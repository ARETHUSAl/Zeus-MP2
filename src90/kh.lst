IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- kh.f90 07/08/15 15:48:57
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** kh   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at kh.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at kh.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 3) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 4) at kh.f90 <line 84> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at kh.f90 <line 84> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[1ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)][d-d%bounds%lbound[].off56 + $$CIV4][d-d%bounds%lbound[].off80 + $$CIV3] =  1.0000000000000000E-099; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[2ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)][d-d%bounds%lbound[].off56 + $$CIV4][d-d%bounds%lbound[].off80 + $$CIV3] =  1.0000000000000000E-099; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32][d-d%bounds%lbound[].off56 + $$CIV4][d-d%bounds%lbound[].off80 + $$CIV3] =  1.0000000000000000E-099; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[3ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)][d-d%bounds%lbound[].off56 + $$CIV4][d-d%bounds%lbound[].off80 + $$CIV3] =  1.0000000000000000E-099; with non-vectorizable strides.
1586-536 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at kh.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV17 * 4ll + d-d%bounds%extent[].off40 % 4ll) + d-d%bounds%lbound[].off32)) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)).
1586-534 (I) Loop (loop index 7) at kh.f90 <line 89> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 8) at kh.f90 <line 90> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 9) at kh.f90 <line 91> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 9) at kh.f90 <line 91> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 10) at kh.f90 <line 103> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 11) at kh.f90 <line 103> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns11.[1ll + (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIVA][d-v2%bounds%lbound[].off600 + $$CIV9] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[1ll + (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-d%bounds%lbound[].off32)][d-d%bounds%lbound[].off56 + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * ((double *)((char *)d-pert%addr  + d-pert%rvo))->pert[].rns4.[2ll + ($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll)][$$CIVA + 1ll][$$CIV9 + 1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns11.[($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552][d-v2%bounds%lbound[].off576 + $$CIVA][d-v2%bounds%lbound[].off600 + $$CIV9] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-d%bounds%lbound[].off32][d-d%bounds%lbound[].off56 + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * ((double *)((char *)d-pert%addr  + d-pert%rvo))->pert[].rns4.[1ll + ($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll)][$$CIVA + 1ll][$$CIV9 + 1ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains operation in ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[1ll + (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-d%bounds%lbound[].off32)][d-d%bounds%lbound[].off56 + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * ((double *)((char *)d-pert%addr  + d-pert%rvo))->pert[].rns4.[2ll + ($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll)][$$CIVA + 1ll][$$CIV9 + 1ll] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains operation in ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-d%bounds%lbound[].off32][d-d%bounds%lbound[].off56 + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * ((double *)((char *)d-pert%addr  + d-pert%rvo))->pert[].rns4.[1ll + ($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll)][$$CIVA + 1ll][$$CIV9 + 1ll] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at kh.f90 <line 103> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) + d-v2%bounds%lbound[].off552) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)).
1586-534 (I) Loop (loop index 13) at kh.f90 <line 104> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 14) at kh.f90 <line 104> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns12.[2ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVD][d-v3%bounds%lbound[].off704 + $$CIVC] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns12.[($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656][d-v3%bounds%lbound[].off680 + $$CIVD][d-v3%bounds%lbound[].off704 + $$CIVC] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns12.[3ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVD][d-v3%bounds%lbound[].off704 + $$CIVC] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns12.[1ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVD][d-v3%bounds%lbound[].off704 + $$CIVC] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-536 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)).
1586-536 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)).
1586-536 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)).
1586-536 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at kh.f90 <line 104> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)).
1586-534 (I) Loop (loop index 16) at kh.f90 <line 109> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 17) at kh.f90 <line 109> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 18) at kh.f90 <line 109> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 18) at kh.f90 <line 109> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 19) at kh.f90 <line 109> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 20) at kh.f90 <line 109> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 21) at kh.f90 <line 109> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 21) at kh.f90 <line 109> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 25) at kh.f90 <line 104> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 26) at kh.f90 <line 104> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 27) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns12.[d-v3%bounds%lbound[].off656 + $$CIVE][d-v3%bounds%lbound[].off680 + $$CIVD][d-v3%bounds%lbound[].off704 + $$CIVC] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-536 (I) Loop (loop index 27) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off656 + $$CIVE) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 27) at kh.f90 <line 104> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off656 + $$CIVE) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at kh.f90 <line 104> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at kh.f90 <line 104> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off656 + $$CIVE) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVD) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIVC)).
1586-534 (I) Loop (loop index 31) at kh.f90 <line 103> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 32) at kh.f90 <line 103> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 33) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns11.[d-v2%bounds%lbound[].off552 + $$CIVB][d-v2%bounds%lbound[].off576 + $$CIVA][d-v2%bounds%lbound[].off600 + $$CIV9] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[d-d%bounds%lbound[].off32 + $$CIVB][d-d%bounds%lbound[].off56 + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * ((double *)((char *)d-pert%addr  + d-pert%rvo))->pert[].rns4.[$$CIVB + 1ll][$$CIVA + 1ll][$$CIV9 + 1ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 33) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off552 + $$CIVB) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 33) at kh.f90 <line 103> was not SIMD vectorized because it contains operation in ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[d-d%bounds%lbound[].off32 + $$CIVB][d-d%bounds%lbound[].off56 + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * ((double *)((char *)d-pert%addr  + d-pert%rvo))->pert[].rns4.[$$CIVB + 1ll][$$CIVA + 1ll][$$CIV9 + 1ll] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 33) at kh.f90 <line 103> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off552 + $$CIVB) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 33) at kh.f90 <line 103> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 33) at kh.f90 <line 103> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off552 + $$CIVB) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIVA) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV9)).
1586-534 (I) Loop (loop index 37) at kh.f90 <line 89> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 38) at kh.f90 <line 90> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 39) at kh.f90 <line 91> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 39) at kh.f90 <line 91> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 43) at kh.f90 <line 84> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 44) at kh.f90 <line 84> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 45) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[d-d%bounds%lbound[].off32 + $$CIV5][d-d%bounds%lbound[].off56 + $$CIV4][d-d%bounds%lbound[].off80 + $$CIV3] =  1.0000000000000000E-099; with non-vectorizable strides.
1586-536 (I) Loop (loop index 45) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(d-d%bounds%lbound[].off32 + $$CIV5) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 45) at kh.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(d-d%bounds%lbound[].off32 + $$CIV5) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 45) at kh.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 45) at kh.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(d-d%bounds%lbound[].off32 + $$CIV5) + (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 + $$CIV4) + (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 + $$CIV3)).
1586-534 (I) Loop (loop index 46) at kh.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 46) at kh.f90 <line 67> was not SIMD vectorized because it contains unsupported loop structure.
1586-534 (I) Loop (loop index 49) at kh.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 50) at kh.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 51) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*($$LoopIV0 + 1ll) + (max((long long) in,0ll) * 8ll)*($$LoopIV1 + 1ll) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 51) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 51) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 52) at kh.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 53) at kh.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *)d-pert%addr  + -8ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*($$LoopIV0 + 1ll) + (max((long long) in,0ll) * 8ll)*($$LoopIV1 + 1ll) + (8ll)*($$LoopIV2 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 53) at kh.f90 <line 69> was not SIMD vectorized because it contains operation in ampl * _sin@7(( 6.2831854820251464E+000 / abs(x1max - x1min)) * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 1ll]) which is not suitable for SIMD vectorization.
1586-554 (I) Loop (loop index 53) at kh.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"13">. Total number of the innermost loops SIMD vectorized <"0">.


     7|         SUBROUTINE kh ()
    26|           v0 =  5.0000000000000000E-001
    27|           p0 =  2.5000000000000000E+000
    28|           rho_in =  2.0000000000000000E+000
    29|           rho_out =  1.0000000000000000E+000
    30|           ampl =  9.9999997764825820E-003
    31|           line_val =  2.5000000000000000E-001
    32|           IF ((myid == 0)) THEN
     7|             |pgen%version = 129
                    |pgen%name_addr = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90"
                    |pgen%name_len = 4
                    |pgen%num_of_items = 6
                    |pgen%nlitems%type.off32 = 4
                    |pgen%nlitems%kind.off40 = 
                    |pgen%nlitems%size.off48 = 
                    |pgen%nlitems%name_addr.off56 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 4
                    |pgen%nlitems%name_len.off64 = 2
                    |pgen%nlitems%item_addr.off72 = loc(v0)
                    |pgen%nlitems%type.off80 = 4
                    |pgen%nlitems%kind.off88 = 
                    |pgen%nlitems%size.off96 = 
                    |pgen%nlitems%name_addr.off104 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 6
                    |pgen%nlitems%name_len.off112 = 2
                    |pgen%nlitems%item_addr.off120 = loc(p0)
                    |pgen%nlitems%type.off128 = 4
                    |pgen%nlitems%kind.off136 = 
                    |pgen%nlitems%size.off144 = 
                    |pgen%nlitems%name_addr.off152 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 8
                    |pgen%nlitems%name_len.off160 = 6
                    |pgen%nlitems%item_addr.off168 = loc(rho_in)
                    |pgen%nlitems%type.off176 = 4
                    |pgen%nlitems%kind.off184 = 
                    |pgen%nlitems%size.off192 = 
                    |pgen%nlitems%name_addr.off200 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 14
                    |pgen%nlitems%name_len.off208 = 7
                    |pgen%nlitems%item_addr.off216 = loc(rho_out)
                    |pgen%nlitems%type.off224 = 4
                    |pgen%nlitems%kind.off232 = 
                    |pgen%nlitems%size.off240 = 
                    |pgen%nlitems%name_addr.off248 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 22
                    |pgen%nlitems%name_len.off256 = 4
                    |pgen%nlitems%item_addr.off264 = loc(ampl)
                    |pgen%nlitems%type.off272 = 4
                    |pgen%nlitems%kind.off280 = 
                    |pgen%nlitems%size.off288 = 
                    |pgen%nlitems%name_addr.off296 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 26
                    |pgen%nlitems%name_len.off304 = 8
                    |pgen%nlitems%item_addr.off312 = loc(line_val)
    33|             #2 = _xlfBeginIO(6,257,#1,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#2),"K-H problem setup.",18,1)
                    _xlfEndIO(%VAL(#2))
    39|             |pgen%name_flags = 0
                    #4 = _xlfBeginIO(1,2,#3,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#4))
    40|             |pgen%name_flags = 0
                    #6 = _xlfBeginIO(2,258,#5,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#6))
    41|             buf_in[].off1744 = v0
    42|             buf_in[].off1752 = p0
    43|             buf_in[].off1760 = rho_in
    44|             buf_in[].off1768 = rho_out
    45|             buf_in[].off1776 = ampl
    46|             buf_in[].off1784 = line_val
    47|           ENDIF
    48|           T_2 = 6
                  T_3 = 1275070495
                  T_4 = 0
                  CALL mpi_bcast(buf_in,T_2,T_3,T_4,comm3d,ierr)
    49|           IF ((myid <> 0)) THEN
    50|             v0 = buf_in[].off1744
    51|             p0 = buf_in[].off1752
    52|             rho_in = buf_in[].off1760
    53|             rho_out = buf_in[].off1768
    54|             ampl = buf_in[].off1776
    55|             line_val = buf_in[].off1784
    56|           ENDIF
    59|           IF ((0 == 8 * (max(int(kn),0) * (max(int(jn),0) * max(int(in),&
                &   0))))) THEN
                    d-pert%addr = NULL
                    lab_6
                    d-pert%addr = __xlf_malloc((8 * (max(int(%VAL(kn)),0) * (&
                &     max(int(%VAL(jn)),0) * max(int(%VAL(in)),0)))),32)
                    CALL __alignx(32,d-pert%addr)
                    IF ((d-pert%addr == NULL)) THEN
    60|             ENDIF
    59|             d-pert%bounds%mult[].off72 = max(int(in),0) * 8
                    d-pert%bounds%mult[].off48 = 8 * (max(int(jn),0) * max(int(&
                &     in),0))
                    d-pert%rvo = -((1 + (max(int(jn),0) * max(int(in),0) + max(&
                &     int(in),0))) * 8)
    60|             IF ((0 == max(int(in),0) * 8)) THEN
                      d-rannum%addr = NULL
                    ELSE
                      lab_11
                      d-rannum%addr = __xlf_malloc((max(int(%VAL(in)),0) * 8),&
                &       32)
                      CALL __alignx(32,d-rannum%addr)
                      IF (.NOT.(d-rannum%addr == NULL)) GOTO lab_13
                      filenameaddr_3 = "kh.f90"
                      filenamelen_3 = 6
                      CALL _xlfErrorExitWithLoc(0,3,108,0,NULL,NULL,filename_3,&
                &       60,NULL)
                      CALL _trap(3)
                      RETURN
                      lab_13
                      lab_12
    61|               CALL _xlirndsg(seedsize,NULL,NULL,NULL,1,0,0,0,0,0)
    67|               IF ((MOD(max(int(kn),0), 4) > 0  .AND.  0 < max(int(kn),0)&
                &       )) THEN
                        $$LoopIV0 = 0
       Id=49            DO $$LoopIV0 = $$LoopIV0, MOD(max(int(kn),0), int(4))&
                &           -1
    68|                   IF ((0 < int(jn))) THEN
                            $$LoopIV1 = 0
       Id=50                DO $$LoopIV1 = $$LoopIV1, int(int(jn))-1
    69|                       IF ((0 < int(in))) THEN
                                $$LoopIV2 = 0
       Id=51                    DO $$LoopIV2 = $$LoopIV2, int(int(in))-1
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,&
                &                   $$LoopIV1 + 1,$$LoopIV0 + 1) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                ENDDO
                              ENDIF
    70|                     ENDDO
                          ENDIF
    71|                 ENDDO
                      ENDIF
    67|               IF ((max(int(kn),0) > MOD(max(int(kn),0), 4)  .AND.  0 < &
                &       max(int(kn),0))) THEN
                        $$LoopIV0 = MOD(max(int(kn),0), int(4))
       Id=46            DO $$LoopIV0 = $$LoopIV0, int(max(int(kn),0))
    68|                   IF ((MOD(int(jn), 4) > 0  .AND.  0 < int(jn))) THEN
                            $$LoopIV1 = 0
       Id=52                DO $$LoopIV1 = $$LoopIV1, MOD(int(jn), int(4))-1
    69|                       IF ((0 < int(in))) THEN
                                $$LoopIV2 = 0
       Id=53                    DO $$LoopIV2 = $$LoopIV2, int(int(in))-1
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,&
                &                   $$LoopIV1 + 1,$$LoopIV0 + 1) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                ENDDO
                              ENDIF
    70|                     ENDDO
                          ENDIF
    71|                 ENDDO
                      ENDIF
    67|               IF ((0 < max(int(kn),0)  .AND.  max(int(kn),0) > MOD(max(&
                &       int(kn),0), 4))) THEN
                        $$CIV16 = int(0)
       Id=1             DO $$CIV16 = $$CIV16, int((((max(int(kn),0) - MOD(max(&
                &           int(kn),0), 4)) - 1) / 4 + 1))-1
    68|                   IF ((0 < int(jn)  .AND.  int(jn) > MOD(int(jn), 4))) &
                &           THEN
                            $$CIV15 = int(0)
       Id=2                 DO $$CIV15 = $$CIV15, int((((int(jn) - MOD(int(jn)&
                &               , 4)) - 1) / 4 + 1))-1
    69|                       IF ((0 < int(in))) THEN
                                $$LoopIV2 = 0
       Id=3                     DO $$LoopIV2 = $$LoopIV2, int(int(in))-1
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                ENDDO
                              ENDIF
    70|                     ENDDO
                          ENDIF
    71|                 ENDDO
                      ENDIF
    84|               IF ((MOD(d-d%bounds%extent[].off40, 4) > 0  .AND.  &
                &       d-d%bounds%extent[].off40 > 0)) THEN
                        $$CIV5 = 0
       Id=43            DO $$CIV5 = $$CIV5, MOD(d-d%bounds%extent[].off40, &
                &           int(4))-1
                          IF ((d-d%bounds%extent[].off64 > 0)) THEN
                            $$CIV4 = 0
       Id=44                DO $$CIV4 = $$CIV4, int(d-d%bounds%extent[].off64)&
                &               -1
                              IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                $$CIV3 = 0
       Id=45                    DO $$CIV3 = $$CIV3, int(&
                &                   d-d%bounds%extent[].off88)-1
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,&
                &                   d-d%bounds%lbound[].off32 + $$CIV5) =  &
                &                   1.0000000000000000E-099
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                      IF ((d-d%bounds%extent[].off40 > 0  .AND.  &
                &       d-d%bounds%extent[].off40 > MOD(d-d%bounds%extent[].off40,&
                &        4))) THEN
                        $$CIV17 = int(0)
       Id=4             DO $$CIV17 = $$CIV17, int((((&
                &           d-d%bounds%extent[].off40 - MOD(&
                &           d-d%bounds%extent[].off40, 4)) - 1) / 4 + 1))-1
                          IF ((d-d%bounds%extent[].off64 > 0)) THEN
                            $$CIV4 = 0
       Id=5                 DO $$CIV4 = $$CIV4, int(d-d%bounds%extent[].off64)&
                &               -1
                              IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                $$CIV3 = 0
       Id=6                     DO $$CIV3 = $$CIV3, int(&
                &                   d-d%bounds%extent[].off88)-1
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,($$CIV17 * &
                &                   4 + MOD(d-d%bounds%extent[].off40, 4)) + &
                &                   d-d%bounds%lbound[].off32) =  &
                &                   1.0000000000000000E-099
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,1 + ((&
                &                   $$CIV17 * 4 + MOD(d-d%bounds%extent[].off40, &
                &                   4)) + d-d%bounds%lbound[].off32)) =  &
                &                   1.0000000000000000E-099
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,2 + ((&
                &                   $$CIV17 * 4 + MOD(d-d%bounds%extent[].off40, &
                &                   4)) + d-d%bounds%lbound[].off32)) =  &
                &                   1.0000000000000000E-099
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,3 + ((&
                &                   $$CIV17 * 4 + MOD(d-d%bounds%extent[].off40, &
                &                   4)) + d-d%bounds%lbound[].off32)) =  &
                &                   1.0000000000000000E-099
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
    85|               T_12 = -iseed
                      CALL ran1(T_12,rn)
    89|               IF ((MOD(int(kn), 2) > 0  .AND.  int(kn) > 0)) THEN
                        $$CIV8 = 0
       Id=37            DO $$CIV8 = $$CIV8, MOD(int(kn), int(2))-1
    90|                   IF ((int(jn) > 0)) THEN
                            $$CIV7 = 0
       Id=38                DO $$CIV7 = $$CIV7, int(int(jn))-1
    91|                       IF ((int(in) > 0)) THEN
                                $$CIV6 = 0
       Id=39                    DO $$CIV6 = $$CIV6, int(int(in))-1
    92|                           IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) >= &
                &                   line_val)) GOTO lab_211
    93|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1) &
                &                   = rho_out
    94|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1)&
                &                    = rho_out * (d-pert%addr%pert[].rns4.($$CIV6 &
                &                   + 1,$$CIV7 + 1,$$CIV8 + 1) - v0)
    95|                           GOTO lab_213
                                  lab_211
                                  IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) < &
                &                   line_val)) GOTO lab_212
    96|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1) &
                &                   = rho_in
    97|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1)&
                &                    = rho_in * (v0 + d-pert%addr%pert[].rns4.(&
                &                   $$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1))
    98|                           lab_212
                                  lab_213
    99|                           d-e%addr%e($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1) &
                &                   = p0 / (gamma -  1.0000000000000000E+000)
   100|                         ENDDO
                              ENDIF
   101|                     ENDDO
                          ENDIF
   102|                 ENDDO
                      ENDIF
    89|               IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 2))) THEN
                        $$CIV18 = int(0)
       Id=7             DO $$CIV18 = $$CIV18, int((((int(kn) - MOD(int(kn), 2)&
                &           ) - 1) / 2 + 1))-1
    90|                   IF ((int(jn) > 0)) THEN
                            $$CIV7 = 0
       Id=8                 DO $$CIV7 = $$CIV7, int(int(jn))-1
    91|                       IF ((int(in) > 0)) THEN
                                $$CIV6 = 0
       Id=9                     DO $$CIV6 = $$CIV6, int(int(in))-1
    92|                           IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) >= &
                &                   line_val)) GOTO lab_41
    93|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,1 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_out
    94|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,1 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_out * (&
                &                   d-pert%addr%pert[].rns4.($$CIV6 + 1,$$CIV7 + &
                &                   1,1 + ($$CIV18 * 2 + MOD(int(kn), 2))) - v0)
    95|                           GOTO lab_42
                                  lab_41
                                  IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) < &
                &                   line_val)) GOTO lab_43
    96|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,1 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_in
    97|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,1 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_in * (&
                &                   v0 + d-pert%addr%pert[].rns4.($$CIV6 + 1,&
                &                   $$CIV7 + 1,1 + ($$CIV18 * 2 + MOD(int(kn), 2))&
                &                   ))
    98|                           lab_43
                                  lab_42
    99|                           d-e%addr%e($$CIV6 + 1,$$CIV7 + 1,1 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = p0 / (gamma -  &
                &                   1.0000000000000000E+000)
    92|                           IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) >= &
                &                   line_val)) GOTO lab_217
    93|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,2 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_out
    94|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,2 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_out * (&
                &                   d-pert%addr%pert[].rns4.($$CIV6 + 1,$$CIV7 + &
                &                   1,2 + ($$CIV18 * 2 + MOD(int(kn), 2))) - v0)
    95|                           GOTO lab_219
                                  lab_217
                                  IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) < &
                &                   line_val)) GOTO lab_218
    96|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,2 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_in
    97|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,2 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_in * (&
                &                   v0 + d-pert%addr%pert[].rns4.($$CIV6 + 1,&
                &                   $$CIV7 + 1,2 + ($$CIV18 * 2 + MOD(int(kn), 2))&
                &                   ))
    98|                           lab_218
                                  lab_219
    99|                           d-e%addr%e($$CIV6 + 1,$$CIV7 + 1,2 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = p0 / (gamma -  &
                &                   1.0000000000000000E+000)
   100|                         ENDDO
                              ENDIF
   101|                     ENDDO
                          ENDIF
   102|                 ENDDO
                      ENDIF
   103|               IF ((MOD(d-v2%bounds%extent[].off560, 2) > 0  .AND.  &
                &       d-v2%bounds%extent[].off560 > 0)) THEN
                        $$CIVB = 0
       Id=31            DO $$CIVB = $$CIVB, MOD(d-v2%bounds%extent[].off560, &
                &           int(2))-1
                          IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                            $$CIVA = 0
       Id=32                DO $$CIVA = $$CIVA, int(&
                &               d-v2%bounds%extent[].off584)-1
                              IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                                $$CIV9 = 0
       Id=33                    DO $$CIV9 = $$CIV9, int(&
                &                   d-v2%bounds%extent[].off608)-1
                                  d-v2%addr%v2(d-v2%bounds%lbound[].off600 + &
                &                   $$CIV9,d-v2%bounds%lbound[].off576 + $$CIVA,&
                &                   d-v2%bounds%lbound[].off552 + $$CIVB) = &
                &                   d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV9,&
                &                   d-d%bounds%lbound[].off56 + $$CIVA,&
                &                   d-d%bounds%lbound[].off32 + $$CIVB) * &
                &                   d-pert%addr%pert[].rns4.($$CIV9 + 1,$$CIVA + &
                &                   1,$$CIVB + 1)
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                      IF ((d-v2%bounds%extent[].off560 > 0  .AND.  &
                &       d-v2%bounds%extent[].off560 > MOD(&
                &       d-v2%bounds%extent[].off560, 2))) THEN
                        $$CIV19 = int(0)
       Id=10            DO $$CIV19 = $$CIV19, int((((&
                &           d-v2%bounds%extent[].off560 - MOD(&
                &           d-v2%bounds%extent[].off560, 2)) - 1) / 2 + 1))-1
                          IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                            $$CIVA = 0
       Id=11                DO $$CIVA = $$CIVA, int(&
                &               d-v2%bounds%extent[].off584)-1
                              IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                                $$CIV9 = 0
       Id=12                    DO $$CIV9 = $$CIV9, int(&
                &                   d-v2%bounds%extent[].off608)-1
                                  d-v2%addr%v2(d-v2%bounds%lbound[].off600 + &
                &                   $$CIV9,d-v2%bounds%lbound[].off576 + $$CIVA,(&
                &                   $$CIV19 * 2 + MOD(d-v2%bounds%extent[].off560,&
                &                    2)) + d-v2%bounds%lbound[].off552) = &
                &                   d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV9,&
                &                   d-d%bounds%lbound[].off56 + $$CIVA,($$CIV19 * &
                &                   2 + MOD(d-v2%bounds%extent[].off560, 2)) + &
                &                   d-d%bounds%lbound[].off32) * &
                &                   d-pert%addr%pert[].rns4.($$CIV9 + 1,$$CIVA + &
                &                   1,1 + ($$CIV19 * 2 + MOD(&
                &                   d-v2%bounds%extent[].off560, 2)))
                                  d-v2%addr%v2(d-v2%bounds%lbound[].off600 + &
                &                   $$CIV9,d-v2%bounds%lbound[].off576 + $$CIVA,1 &
                &                   + (($$CIV19 * 2 + MOD(&
                &                   d-v2%bounds%extent[].off560, 2)) + &
                &                   d-v2%bounds%lbound[].off552)) = d-d%addr%d(&
                &                   d-d%bounds%lbound[].off80 + $$CIV9,&
                &                   d-d%bounds%lbound[].off56 + $$CIVA,1 + ((&
                &                   $$CIV19 * 2 + MOD(d-v2%bounds%extent[].off560,&
                &                    2)) + d-d%bounds%lbound[].off32)) * &
                &                   d-pert%addr%pert[].rns4.($$CIV9 + 1,$$CIVA + &
                &                   1,2 + ($$CIV19 * 2 + MOD(&
                &                   d-v2%bounds%extent[].off560, 2)))
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
   104|               IF ((MOD(d-v3%bounds%extent[].off664, 4) > 0  .AND.  &
                &       d-v3%bounds%extent[].off664 > 0)) THEN
                        $$CIVE = 0
       Id=25            DO $$CIVE = $$CIVE, MOD(d-v3%bounds%extent[].off664, &
                &           int(4))-1
                          IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                            $$CIVD = 0
       Id=26                DO $$CIVD = $$CIVD, int(&
                &               d-v3%bounds%extent[].off688)-1
                              IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                                $$CIVC = 0
       Id=27                    DO $$CIVC = $$CIVC, int(&
                &                   d-v3%bounds%extent[].off712)-1
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,&
                &                   d-v3%bounds%lbound[].off656 + $$CIVE) =  &
                &                   0.0000000000000000E+000
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                      IF ((d-v3%bounds%extent[].off664 > 0  .AND.  &
                &       d-v3%bounds%extent[].off664 > MOD(&
                &       d-v3%bounds%extent[].off664, 4))) THEN
                        $$CIV1A = int(0)
       Id=13            DO $$CIV1A = $$CIV1A, int((((&
                &           d-v3%bounds%extent[].off664 - MOD(&
                &           d-v3%bounds%extent[].off664, 4)) - 1) / 4 + 1))-1
                          IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                            $$CIVD = 0
       Id=14                DO $$CIVD = $$CIVD, int(&
                &               d-v3%bounds%extent[].off688)-1
                              IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                                $$CIVC = 0
       Id=15                    DO $$CIVC = $$CIVC, int(&
                &                   d-v3%bounds%extent[].off712)-1
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,(&
                &                   $$CIV1A * 4 + MOD(d-v3%bounds%extent[].off664,&
                &                    4)) + d-v3%bounds%lbound[].off656) =  &
                &                   0.0000000000000000E+000
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,1 &
                &                   + (($$CIV1A * 4 + MOD(&
                &                   d-v3%bounds%extent[].off664, 4)) + &
                &                   d-v3%bounds%lbound[].off656)) =  &
                &                   0.0000000000000000E+000
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,2 &
                &                   + (($$CIV1A * 4 + MOD(&
                &                   d-v3%bounds%extent[].off664, 4)) + &
                &                   d-v3%bounds%lbound[].off656)) =  &
                &                   0.0000000000000000E+000
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,3 &
                &                   + (($$CIV1A * 4 + MOD(&
                &                   d-v3%bounds%extent[].off664, 4)) + &
                &                   d-v3%bounds%lbound[].off656)) =  &
                &                   0.0000000000000000E+000
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
   106|               IF ((d-pert%addr <> NULL)) THEN
                        CALL free(d-pert%addr)
                      ENDIF
   107|               IF ((myid_w == 0)) THEN
   108|                 #8 = _xlfBeginIO(6,257,#7,32768,NULL,0,NULL)
                        CALL _xlfWriteLDChar(%VAL(#8),"KH SETUP FINISHED",17,1)
                        _xlfEndIO(%VAL(#8))
   109|                 #10 = _xlfBeginIO(6,257,#9,32768,NULL,0,NULL)
                        T_40 = -1.7976931348623157E+308
                        IF ((d-d%bounds%extent[].off40 > 0)) THEN
                          $$CIV11 = 0
       Id=16              DO $$CIV11 = $$CIV11, int(d-d%bounds%extent[].off40)&
                &             -1
                            IF ((d-d%bounds%extent[].off64 > 0)) THEN
                              $$CIV10 = 0
       Id=17                  DO $$CIV10 = $$CIV10, int(&
                &                 d-d%bounds%extent[].off64)-1
                                IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                  $$CIVF = 0
       Id=18                      DO $$CIVF = $$CIVF, int(&
                &                     d-d%bounds%extent[].off88)-1
                                    IF ((d-d%addr%d(d-d%bounds%lbound[].off80 + &
                &                     $$CIVF,d-d%bounds%lbound[].off56 + $$CIV10,&
                &                     d-d%bounds%lbound[].off32 + $$CIV11) > T_40)&
                &                     ) THEN
                                      T_40 = d-d%addr%d((&
                &                       d-d%bounds%extent[].off88 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off80) + $$CIVF,(&
                &                       d-d%bounds%extent[].off64 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off56) + $$CIV10,(&
                &                       d-d%bounds%extent[].off40 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off32) + $$CIV11)
                                    ENDIF
                                  ENDDO
                                ENDIF
                              ENDDO
                            ENDIF
                          ENDDO
                        ENDIF
                        CALL _xlfWriteLDReal(%VAL(#10),T_40,8,8)
                        T_44 =  1.7976931348623157E+308
                        IF ((d-d%bounds%extent[].off40 > 0)) THEN
                          $$CIV14 = 0
       Id=19              DO $$CIV14 = $$CIV14, int(d-d%bounds%extent[].off40)&
                &             -1
                            IF ((d-d%bounds%extent[].off64 > 0)) THEN
                              $$CIV13 = 0
       Id=20                  DO $$CIV13 = $$CIV13, int(&
                &                 d-d%bounds%extent[].off64)-1
                                IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                  $$CIV12 = 0
       Id=21                      DO $$CIV12 = $$CIV12, int(&
                &                     d-d%bounds%extent[].off88)-1
                                    IF ((d-d%addr%d(d-d%bounds%lbound[].off80 + &
                &                     $$CIV12,d-d%bounds%lbound[].off56 + $$CIV13,&
                &                     d-d%bounds%lbound[].off32 + $$CIV14) < T_44)&
                &                     ) THEN
                                      T_44 = d-d%addr%d((&
                &                       d-d%bounds%extent[].off88 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off80) + $$CIV12,(&
                &                       d-d%bounds%extent[].off64 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off56) + $$CIV13,(&
                &                       d-d%bounds%extent[].off40 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off32) + $$CIV14)
                                    ENDIF
                                  ENDDO
                                ENDIF
                              ENDDO
                            ENDIF
                          ENDDO
                        ENDIF
                        CALL _xlfWriteLDReal(%VAL(#10),T_44,8,8)
                        _xlfEndIO(%VAL(#10))
   111|               ENDIF
                      IF (.NOT.(d-rannum%addr <> NULL)) GOTO lab_173
                      CALL free(d-rannum%addr)
   112|             ELSE
    59|               lab_173
                      RETURN
                      filenameaddr_1 = "kh.f90"
                      filenamelen_1 = 6
                      CALL _xlfErrorExitWithLoc(0,3,108,0,NULL,NULL,filename_1,&
                &       59,NULL)
                      CALL _trap(3)
                      RETURN
   112|             END SUBROUTINE kh


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            67            49    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            68            50    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*($$LoopIV0 
                                          + 1ll) + (max((long long) in,0ll) * 8ll)*($$LoopIV1 + 
                                          1ll) + (8ll)*($$LoopIV2 + 1ll))  with 
                                          non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            67            46    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            67            46    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            68            52    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*($$LoopIV0 
                                          + 1ll) + (max((long long) in,0ll) * 8ll)*($$LoopIV1 + 
                                          1ll) + (8ll)*($$LoopIV2 + 1ll))  with 
                                          non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            67             1    Loop interchanging applied to loop nest.
         0            67             1    Outer loop has been unrolled 4 time(s).
         0            67             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            68             2    Outer loop has been unrolled 4 time(s).
         0            68             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(1ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(1ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(1ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(1ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(2ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(2ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(2ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(2ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(3ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(3ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(3ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(3ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(4ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(1ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(4ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(2ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(4ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(3ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-pert%addr  + -8ll - (8ll 
                                          * (max((long long) jn,0ll) * max((long long) in,0ll)) 
                                          + max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(4ll + 
                                          ($$CIV16 * 4ll + max((long long) kn,0ll) % 4ll)) + 
                                          (max((long long) in,0ll) * 8ll)*(4ll + ($$CIV15 * 4ll 
                                          + (long long) jn % 4ll)) + (8ll)*($$LoopIV2 + 1ll))  
                                          with non-vectorizable alignment.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          operation in ampl * _sin@7(( 6.2831854820251464E+000 
                                          / abs(x1max - x1min)) * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns5.[$$LoopIV2 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84            43    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84            44    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(d-d%bounds%lbound[].off32 
                                          + $$CIV5) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(d-d%bounds%lbound[].off32 
                                          + $$CIV5) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(d-d%bounds%lbound[].off32 
                                          + $$CIV5) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)).
         0            84             4    Outer loop has been unrolled 4 time(s).
         0            84             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)).
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 
                                          * 4ll + d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)).
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV17 
                                          * 4ll + d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)).
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV17 * 4ll + 
                                          d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV17 
                                          * 4ll + d-d%bounds%extent[].off40 % 4ll) + 
                                          d-d%bounds%lbound[].off32)) + 
                                          (d-d%bounds%mult[].off72)*(d-d%bounds%lbound[].off56 
                                          + $$CIV4) + 
                                          (d-d%bounds%mult[].off96)*(d-d%bounds%lbound[].off80 
                                          + $$CIV3)).
         0            89            37    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            90            38    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            91            39    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            91            39    Loop was not SIMD vectorized because it contains 
                                          control flow.
         0            89             7    Outer loop has been unrolled 2 time(s).
         0            89             7    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            90             8    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            91             9    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            91             9    Loop was not SIMD vectorized because it contains 
                                          control flow.
         0           103            31    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           103            32    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off5
                                          52 + $$CIVB) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9))  with non-vectorizable alignment.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns8.[d-d%bounds%lbound[].off32 + 
                                          $$CIVB][d-d%bounds%lbound[].off56 + 
                                          $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * 
                                          ((double *)((char *)d-pert%addr  + 
                                          d-pert%rvo))->pert[].rns4.[$$CIVB + 1ll][$$CIVA + 
                                          1ll][$$CIV9 + 1ll] which is not  suitable for SIMD 
                                          vectorization.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off5
                                          52 + $$CIVB) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9)) with  non-vectorizable strides.
         0           103                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off5
                                          52 + $$CIVB) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9)).
         0           103            10    Outer loop has been unrolled 2 time(s).
         0           103            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           103            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-v2%bounds%lbound[].off552) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9))  with non-vectorizable alignment.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns8.[($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-d%bounds%lbound[].off32][d-d%bounds%lbound[].off56 
                                          + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * 
                                          ((double *)((char *)d-pert%addr  + 
                                          d-pert%rvo))->pert[].rns4.[1ll + ($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll)][$$CIVA + 
                                          1ll][$$CIV9 + 1ll] which is not  suitable for SIMD 
                                          vectorization.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-v2%bounds%lbound[].off552) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9)) with  non-vectorizable strides.
         0           103                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV19 * 
                                          2ll + d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-v2%bounds%lbound[].off552) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9)).
         0           103                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(1ll + (($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9))  with non-vectorizable alignment.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns8.[1ll + (($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-d%bounds%lbound[].off32)][d-d%bounds%lbound[].off56 
                                          + $$CIVA][d-d%bounds%lbound[].off80 + $$CIV9] * 
                                          ((double *)((char *)d-pert%addr  + 
                                          d-pert%rvo))->pert[].rns4.[2ll + ($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll)][$$CIVA + 
                                          1ll][$$CIV9 + 1ll] which is not  suitable for SIMD 
                                          vectorization.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(1ll + (($$CIV19 * 2ll + 
                                          d-v2%bounds%extent[].off560 % 2ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9)) with  non-vectorizable strides.
         0           103                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           103                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + 
                                          (($$CIV19 * 2ll + d-v2%bounds%extent[].off560 % 2ll) 
                                          + d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIVA) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV9)).
         0           104            25    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           104            26    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off6
                                          56 + $$CIVE) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC))  with non-vectorizable alignment.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off6
                                          56 + $$CIVE) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)) with  non-vectorizable strides.
         0           104                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off6
                                          56 + $$CIVE) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)).
         0           104            13    Outer loop has been unrolled 4 time(s).
         0           104            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           104            14    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC))  with non-vectorizable alignment.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)) with  non-vectorizable strides.
         0           104                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV1A * 
                                          4ll + d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)).
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(1ll + (($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC))  with non-vectorizable alignment.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(1ll + (($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)) with  non-vectorizable strides.
         0           104                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + 
                                          (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) 
                                          + d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)).
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(2ll + (($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC))  with non-vectorizable alignment.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(2ll + (($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)) with  non-vectorizable strides.
         0           104                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + 
                                          (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) 
                                          + d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)).
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(3ll + (($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC))  with non-vectorizable alignment.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(3ll + (($$CIV1A * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)) with  non-vectorizable strides.
         0           104                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           104                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + 
                                          (($$CIV1A * 4ll + d-v3%bounds%extent[].off664 % 4ll) 
                                          + d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVD) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIVC)).
         0           109            16    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           109            17    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           109            18    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0           109            18    Loop was not SIMD vectorized because it contains 
                                          control flow.
         0           109            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           109            20    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           109            21    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0           109            21    Loop was not SIMD vectorized because it contains 
                                          control flow.


     7|         SUBROUTINE kh ()
    26|           v0 =  5.0000000000000000E-001
    27|           p0 =  2.5000000000000000E+000
    28|           rho_in =  2.0000000000000000E+000
    29|           rho_out =  1.0000000000000000E+000
    30|           ampl =  9.9999997764825820E-003
    31|           line_val =  2.5000000000000000E-001
    32|           IF ((myid == 0)) THEN
     7|             |pgen%version = 129
                    |pgen%name_addr = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90"
                    |pgen%name_len = 4
                    |pgen%num_of_items = 6
                    |pgen%nlitems%type.off32 = 4
                    |pgen%nlitems%kind.off40 = 
                    |pgen%nlitems%size.off48 = 
                    |pgen%nlitems%name_addr.off56 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 4
                    |pgen%nlitems%name_len.off64 = 2
                    |pgen%nlitems%item_addr.off72 = loc(v0)
                    |pgen%nlitems%type.off80 = 4
                    |pgen%nlitems%kind.off88 = 
                    |pgen%nlitems%size.off96 = 
                    |pgen%nlitems%name_addr.off104 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 6
                    |pgen%nlitems%name_len.off112 = 2
                    |pgen%nlitems%item_addr.off120 = loc(p0)
                    |pgen%nlitems%type.off128 = 4
                    |pgen%nlitems%kind.off136 = 
                    |pgen%nlitems%size.off144 = 
                    |pgen%nlitems%name_addr.off152 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 8
                    |pgen%nlitems%name_len.off160 = 6
                    |pgen%nlitems%item_addr.off168 = loc(rho_in)
                    |pgen%nlitems%type.off176 = 4
                    |pgen%nlitems%kind.off184 = 
                    |pgen%nlitems%size.off192 = 
                    |pgen%nlitems%name_addr.off200 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 14
                    |pgen%nlitems%name_len.off208 = 7
                    |pgen%nlitems%item_addr.off216 = loc(rho_out)
                    |pgen%nlitems%type.off224 = 4
                    |pgen%nlitems%kind.off232 = 
                    |pgen%nlitems%size.off240 = 
                    |pgen%nlitems%name_addr.off248 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 22
                    |pgen%nlitems%name_len.off256 = 4
                    |pgen%nlitems%item_addr.off264 = loc(ampl)
                    |pgen%nlitems%type.off272 = 4
                    |pgen%nlitems%kind.off280 = 
                    |pgen%nlitems%size.off288 = 
                    |pgen%nlitems%name_addr.off296 = &
                &     "pgenv0p0rho_inrho_outIamplline_valkh.f90" + 26
                    |pgen%nlitems%name_len.off304 = 8
                    |pgen%nlitems%item_addr.off312 = loc(line_val)
    33|             #2 = _xlfBeginIO(6,257,#1,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#2),"K-H problem setup.",18,1)
                    _xlfEndIO(%VAL(#2))
    39|             |pgen%name_flags = 0
                    #4 = _xlfBeginIO(1,2,#3,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#4))
    40|             |pgen%name_flags = 0
                    #6 = _xlfBeginIO(2,258,#5,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#6))
    41|             buf_in[].off1744 = v0
    42|             buf_in[].off1752 = p0
    43|             buf_in[].off1760 = rho_in
    44|             buf_in[].off1768 = rho_out
    45|             buf_in[].off1776 = ampl
    46|             buf_in[].off1784 = line_val
    47|           ENDIF
    48|           T_2 = 6
                  T_3 = 1275070495
                  T_4 = 0
                  CALL mpi_bcast(buf_in,T_2,T_3,T_4,comm3d,ierr)
    49|           IF ((myid <> 0)) THEN
    50|             v0 = buf_in[].off1744
    51|             p0 = buf_in[].off1752
    52|             rho_in = buf_in[].off1760
    53|             rho_out = buf_in[].off1768
    54|             ampl = buf_in[].off1776
    55|             line_val = buf_in[].off1784
    56|           ENDIF
    59|           IF ((0 == 8 * (max(int(kn),0) * (max(int(jn),0) * max(int(in),&
                &   0))))) THEN
                    d-pert%addr = NULL
                    lab_6
                    d-pert%addr = __xlf_malloc(8 * (max(int(%VAL(kn)),0) * (max(&
                &     int(%VAL(jn)),0) * max(int(%VAL(in)),0))),32)
                    CALL __alignx(32,d-pert%addr)
                    IF ((d-pert%addr == NULL)) THEN
    60|             ENDIF
    59|             d-pert%bounds%mult[].off72 = max(int(in),0) * 8
                    d-pert%bounds%mult[].off48 = 8 * (max(int(jn),0) * max(int(&
                &     in),0))
                    d-pert%rvo = -((1 + (max(int(jn),0) * max(int(in),0) + max(&
                &     int(in),0))) * 8)
    60|             $$csx0 = max(int(in),0) * 8
                    IF ((0 == $$csx0)) THEN
                      d-rannum%addr = NULL
                    ELSE
                      lab_11
                      d-rannum%addr = __xlf_malloc(%VAL($$csx0),32)
                      CALL __alignx(32,d-rannum%addr)
                      IF (.NOT.(d-rannum%addr == NULL)) GOTO lab_13
                      filenameaddr_3 = "kh.f90"
                      filenamelen_3 = 6
                      CALL _xlfErrorExitWithLoc(0,3,108,0,NULL,NULL,filename_3,&
                &       60,NULL)
                      CALL _trap(3)
                      RETURN
                      lab_13
                      lab_12
    61|               CALL _xlirndsg(seedsize,NULL,NULL,NULL,1,0,0,0,0,0)
    67|               IF ((MOD(max(int(kn),0), 4) > 0  .AND.  0 < max(int(kn),0)&
                &       )) THEN
                        $$LoopIV0 = 0
       Id=49            DO $$LoopIV0 = $$LoopIV0, MOD(max(int(kn),0), int(4))&
                &           -1
    68|                   IF ((0 < int(jn))) THEN
                            $$LoopIV1 = 0
       Id=50                DO $$LoopIV1 = $$LoopIV1, int(int(jn))-1
    69|                       IF ((0 < int(in))) THEN
                                $$LoopIV2 = 0
       Id=51                    DO $$LoopIV2 = $$LoopIV2, int(int(in))-1
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,&
                &                   $$LoopIV1 + 1,$$LoopIV0 + 1) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                ENDDO
                              ENDIF
    70|                     ENDDO
                          ENDIF
    71|                 ENDDO
                      ENDIF
    67|               $$csx1 = max(int(kn),0) > MOD(max(int(kn),0), 4)  .AND.  &
                &       0 < max(int(kn),0)
                      IF ($$csx1) THEN
                        $$LoopIV0 = MOD(max(int(kn),0), int(4))
       Id=46            DO $$LoopIV0 = $$LoopIV0, int(max(int(kn),0))
    68|                   IF ((MOD(int(jn), 4) > 0  .AND.  0 < int(jn))) THEN
                            $$LoopIV1 = 0
       Id=52                DO $$LoopIV1 = $$LoopIV1, MOD(int(jn), int(4))-1
    69|                       IF ((0 < int(in))) THEN
                                $$LoopIV2 = 0
       Id=53                    DO $$LoopIV2 = $$LoopIV2, int(int(in))-1
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,&
                &                   $$LoopIV1 + 1,$$LoopIV0 + 1) = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                ENDDO
                              ENDIF
    70|                     ENDDO
                          ENDIF
    71|                 ENDDO
                      ENDIF
    67|               IF ($$csx1) THEN
                        $$CIV16 = int(0)
       Id=1             DO $$CIV16 = $$CIV16, int((((max(int(kn),0) - MOD(max(&
                &           int(kn),0), 4)) - 1) / 4 + 1))-1
    68|                   IF ((0 < int(jn)  .AND.  int(jn) > MOD(int(jn), 4))) &
                &           THEN
                            $$CIV15 = int(0)
       Id=2                 DO $$CIV15 = $$CIV15, int((((int(jn) - MOD(int(jn)&
                &               , 4)) - 1) / 4 + 1))-1
    69|                       IF ((0 < int(in))) THEN
                                $$LoopIV2 = 0
       Id=3                     DO $$LoopIV2 = $$LoopIV2, int(int(in))-1
                                  $$csx2 = ampl * _sin(( &
                &                   6.2831854820251464E+000 / abs(%VAL(x1max) - &
                &                   %VAL(x1min))) * d-x1b%addr%x1b(%VAL($$LoopIV2)&
                &                    + 1))
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),1 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),2 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),3 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,1 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,2 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,3 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                  d-pert%addr%pert[].rns4.($$LoopIV2 + 1,4 + (&
                &                   $$CIV15 * 4 + MOD(int(jn), 4)),4 + ($$CIV16 * &
                &                   4 + MOD(max(int(kn),0), 4))) = $$csx2
                                ENDDO
                              ENDIF
    70|                     ENDDO
                          ENDIF
    71|                 ENDDO
                      ENDIF
    84|               IF ((MOD(d-d%bounds%extent[].off40, 4) > 0  .AND.  &
                &       d-d%bounds%extent[].off40 > 0)) THEN
                        $$CIV5 = 0
       Id=43            DO $$CIV5 = $$CIV5, MOD(d-d%bounds%extent[].off40, &
                &           int(4))-1
                          IF ((d-d%bounds%extent[].off64 > 0)) THEN
                            $$CIV4 = 0
       Id=44                DO $$CIV4 = $$CIV4, int(d-d%bounds%extent[].off64)&
                &               -1
                              IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                $$CIV3 = 0
       Id=45                    DO $$CIV3 = $$CIV3, int(&
                &                   d-d%bounds%extent[].off88)-1
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,&
                &                   d-d%bounds%lbound[].off32 + $$CIV5) =  &
                &                   1.0000000000000000E-099
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                      IF ((d-d%bounds%extent[].off40 > 0  .AND.  &
                &       d-d%bounds%extent[].off40 > MOD(d-d%bounds%extent[].off40,&
                &        4))) THEN
                        $$CIV17 = int(0)
       Id=4             DO $$CIV17 = $$CIV17, int((((&
                &           d-d%bounds%extent[].off40 - MOD(&
                &           d-d%bounds%extent[].off40, 4)) - 1) / 4 + 1))-1
                          IF ((d-d%bounds%extent[].off64 > 0)) THEN
                            $$CIV4 = 0
       Id=5                 DO $$CIV4 = $$CIV4, int(d-d%bounds%extent[].off64)&
                &               -1
                              IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                $$CIV3 = 0
       Id=6                     DO $$CIV3 = $$CIV3, int(&
                &                   d-d%bounds%extent[].off88)-1
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,($$CIV17 * &
                &                   4 + MOD(d-d%bounds%extent[].off40, 4)) + &
                &                   d-d%bounds%lbound[].off32) =  &
                &                   1.0000000000000000E-099
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,1 + ((&
                &                   $$CIV17 * 4 + MOD(d-d%bounds%extent[].off40, &
                &                   4)) + d-d%bounds%lbound[].off32)) =  &
                &                   1.0000000000000000E-099
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,2 + ((&
                &                   $$CIV17 * 4 + MOD(d-d%bounds%extent[].off40, &
                &                   4)) + d-d%bounds%lbound[].off32)) =  &
                &                   1.0000000000000000E-099
                                  d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV3,&
                &                   d-d%bounds%lbound[].off56 + $$CIV4,3 + ((&
                &                   $$CIV17 * 4 + MOD(d-d%bounds%extent[].off40, &
                &                   4)) + d-d%bounds%lbound[].off32)) =  &
                &                   1.0000000000000000E-099
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
    85|               T_12 = -iseed
                      CALL ran1(T_12,rn)
    89|               IF ((MOD(int(kn), 2) > 0  .AND.  int(kn) > 0)) THEN
                        $$CIV8 = 0
       Id=37            DO $$CIV8 = $$CIV8, MOD(int(kn), int(2))-1
    90|                   IF ((int(jn) > 0)) THEN
                            $$CIV7 = 0
       Id=38                DO $$CIV7 = $$CIV7, int(int(jn))-1
    91|                       IF ((int(in) > 0)) THEN
                                $$CIV6 = 0
       Id=39                    DO $$CIV6 = $$CIV6, int(int(in))-1
    92|                           IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) >= &
                &                   line_val)) GOTO lab_211
    93|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1) &
                &                   = rho_out
    94|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1)&
                &                    = rho_out * (d-pert%addr%pert[].rns4.($$CIV6 &
                &                   + 1,$$CIV7 + 1,$$CIV8 + 1) - v0)
    95|                           GOTO lab_213
                                  lab_211
                                  IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) < &
                &                   line_val)) GOTO lab_212
    96|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1) &
                &                   = rho_in
    97|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1)&
                &                    = rho_in * (v0 + d-pert%addr%pert[].rns4.(&
                &                   $$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1))
    98|                           lab_212
                                  lab_213
    99|                           d-e%addr%e($$CIV6 + 1,$$CIV7 + 1,$$CIV8 + 1) &
                &                   = p0 / (gamma -  1.0000000000000000E+000)
   100|                         ENDDO
                              ENDIF
   101|                     ENDDO
                          ENDIF
   102|                 ENDDO
                      ENDIF
    89|               IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 2))) THEN
                        $$CIV18 = int(0)
       Id=7             DO $$CIV18 = $$CIV18, int((((int(kn) - MOD(int(kn), 2)&
                &           ) - 1) / 2 + 1))-1
    90|                   IF ((int(jn) > 0)) THEN
                            $$CIV7 = 0
       Id=8                 DO $$CIV7 = $$CIV7, int(int(jn))-1
    91|                       IF ((int(in) > 0)) THEN
                                $$CIV6 = 0
       Id=9                     DO $$CIV6 = $$CIV6, int(int(in))-1
    92|                           IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) >= &
                &                   line_val)) GOTO lab_41
    93|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,1 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_out
    94|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,1 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_out * (&
                &                   d-pert%addr%pert[].rns4.($$CIV6 + 1,$$CIV7 + &
                &                   1,1 + ($$CIV18 * 2 + MOD(int(kn), 2))) - v0)
    95|                           GOTO lab_42
                                  lab_41
                                  IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) < &
                &                   line_val)) GOTO lab_43
    96|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,1 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_in
    97|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,1 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_in * (&
                &                   v0 + d-pert%addr%pert[].rns4.($$CIV6 + 1,&
                &                   $$CIV7 + 1,1 + ($$CIV18 * 2 + MOD(int(kn), 2))&
                &                   ))
    98|                           lab_43
                                  lab_42
    99|                           d-e%addr%e($$CIV6 + 1,$$CIV7 + 1,1 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = p0 / (gamma -  &
                &                   1.0000000000000000E+000)
    92|                           IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) >= &
                &                   line_val)) GOTO lab_217
    93|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,2 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_out
    94|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,2 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_out * (&
                &                   d-pert%addr%pert[].rns4.($$CIV6 + 1,$$CIV7 + &
                &                   1,2 + ($$CIV18 * 2 + MOD(int(kn), 2))) - v0)
    95|                           GOTO lab_219
                                  lab_217
                                  IF (.NOT.(abs(d-x2b%addr%x2b($$CIV7 + 1)) < &
                &                   line_val)) GOTO lab_218
    96|                           d-d%addr%d($$CIV6 + 1,$$CIV7 + 1,2 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = rho_in
    97|                           d-v1%addr%v1($$CIV6 + 1,$$CIV7 + 1,2 + (&
                &                   $$CIV18 * 2 + MOD(int(kn), 2))) = rho_in * (&
                &                   v0 + d-pert%addr%pert[].rns4.($$CIV6 + 1,&
                &                   $$CIV7 + 1,2 + ($$CIV18 * 2 + MOD(int(kn), 2))&
                &                   ))
    98|                           lab_218
                                  lab_219
    99|                           d-e%addr%e($$CIV6 + 1,$$CIV7 + 1,2 + ($$CIV18 &
                &                   * 2 + MOD(int(kn), 2))) = p0 / (gamma -  &
                &                   1.0000000000000000E+000)
   100|                         ENDDO
                              ENDIF
   101|                     ENDDO
                          ENDIF
   102|                 ENDDO
                      ENDIF
   103|               IF ((MOD(d-v2%bounds%extent[].off560, 2) > 0  .AND.  &
                &       d-v2%bounds%extent[].off560 > 0)) THEN
                        $$CIVB = 0
       Id=31            DO $$CIVB = $$CIVB, MOD(d-v2%bounds%extent[].off560, &
                &           int(2))-1
                          IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                            $$CIVA = 0
       Id=32                DO $$CIVA = $$CIVA, int(&
                &               d-v2%bounds%extent[].off584)-1
                              IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                                $$CIV9 = 0
       Id=33                    DO $$CIV9 = $$CIV9, int(&
                &                   d-v2%bounds%extent[].off608)-1
                                  d-v2%addr%v2(d-v2%bounds%lbound[].off600 + &
                &                   $$CIV9,d-v2%bounds%lbound[].off576 + $$CIVA,&
                &                   d-v2%bounds%lbound[].off552 + $$CIVB) = &
                &                   d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV9,&
                &                   d-d%bounds%lbound[].off56 + $$CIVA,&
                &                   d-d%bounds%lbound[].off32 + $$CIVB) * &
                &                   d-pert%addr%pert[].rns4.($$CIV9 + 1,$$CIVA + &
                &                   1,$$CIVB + 1)
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                      IF ((d-v2%bounds%extent[].off560 > 0  .AND.  &
                &       d-v2%bounds%extent[].off560 > MOD(&
                &       d-v2%bounds%extent[].off560, 2))) THEN
                        $$CIV19 = int(0)
       Id=10            DO $$CIV19 = $$CIV19, int((((&
                &           d-v2%bounds%extent[].off560 - MOD(&
                &           d-v2%bounds%extent[].off560, 2)) - 1) / 2 + 1))-1
                          IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                            $$CIVA = 0
       Id=11                DO $$CIVA = $$CIVA, int(&
                &               d-v2%bounds%extent[].off584)-1
                              IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                                $$CIV9 = 0
       Id=12                    DO $$CIV9 = $$CIV9, int(&
                &                   d-v2%bounds%extent[].off608)-1
                                  d-v2%addr%v2(d-v2%bounds%lbound[].off600 + &
                &                   $$CIV9,d-v2%bounds%lbound[].off576 + $$CIVA,(&
                &                   $$CIV19 * 2 + MOD(d-v2%bounds%extent[].off560,&
                &                    2)) + d-v2%bounds%lbound[].off552) = &
                &                   d-d%addr%d(d-d%bounds%lbound[].off80 + $$CIV9,&
                &                   d-d%bounds%lbound[].off56 + $$CIVA,($$CIV19 * &
                &                   2 + MOD(d-v2%bounds%extent[].off560, 2)) + &
                &                   d-d%bounds%lbound[].off32) * &
                &                   d-pert%addr%pert[].rns4.($$CIV9 + 1,$$CIVA + &
                &                   1,1 + ($$CIV19 * 2 + MOD(&
                &                   d-v2%bounds%extent[].off560, 2)))
                                  d-v2%addr%v2(d-v2%bounds%lbound[].off600 + &
                &                   $$CIV9,d-v2%bounds%lbound[].off576 + $$CIVA,1 &
                &                   + (($$CIV19 * 2 + MOD(&
                &                   d-v2%bounds%extent[].off560, 2)) + &
                &                   d-v2%bounds%lbound[].off552)) = d-d%addr%d(&
                &                   d-d%bounds%lbound[].off80 + $$CIV9,&
                &                   d-d%bounds%lbound[].off56 + $$CIVA,1 + ((&
                &                   $$CIV19 * 2 + MOD(d-v2%bounds%extent[].off560,&
                &                    2)) + d-d%bounds%lbound[].off32)) * &
                &                   d-pert%addr%pert[].rns4.($$CIV9 + 1,$$CIVA + &
                &                   1,2 + ($$CIV19 * 2 + MOD(&
                &                   d-v2%bounds%extent[].off560, 2)))
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
   104|               IF ((MOD(d-v3%bounds%extent[].off664, 4) > 0  .AND.  &
                &       d-v3%bounds%extent[].off664 > 0)) THEN
                        $$CIVE = 0
       Id=25            DO $$CIVE = $$CIVE, MOD(d-v3%bounds%extent[].off664, &
                &           int(4))-1
                          IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                            $$CIVD = 0
       Id=26                DO $$CIVD = $$CIVD, int(&
                &               d-v3%bounds%extent[].off688)-1
                              IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                                $$CIVC = 0
       Id=27                    DO $$CIVC = $$CIVC, int(&
                &                   d-v3%bounds%extent[].off712)-1
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,&
                &                   d-v3%bounds%lbound[].off656 + $$CIVE) =  &
                &                   0.0000000000000000E+000
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                      IF ((d-v3%bounds%extent[].off664 > 0  .AND.  &
                &       d-v3%bounds%extent[].off664 > MOD(&
                &       d-v3%bounds%extent[].off664, 4))) THEN
                        $$CIV1A = int(0)
       Id=13            DO $$CIV1A = $$CIV1A, int((((&
                &           d-v3%bounds%extent[].off664 - MOD(&
                &           d-v3%bounds%extent[].off664, 4)) - 1) / 4 + 1))-1
                          IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                            $$CIVD = 0
       Id=14                DO $$CIVD = $$CIVD, int(&
                &               d-v3%bounds%extent[].off688)-1
                              IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                                $$CIVC = 0
       Id=15                    DO $$CIVC = $$CIVC, int(&
                &                   d-v3%bounds%extent[].off712)-1
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,(&
                &                   $$CIV1A * 4 + MOD(d-v3%bounds%extent[].off664,&
                &                    4)) + d-v3%bounds%lbound[].off656) =  &
                &                   0.0000000000000000E+000
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,1 &
                &                   + (($$CIV1A * 4 + MOD(&
                &                   d-v3%bounds%extent[].off664, 4)) + &
                &                   d-v3%bounds%lbound[].off656)) =  &
                &                   0.0000000000000000E+000
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,2 &
                &                   + (($$CIV1A * 4 + MOD(&
                &                   d-v3%bounds%extent[].off664, 4)) + &
                &                   d-v3%bounds%lbound[].off656)) =  &
                &                   0.0000000000000000E+000
                                  d-v3%addr%v3(d-v3%bounds%lbound[].off704 + &
                &                   $$CIVC,d-v3%bounds%lbound[].off680 + $$CIVD,3 &
                &                   + (($$CIV1A * 4 + MOD(&
                &                   d-v3%bounds%extent[].off664, 4)) + &
                &                   d-v3%bounds%lbound[].off656)) =  &
                &                   0.0000000000000000E+000
                                ENDDO
                              ENDIF
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
   106|               IF ((d-pert%addr <> NULL)) THEN
                        CALL free(d-pert%addr)
                      ENDIF
   107|               IF ((myid_w == 0)) THEN
   108|                 #8 = _xlfBeginIO(6,257,#7,32768,NULL,0,NULL)
                        CALL _xlfWriteLDChar(%VAL(#8),"KH SETUP FINISHED",17,1)
                        _xlfEndIO(%VAL(#8))
   109|                 #10 = _xlfBeginIO(6,257,#9,32768,NULL,0,NULL)
                        T_40 = -1.7976931348623157E+308
                        IF ((d-d%bounds%extent[].off40 > 0)) THEN
                          $$CIV11 = 0
                          $$ICM.T_400 = T_40
       Id=16              DO $$CIV11 = $$CIV11, int(d-d%bounds%extent[].off40)&
                &             -1
                            IF ((d-d%bounds%extent[].off64 > 0)) THEN
                              $$CIV10 = 0
       Id=17                  DO $$CIV10 = $$CIV10, int(&
                &                 d-d%bounds%extent[].off64)-1
                                IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                  $$CIVF = 0
       Id=18                      DO $$CIVF = $$CIVF, int(&
                &                     d-d%bounds%extent[].off88)-1
                                    IF ((d-d%addr%d(d-d%bounds%lbound[].off80 + &
                &                     $$CIVF,d-d%bounds%lbound[].off56 + $$CIV10,&
                &                     d-d%bounds%lbound[].off32 + $$CIV11) > &
                &                     $$ICM.T_400)) THEN
                                      $$ICM.T_400 = d-d%addr%d((&
                &                       d-d%bounds%extent[].off88 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off80) + $$CIVF,(&
                &                       d-d%bounds%extent[].off64 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off56) + $$CIV10,(&
                &                       d-d%bounds%extent[].off40 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off32) + $$CIV11)
                                    ENDIF
                                  ENDDO
                                ENDIF
                              ENDDO
                            ENDIF
                          ENDDO
                          T_40 = $$ICM.T_400
                        ENDIF
                        CALL _xlfWriteLDReal(%VAL(#10),T_40,8,8)
                        T_44 =  1.7976931348623157E+308
                        IF ((d-d%bounds%extent[].off40 > 0)) THEN
                          $$CIV14 = 0
                          $$ICM.T_441 = T_44
       Id=19              DO $$CIV14 = $$CIV14, int(d-d%bounds%extent[].off40)&
                &             -1
                            IF ((d-d%bounds%extent[].off64 > 0)) THEN
                              $$CIV13 = 0
       Id=20                  DO $$CIV13 = $$CIV13, int(&
                &                 d-d%bounds%extent[].off64)-1
                                IF ((d-d%bounds%extent[].off88 > 0)) THEN
                                  $$CIV12 = 0
       Id=21                      DO $$CIV12 = $$CIV12, int(&
                &                     d-d%bounds%extent[].off88)-1
                                    IF ((d-d%addr%d(d-d%bounds%lbound[].off80 + &
                &                     $$CIV12,d-d%bounds%lbound[].off56 + $$CIV13,&
                &                     d-d%bounds%lbound[].off32 + $$CIV14) < &
                &                     $$ICM.T_441)) THEN
                                      $$ICM.T_441 = d-d%addr%d((&
                &                       d-d%bounds%extent[].off88 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off80) + $$CIV12,(&
                &                       d-d%bounds%extent[].off64 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off56) + $$CIV13,(&
                &                       d-d%bounds%extent[].off40 == 0 ? 1 : &
                &                       d-d%bounds%lbound[].off32) + $$CIV14)
                                    ENDIF
                                  ENDDO
                                ENDIF
                              ENDDO
                            ENDIF
                          ENDDO
                          T_44 = $$ICM.T_441
                        ENDIF
                        CALL _xlfWriteLDReal(%VAL(#10),T_44,8,8)
                        _xlfEndIO(%VAL(#10))
   111|               ENDIF
                      IF (.NOT.(d-rannum%addr <> NULL)) GOTO lab_173
                      CALL free(d-rannum%addr)
   112|             ELSE
    59|               filenameaddr_1 = "kh.f90"
                      filenamelen_1 = 6
                      CALL _xlfErrorExitWithLoc(0,3,108,0,NULL,NULL,filename_1,&
                &       59,NULL)
                      CALL _trap(3)
                      lab_173
                      RETURN
   112|             END SUBROUTINE kh

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- --ss ssss ssss
 CCR's set/used:   ssss ssss
     | 000000                           PDEF     kh
    7|                                  PROC      
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 stfd     DB61FFD8   1     STFL      #stack(gr1,-40)=fp27
    0| 000014 stfd     DB41FFD0   1     STFL      #stack(gr1,-48)=fp26
    0| 000018 stfd     DB21FFC8   1     STFL      #stack(gr1,-56)=fp25
    0| 00001C stfd     DB01FFC0   1     STFL      #stack(gr1,-64)=fp24
    0| 000020 stfd     DAE1FFB8   1     STFL      #stack(gr1,-72)=fp23
    0| 000024 stfd     DAC1FFB0   1     STFL      #stack(gr1,-80)=fp22
    0| 000028 std      FBE1FFA8   1     ST8       #stack(gr1,-88)=gr31
    0| 00002C std      FBC1FFA0   1     ST8       #stack(gr1,-96)=gr30
    0| 000030 std      FBA1FF98   1     ST8       #stack(gr1,-104)=gr29
    0| 000034 std      FB81FF90   1     ST8       #stack(gr1,-112)=gr28
    0| 000038 std      FB61FF88   1     ST8       #stack(gr1,-120)=gr27
    0| 00003C std      FB41FF80   1     ST8       #stack(gr1,-128)=gr26
    0| 000040 std      FB21FF78   1     ST8       #stack(gr1,-136)=gr25
    0| 000044 std      FB01FF70   1     ST8       #stack(gr1,-144)=gr24
    0| 000048 std      FAE1FF68   1     ST8       #stack(gr1,-152)=gr23
    0| 00004C std      FAC1FF60   1     ST8       #stack(gr1,-160)=gr22
    0| 000050 std      FAA1FF58   1     ST8       #stack(gr1,-168)=gr21
    0| 000054 std      FA81FF50   1     ST8       #stack(gr1,-176)=gr20
    0| 000058 std      FA61FF48   1     ST8       #stack(gr1,-184)=gr19
    0| 00005C std      FA41FF40   1     ST8       #stack(gr1,-192)=gr18
    0| 000060 std      FA21FF38   1     ST8       #stack(gr1,-200)=gr17
    0| 000064 std      FA01FF30   1     ST8       #stack(gr1,-208)=gr16
    0| 000068 std      F9E1FF28   1     ST8       #stack(gr1,-216)=gr15
    0| 00006C std      F9C1FF20   1     ST8       #stack(gr1,-224)=gr14
    0| 000070 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 000074 mfcr     7D800026   1     LFCR      gr12=cr[234],2
    0| 000078 stw      91810008   1     ST4A      #stack(gr1,8)=gr12
    0| 00007C std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000080 stdu     F821FB01   1     ST8U      gr1,#stack(gr1,-1280)=gr1
   32| 000084 ld       E9020000   1     L8        gr8=.&&N&&mpipar(gr2,0)
   26| 000088 ld       E9220000   1     L8        gr9=.+CONSTANT_AREA(gr2,0)
   26| 00008C addi     380001FF   1     LI        gr0=511
   27| 000090 addi     38601001   1     LI        gr3=4097
   28| 000094 addi     38800001   1     LI        gr4=1
   26| 000098 rldicr   7800AA86   1     SLL8      gr0=gr0,53
   32| 00009C lwz      80C80004   1     L4Z       gr6=<s119:d4:l4>(gr8,4)
   26| 0000A0 std      F8010098   1     ST8       v0(gr1,152)=gr0
   30| 0000A4 lfs      C3E90040   1     LFS       fp31=+CONSTANT_AREA(gr9,64)
   29| 0000A8 addi     38A003FF   1     LI        gr5=1023
   31| 0000AC addi     38E003FD   1     LI        gr7=1021
   27| 0000B0 rldicr   78639346   1     SLL8      gr3=gr3,50
   28| 0000B4 rldicr   7880F046   1     SLL8      gr0=gr4,62
   27| 0000B8 std      F86100A0   1     ST8       p0(gr1,160)=gr3
   28| 0000BC std      F80100A8   1     ST8       rho_in(gr1,168)=gr0
   29| 0000C0 rldicr   78A3A2C6   1     SLL8      gr3=gr5,52
   31| 0000C4 rldicr   78E0A2C6   1     SLL8      gr0=gr7,52
   29| 0000C8 std      F86100B0   1     ST8       rho_out(gr1,176)=gr3
   30| 0000CC stfd     DBE100B8   1     STFL      ampl(gr1,184)=fp31
   32| 0000D0 cmpdi    2C260000   1     C8        cr0=gr6,0
   31| 0000D4 std      F80100C0   1     ST8       line_val(gr1,192)=gr0
   26| 0000D8 lfs      C3C90030   1     LFS       fp30=+CONSTANT_AREA(gr9,48)
   27| 0000DC lfs      C3A90034   1     LFS       fp29=+CONSTANT_AREA(gr9,52)
   28| 0000E0 lfs      C3890038   1     LFS       fp28=+CONSTANT_AREA(gr9,56)
   29| 0000E4 lfs      C369003C   1     LFS       fp27=+CONSTANT_AREA(gr9,60)
   31| 0000E8 lfs      C2C90044   1     LFS       fp22=+CONSTANT_AREA(gr9,68)
   32| 0000EC bc       41821984   1     BT        CL.1547,cr0,0x4/eq,taken=40%(40,60)
   47|                              CL.2:
   48| 0000F0 addis    3C604C00   1     LIU       gr3=19456
   48| 0000F4 addi     3BA00006   1     LI        gr29=6
   48| 0000F8 addi     3803081F   1     AI        gr0=gr3,2079
   48| 0000FC or       7D0E4378   1     LR        gr14=gr8
   48| 000100 stw      93A10080   1     ST4Z      T_2(gr1,128)=gr29
   48| 000104 addi     3BE00000   1     LI        gr31=0
   48| 000108 stw      90010084   1     ST4Z      T_3(gr1,132)=gr0
   48| 00010C stw      93E10088   1     ST4Z      T_4(gr1,136)=gr31
   48| 000110 addi     39080014   1     AI        gr8=gr8,20
   48| 000114 addi     38EE0020   1     AI        gr7=gr14,32
   48| 000118 addi     38C10088   1     AI        gr6=gr1,136
   48| 00011C addi     38A10084   1     AI        gr5=gr1,132
   48| 000120 addi     38810080   1     AI        gr4=gr1,128
   48| 000124 addi     386E06D0   1     AI        gr3=gr14,1744
   48| 000128 bl       48000001   1     CALL      mpi_bcast,6,buf_in[]",gr3,T_2",gr4,T_3",gr5,T_4",gr6,comm3d",gr7,ierr",gr8,mpi_bcast",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   48| 00012C ori      60000000   1
   49| 000130 lwz      800E0004   1     L4Z       gr0=<s119:d4:l4>(gr14,4)
   49| 000134 cmpdi    2C200000   1     C8        cr0=gr0,0
   49| 000138 bc       4182001C   1     BT        CL.3,cr0,0x4/eq,taken=50%(0,0)
   50| 00013C lfd      CBCE06D0   1     LFL       fp30=<s119:d1744:l8>(gr14,1744)
   51| 000140 lfd      CBAE06D8   1     LFL       fp29=<s119:d1752:l8>(gr14,1752)
   52| 000144 lfd      CB8E06E0   1     LFL       fp28=<s119:d1760:l8>(gr14,1760)
   53| 000148 lfd      CB6E06E8   1     LFL       fp27=<s119:d1768:l8>(gr14,1768)
   54| 00014C lfd      CBEE06F0   1     LFL       fp31=<s119:d1776:l8>(gr14,1776)
   55| 000150 lfd      CACE06F8   1     LFL       fp22=<s119:d1784:l8>(gr14,1784)
   56|                              CL.3:
   59| 000154 ld       E8A20000   1     L8        gr5=.&&N&&param(gr2,0)
   59| 000158 lwa      EBC50006   1     L4A       gr30=<s145:d4:l4>(gr5,4)
   59| 00015C lwa      E8C50002   1     L4A       gr6=<s145:d0:l4>(gr5,0)
   59| 000160 lwa      E865000A   1     L4A       gr3=<s145:d8:l4>(gr5,8)
   59| 000164 sradi    7FC4FE76   1     SRA8      gr4=gr30,63,ca"
   59| 000168 sradi    7CC0FE76   1     SRA8      gr0=gr6,63,ca"
   59| 00016C std      F8C10298   1     ST8       #SPILL0(gr1,664)=gr6
   59| 000170 andc     7FD12078   1     ANDC      gr17=gr30,gr4
   59| 000174 andc     7CDF0078   1     ANDC      gr31=gr6,gr0
   59| 000178 mulld    7CF1F9D2   1     M         gr7=gr17,gr31
   59| 00017C sradi    7C60FE76   1     SRA8      gr0=gr3,63,ca"
   59| 000180 std      F8E102A0   1     ST8       #SPILL1(gr1,672)=gr7
   59| 000184 andc     7C7C0078   1     ANDC      gr28=gr3,gr0
   59| 000188 mulld    7C07E1D2   1     M         gr0=gr7,gr28
   59| 00018C rldicr   78031F25   1     SLL8_R    gr3,cr0=gr0,3
   59| 000190 bc       40821888   1     BF        CL.6,cr0,0x4/eq,taken=40%(40,60)
   59| 000194 addi     38000000   1     LI        gr0=0
   59| 000198 std      F80102A8   1     ST8       #SPILL2(gr1,680)=gr0
   60|                              CL.9:
   59| 00019C ld       E88102A0   1     L8        gr4=#SPILL1(gr1,672)
   59| 0001A0 rldicr   7BE51F25   1     SLL8_R    gr5,cr0=gr31,3
   59| 0001A4 std      F8A102B0   1     ST8       #SPILL3(gr1,688)=gr5
   59| 0001A8 add      7C64FA14   1     A         gr3=gr4,gr31
   59| 0001AC rldicr   78861F24   1     SLL8      gr6=gr4,3
   59| 0001B0 addi     38030001   1     AI        gr0=gr3,1
   59| 0001B4 std      F8C102B8   1     ST8       #SPILL4(gr1,696)=gr6
   59| 0001B8 rldicr   78071F24   1     SLL8      gr7=gr0,3
   59| 0001BC std      F8E102C0   1     ST8       #SPILL5(gr1,704)=gr7
   60| 0001C0 bc       408217FC   1     BF        CL.11,cr0,0x4/eq,taken=40%(40,60)
   60| 0001C4 addi     38000000   1     LI        gr0=0
   60| 0001C8 std      F8010408   1     ST8       #SPILL46(gr1,1032)=gr0
   60|                              CL.12:
   61| 0001CC addi     3BA00000   1     LI        gr29=0
   61| 0001D0 addi     3861008C   1     AI        gr3=gr1,140
   61| 0001D4 std      FBA10070   1     ST8       #MX_TEMP1(gr1,112)=gr29
   61| 0001D8 addi     38800000   1     LI        gr4=0
   61| 0001DC addi     38A00000   1     LI        gr5=0
   61| 0001E0 addi     38C00000   1     LI        gr6=0
   61| 0001E4 addi     38E00001   1     LI        gr7=1
   61| 0001E8 addi     39000000   1     LI        gr8=0
   61| 0001EC addi     39200000   1     LI        gr9=0
   61| 0001F0 addi     39400000   1     LI        gr10=0
   61| 0001F4 std      FBA10078   1     ST8       #MX_TEMP1(gr1,120)=gr29
   61| 0001F8 bl       48000001   1     CALL      _xlirndsg,10,seedsize",gr3,@PALI_SHADOW.rns2.",gr4,@PALI_SHADOW.rns2.",gr5,@PALI_SHADOW.rns2.",gr6-gr10,_xlirndsg",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   61| 0001FC ori      60000000   1
   61| 000200 sradi    7F801674   1     SRA8CA    gr0,ca=gr28,2
   67| 000204 cmpdi    2DBC0000   1     C8        cr3=gr28,0
   61| 000208 addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
   67| 00020C rldicr   781A1764   1     SLL8      gr26=gr0,2
   67| 000210 subf     7F7AE051   1     S_R       gr27,cr0=gr28,gr26
   67| 000214 std      FB4102C8   1     ST8       #SPILL6(gr1,712)=gr26
   67| 000218 crand    4C2D0A02   1     CR_N      cr0=cr[30],0x2/gt,0x2/gt,0x2/gt,cr0
   67| 00021C bc       408100E4   1     BF        CL.243,cr0,0x2/gt,taken=50%(0,0)
   69| 000220 ld       E8620000   1     L8        gr3=.&&N&grid(gr2,0)
   69| 000224 ld       E8820000   1     L8        gr4=.&&N&&domain(gr2,0)
    0| 000228 cmpwi    2C1E0000   1     C4        cr0=gr30,0
   69| 00022C ld       E80306C8   1     L8        gr0=<s82:d1736:l8>(gr3,1736)
   69| 000230 ld       E86306E0   1     L8        gr3=<s82:d1760:l8>(gr3,1760)
   69| 000234 lfd      CB440008   1     LFL       fp26=<s165:d8:l8>(gr4,8)
   69| 000238 lfd      CB240000   1     LFL       fp25=<s165:d0:l8>(gr4,0)
    0| 00023C bc       4081176C   1     BF        CL.716,cr0,0x2/gt,taken=40%(40,60)
    0| 000240 rldicr   7A241F24   1     SLL8      gr4=gr17,3
    0| 000244 ld       E8A102A8   1     L8        gr5=#SPILL2(gr1,680)
   69| 000248 add      7F001A14   1     A         gr24=gr0,gr3
    0| 00024C ld       E8010298   1     L8        gr0=#SPILL0(gr1,664)
    0| 000250 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
    0| 000254 mulld    7F24F9D2   1     M         gr25=gr4,gr31
    0| 000258 addi     3B45FFF8   1     AI        gr26=gr5,-8
    0| 00025C cmpdi    2E200000   1     C8        cr4=gr0,0
    0| 000260 lfs      C303005C   1     LFS       fp24=+CONSTANT_AREA(gr3,92)
   69| 000264 ld       EA410298   1     L8        gr18=#SPILL0(gr1,664)
   67|                              CL.238:
   68| 000268 addi     3AE00000   1     LI        gr23=0
    0| 00026C bc       40910084   1     BF        CL.242,cr4,0x2/gt,taken=50%(0,0)
    0| 000270 fsub     FC3AC828   1     SFL       fp1=fp26,fp25,fcr
    0| 000274 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
    0| 000278 or       7F56D378   1     LR        gr22=gr26
    0| 00027C fabs     FC600A10   1     ABSFL     fp3=fp1
    0| 000280 lfs      C003003C   2     LFS       fp0=+CONSTANT_AREA(gr3,60)
    0| 000284 qvfre    10201830   1     QVFRE     fp1=fp3
    0| 000288 fmsub    FC430078   1     FMS       fp2=fp0,fp3,fp1,fcr
    0| 00028C fnmsub   FC2108BC   2     FNMS      fp1=fp1,fp1,fp2,fcr
    0| 000290 fmsub    FC030078   2     FMS       fp0=fp0,fp3,fp1,fcr
    0| 000294 fnmsub   FC01083C   2     FNMS      fp0=fp1,fp1,fp0,fcr
    0| 000298 fmul     FC380032   2     MFL       fp1=fp24,fp0,fcr
    0| 00029C fmsub    FC43C078   2     FMS       fp2=fp24,fp3,fp1,fcr
    0| 0002A0 fnmsub   FEE008BC   2     FNMS      fp23=fp1,fp0,fp2,fcr
   68|                              CL.239:
    0| 0002A4 addi     3AF70001   1     AI        gr23=gr23,1
   69| 0002A8 or       7ED5B378   1     LR        gr21=gr22
   69| 0002AC addi     3A800000   1     LI        gr20=0
   69| 0002B0 or       7F13C378   1     LR        gr19=gr24
    0| 0002B4 ori      60210000   1     XNOP      
    0| 0002B8 ori      60210000   1     XNOP      
   69|                              CL.240:
   69| 0002BC lfdu     CC130008   1     LFDU      fp0,gr19=x1b(gr19,8)
   69| 0002C0 addi     3A940001   1     AI        gr20=gr20,1
   69| 0002C4 fmul     FC370032   1     MFL       fp1=fp23,fp0,fcr
   69| 0002C8 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   69| 0002CC ori      60000000   1
   69| 0002D0 cmpld    7C349040   1     CL8       cr0=gr20,gr18
   69| 0002D4 fmul     FC1F0072   1     MFL       fp0=fp31,fp1,fcr
   69| 0002D8 stfdu    DC150008   2     STFDU     gr21,pert(gr21,8)=fp0
   69| 0002DC bc       4180FFE0   1     BT        CL.240,cr0,0x8/llt,taken=80%(80,20)
    0| 0002E0 ld       E80102B0   1     L8        gr0=#SPILL3(gr1,688)
   70| 0002E4 cmpld    7C37F040   1     CL8       cr0=gr23,gr30
    0| 0002E8 add      7EC0B214   1     A         gr22=gr0,gr22
   70| 0002EC bc       4180FFB8   1     BT        CL.239,cr0,0x8/llt,taken=80%(80,20)
   70|                              CL.242:
   71| 0002F0 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 0002F4 add      7F59D214   1     A         gr26=gr25,gr26
   71| 0002F8 cmpd     7C3BE800   1     C8        cr0=gr27,gr29
   71| 0002FC bc       4181FF6C   1     BT        CL.238,cr0,0x2/gt,taken=80%(80,20)
   71|                              CL.243:
   67| 000300 cmpd     7C3CD800   1     C8        cr0=gr28,gr27
   67| 000304 crand    4DA16A02   1     CR_N      cr3=cr[03],0x2/gt,0x2/gt,0x2/gt,cr3
   67| 000308 bc       408D0570   1     BF        CL.131,cr3,0x2/gt,taken=50%(0,0)
    0| 00030C rldicr   7A231F24   1     SLL8      gr3=gr17,3
   69| 000310 sradi    7FC01674   1     SRA8CA    gr0,ca=gr30,2
    0| 000314 mulld    7F43F9D2   1     M         gr26=gr3,gr31
   69| 000318 addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
    0| 00031C ld       E8C102A8   1     L8        gr6=#SPILL2(gr1,680)
   68| 000320 rldicr   780F1764   1     SLL8      gr15=gr0,2
   69| 000324 ld       E8A20000   1     L8        gr5=.&&N&grid(gr2,0)
   69| 000328 ld       E8620000   1     L8        gr3=.&&N&&domain(gr2,0)
    0| 00032C mulld    7C9BD1D2   1     M         gr4=gr27,gr26
   68| 000330 subf     7FAFF051   1     S_R       gr29,cr0=gr30,gr15
   68| 000334 cmpwi    2E1E0000   1     C4        cr4=gr30,0
    0| 000338 addi     3806FFF8   1     AI        gr0=gr6,-8
   68| 00033C crand    4C310A02   1     CR_N      cr0=cr[40],0x2/gt,0x2/gt,0x2/gt,cr0
   67| 000340 or       7F79DB78   1     LR        gr25=gr27
   69| 000344 ld       E9C506C8   1     L8        gr14=<s82:d1736:l8>(gr5,1736)
   69| 000348 ld       EA0506E0   1     L8        gr16=<s82:d1760:l8>(gr5,1760)
   69| 00034C lfd      CB430008   1     LFL       fp26=<s165:d8:l8>(gr3,8)
   69| 000350 lfd      CAE30000   1     LFL       fp23=<s165:d0:l8>(gr3,0)
    0| 000354 bc       408100C0   1     BF        CL.668,cr0,0x2/gt,taken=20%(20,80)
    0| 000358 add      7EE02214   1     A         gr23=gr0,gr4
    0| 00035C ld       E8010298   1     L8        gr0=#SPILL0(gr1,664)
    0| 000360 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
   69| 000364 add      7F0E8214   1     A         gr24=gr14,gr16
    0| 000368 cmpdi    2D200000   1     C8        cr2=gr0,0
    0| 00036C lfs      C323005C   1     LFS       fp25=+CONSTANT_AREA(gr3,92)
   67|                              CL.232:
   68| 000370 addi     3AC00000   1     LI        gr22=0
    0| 000374 bc       40890090   1     BF        CL.247,cr2,0x2/gt,taken=50%(0,0)
    0| 000378 fsub     FC3AB828   1     SFL       fp1=fp26,fp23,fcr
    0| 00037C ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
    0| 000380 or       7EF5BB78   1     LR        gr21=gr23
    0| 000384 fabs     FC600A10   1     ABSFL     fp3=fp1
    0| 000388 lfs      C003003C   2     LFS       fp0=+CONSTANT_AREA(gr3,60)
    0| 00038C qvfre    10201830   1     QVFRE     fp1=fp3
    0| 000390 fmsub    FC430078   1     FMS       fp2=fp0,fp3,fp1,fcr
    0| 000394 fnmsub   FC2108BC   2     FNMS      fp1=fp1,fp1,fp2,fcr
    0| 000398 fmsub    FC030078   2     FMS       fp0=fp0,fp3,fp1,fcr
    0| 00039C fnmsub   FC01083C   2     FNMS      fp0=fp1,fp1,fp0,fcr
    0| 0003A0 fmul     FC390032   2     MFL       fp1=fp25,fp0,fcr
    0| 0003A4 fmsub    FC43C878   2     FMS       fp2=fp25,fp3,fp1,fcr
    0| 0003A8 fnmsub   FF0008BC   2     FNMS      fp24=fp1,fp0,fp2,fcr
   68|                              CL.244:
   69| 0003AC std      FBE102D0   1     ST8       #SPILL7(gr1,720)=gr31
    0| 0003B0 addi     3AD60001   1     AI        gr22=gr22,1
   69| 0003B4 or       7EB4AB78   1     LR        gr20=gr21
   69| 0003B8 addi     3A600000   1     LI        gr19=0
   69| 0003BC or       7F12C378   1     LR        gr18=gr24
   69| 0003C0 ld       EBE10298   1     L8        gr31=#SPILL0(gr1,664)
    0| 0003C4 ori      60210000   1     XNOP      
    0| 0003C8 ori      60210000   1     XNOP      
   69|                              CL.245:
   69| 0003CC lfdu     CC120008   1     LFDU      fp0,gr18=x1b(gr18,8)
   69| 0003D0 addi     3A730001   1     AI        gr19=gr19,1
   69| 0003D4 fmul     FC380032   1     MFL       fp1=fp24,fp0,fcr
   69| 0003D8 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   69| 0003DC ori      60000000   1
   69| 0003E0 cmpld    7CB3F840   1     CL8       cr1=gr19,gr31
   69| 0003E4 fmul     FC1F0072   1     MFL       fp0=fp31,fp1,fcr
   69| 0003E8 stfdu    DC140008   2     STFDU     gr20,pert(gr20,8)=fp0
   69| 0003EC bc       4184FFE0   1     BT        CL.245,cr1,0x8/llt,taken=80%(80,20)
    0| 0003F0 ld       E80102B0   1     L8        gr0=#SPILL3(gr1,688)
   70| 0003F4 cmpd     7CB6E800   1     C8        cr1=gr22,gr29
   69| 0003F8 ld       EBE102D0   1     L8        gr31=#SPILL7(gr1,720)
    0| 0003FC add      7EA0AA14   1     A         gr21=gr0,gr21
   70| 000400 bc       4184FFAC   1     BT        CL.244,cr1,0x1/lt,taken=80%(80,20)
   70|                              CL.247:
   71| 000404 addi     3B390001   1     AI        gr25=gr25,1
    0| 000408 add      7EF7D214   1     A         gr23=gr23,gr26
   71| 00040C cmpld    7CB9E040   1     CL8       cr1=gr25,gr28
   71| 000410 bc       4184FF60   1     BT        CL.232,cr1,0x8/llt,taken=80%(80,20)
    0|                              CL.668:
   67| 000414 bc       408D0464   1     BF        CL.131,cr3,0x2/gt,taken=50%(0,0)
    0| 000418 rldicr   7A201F24   1     SLL8      gr0=gr17,3
    0| 00041C ld       E96102B0   1     L8        gr11=#SPILL3(gr1,688)
    0| 000420 mulld    7C00F9D2   1     M         gr0=gr0,gr31
    0| 000424 rldicr   7A231764   1     SLL8      gr3=gr17,2
    0| 000428 ld       E98102A8   1     L8        gr12=#SPILL2(gr1,680)
    0| 00042C subf     7C711850   1     S         gr3=gr3,gr17
    0| 000430 mulld    7CABE9D2   1     M         gr5=gr11,gr29
   71| 000434 ld       EB8102C8   1     L8        gr28=#SPILL6(gr1,712)
    0| 000438 rldicr   7A2426E4   1     SLL8      gr4=gr17,4
    0| 00043C rldicr   78631F24   1     SLL8      gr3=gr3,3
    0| 000440 mulld    7CC0D9D2   1     M         gr6=gr0,gr27
    0| 000444 addi     38ECFFF8   1     AI        gr7=gr12,-8
   71| 000448 addi     391CFFFF   1     AI        gr8=gr28,-1
    0| 00044C rldicr   7BFB2EA4   1     SLL8      gr27=gr31,5
    0| 000450 add      7D253A14   1     A         gr9=gr5,gr7
    0| 000454 std      FB6102C8   1     ST8       #SPILL6(gr1,712)=gr27
    0| 000458 rldicr   7A272EA4   1     SLL8      gr7=gr17,5
    0| 00045C mulld    7CA4F9D2   1     M         gr5=gr4,gr31
    0| 000460 mulld    7C83F9D2   1     M         gr4=gr3,gr31
   71| 000464 sradi    7D081674   1     SRA8CA    gr8,ca=gr8,2
   68| 000468 cmpd     7CBEE800   1     C8        cr1=gr30,gr29
    0| 00046C add      7FC64A14   1     A         gr30=gr6,gr9
    0| 000470 subf     7D4BD850   1     S         gr10=gr27,gr11
    0| 000474 std      FBC102D0   1     ST8       #SPILL7(gr1,720)=gr30
    0| 000478 rldicr   7BE926E4   1     SLL8      gr9=gr31,4
   71| 00047C addze    7C680194   1     ADDE      gr3,ca=gr8,0,ca
    0| 000480 add      7FABF214   1     A         gr29=gr11,gr30
    0| 000484 add      7F4AF214   1     A         gr26=gr10,gr30
    0| 000488 std      FBA102D8   1     ST8       #SPILL8(gr1,728)=gr29
    0| 00048C std      FB4102E0   1     ST8       #SPILL9(gr1,736)=gr26
    0| 000490 add      7F29F214   1     A         gr25=gr9,gr30
   67| 000494 addi     3B000000   1     LI        gr24=0
    0| 000498 std      FB2102E8   1     ST8       #SPILL10(gr1,744)=gr25
   67| 00049C std      FB0102F0   1     ST8       #SPILL11(gr1,752)=gr24
    0| 0004A0 addi     38CFFFFF   1     AI        gr6=gr15,-1
   68| 0004A4 crand    4C312A02   1     CR_N      cr0=cr[41],0x2/gt,0x2/gt,0x2/gt,cr0
    0| 0004A8 bc       408103D0   1     BF        CL.131,cr0,0x2/gt,taken=20%(20,80)
    0| 0004AC mulld    7D27F9D2   1     M         gr9=gr7,gr31
    0| 0004B0 add      7CE5F214   1     A         gr7=gr5,gr30
    0| 0004B4 std      F9210300   1     ST8       #SPILL13(gr1,768)=gr9
    0| 0004B8 std      F8E10308   1     ST8       #SPILL14(gr1,776)=gr7
    0| 0004BC add      7D40F214   1     A         gr10=gr0,gr30
    0| 0004C0 add      7FE4F214   1     A         gr31=gr4,gr30
    0| 0004C4 std      F9410310   1     ST8       #SPILL15(gr1,784)=gr10
    0| 0004C8 std      FBE10318   1     ST8       #SPILL16(gr1,792)=gr31
    0| 0004CC add      7F80D214   1     A         gr28=gr0,gr26
    0| 0004D0 add      7EE5D214   1     A         gr23=gr5,gr26
    0| 0004D4 std      FB810320   1     ST8       #SPILL17(gr1,800)=gr28
    0| 0004D8 std      FAE10328   1     ST8       #SPILL18(gr1,808)=gr23
    0| 0004DC add      7EC4D214   1     A         gr22=gr4,gr26
    0| 0004E0 add      7EA0CA14   1     A         gr21=gr0,gr25
    0| 0004E4 std      FAC10330   1     ST8       #SPILL19(gr1,816)=gr22
    0| 0004E8 std      FAA10338   1     ST8       #SPILL20(gr1,824)=gr21
    0| 0004EC add      7E80EA14   1     A         gr20=gr0,gr29
    0| 0004F0 add      7C05CA14   1     A         gr0=gr5,gr25
    0| 0004F4 std      FA810340   1     ST8       #SPILL21(gr1,832)=gr20
    0| 0004F8 std      F8010348   1     ST8       #SPILL22(gr1,840)=gr0
    0| 0004FC add      7E64CA14   1     A         gr19=gr4,gr25
    0| 000500 add      7E45EA14   1     A         gr18=gr5,gr29
    0| 000504 std      FA610350   1     ST8       #SPILL23(gr1,848)=gr19
    0| 000508 std      FA410358   1     ST8       #SPILL24(gr1,856)=gr18
    0| 00050C add      7CA4EA14   1     A         gr5=gr4,gr29
    0| 000510 addi     38830001   1     AI        gr4=gr3,1
    0| 000514 std      F8A10360   1     ST8       #SPILL25(gr1,864)=gr5
    0| 000518 ld       E8610298   1     L8        gr3=#SPILL0(gr1,664)
    0| 00051C ld       EA220000   1     L8        gr17=.+CONSTANT_AREA(gr2,0)
    0| 000520 std      F8810368   1     ST8       #SPILL26(gr1,872)=gr4
    0| 000524 sradi    7CC61674   1     SRA8CA    gr6,ca=gr6,2
   69| 000528 add      7D0E8214   1     A         gr8=gr14,gr16
    0| 00052C addze    7E060194   1     ADDE      gr16,ca=gr6,0,ca
   69| 000530 std      F90102F8   1     ST8       #SPILL12(gr1,760)=gr8
    0| 000534 std      FA010370   1     ST8       #SPILL27(gr1,880)=gr16
    0| 000538 cmpdi    2E230000   1     C8        cr4=gr3,0
    0| 00053C lfs      C311005C   1     LFS       fp24=+CONSTANT_AREA(gr17,92)
   67|                              CL.132:
   68| 000540 addi     38000000   1     LI        gr0=0
   68| 000544 std      F8010378   1     ST8       #SPILL28(gr1,888)=gr0
    0| 000548 bc       40910254   1     BF        CL.133,cr4,0x2/gt,taken=20%(20,80)
    0| 00054C fsub     FC3AB828   1     SFL       fp1=fp26,fp23,fcr
    0| 000550 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
    0| 000554 ld       EB010370   1     L8        gr24=#SPILL27(gr1,880)
    0| 000558 ld       E8810358   1     L8        gr4=#SPILL24(gr1,856)
    0| 00055C ld       E8A10318   1     L8        gr5=#SPILL16(gr1,792)
    0| 000560 ld       E8C10340   1     L8        gr6=#SPILL21(gr1,832)
    0| 000564 fabs     FC600A10   1     ABSFL     fp3=fp1
    0| 000568 lfs      C003003C   2     LFS       fp0=+CONSTANT_AREA(gr3,60)
    0| 00056C qvfre    10201830   1     QVFRE     fp1=fp3
    0| 000570 std      F8810380   1     ST8       #SPILL29(gr1,896)=gr4
    0| 000574 std      F8A10388   1     ST8       #SPILL30(gr1,904)=gr5
    0| 000578 std      F8C10390   1     ST8       #SPILL31(gr1,912)=gr6
    0| 00057C ld       E8E10360   1     L8        gr7=#SPILL25(gr1,864)
    0| 000580 ld       E90102D8   1     L8        gr8=#SPILL8(gr1,728)
    0| 000584 fmsub    FC430078   1     FMS       fp2=fp0,fp3,fp1,fcr
    0| 000588 ld       E9210328   1     L8        gr9=#SPILL18(gr1,808)
    0| 00058C ld       E9410348   1     L8        gr10=#SPILL22(gr1,840)
    0| 000590 ld       E9610310   1     L8        gr11=#SPILL15(gr1,784)
    0| 000594 std      F8E10398   1     ST8       #SPILL32(gr1,920)=gr7
    0| 000598 std      F90103A0   1     ST8       #SPILL33(gr1,928)=gr8
    0| 00059C fnmsub   FC2108BC   1     FNMS      fp1=fp1,fp1,fp2,fcr
    0| 0005A0 std      F92103A8   1     ST8       #SPILL34(gr1,936)=gr9
    0| 0005A4 std      F94103B0   1     ST8       #SPILL35(gr1,944)=gr10
    0| 0005A8 std      F96103B8   1     ST8       #SPILL36(gr1,952)=gr11
    0| 0005AC ld       E98102D0   1     L8        gr12=#SPILL7(gr1,720)
    0| 0005B0 ld       EBE10308   1     L8        gr31=#SPILL14(gr1,776)
    0| 0005B4 fmsub    FC030078   1     FMS       fp0=fp0,fp3,fp1,fcr
    0| 0005B8 ld       EBC10338   1     L8        gr30=#SPILL20(gr1,824)
    0| 0005BC ld       EBA10320   1     L8        gr29=#SPILL17(gr1,800)
    0| 0005C0 ld       EB810350   1     L8        gr28=#SPILL23(gr1,848)
    0| 0005C4 std      F98103C0   1     ST8       #SPILL37(gr1,960)=gr12
    0| 0005C8 std      FBE103C8   1     ST8       #SPILL38(gr1,968)=gr31
    0| 0005CC fnmsub   FC01083C   1     FNMS      fp0=fp1,fp1,fp0,fcr
    0| 0005D0 std      FBC103D0   1     ST8       #SPILL39(gr1,976)=gr30
    0| 0005D4 std      FBA103D8   1     ST8       #SPILL40(gr1,984)=gr29
    0| 0005D8 std      FB8103E0   1     ST8       #SPILL41(gr1,992)=gr28
    0| 0005DC ld       EB610330   1     L8        gr27=#SPILL19(gr1,816)
    0| 0005E0 ld       EB4102E8   1     L8        gr26=#SPILL10(gr1,744)
    0| 0005E4 fmul     FC380032   1     MFL       fp1=fp24,fp0,fcr
    0| 0005E8 ld       EB2102E0   1     L8        gr25=#SPILL9(gr1,736)
    0| 0005EC addi     3AF80001   1     AI        gr23=gr24,1
    0| 0005F0 std      FB6103E8   1     ST8       #SPILL42(gr1,1000)=gr27
    0| 0005F4 std      FB4103F0   1     ST8       #SPILL43(gr1,1008)=gr26
    0| 0005F8 fmsub    FC43C078   1     FMS       fp2=fp24,fp3,fp1,fcr
    0| 0005FC std      FB2103F8   1     ST8       #SPILL44(gr1,1016)=gr25
    0| 000600 std      FAE10400   1     ST8       #SPILL45(gr1,1024)=gr23
    0| 000604 fnmsub   FF2008BC   1     FNMS      fp25=fp1,fp0,fp2,fcr
   68|                              CL.134:
   69| 000608 ld       EBE10398   1     L8        gr31=#SPILL32(gr1,920)
   69| 00060C ld       EBC103E0   1     L8        gr30=#SPILL41(gr1,992)
   69| 000610 ld       EBA103B8   1     L8        gr29=#SPILL36(gr1,952)
   69| 000614 ld       EB8103C8   1     L8        gr28=#SPILL38(gr1,968)
   69| 000618 ld       EB6103C0   1     L8        gr27=#SPILL37(gr1,960)
   69| 00061C ld       EB410388   1     L8        gr26=#SPILL30(gr1,904)
   69| 000620 ld       EB2103E8   1     L8        gr25=#SPILL42(gr1,1000)
   69| 000624 ld       EB0103F8   1     L8        gr24=#SPILL44(gr1,1016)
   69| 000628 ld       EAE103D8   1     L8        gr23=#SPILL40(gr1,984)
   69| 00062C ld       EAC103A8   1     L8        gr22=#SPILL34(gr1,936)
   69| 000630 ld       EAA103F0   1     L8        gr21=#SPILL43(gr1,1008)
   69| 000634 ld       EA8103A0   1     L8        gr20=#SPILL33(gr1,928)
   69| 000638 ld       EA6103D0   1     L8        gr19=#SPILL39(gr1,976)
   69| 00063C ld       EA4103B0   1     L8        gr18=#SPILL35(gr1,944)
   69| 000640 ld       EA210390   1     L8        gr17=#SPILL31(gr1,912)
   69| 000644 ld       EA010380   1     L8        gr16=#SPILL29(gr1,896)
   69| 000648 ld       E9E102F8   1     L8        gr15=#SPILL12(gr1,760)
   69| 00064C addi     39C00001   1     LI        gr14=1
    0| 000650 ori      60210000   1     XNOP      
    0| 000654 ori      60210000   1     XNOP      
    0| 000658 ori      60210000   1     XNOP      
   69|                              CL.136:
   69| 00065C lfdu     CC0F0008   1     LFDU      fp0,gr15=x1b(gr15,8)
   69| 000660 fmul     FC390032   1     MFL       fp1=fp25,fp0,fcr
   69| 000664 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   69| 000668 ori      60000000   1
   69| 00066C ld       E8010298   1     L8        gr0=#SPILL0(gr1,664)
   69| 000670 fmul     FC1F0072   1     MFL       fp0=fp31,fp1,fcr
   69| 000674 cmpld    7C2E0040   1     CL8       cr0=gr14,gr0
   69| 000678 addi     39CE0001   1     AI        gr14=gr14,1
   69| 00067C stfdu    DC1B0008   1     STFDU     gr27,pert(gr27,8)=fp0
   69| 000680 stfdu    DC140008   1     STFDU     gr20,pert(gr20,8)=fp0
   69| 000684 stfdu    DC150008   1     STFDU     gr21,pert(gr21,8)=fp0
   69| 000688 stfdu    DC180008   1     STFDU     gr24,pert(gr24,8)=fp0
   69| 00068C stfdu    DC1D0008   1     STFDU     gr29,pert(gr29,8)=fp0
   69| 000690 stfdu    DC110008   1     STFDU     gr17,pert(gr17,8)=fp0
   69| 000694 stfdu    DC130008   1     STFDU     gr19,pert(gr19,8)=fp0
   69| 000698 stfdu    DC170008   1     STFDU     gr23,pert(gr23,8)=fp0
   69| 00069C stfdu    DC1C0008   1     STFDU     gr28,pert(gr28,8)=fp0
   69| 0006A0 stfdu    DC100008   1     STFDU     gr16,pert(gr16,8)=fp0
   69| 0006A4 stfdu    DC120008   1     STFDU     gr18,pert(gr18,8)=fp0
   69| 0006A8 stfdu    DC160008   1     STFDU     gr22,pert(gr22,8)=fp0
   69| 0006AC stfdu    DC1A0008   1     STFDU     gr26,pert(gr26,8)=fp0
   69| 0006B0 stfdu    DC1F0008   1     STFDU     gr31,pert(gr31,8)=fp0
   69| 0006B4 stfdu    DC1E0008   1     STFDU     gr30,pert(gr30,8)=fp0
   69| 0006B8 stfdu    DC190008   1     STFDU     gr25,pert(gr25,8)=fp0
   69| 0006BC bc       4180FFA0   1     BT        CL.136,cr0,0x8/llt,taken=80%(80,20)
   70| 0006C0 ld       E8610378   1     L8        gr3=#SPILL28(gr1,888)
    0| 0006C4 ld       E80102C8   1     L8        gr0=#SPILL6(gr1,712)
    0| 0006C8 ld       E8810380   1     L8        gr4=#SPILL29(gr1,896)
   70| 0006CC ld       E8A10400   1     L8        gr5=#SPILL45(gr1,1024)
    0| 0006D0 ld       E8C10388   1     L8        gr6=#SPILL30(gr1,904)
    0| 0006D4 ld       E8E10390   1     L8        gr7=#SPILL31(gr1,912)
    0| 0006D8 ld       E9010398   1     L8        gr8=#SPILL32(gr1,920)
    0| 0006DC ld       E92103A0   1     L8        gr9=#SPILL33(gr1,928)
    0| 0006E0 ld       E94103A8   1     L8        gr10=#SPILL34(gr1,936)
    0| 0006E4 ld       E96103B0   1     L8        gr11=#SPILL35(gr1,944)
    0| 0006E8 ld       E98103B8   1     L8        gr12=#SPILL36(gr1,952)
    0| 0006EC ld       EBE103C0   1     L8        gr31=#SPILL37(gr1,960)
    0| 0006F0 ld       EBC103C8   1     L8        gr30=#SPILL38(gr1,968)
    0| 0006F4 ld       EBA103D0   1     L8        gr29=#SPILL39(gr1,976)
    0| 0006F8 ld       EB8103D8   1     L8        gr28=#SPILL40(gr1,984)
    0| 0006FC ld       EB6103E0   1     L8        gr27=#SPILL41(gr1,992)
    0| 000700 ld       EB4103E8   1     L8        gr26=#SPILL42(gr1,1000)
    0| 000704 ld       EB2103F0   1     L8        gr25=#SPILL43(gr1,1008)
    0| 000708 ld       EB0103F8   1     L8        gr24=#SPILL44(gr1,1016)
   70| 00070C addi     38630001   1     AI        gr3=gr3,1
    0| 000710 add      7C802214   1     A         gr4=gr0,gr4
   70| 000714 std      F8610378   1     ST8       #SPILL28(gr1,888)=gr3
    0| 000718 std      F8810380   1     ST8       #SPILL29(gr1,896)=gr4
   70| 00071C cmpld    7C232840   1     CL8       cr0=gr3,gr5
    0| 000720 add      7CC03214   1     A         gr6=gr0,gr6
    0| 000724 add      7CE03A14   1     A         gr7=gr0,gr7
    0| 000728 std      F8C10388   1     ST8       #SPILL30(gr1,904)=gr6
    0| 00072C std      F8E10390   1     ST8       #SPILL31(gr1,912)=gr7
    0| 000730 add      7D004214   1     A         gr8=gr0,gr8
    0| 000734 add      7D204A14   1     A         gr9=gr0,gr9
    0| 000738 std      F9010398   1     ST8       #SPILL32(gr1,920)=gr8
    0| 00073C std      F92103A0   1     ST8       #SPILL33(gr1,928)=gr9
    0| 000740 add      7D405214   1     A         gr10=gr0,gr10
    0| 000744 add      7D605A14   1     A         gr11=gr0,gr11
    0| 000748 std      F94103A8   1     ST8       #SPILL34(gr1,936)=gr10
    0| 00074C std      F96103B0   1     ST8       #SPILL35(gr1,944)=gr11
    0| 000750 add      7D806214   1     A         gr12=gr0,gr12
    0| 000754 add      7FE0FA14   1     A         gr31=gr0,gr31
    0| 000758 std      F98103B8   1     ST8       #SPILL36(gr1,952)=gr12
    0| 00075C std      FBE103C0   1     ST8       #SPILL37(gr1,960)=gr31
    0| 000760 add      7FC0F214   1     A         gr30=gr0,gr30
    0| 000764 add      7FA0EA14   1     A         gr29=gr0,gr29
    0| 000768 std      FBC103C8   1     ST8       #SPILL38(gr1,968)=gr30
    0| 00076C std      FBA103D0   1     ST8       #SPILL39(gr1,976)=gr29
    0| 000770 add      7F80E214   1     A         gr28=gr0,gr28
    0| 000774 add      7F60DA14   1     A         gr27=gr0,gr27
    0| 000778 std      FB8103D8   1     ST8       #SPILL40(gr1,984)=gr28
    0| 00077C std      FB6103E0   1     ST8       #SPILL41(gr1,992)=gr27
    0| 000780 add      7F40D214   1     A         gr26=gr0,gr26
    0| 000784 add      7F20CA14   1     A         gr25=gr0,gr25
    0| 000788 std      FB4103E8   1     ST8       #SPILL42(gr1,1000)=gr26
    0| 00078C std      FB2103F0   1     ST8       #SPILL43(gr1,1008)=gr25
    0| 000790 add      7F00C214   1     A         gr24=gr0,gr24
    0| 000794 std      FB0103F8   1     ST8       #SPILL44(gr1,1016)=gr24
   70| 000798 bc       4180FE70   1     BT        CL.134,cr0,0x8/llt,taken=80%(80,20)
   70|                              CL.133:
   71| 00079C ld       E86102F0   1     L8        gr3=#SPILL11(gr1,752)
    0| 0007A0 ld       E8010300   1     L8        gr0=#SPILL13(gr1,768)
    0| 0007A4 ld       E88102D8   1     L8        gr4=#SPILL8(gr1,728)
   71| 0007A8 ld       E8A10368   1     L8        gr5=#SPILL26(gr1,872)
    0| 0007AC ld       E8C10308   1     L8        gr6=#SPILL14(gr1,776)
    0| 0007B0 ld       E8E10310   1     L8        gr7=#SPILL15(gr1,784)
    0| 0007B4 ld       E90102E0   1     L8        gr8=#SPILL9(gr1,736)
    0| 0007B8 ld       E9210318   1     L8        gr9=#SPILL16(gr1,792)
    0| 0007BC ld       E94102E8   1     L8        gr10=#SPILL10(gr1,744)
    0| 0007C0 ld       E96102D0   1     L8        gr11=#SPILL7(gr1,720)
    0| 0007C4 ld       E9810320   1     L8        gr12=#SPILL17(gr1,800)
    0| 0007C8 ld       EBE10328   1     L8        gr31=#SPILL18(gr1,808)
    0| 0007CC ld       EBC10330   1     L8        gr30=#SPILL19(gr1,816)
    0| 0007D0 ld       EBA10338   1     L8        gr29=#SPILL20(gr1,824)
    0| 0007D4 ld       EB810340   1     L8        gr28=#SPILL21(gr1,832)
    0| 0007D8 ld       EB610348   1     L8        gr27=#SPILL22(gr1,840)
    0| 0007DC ld       EB410350   1     L8        gr26=#SPILL23(gr1,848)
    0| 0007E0 ld       EB210358   1     L8        gr25=#SPILL24(gr1,856)
    0| 0007E4 ld       EB010360   1     L8        gr24=#SPILL25(gr1,864)
   71| 0007E8 addi     38630001   1     AI        gr3=gr3,1
    0| 0007EC add      7C802214   1     A         gr4=gr0,gr4
   71| 0007F0 std      F86102F0   1     ST8       #SPILL11(gr1,752)=gr3
    0| 0007F4 std      F88102D8   1     ST8       #SPILL8(gr1,728)=gr4
   71| 0007F8 cmpld    7C232840   1     CL8       cr0=gr3,gr5
    0| 0007FC add      7CC03214   1     A         gr6=gr0,gr6
    0| 000800 add      7CE03A14   1     A         gr7=gr0,gr7
    0| 000804 std      F8C10308   1     ST8       #SPILL14(gr1,776)=gr6
    0| 000808 std      F8E10310   1     ST8       #SPILL15(gr1,784)=gr7
    0| 00080C add      7D004214   1     A         gr8=gr0,gr8
    0| 000810 add      7D204A14   1     A         gr9=gr0,gr9
    0| 000814 std      F90102E0   1     ST8       #SPILL9(gr1,736)=gr8
    0| 000818 std      F9210318   1     ST8       #SPILL16(gr1,792)=gr9
    0| 00081C add      7D405214   1     A         gr10=gr0,gr10
    0| 000820 add      7D605A14   1     A         gr11=gr0,gr11
    0| 000824 std      F94102E8   1     ST8       #SPILL10(gr1,744)=gr10
    0| 000828 std      F96102D0   1     ST8       #SPILL7(gr1,720)=gr11
    0| 00082C add      7D806214   1     A         gr12=gr0,gr12
    0| 000830 add      7FE0FA14   1     A         gr31=gr0,gr31
    0| 000834 std      F9810320   1     ST8       #SPILL17(gr1,800)=gr12
    0| 000838 std      FBE10328   1     ST8       #SPILL18(gr1,808)=gr31
    0| 00083C add      7FC0F214   1     A         gr30=gr0,gr30
    0| 000840 add      7FA0EA14   1     A         gr29=gr0,gr29
    0| 000844 std      FBC10330   1     ST8       #SPILL19(gr1,816)=gr30
    0| 000848 std      FBA10338   1     ST8       #SPILL20(gr1,824)=gr29
    0| 00084C add      7F80E214   1     A         gr28=gr0,gr28
    0| 000850 add      7F60DA14   1     A         gr27=gr0,gr27
    0| 000854 std      FB810340   1     ST8       #SPILL21(gr1,832)=gr28
    0| 000858 std      FB610348   1     ST8       #SPILL22(gr1,840)=gr27
    0| 00085C add      7F40D214   1     A         gr26=gr0,gr26
    0| 000860 add      7F20CA14   1     A         gr25=gr0,gr25
    0| 000864 std      FB410350   1     ST8       #SPILL23(gr1,848)=gr26
    0| 000868 std      FB210358   1     ST8       #SPILL24(gr1,856)=gr25
    0| 00086C add      7F00C214   1     A         gr24=gr0,gr24
    0| 000870 std      FB010360   1     ST8       #SPILL25(gr1,864)=gr24
   71| 000874 bc       4180FCCC   1     BT        CL.132,cr0,0x8/llt,taken=80%(80,20)
   71|                              CL.131:
   84| 000878 ld       E9220000   1     L8        gr9=.&&N&field(gr2,0)
   84| 00087C ld       E9690028   1     L8        gr11=<s35:d40:l8>(gr9,40)
   84| 000880 ld       E8090040   1     L8        gr0=<s35:d64:l8>(gr9,64)
   84| 000884 ld       E9490058   1     L8        gr10=<s35:d88:l8>(gr9,88)
   84| 000888 ld       E8A90020   1     L8        gr5=<s35:d32:l8>(gr9,32)
   84| 00088C ld       E8C90038   1     L8        gr6=<s35:d56:l8>(gr9,56)
   84| 000890 ld       E8E90050   1     L8        gr7=<s35:d80:l8>(gr9,80)
   84| 000894 sradi    7D631674   1     SRA8CA    gr3,ca=gr11,2
   84| 000898 cmpdi    2F2B0000   1     C8        cr6=gr11,0
   84| 00089C addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
   84| 0008A0 rldicr   78641764   1     SLL8      gr4=gr3,2
   84| 0008A4 subf     7D045851   1     S_R       gr8,cr0=gr11,gr4
   84| 0008A8 crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
   84| 0008AC bc       40810094   1     BF        CL.231,cr0,0x2/gt,taken=50%(0,0)
   84| 0008B0 ld       E9890030   1     L8        gr12=<s35:d48:l8>(gr9,48)
   84| 0008B4 ld       EB490048   1     L8        gr26=<s35:d72:l8>(gr9,72)
   84| 0008B8 ld       E8690060   1     L8        gr3=<s35:d96:l8>(gr9,96)
   84| 0008BC ld       EBE90000   1     L8        gr31=<s35:d0:l8>(gr9,0)
   84| 0008C0 ld       EBC90018   1     L8        gr30=<s35:d24:l8>(gr9,24)
    0| 0008C4 cmpdi    2C200000   1     C8        cr0=gr0,0
   84| 0008C8 addi     39200000   1     LI        gr9=0
    0| 0008CC bc       408110C8   1     BF        CL.731,cr0,0x2/gt,taken=40%(40,60)
    0| 0008D0 mulld    7FA561D2   1     M         gr29=gr5,gr12
    0| 0008D4 mulld    7F8339D2   1     M         gr28=gr3,gr7
    0| 0008D8 mulld    7F66D1D2   1     M         gr27=gr6,gr26
    0| 0008DC add      7FFFF214   1     A         gr31=gr31,gr30
    0| 0008E0 ld       EB220000   1     L8        gr25=.+CONSTANT_AREA(gr2,0)
    0| 0008E4 add      7FFDFA14   1     A         gr31=gr29,gr31
    0| 0008E8 add      7FDBE214   1     A         gr30=gr27,gr28
    0| 0008EC subf     7FE3F850   1     S         gr31=gr31,gr3
    0| 0008F0 cmpdi    2C2A0000   1     C8        cr0=gr10,0
    0| 0008F4 add      7FBEFA14   1     A         gr29=gr30,gr31
    0| 0008F8 lfd      C8190060   1     LFL       fp0=+CONSTANT_AREA(gr25,96)
   84|                              CL.226:
   84| 0008FC addi     3BE00000   1     LI        gr31=0
    0| 000900 bc       40810030   1     BF        CL.230,cr0,0x2/gt,taken=20%(20,80)
    0| 000904 or       7FBCEB78   1     LR        gr28=gr29
   84|                              CL.227:
   84| 000908 or       7F9EE378   1     LR        gr30=gr28
    0| 00090C mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 000910 ori      60210000   1     XNOP      
    0| 000914 ori      60210000   1     XNOP      
   84|                              CL.228:
   84| 000918 stfdux   7C1E1DEE   1     STFDU     gr30,d(gr30,gr3,0)=fp0
   84| 00091C bc       4200FFFC   1     BCT       ctr=CL.228,taken=100%(100,0)
   84| 000920 addi     3BFF0001   1     AI        gr31=gr31,1
    0| 000924 add      7F9AE214   1     A         gr28=gr26,gr28
   84| 000928 cmpld    7CBF0040   1     CL8       cr1=gr31,gr0
   84| 00092C bc       4184FFDC   1     BT        CL.227,cr1,0x8/llt,taken=80%(80,20)
   84|                              CL.230:
   84| 000930 addi     39290001   1     AI        gr9=gr9,1
    0| 000934 add      7FACEA14   1     A         gr29=gr12,gr29
   84| 000938 cmpd     7CA84800   1     C8        cr1=gr8,gr9
   84| 00093C bc       4185FFC0   1     BT        CL.226,cr1,0x2/gt,taken=80%(80,20)
   84|                              CL.231:
   84| 000940 cmpd     7CAB4000   1     C8        cr1=gr11,gr8
   84| 000944 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
   84| 000948 bc       40810100   1     BF        CL.137,cr0,0x2/gt,taken=50%(0,0)
   84| 00094C ld       EBC20000   1     L8        gr30=.&&N&field(gr2,0)
   84| 000950 addi     3884FFFF   1     AI        gr4=gr4,-1
    0| 000954 cmpdi    2C200000   1     C8        cr0=gr0,0
   84| 000958 sradi    7C841674   1     SRA8CA    gr4,ca=gr4,2
   84| 00095C ld       E93E0030   1     L8        gr9=<s35:d48:l8>(gr30,48)
   84| 000960 ld       E97E0048   1     L8        gr11=<s35:d72:l8>(gr30,72)
   84| 000964 ld       E87E0060   1     L8        gr3=<s35:d96:l8>(gr30,96)
   84| 000968 ld       E99E0000   1     L8        gr12=<s35:d0:l8>(gr30,0)
   84| 00096C ld       EBFE0018   1     L8        gr31=<s35:d24:l8>(gr30,24)
    0| 000970 mulld    7CA549D2   1     M         gr5=gr5,gr9
    0| 000974 mulld    7CE719D2   1     M         gr7=gr7,gr3
    0| 000978 mulld    7CC659D2   1     M         gr6=gr6,gr11
    0| 00097C add      7D8CFA14   1     A         gr12=gr12,gr31
    0| 000980 mulld    7D0849D2   1     M         gr8=gr8,gr9
    0| 000984 add      7CA56214   1     A         gr5=gr5,gr12
    0| 000988 add      7CC63A14   1     A         gr6=gr6,gr7
    0| 00098C subf     7CA32850   1     S         gr5=gr5,gr3
    0| 000990 rldicr   792C1764   1     SLL8      gr12=gr9,2
    0| 000994 add      7CC62A14   1     A         gr6=gr6,gr5
   84| 000998 addze    7CA40194   1     ADDE      gr5,ca=gr4,0,ca
    0| 00099C add      7FE64214   1     A         gr31=gr6,gr8
    0| 0009A0 subf     7CE96050   1     S         gr7=gr12,gr9
    0| 0009A4 rldicr   79260FA4   1     SLL8      gr6=gr9,1
   84| 0009A8 addi     38800000   1     LI        gr4=0
    0| 0009AC bc       4081009C   1     BF        CL.137,cr0,0x2/gt,taken=20%(20,80)
    0| 0009B0 addi     3B650001   1     AI        gr27=gr5,1
    0| 0009B4 ld       E8A20000   1     L8        gr5=.+CONSTANT_AREA(gr2,0)
    0| 0009B8 add      7FC9FA14   1     A         gr30=gr9,gr31
    0| 0009BC add      7FA7FA14   1     A         gr29=gr7,gr31
    0| 0009C0 add      7F86FA14   1     A         gr28=gr6,gr31
    0| 0009C4 cmpdi    2C2A0000   1     C8        cr0=gr10,0
    0| 0009C8 lfd      C8050060   1     LFL       fp0=+CONSTANT_AREA(gr5,96)
   84|                              CL.138:
   84| 0009CC addi     38A00000   1     LI        gr5=0
    0| 0009D0 bc       4081005C   1     BF        CL.139,cr0,0x2/gt,taken=20%(20,80)
    0| 0009D4 or       7FFAFB78   1     LR        gr26=gr31
    0| 0009D8 or       7FB9EB78   1     LR        gr25=gr29
    0| 0009DC or       7F98E378   1     LR        gr24=gr28
    0| 0009E0 or       7FD7F378   1     LR        gr23=gr30
   84|                              CL.140:
   84| 0009E4 or       7F26CB78   1     LR        gr6=gr25
   84| 0009E8 or       7F07C378   1     LR        gr7=gr24
   84| 0009EC or       7EE8BB78   1     LR        gr8=gr23
   84| 0009F0 or       7F49D378   1     LR        gr9=gr26
    0| 0009F4 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 0009F8 ori      60210000   1     XNOP      
    0|                              CL.1136:
   84| 0009FC stfdux   7C091DEE   1     STFDU     gr9,d(gr9,gr3,0)=fp0
   84| 000A00 stfdux   7C081DEE   1     STFDU     gr8,d(gr8,gr3,0)=fp0
   84| 000A04 stfdux   7C071DEE   1     STFDU     gr7,d(gr7,gr3,0)=fp0
   84| 000A08 stfdux   7C061DEE   1     STFDU     gr6,d(gr6,gr3,0)=fp0
    0| 000A0C bc       4200FFF0   1     BCT       ctr=CL.1136,taken=100%(100,0)
   84| 000A10 addi     38A50001   1     AI        gr5=gr5,1
    0| 000A14 add      7F4BD214   1     A         gr26=gr11,gr26
   84| 000A18 cmpld    7CA50040   1     CL8       cr1=gr5,gr0
    0| 000A1C add      7F2BCA14   1     A         gr25=gr11,gr25
    0| 000A20 add      7F0BC214   1     A         gr24=gr11,gr24
    0| 000A24 add      7EEBBA14   1     A         gr23=gr11,gr23
   84| 000A28 bc       4184FFBC   1     BT        CL.140,cr1,0x8/llt,taken=80%(80,20)
   84|                              CL.139:
   84| 000A2C addi     38840001   1     AI        gr4=gr4,1
    0| 000A30 add      7FCCF214   1     A         gr30=gr12,gr30
   84| 000A34 cmpld    7CA4D840   1     CL8       cr1=gr4,gr27
    0| 000A38 add      7FACEA14   1     A         gr29=gr12,gr29
    0| 000A3C add      7F8CE214   1     A         gr28=gr12,gr28
    0| 000A40 add      7FECFA14   1     A         gr31=gr12,gr31
   84| 000A44 bc       4184FF88   1     BT        CL.138,cr1,0x8/llt,taken=80%(80,20)
   84|                              CL.137:
   85| 000A48 ld       EBE20000   1     L8        gr31=.$STATIC(gr2,0)
   85| 000A4C addi     388100C8   1     AI        gr4=gr1,200
   85| 000A50 addi     38610090   1     AI        gr3=gr1,144
   85| 000A54 lwa      E81F0002   1     L4A       gr0=iseed(gr31,0)
   85| 000A58 neg      7C0000D0   1     COMP      gr0=gr0
   85| 000A5C stw      90010090   1     ST4Z      T_12(gr1,144)=gr0
   85| 000A60 bl       48000001   1     CALL      ran1,2,T_12",gr3,rn",gr4,ran1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   85| 000A64 ori      60000000   1
   89| 000A68 ld       EBC20000   1     L8        gr30=.&&N&&param(gr2,0)
   89| 000A6C lwa      EBBE000A   1     L4A       gr29=<s145:d8:l4>(gr30,8)
   89| 000A70 sradi    7FA00E74   1     SRA8CA    gr0,ca=gr29,1
   89| 000A74 cmpwi    2F1D0000   1     C4        cr6=gr29,0
   89| 000A78 addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
   89| 000A7C std      FBA10298   1     ST8       #SPILL0(gr1,664)=gr29
   89| 000A80 rldicr   781C0FA4   1     SLL8      gr28=gr0,1
   89| 000A84 subf     7C7CE851   1     S_R       gr3,cr0=gr29,gr28
   89| 000A88 std      FB8102C8   1     ST8       #SPILL6(gr1,712)=gr28
   89| 000A8C crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
   89| 000A90 bc       408101AC   1     BF        CL.216,cr0,0x2/gt,taken=50%(0,0)
   93| 000A94 ld       EAC20000   1     L8        gr22=.&&N&field(gr2,0)
   90| 000A98 lwa      E81E0006   1     L4A       gr0=<s145:d4:l4>(gr30,4)
   92| 000A9C ld       E8820000   1     L8        gr4=.&&N&grid(gr2,0)
   99| 000AA0 ld       E8A20000   1     L8        gr5=.&&N&&root(gr2,0)
   91| 000AA4 ld       EAA20000   1     L8        gr21=.&&N&&param(gr2,0)
   93| 000AA8 ld       EA960030   1     L8        gr20=<s35:d48:l8>(gr22,48)
   99| 000AAC ld       E9760068   1     L8        gr11=<s35:d104:l8>(gr22,104)
   99| 000AB0 ld       EBB60080   1     L8        gr29=<s35:d128:l8>(gr22,128)
   93| 000AB4 ld       E9160000   1     L8        gr8=<s35:d0:l8>(gr22,0)
   93| 000AB8 ld       EBD60018   1     L8        gr30=<s35:d24:l8>(gr22,24)
    0| 000ABC cmpwi    2C800000   1     C4        cr1=gr0,0
   93| 000AC0 std      FA8102D0   1     ST8       #SPILL7(gr1,720)=gr20
   92| 000AC4 ld       E9840700   1     L8        gr12=<s82:d1792:l8>(gr4,1792)
   92| 000AC8 ld       EBE40718   1     L8        gr31=<s82:d1816:l8>(gr4,1816)
   94| 000ACC ld       E93601A0   1     L8        gr9=<s35:d416:l8>(gr22,416)
   94| 000AD0 ld       E95601B8   1     L8        gr10=<s35:d440:l8>(gr22,440)
   93| 000AD4 ld       EB760048   1     L8        gr27=<s35:d72:l8>(gr22,72)
   99| 000AD8 ld       EB5600B0   1     L8        gr26=<s35:d176:l8>(gr22,176)
   94| 000ADC ld       EB3601E8   1     L8        gr25=<s35:d488:l8>(gr22,488)
   91| 000AE0 lwa      EB150002   1     L4A       gr24=<s145:d0:l4>(gr21,0)
   99| 000AE4 ld       E9D60098   1     L8        gr14=<s35:d152:l8>(gr22,152)
   94| 000AE8 ld       EAF601D0   1     L8        gr23=<s35:d464:l8>(gr22,464)
   89| 000AEC addi     38800000   1     LI        gr4=0
   99| 000AF0 lfd      C80500E0   1     LFL       fp0=<s178:d224:l8>(gr5,224)
   93| 000AF4 ld       E8B60060   1     L8        gr5=<s35:d96:l8>(gr22,96)
   99| 000AF8 ld       E8D600C8   1     L8        gr6=<s35:d200:l8>(gr22,200)
   94| 000AFC ld       E8F60200   1     L8        gr7=<s35:d512:l8>(gr22,512)
    0| 000B00 bc       40850E80   1     BF        CL.742,cr1,0x2/gt,taken=40%(40,60)
    0| 000B04 add      7D6BEA14   1     A         gr11=gr11,gr29
    0| 000B08 ld       EBA102B0   1     L8        gr29=#SPILL3(gr1,688)
    0| 000B0C ld       EA4102C0   1     L8        gr18=#SPILL5(gr1,704)
    0| 000B10 add      7D08F214   1     A         gr8=gr8,gr30
    0| 000B14 ld       EBC102A8   1     L8        gr30=#SPILL2(gr1,680)
   92| 000B18 add      7E2CFA14   1     A         gr17=gr12,gr31
    0| 000B1C ld       E98102B8   1     L8        gr12=#SPILL4(gr1,696)
    0| 000B20 ld       EBE102D0   1     L8        gr31=#SPILL7(gr1,720)
    0| 000B24 ld       EA020000   1     L8        gr16=.+CONSTANT_AREA(gr2,0)
   92| 000B28 std      FA2102D8   1     ST8       #SPILL8(gr1,728)=gr17
    0| 000B2C add      7D495214   1     A         gr10=gr9,gr10
    0| 000B30 subf     7D32E850   1     S         gr9=gr29,gr18
    0| 000B34 add      7D6BD214   1     A         gr11=gr11,gr26
    0| 000B38 add      7D4ACA14   1     A         gr10=gr10,gr25
    0| 000B3C add      7D29F214   1     A         gr9=gr9,gr30
    0| 000B40 add      7D08DA14   1     A         gr8=gr8,gr27
    0| 000B44 add      7ECB7214   1     A         gr22=gr11,gr14
    0| 000B48 add      7EAABA14   1     A         gr21=gr10,gr23
    0| 000B4C add      7E896214   1     A         gr20=gr9,gr12
    0| 000B50 add      7E68FA14   1     A         gr19=gr8,gr31
    0| 000B54 cmpdi    2C380000   1     C8        cr0=gr24,0
    0| 000B58 lfs      C0300068   1     LFS       fp1=+CONSTANT_AREA(gr16,104)
   89|                              CL.208:
   90| 000B5C addi     39000000   1     LI        gr8=0
    0| 000B60 bc       408100B8   1     BF        CL.215,cr0,0x2/gt,taken=50%(0,0)
    0| 000B64 fadd     FC40082A   1     AFL       fp2=fp0,fp1,fcr
   92| 000B68 ld       E92102D8   1     L8        gr9=#SPILL8(gr1,728)
    0| 000B6C or       7E729B78   1     LR        gr18=gr19
    0| 000B70 or       7E91A378   1     LR        gr17=gr20
    0| 000B74 or       7EB0AB78   1     LR        gr16=gr21
    0| 000B78 or       7ECFB378   1     LR        gr15=gr22
    0| 000B7C fdiv     FCBD1024   1     DFL       fp5=fp29,fp2,fcr
   90|                              CL.209:
   92| 000B80 lfdu     CC490008   1     LFDU      fp2,gr9=x2b(gr9,8)
   94| 000B84 or       7E2A8B78   1     LR        gr10=gr17
    0| 000B88 addi     39080001   1     AI        gr8=gr8,1
   97| 000B8C or       7E2B8B78   1     LR        gr11=gr17
   96| 000B90 or       7E4C9378   1     LR        gr12=gr18
    0| 000B94 mtspr    7F0903A6   1     LCTR      ctr=gr24
   99| 000B98 or       7DFF7B78   1     LR        gr31=gr15
   92| 000B9C fabs     FC401210   1     ABSFL     fp2=fp2
   97| 000BA0 or       7E1E8378   2     LR        gr30=gr16
    0| 000BA4 fcmpu    FC82B000   1     CFL       cr1=fp2,fp22
   93| 000BA8 or       7E5D9378   2     LR        gr29=gr18
   99| 000BAC or       7DFC7B78   1     LR        gr28=gr15
    0| 000BB0 bc       41840D8C   1     BT        CL.738,cr1,0x20/flt,taken=50%(0,0)
   94| 000BB4 lfdu     CC4A0008   1     LFDU      fp2,gr10=pert(gr10,8)
   93| 000BB8 stfdux   7F7D2DEE   1     STFDU     gr29,d(gr29,gr5,0)=fp27
   94| 000BBC or       7E0B8378   1     LR        gr11=gr16
   94| 000BC0 fsub     FC42F028   1     SFL       fp2=fp2,fp30,fcr
    0| 000BC4 bc       4240002C   1     BCF       ctr=CL.1137,taken=0%(0,100)
    0| 000BC8 ori      60210000   1     XNOP      
    0| 000BCC ori      60210000   1     XNOP      
    0| 000BD0 ori      60210000   1     XNOP      
    0|                              CL.1138:
   94| 000BD4 lfdu     CC6A0008   1     LFDU      fp3,gr10=pert(gr10,8)
   94| 000BD8 fmul     FC9B00B2   1     MFL       fp4=fp27,fp2,fcr
   99| 000BDC stfdux   7CBC35EE   1     STFDU     gr28,e(gr28,gr6,0)=fp5
   94| 000BE0 fsub     FC43F028   1     SFL       fp2=fp3,fp30,fcr
   94| 000BE4 stfdux   7C8B3DEE   1     STFDU     gr11,v1(gr11,gr7,0)=fp4
   93| 000BE8 stfdux   7F7D2DEE   1     STFDU     gr29,d(gr29,gr5,0)=fp27
    0| 000BEC bc       4200FFE8   1     BCT       ctr=CL.1138,taken=100%(100,0)
    0|                              CL.1137:
   99| 000BF0 stfdux   7CBC35EE   1     STFDU     gr28,e(gr28,gr6,0)=fp5
   94| 000BF4 fmul     FC5B00B2   1     MFL       fp2=fp27,fp2,fcr
   94| 000BF8 stfdux   7C4B3DEE   2     STFDU     gr11,v1(gr11,gr7,0)=fp2
  100|                              CL.214:
    0| 000BFC ld       E94102B0   1     L8        gr10=#SPILL3(gr1,688)
  101| 000C00 cmpld    7CA80040   1     CL8       cr1=gr8,gr0
    0| 000C04 add      7E52DA14   1     A         gr18=gr18,gr27
    0| 000C08 add      7E10CA14   1     A         gr16=gr16,gr25
    0| 000C0C add      7DEFD214   1     A         gr15=gr15,gr26
    0| 000C10 add      7E2A8A14   1     A         gr17=gr10,gr17
  101| 000C14 bc       4184FF6C   1     BT        CL.209,cr1,0x8/llt,taken=80%(80,20)
  101|                              CL.215:
    0| 000C18 ld       E90102D0   1     L8        gr8=#SPILL7(gr1,720)
    0| 000C1C ld       E92102B8   1     L8        gr9=#SPILL4(gr1,696)
  102| 000C20 addi     38840001   1     AI        gr4=gr4,1
    0| 000C24 add      7EB5BA14   1     A         gr21=gr21,gr23
  102| 000C28 cmpd     7CA32000   1     C8        cr1=gr3,gr4
    0| 000C2C add      7ECEB214   1     A         gr22=gr14,gr22
    0| 000C30 add      7E689A14   1     A         gr19=gr8,gr19
    0| 000C34 add      7E89A214   1     A         gr20=gr9,gr20
  102| 000C38 bc       4185FF24   1     BT        CL.208,cr1,0x2/gt,taken=80%(80,20)
  102|                              CL.216:
   89| 000C3C ld       E8010298   1     L8        gr0=#SPILL0(gr1,664)
   89| 000C40 cmpd     7CA01800   1     C8        cr1=gr0,gr3
   89| 000C44 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
   89| 000C48 bc       408102DC   1     BF        CL.143,cr0,0x2/gt,taken=50%(0,0)
   93| 000C4C ld       EAC20000   1     L8        gr22=.&&N&field(gr2,0)
    0| 000C50 ld       E9E102B8   1     L8        gr15=#SPILL4(gr1,696)
   92| 000C54 ld       E9020000   1     L8        gr8=.&&N&grid(gr2,0)
    0| 000C58 ld       EA2102B0   1     L8        gr17=#SPILL3(gr1,688)
    0| 000C5C ld       EA0102C0   1     L8        gr16=#SPILL5(gr1,704)
   90| 000C60 ld       EA620000   1     L8        gr19=.&&N&&param(gr2,0)
   93| 000C64 ld       E8D60030   1     L8        gr6=<s35:d48:l8>(gr22,48)
   99| 000C68 ld       E9560098   1     L8        gr10=<s35:d152:l8>(gr22,152)
   94| 000C6C ld       E8B601D0   1     L8        gr5=<s35:d464:l8>(gr22,464)
   93| 000C70 ld       E9360000   1     L8        gr9=<s35:d0:l8>(gr22,0)
   99| 000C74 ld       E9960068   1     L8        gr12=<s35:d104:l8>(gr22,104)
   94| 000C78 ld       E8F601A0   1     L8        gr7=<s35:d416:l8>(gr22,416)
   93| 000C7C ld       E9760018   1     L8        gr11=<s35:d24:l8>(gr22,24)
   94| 000C80 ld       EBF601B8   1     L8        gr31=<s35:d440:l8>(gr22,440)
   99| 000C84 ld       EBD60080   1     L8        gr30=<s35:d128:l8>(gr22,128)
   99| 000C88 ld       EB9600B0   1     L8        gr28=<s35:d176:l8>(gr22,176)
   94| 000C8C ld       EB7601E8   1     L8        gr27=<s35:d488:l8>(gr22,488)
   93| 000C90 ld       E8160048   1     L8        gr0=<s35:d72:l8>(gr22,72)
    0| 000C94 mulld    7C8331D2   1     M         gr4=gr3,gr6
  102| 000C98 ld       EA4102C8   1     L8        gr18=#SPILL6(gr1,712)
    0| 000C9C add      7D8CF214   1     A         gr12=gr12,gr30
    0| 000CA0 add      7EE7FA14   1     A         gr23=gr7,gr31
    0| 000CA4 add      7FE95A14   1     A         gr31=gr9,gr11
   92| 000CA8 ld       EBC80700   1     L8        gr30=<s82:d1792:l8>(gr8,1792)
   92| 000CAC ld       EBA80718   1     L8        gr29=<s82:d1816:l8>(gr8,1816)
    0| 000CB0 mulld    7D6351D2   1     M         gr11=gr3,gr10
    0| 000CB4 mulld    7D2329D2   1     M         gr9=gr3,gr5
    0| 000CB8 mulld    7D0379D2   1     M         gr8=gr3,gr15
    0| 000CBC ld       E86102A8   1     L8        gr3=#SPILL2(gr1,680)
    0| 000CC0 subf     7CF08850   1     S         gr7=gr17,gr16
    0| 000CC4 add      7D8CE214   1     A         gr12=gr12,gr28
    0| 000CC8 add      7EF7DA14   1     A         gr23=gr23,gr27
   99| 000CCC ld       EA820000   1     L8        gr20=.&&N&&root(gr2,0)
   90| 000CD0 lwa      EB530006   1     L4A       gr26=<s145:d4:l4>(gr19,4)
  102| 000CD4 addi     3B12FFFF   1     AI        gr24=gr18,-1
   91| 000CD8 lwa      EB330002   1     L4A       gr25=<s145:d0:l4>(gr19,0)
    0| 000CDC rldicr   794E0FA4   1     SLL8      gr14=gr10,1
    0| 000CE0 rldicr   78B30FA4   1     SLL8      gr19=gr5,1
    0| 000CE4 std      F9C10298   1     ST8       #SPILL0(gr1,664)=gr14
    0| 000CE8 std      FA6102C8   1     ST8       #SPILL6(gr1,712)=gr19
    0| 000CEC ld       EA4102A0   1     L8        gr18=#SPILL1(gr1,672)
    0| 000CF0 add      7CE33A14   1     A         gr7=gr3,gr7
    0| 000CF4 add      7C60FA14   1     A         gr3=gr0,gr31
    0| 000CF8 add      7FEA6214   1     A         gr31=gr10,gr12
    0| 000CFC add      7D45BA14   1     A         gr10=gr5,gr23
    0| 000D00 rldicr   78C50FA4   1     SLL8      gr5=gr6,1
   99| 000D04 lfd      C81400E0   1     LFL       fp0=<s178:d224:l8>(gr20,224)
    0| 000D08 std      F8A102D8   1     ST8       #SPILL8(gr1,728)=gr5
   93| 000D0C ld       EA820000   1     L8        gr20=.&&N&field(gr2,0)
  102| 000D10 sradi    7F180E74   1     SRA8CA    gr24,ca=gr24,1
    0| 000D14 rldicr   7A5626E4   1     SLL8      gr22=gr18,4
  102| 000D18 addze    7EB80194   1     ADDE      gr21,ca=gr24,0,ca
    0| 000D1C std      FAC102D0   1     ST8       #SPILL7(gr1,720)=gr22
    0| 000D20 cmpwi    2C1A0000   1     C4        cr0=gr26,0
    0| 000D24 add      7F0C7214   1     A         gr24=gr12,gr14
    0| 000D28 add      7D93BA14   1     A         gr12=gr19,gr23
    0| 000D2C add      7EE7B214   1     A         gr23=gr7,gr22
    0| 000D30 add      7EC77A14   1     A         gr22=gr7,gr15
    0| 000D34 add      7CE32214   1     A         gr7=gr3,gr4
   89| 000D38 addi     38800000   1     LI        gr4=0
   93| 000D3C ld       E8740060   1     L8        gr3=<s35:d96:l8>(gr20,96)
   89| 000D40 std      F88102E0   1     ST8       #SPILL9(gr1,736)=gr4
   99| 000D44 ld       E89400C8   1     L8        gr4=<s35:d200:l8>(gr20,200)
   94| 000D48 ld       E8B40200   1     L8        gr5=<s35:d512:l8>(gr20,512)
    0| 000D4C bc       408101D8   1     BF        CL.143,cr0,0x2/gt,taken=20%(20,80)
   92| 000D50 add      7FBDF214   1     A         gr29=gr29,gr30
    0| 000D54 add      7FCBC214   1     A         gr30=gr11,gr24
   92| 000D58 std      FBA102E8   1     ST8       #SPILL10(gr1,744)=gr29
    0| 000D5C std      FBC102F0   1     ST8       #SPILL11(gr1,752)=gr30
    0| 000D60 add      7D6BFA14   1     A         gr11=gr11,gr31
    0| 000D64 add      7FE96214   1     A         gr31=gr9,gr12
    0| 000D68 std      F96102F8   1     ST8       #SPILL12(gr1,760)=gr11
    0| 000D6C std      FBE10300   1     ST8       #SPILL13(gr1,768)=gr31
    0| 000D70 add      7D895214   1     A         gr12=gr9,gr10
    0| 000D74 add      7D28BA14   1     A         gr9=gr8,gr23
    0| 000D78 std      F9810308   1     ST8       #SPILL14(gr1,776)=gr12
    0| 000D7C add      7DC8B214   1     A         gr14=gr8,gr22
    0| 000D80 ld       E90102D8   1     L8        gr8=#SPILL8(gr1,728)
    0| 000D84 std      F9210310   1     ST8       #SPILL15(gr1,784)=gr9
    0| 000D88 add      7F063A14   1     A         gr24=gr6,gr7
    0| 000D8C addi     38D50001   1     AI        gr6=gr21,1
    0| 000D90 cmpdi    2C390000   1     C8        cr0=gr25,0
    0| 000D94 std      F8C10318   1     ST8       #SPILL16(gr1,792)=gr6
    0| 000D98 add      7DE74214   1     A         gr15=gr7,gr8
    0| 000D9C ld       E8E20000   1     L8        gr7=.+CONSTANT_AREA(gr2,0)
    0| 000DA0 lfs      C0270068   1     LFS       fp1=+CONSTANT_AREA(gr7,104)
   89|                              CL.144:
   90| 000DA4 addi     38C00000   1     LI        gr6=0
    0| 000DA8 bc       4081010C   1     BF        CL.145,cr0,0x2/gt,taken=20%(20,80)
    0| 000DAC fadd     FC40082A   1     AFL       fp2=fp0,fp1,fcr
   92| 000DB0 ld       E8E102E8   1     L8        gr7=#SPILL10(gr1,744)
    0| 000DB4 or       7F17C378   1     LR        gr23=gr24
    0| 000DB8 or       7DF67B78   1     LR        gr22=gr15
    0| 000DBC or       7DD57378   1     LR        gr21=gr14
    0| 000DC0 ld       EA810310   1     L8        gr20=#SPILL15(gr1,784)
    0| 000DC4 fdiv     FD1D1024   1     DFL       fp8=fp29,fp2,fcr
    0| 000DC8 ld       EA610308   1     L8        gr19=#SPILL14(gr1,776)
    0| 000DCC ld       EA410300   1     L8        gr18=#SPILL13(gr1,768)
    0| 000DD0 ld       EA2102F8   1     L8        gr17=#SPILL12(gr1,760)
    0| 000DD4 ld       EA0102F0   1     L8        gr16=#SPILL11(gr1,752)
   90|                              CL.146:
   92| 000DD8 lfdu     CC470008   1     LFDU      fp2,gr7=x2b(gr7,8)
   94| 000DDC or       7EA8AB78   1     LR        gr8=gr21
   94| 000DE0 or       7E89A378   1     LR        gr9=gr20
    0| 000DE4 addi     38C60001   1     AI        gr6=gr6,1
   97| 000DE8 or       7EACAB78   1     LR        gr12=gr21
   97| 000DEC or       7E9FA378   1     LR        gr31=gr20
    0| 000DF0 mtspr    7F2903A6   1     LCTR      ctr=gr25
   92| 000DF4 fabs     FC401210   1     ABSFL     fp2=fp2
   93| 000DF8 or       7EEABB78   1     LR        gr10=gr23
    0| 000DFC fcmpu    FC82B000   1     CFL       cr1=fp2,fp22
   96| 000E00 or       7EFEBB78   1     LR        gr30=gr23
   99| 000E04 or       7E2B8B78   1     LR        gr11=gr17
    0| 000E08 bc       41840AA8   1     BT        CL.744,cr1,0x20/flt,taken=50%(0,0)
   93| 000E0C stfdux   7F6A1DEE   1     STFDU     gr10,d(gr10,gr3,0)=fp27
   94| 000E10 lfdu     CC680008   1     LFDU      fp3,gr8=pert(gr8,8)
   94| 000E14 lfdu     CC490008   1     LFDU      fp2,gr9=pert(gr9,8)
   93| 000E18 or       7ECCB378   1     LR        gr12=gr22
   94| 000E1C or       7E7F9B78   1     LR        gr31=gr19
   93| 000E20 stfdux   7F6C1DEE   1     STFDU     gr12,d(gr12,gr3,0)=fp27
   99| 000E24 or       7E1E8378   1     LR        gr30=gr16
   94| 000E28 or       7E5D9378   1     LR        gr29=gr18
   94| 000E2C fsub     FCE3F028   1     SFL       fp7=fp3,fp30,fcr
   94| 000E30 fsub     FC42F028   2     SFL       fp2=fp2,fp30,fcr
    0| 000E34 bc       4240003C   1     BCF       ctr=CL.1141,taken=0%(0,100)
    0| 000E38 ori      60210000   1     XNOP      
    0|                              CL.1142:
   94| 000E3C lfdu     CC680008   1     LFDU      fp3,gr8=pert(gr8,8)
   94| 000E40 lfdu     CC890008   1     LFDU      fp4,gr9=pert(gr9,8)
   94| 000E44 fmul     FCBB01F2   1     MFL       fp5=fp27,fp7,fcr
   94| 000E48 fmul     FCDB00B2   2     MFL       fp6=fp27,fp2,fcr
   93| 000E4C stfdux   7F6A1DEE   2     STFDU     gr10,d(gr10,gr3,0)=fp27
   99| 000E50 stfdux   7D0B25EE   1     STFDU     gr11,e(gr11,gr4,0)=fp8
   99| 000E54 stfdux   7D1E25EE   1     STFDU     gr30,e(gr30,gr4,0)=fp8
   94| 000E58 fsub     FCE3F028   1     SFL       fp7=fp3,fp30,fcr
   94| 000E5C fsub     FC44F028   2     SFL       fp2=fp4,fp30,fcr
   94| 000E60 stfdux   7CBF2DEE   1     STFDU     gr31,v1(gr31,gr5,0)=fp5
   94| 000E64 stfdux   7CDD2DEE   1     STFDU     gr29,v1(gr29,gr5,0)=fp6
   93| 000E68 stfdux   7F6C1DEE   1     STFDU     gr12,d(gr12,gr3,0)=fp27
    0| 000E6C bc       4200FFD0   1     BCT       ctr=CL.1142,taken=100%(100,0)
    0|                              CL.1141:
   94| 000E70 fmul     FC5B00B2   1     MFL       fp2=fp27,fp2,fcr
   99| 000E74 stfdux   7D0B25EE   1     STFDU     gr11,e(gr11,gr4,0)=fp8
   94| 000E78 fmul     FC7B01F2   1     MFL       fp3=fp27,fp7,fcr
   99| 000E7C stfdux   7D1E25EE   1     STFDU     gr30,e(gr30,gr4,0)=fp8
   94| 000E80 stfdux   7C7F2DEE   1     STFDU     gr31,v1(gr31,gr5,0)=fp3
   94| 000E84 stfdux   7C5D2DEE   1     STFDU     gr29,v1(gr29,gr5,0)=fp2
  100|                              CL.147:
    0| 000E88 ld       E90102B0   1     L8        gr8=#SPILL3(gr1,688)
  101| 000E8C cmpld    7CA6D040   1     CL8       cr1=gr6,gr26
    0| 000E90 add      7EE0BA14   1     A         gr23=gr0,gr23
    0| 000E94 add      7EC0B214   1     A         gr22=gr0,gr22
    0| 000E98 add      7E73DA14   1     A         gr19=gr19,gr27
    0| 000E9C add      7E52DA14   1     A         gr18=gr18,gr27
    0| 000EA0 add      7EA8AA14   1     A         gr21=gr8,gr21
    0| 000EA4 add      7E88A214   1     A         gr20=gr8,gr20
    0| 000EA8 add      7E31E214   1     A         gr17=gr17,gr28
    0| 000EAC add      7E10E214   1     A         gr16=gr16,gr28
  101| 000EB0 bc       4184FF28   1     BT        CL.146,cr1,0x8/llt,taken=80%(80,20)
  101|                              CL.145:
  102| 000EB4 ld       E8C102E0   1     L8        gr6=#SPILL9(gr1,736)
    0| 000EB8 ld       E8E10298   1     L8        gr7=#SPILL0(gr1,664)
    0| 000EBC ld       E90102F0   1     L8        gr8=#SPILL11(gr1,752)
  102| 000EC0 ld       E9210318   1     L8        gr9=#SPILL16(gr1,792)
    0| 000EC4 ld       E94102F8   1     L8        gr10=#SPILL12(gr1,760)
    0| 000EC8 ld       E96102C8   1     L8        gr11=#SPILL6(gr1,712)
    0| 000ECC ld       E9810300   1     L8        gr12=#SPILL13(gr1,768)
    0| 000ED0 ld       EBE10308   1     L8        gr31=#SPILL14(gr1,776)
    0| 000ED4 ld       EBC102D0   1     L8        gr30=#SPILL7(gr1,720)
    0| 000ED8 ld       EBA10310   1     L8        gr29=#SPILL15(gr1,784)
    0| 000EDC ld       EAE102D8   1     L8        gr23=#SPILL8(gr1,728)
  102| 000EE0 addi     38C60001   1     AI        gr6=gr6,1
    0| 000EE4 add      7D074214   1     A         gr8=gr7,gr8
  102| 000EE8 std      F8C102E0   1     ST8       #SPILL9(gr1,736)=gr6
    0| 000EEC std      F90102F0   1     ST8       #SPILL11(gr1,752)=gr8
  102| 000EF0 cmpld    7CA64840   1     CL8       cr1=gr6,gr9
    0| 000EF4 add      7D475214   1     A         gr10=gr7,gr10
    0| 000EF8 add      7D8B6214   1     A         gr12=gr11,gr12
    0| 000EFC std      F94102F8   1     ST8       #SPILL12(gr1,760)=gr10
    0| 000F00 std      F9810300   1     ST8       #SPILL13(gr1,768)=gr12
    0| 000F04 add      7FEBFA14   1     A         gr31=gr11,gr31
    0| 000F08 add      7FBDF214   1     A         gr29=gr29,gr30
    0| 000F0C std      FBE10308   1     ST8       #SPILL14(gr1,776)=gr31
    0| 000F10 std      FBA10310   1     ST8       #SPILL15(gr1,784)=gr29
    0| 000F14 add      7DCEF214   1     A         gr14=gr14,gr30
    0| 000F18 add      7DEFBA14   1     A         gr15=gr15,gr23
    0| 000F1C add      7F17C214   1     A         gr24=gr23,gr24
  102| 000F20 bc       4184FE84   1     BT        CL.144,cr1,0x8/llt,taken=80%(80,20)
  102|                              CL.143:
  103| 000F24 ld       E8820000   1     L8        gr4=.&&N&field(gr2,0)
  103| 000F28 ld       EB840230   1     L8        gr28=<s35:d560:l8>(gr4,560)
  103| 000F2C ld       E9440020   1     L8        gr10=<s35:d32:l8>(gr4,32)
  103| 000F30 ld       E9640038   1     L8        gr11=<s35:d56:l8>(gr4,56)
  103| 000F34 ld       E8040248   1     L8        gr0=<s35:d584:l8>(gr4,584)
  103| 000F38 ld       E8C40260   1     L8        gr6=<s35:d608:l8>(gr4,608)
  103| 000F3C ld       E8A40228   1     L8        gr5=<s35:d552:l8>(gr4,552)
  103| 000F40 sradi    7F830E74   1     SRA8CA    gr3,ca=gr28,1
  103| 000F44 std      F9410298   1     ST8       #SPILL0(gr1,664)=gr10
  103| 000F48 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  103| 000F4C cmpdi    2F3C0000   1     C8        cr6=gr28,0
  103| 000F50 rldicr   78710FA4   1     SLL8      gr17=gr3,1
  103| 000F54 std      F96102C8   1     ST8       #SPILL6(gr1,712)=gr11
  103| 000F58 subf     7D31E051   1     S_R       gr9,cr0=gr28,gr17
  103| 000F5C ld       E8E40240   1     L8        gr7=<s35:d576:l8>(gr4,576)
  103| 000F60 crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  103| 000F64 ld       E9040258   1     L8        gr8=<s35:d600:l8>(gr4,600)
  103| 000F68 ld       EBE40050   1     L8        gr31=<s35:d80:l8>(gr4,80)
  103| 000F6C bc       4081012C   1     BF        CL.198,cr0,0x2/gt,taken=50%(0,0)
  103| 000F70 ld       EB440238   1     L8        gr26=<s35:d568:l8>(gr4,568)
  103| 000F74 ld       E8640268   1     L8        gr3=<s35:d616:l8>(gr4,616)
  103| 000F78 or       7C8E2378   1     LR        gr14=gr4
    0| 000F7C cmpdi    2C200000   1     C8        cr0=gr0,0
  103| 000F80 ld       EB640030   1     L8        gr27=<s35:d48:l8>(gr4,48)
  103| 000F84 ld       EB240048   1     L8        gr25=<s35:d72:l8>(gr4,72)
  103| 000F88 ld       E8840060   1     L8        gr4=<s35:d96:l8>(gr4,96)
  103| 000F8C ld       EB0E0250   1     L8        gr24=<s35:d592:l8>(gr14,592)
  103| 000F90 ld       E96E0000   1     L8        gr11=<s35:d0:l8>(gr14,0)
  103| 000F94 ld       E98E0018   1     L8        gr12=<s35:d24:l8>(gr14,24)
  103| 000F98 addi     39400000   1     LI        gr10=0
  103| 000F9C ld       EBCE0208   1     L8        gr30=<s35:d520:l8>(gr14,520)
  103| 000FA0 ld       EBAE0220   1     L8        gr29=<s35:d544:l8>(gr14,544)
    0| 000FA4 bc       408108F8   1     BF        CL.753,cr0,0x2/gt,taken=40%(40,60)
    0| 000FA8 ld       EA010298   1     L8        gr16=#SPILL0(gr1,664)
    0| 000FAC ld       E9E102C8   1     L8        gr15=#SPILL6(gr1,712)
    0| 000FB0 mulld    7E6341D2   1     M         gr19=gr3,gr8
    0| 000FB4 mulld    7EE5D1D2   1     M         gr23=gr5,gr26
    0| 000FB8 mulld    7E50D9D2   1     M         gr18=gr16,gr27
    0| 000FBC mulld    7EA4F9D2   1     M         gr21=gr4,gr31
    0| 000FC0 mulld    7E8FC9D2   1     M         gr20=gr15,gr25
    0| 000FC4 add      7D6B6214   1     A         gr11=gr11,gr12
    0| 000FC8 subf     7D83F050   1     S         gr12=gr30,gr3
    0| 000FCC add      7FD3BA14   1     A         gr30=gr19,gr23
    0| 000FD0 ld       EA6102B0   1     L8        gr19=#SPILL3(gr1,688)
    0| 000FD4 ld       EAE102C0   1     L8        gr23=#SPILL5(gr1,704)
    0| 000FD8 mulld    7EC7C1D2   1     M         gr22=gr7,gr24
    0| 000FDC add      7D6B9214   1     A         gr11=gr11,gr18
    0| 000FE0 ld       EA4102A8   1     L8        gr18=#SPILL2(gr1,680)
    0| 000FE4 add      7EB4AA14   1     A         gr21=gr20,gr21
    0| 000FE8 ld       EA8102B8   1     L8        gr20=#SPILL4(gr1,696)
    0| 000FEC subf     7EF79850   1     S         gr23=gr19,gr23
    0| 000FF0 add      7D8CEA14   1     A         gr12=gr12,gr29
    0| 000FF4 add      7FD6F214   1     A         gr30=gr22,gr30
    0| 000FF8 add      7FB2BA14   1     A         gr29=gr18,gr23
    0| 000FFC subf     7D645850   1     S         gr11=gr11,gr4
    0| 001000 add      7EECF214   1     A         gr23=gr12,gr30
    0| 001004 add      7ED4EA14   1     A         gr22=gr20,gr29
    0| 001008 add      7EB55A14   1     A         gr21=gr21,gr11
    0| 00100C cmpdi    2C260000   1     C8        cr0=gr6,0
  103|                              CL.193:
  103| 001010 addi     39600000   1     LI        gr11=0
    0| 001014 bc       40810068   1     BF        CL.197,cr0,0x2/gt,taken=50%(0,0)
    0| 001018 or       7EB4AB78   1     LR        gr20=gr21
    0| 00101C or       7ED3B378   1     LR        gr19=gr22
    0| 001020 or       7EF2BB78   1     LR        gr18=gr23
  103|                              CL.194:
  103| 001024 or       7E6C9B78   1     LR        gr12=gr19
  103| 001028 or       7E9EA378   1     LR        gr30=gr20
  103| 00102C lfdu     CC0C0008   1     LFDU      fp0,gr12=pert(gr12,8)
  103| 001030 lfdux    7C3E24EE   1     LFDU      fp1,gr30=d(gr30,gr4,0)
    0| 001034 mtspr    7CC903A6   1     LCTR      ctr=gr6
    0| 001038 addi     396B0001   1     AI        gr11=gr11,1
  103| 00103C or       7E5D9378   1     LR        gr29=gr18
  103| 001040 fmul     FC010032   1     MFL       fp0=fp1,fp0,fcr
    0| 001044 bc       4240001C   1     BCF       ctr=CL.1145,taken=0%(0,100)
    0| 001048 ori      60210000   1     XNOP      
    0|                              CL.1146:
  103| 00104C lfdux    7C3E24EE   1     LFDU      fp1,gr30=d(gr30,gr4,0)
  103| 001050 lfdu     CC4C0008   1     LFDU      fp2,gr12=pert(gr12,8)
  103| 001054 stfdux   7C1D1DEE   1     STFDU     gr29,v2(gr29,gr3,0)=fp0
  103| 001058 fmul     FC0100B2   1     MFL       fp0=fp1,fp2,fcr
    0| 00105C bc       4200FFF0   1     BCT       ctr=CL.1146,taken=100%(100,0)
    0|                              CL.1145:
    0| 001060 ld       E98102B0   1     L8        gr12=#SPILL3(gr1,688)
  103| 001064 cmpld    7CAB0040   1     CL8       cr1=gr11,gr0
  103| 001068 stfdux   7C1D1DEE   1     STFDU     gr29,v2(gr29,gr3,0)=fp0
    0| 00106C add      7E94CA14   1     A         gr20=gr20,gr25
    0| 001070 add      7E52C214   1     A         gr18=gr18,gr24
    0| 001074 add      7E6C9A14   1     A         gr19=gr12,gr19
  103| 001078 bc       4184FFAC   1     BT        CL.194,cr1,0x8/llt,taken=80%(80,20)
  103|                              CL.197:
    0| 00107C ld       E96102B8   1     L8        gr11=#SPILL4(gr1,696)
  103| 001080 addi     394A0001   1     AI        gr10=gr10,1
    0| 001084 add      7EB5DA14   1     A         gr21=gr21,gr27
  103| 001088 cmpd     7CA95000   1     C8        cr1=gr9,gr10
    0| 00108C add      7EF7D214   1     A         gr23=gr23,gr26
    0| 001090 add      7ECBB214   1     A         gr22=gr11,gr22
  103| 001094 bc       4185FF7C   1     BT        CL.193,cr1,0x2/gt,taken=80%(80,20)
  103|                              CL.198:
  103| 001098 cmpd     7CBC4800   1     C8        cr1=gr28,gr9
  103| 00109C crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  103| 0010A0 bc       40810248   1     BF        CL.149,cr0,0x2/gt,taken=50%(0,0)
  103| 0010A4 ld       EAC20000   1     L8        gr22=.&&N&field(gr2,0)
    0| 0010A8 ld       EAA10298   1     L8        gr21=#SPILL0(gr1,664)
    0| 0010AC ld       EA8102C8   1     L8        gr20=#SPILL6(gr1,712)
    0| 0010B0 ld       EA6102B0   1     L8        gr19=#SPILL3(gr1,688)
    0| 0010B4 ld       EA4102C0   1     L8        gr18=#SPILL5(gr1,704)
    0| 0010B8 ld       EA0102A0   1     L8        gr16=#SPILL1(gr1,672)
  103| 0010BC ld       E9560030   1     L8        gr10=<s35:d48:l8>(gr22,48)
  103| 0010C0 ld       E9760238   1     L8        gr11=<s35:d568:l8>(gr22,568)
  103| 0010C4 ld       E8760268   1     L8        gr3=<s35:d616:l8>(gr22,616)
  103| 0010C8 ld       E9960000   1     L8        gr12=<s35:d0:l8>(gr22,0)
  103| 0010CC ld       EB960018   1     L8        gr28=<s35:d24:l8>(gr22,24)
  103| 0010D0 ld       EBB60250   1     L8        gr29=<s35:d592:l8>(gr22,592)
  103| 0010D4 ld       EB760208   1     L8        gr27=<s35:d520:l8>(gr22,520)
    0| 0010D8 mulld    7F4AA9D2   1     M         gr26=gr10,gr21
  103| 0010DC ld       EBD60048   1     L8        gr30=<s35:d72:l8>(gr22,72)
  103| 0010E0 ld       E8960060   1     L8        gr4=<s35:d96:l8>(gr22,96)
    0| 0010E4 add      7D8CE214   1     A         gr12=gr12,gr28
    0| 0010E8 mulld    7D0819D2   1     M         gr8=gr8,gr3
    0| 0010EC mulld    7CA559D2   1     M         gr5=gr5,gr11
    0| 0010F0 subf     7F83D850   1     S         gr28=gr27,gr3
    0| 0010F4 add      7F6CD214   1     A         gr27=gr12,gr26
  103| 0010F8 addi     3B51FFFF   1     AI        gr26=gr17,-1
    0| 0010FC ld       EA2102B8   1     L8        gr17=#SPILL4(gr1,696)
  103| 001100 ld       EB360220   1     L8        gr25=<s35:d544:l8>(gr22,544)
    0| 001104 mulld    7CE7E9D2   1     M         gr7=gr7,gr29
    0| 001108 mulld    7F04F9D2   1     M         gr24=gr4,gr31
    0| 00110C mulld    7EF4F1D2   1     M         gr23=gr20,gr30
    0| 001110 add      7D054214   1     A         gr8=gr5,gr8
    0| 001114 ld       E9E102A8   1     L8        gr15=#SPILL2(gr1,680)
    0| 001118 mulld    7D8959D2   1     M         gr12=gr9,gr11
    0| 00111C add      7F99E214   1     A         gr28=gr25,gr28
    0| 001120 add      7CE74214   1     A         gr7=gr7,gr8
    0| 001124 subf     7F329850   1     S         gr25=gr19,gr18
    0| 001128 mulld    7CA951D2   1     M         gr5=gr9,gr10
    0| 00112C mulld    7D0989D2   1     M         gr8=gr9,gr17
    0| 001130 add      7F17C214   1     A         gr24=gr23,gr24
    0| 001134 subf     7F64D850   1     S         gr27=gr27,gr4
  103| 001138 sradi    7F490E74   1     SRA8CA    gr9,ca=gr26,1
    0| 00113C add      7F47E214   1     A         gr26=gr7,gr28
    0| 001140 rldicr   7A1C26E4   1     SLL8      gr28=gr16,4
    0| 001144 add      7F2FCA14   1     A         gr25=gr15,gr25
    0| 001148 add      7F18DA14   1     A         gr24=gr24,gr27
  103| 00114C addze    7CE90194   1     ADDE      gr7,ca=gr9,0,ca
    0| 001150 cmpdi    2C200000   1     C8        cr0=gr0,0
    0| 001154 add      7F6CD214   1     A         gr27=gr12,gr26
    0| 001158 add      7D39E214   1     A         gr9=gr25,gr28
    0| 00115C add      7D91CA14   1     A         gr12=gr17,gr25
    0| 001160 add      7F45C214   1     A         gr26=gr5,gr24
    0| 001164 rldicl   78D9F842   1     SRL8      gr25=gr6,1
  103| 001168 addi     38A00000   1     LI        gr5=0
    0| 00116C bc       4081017C   1     BF        CL.149,cr0,0x2/gt,taken=20%(20,80)
    0| 001170 rldicr   79720FA4   1     SLL8      gr18=gr11,1
    0| 001174 add      7EE84A14   1     A         gr23=gr8,gr9
    0| 001178 std      FA4102A0   1     ST8       #SPILL1(gr1,672)=gr18
    0| 00117C add      7EC86214   1     A         gr22=gr8,gr12
    0| 001180 addi     39070001   1     AI        gr8=gr7,1
    0| 001184 add      7F0BDA14   1     A         gr24=gr11,gr27
    0| 001188 std      F90102B8   1     ST8       #SPILL4(gr1,696)=gr8
    0| 00118C rldicr   794E0FA4   1     SLL8      gr14=gr10,1
    0| 001190 add      7EAAD214   1     A         gr21=gr10,gr26
    0| 001194 cmpdi    2CA60000   1     C8        cr1=gr6,0
    0| 001198 andi.    70C60001   1     RN4_R     gr6,cr0=gr6,0,0x1
    0| 00119C cmpdi    2FB90000   1     C8        cr7=gr25,0
  103|                              CL.150:
  103| 0011A0 addi     38C00000   1     LI        gr6=0
    0| 0011A4 bc       40850118   1     BF        CL.151,cr1,0x2/gt,taken=20%(20,80)
    0| 0011A8 or       7F54D378   1     LR        gr20=gr26
    0| 0011AC or       7EB3AB78   1     LR        gr19=gr21
    0| 0011B0 or       7ED2B378   1     LR        gr18=gr22
    0| 0011B4 or       7EF1BB78   1     LR        gr17=gr23
    0| 0011B8 or       7F70DB78   1     LR        gr16=gr27
    0| 0011BC or       7F0FC378   1     LR        gr15=gr24
  103|                              CL.152:
  103| 0011C0 or       7E479378   1     LR        gr7=gr18
  103| 0011C4 or       7E88A378   1     LR        gr8=gr20
  103| 0011C8 or       7E699B78   1     LR        gr9=gr19
  103| 0011CC or       7E2A8B78   1     LR        gr10=gr17
  103| 0011D0 or       7E0B8378   1     LR        gr11=gr16
  103| 0011D4 or       7DEC7B78   1     LR        gr12=gr15
    0| 0011D8 mtspr    7F2903A6   1     LCTR      ctr=gr25
    0| 0011DC addi     38C60001   1     AI        gr6=gr6,1
    0| 0011E0 bc       41820028   1     BT        CL.1119,cr0,0x4/eq,taken=50%(0,0)
  103| 0011E4 lfdu     CC070008   1     LFDU      fp0,gr7=pert(gr7,8)
  103| 0011E8 lfdux    7C2824EE   1     LFDU      fp1,gr8=d(gr8,gr4,0)
  103| 0011EC lfdux    7C4924EE   1     LFDU      fp2,gr9=d(gr9,gr4,0)
  103| 0011F0 lfdu     CC6A0008   1     LFDU      fp3,gr10=pert(gr10,8)
  103| 0011F4 fmul     FC010032   1     MFL       fp0=fp1,fp0,fcr
  103| 0011F8 fmul     FC2200F2   2     MFL       fp1=fp2,fp3,fcr
  103| 0011FC stfdux   7C0B1DEE   2     STFDU     gr11,v2(gr11,gr3,0)=fp0
  103| 001200 stfdux   7C2C1DEE   1     STFDU     gr12,v2(gr12,gr3,0)=fp1
    0| 001204 bc       419E0094   1     BT        CL.1027,cr7,0x4/eq,taken=20%(20,80)
    0|                              CL.1119:
  103| 001208 lfdu     CC070010   1     LFDU      fp0,gr7=pert(gr7,16)
  103| 00120C lfdux    7C2824EE   1     LFDU      fp1,gr8=d(gr8,gr4,0)
  103| 001210 lfdux    7C4924EE   1     LFDU      fp2,gr9=d(gr9,gr4,0)
  103| 001214 lfd      C86A0008   1     LFL       fp3=pert(gr10,8)
  103| 001218 lfd      C887FFF8   1     LFL       fp4=pert(gr7,-8)
  103| 00121C lfdux    7CA824EE   1     LFDU      fp5,gr8=d(gr8,gr4,0)
    0| 001220 bc       42400050   1     BCF       ctr=CL.1147,taken=0%(0,100)
    0| 001224 ori      60210000   1     XNOP      
    0|                              CL.1148:
  103| 001228 lfdu     CCC70010   1     LFDU      fp6,gr7=pert(gr7,16)
  103| 00122C lfdu     CCEA0010   1     LFDU      fp7,gr10=pert(gr10,16)
  103| 001230 lfdux    7D0924EE   1     LFDU      fp8,gr9=d(gr9,gr4,0)
  103| 001234 fmul     FD210132   1     MFL       fp9=fp1,fp4,fcr
  103| 001238 fmul     FC4200F2   2     MFL       fp2=fp2,fp3,fcr
  103| 00123C fmul     FC050032   2     MFL       fp0=fp5,fp0,fcr
  103| 001240 lfd      C887FFF8   1     LFL       fp4=pert(gr7,-8)
  103| 001244 lfdux    7C2824EE   1     LFDU      fp1,gr8=d(gr8,gr4,0)
  103| 001248 lfd      C86A0008   1     LFL       fp3=pert(gr10,8)
  103| 00124C fmul     FCE801F2   1     MFL       fp7=fp8,fp7,fcr
  103| 001250 stfdux   7D2B1DEE   2     STFDU     gr11,v2(gr11,gr3,0)=fp9
  103| 001254 stfdux   7C4C1DEE   1     STFDU     gr12,v2(gr12,gr3,0)=fp2
  103| 001258 stfdux   7C0B1DEE   1     STFDU     gr11,v2(gr11,gr3,0)=fp0
  103| 00125C lfdux    7C4924EE   1     LFDU      fp2,gr9=d(gr9,gr4,0)
  103| 001260 lfdux    7CA824EE   1     LFDU      fp5,gr8=d(gr8,gr4,0)
  103| 001264 stfdux   7CEC1DEE   1     STFDU     gr12,v2(gr12,gr3,0)=fp7
    0| 001268 fmr      FC003090   1     LRFL      fp0=fp6
    0| 00126C bc       4200FFBC   1     BCT       ctr=CL.1148,taken=100%(100,0)
    0|                              CL.1147:
  103| 001270 lfdu     CCCA0010   1     LFDU      fp6,gr10=pert(gr10,16)
  103| 001274 fmul     FC4200F2   1     MFL       fp2=fp2,fp3,fcr
  103| 001278 lfdux    7C6924EE   1     LFDU      fp3,gr9=d(gr9,gr4,0)
  103| 00127C fmul     FC210132   1     MFL       fp1=fp1,fp4,fcr
  103| 001280 fmul     FC050032   2     MFL       fp0=fp5,fp0,fcr
  103| 001284 fmul     FC6301B2   2     MFL       fp3=fp3,fp6,fcr
  103| 001288 stfdux   7C2B1DEE   2     STFDU     gr11,v2(gr11,gr3,0)=fp1
  103| 00128C stfdux   7C4C1DEE   1     STFDU     gr12,v2(gr12,gr3,0)=fp2
  103| 001290 stfdux   7C0B1DEE   1     STFDU     gr11,v2(gr11,gr3,0)=fp0
  103| 001294 stfdux   7C6C1DEE   1     STFDU     gr12,v2(gr12,gr3,0)=fp3
    0|                              CL.1027:
    0| 001298 ld       E8E102B0   1     L8        gr7=#SPILL3(gr1,688)
  103| 00129C cmpld    7F260040   1     CL8       cr6=gr6,gr0
    0| 0012A0 add      7E94F214   1     A         gr20=gr20,gr30
    0| 0012A4 add      7E73F214   1     A         gr19=gr19,gr30
    0| 0012A8 add      7E10EA14   1     A         gr16=gr16,gr29
    0| 0012AC add      7DEFEA14   1     A         gr15=gr15,gr29
    0| 0012B0 add      7E479214   1     A         gr18=gr7,gr18
    0| 0012B4 add      7E278A14   1     A         gr17=gr7,gr17
  103| 0012B8 bc       4198FF08   1     BT        CL.152,cr6,0x8/llt,taken=80%(80,20)
  103|                              CL.151:
  103| 0012BC ld       E8E102B8   1     L8        gr7=#SPILL4(gr1,696)
    0| 0012C0 ld       E8C102A0   1     L8        gr6=#SPILL1(gr1,672)
  103| 0012C4 addi     38A50001   1     AI        gr5=gr5,1
    0| 0012C8 add      7EF7E214   1     A         gr23=gr23,gr28
    0| 0012CC add      7ED6E214   1     A         gr22=gr22,gr28
    0| 0012D0 add      7EAEAA14   1     A         gr21=gr14,gr21
  103| 0012D4 cmpld    7F253840   1     CL8       cr6=gr5,gr7
    0| 0012D8 add      7F06C214   1     A         gr24=gr6,gr24
    0| 0012DC add      7F66DA14   1     A         gr27=gr6,gr27
    0| 0012E0 add      7F4ED214   1     A         gr26=gr14,gr26
  103| 0012E4 bc       4198FEBC   1     BT        CL.150,cr6,0x8/llt,taken=80%(80,20)
  103|                              CL.149:
  104| 0012E8 ld       E9220000   1     L8        gr9=.&&N&field(gr2,0)
  104| 0012EC ld       E9690298   1     L8        gr11=<s35:d664:l8>(gr9,664)
  104| 0012F0 ld       E80902B0   1     L8        gr0=<s35:d688:l8>(gr9,688)
  104| 0012F4 ld       E94902C8   1     L8        gr10=<s35:d712:l8>(gr9,712)
  104| 0012F8 ld       E8A90290   1     L8        gr5=<s35:d656:l8>(gr9,656)
  104| 0012FC ld       E8C902A8   1     L8        gr6=<s35:d680:l8>(gr9,680)
  104| 001300 ld       E8E902C0   1     L8        gr7=<s35:d704:l8>(gr9,704)
  104| 001304 sradi    7D631674   1     SRA8CA    gr3,ca=gr11,2
  104| 001308 cmpdi    2F2B0000   1     C8        cr6=gr11,0
  104| 00130C addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  104| 001310 rldicr   78641764   1     SLL8      gr4=gr3,2
  104| 001314 subf     7D045851   1     S_R       gr8,cr0=gr11,gr4
  104| 001318 crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  104| 00131C bc       40810094   1     BF        CL.186,cr0,0x2/gt,taken=50%(0,0)
  104| 001320 ld       E98902A0   1     L8        gr12=<s35:d672:l8>(gr9,672)
  104| 001324 ld       E86902D0   1     L8        gr3=<s35:d720:l8>(gr9,720)
  104| 001328 ld       EBC902B8   1     L8        gr30=<s35:d696:l8>(gr9,696)
  104| 00132C ld       EB620000   1     L8        gr27=.&&N&field(gr2,0)
    0| 001330 cmpdi    2C200000   1     C8        cr0=gr0,0
  104| 001334 addi     39200000   1     LI        gr9=0
  104| 001338 ld       EBBB0270   1     L8        gr29=<s35:d624:l8>(gr27,624)
  104| 00133C ld       EB9B0288   1     L8        gr28=<s35:d648:l8>(gr27,648)
    0| 001340 bc       40810548   1     BF        CL.763,cr0,0x2/gt,taken=40%(40,60)
    0| 001344 mulld    7F6339D2   1     M         gr27=gr3,gr7
    0| 001348 mulld    7F4561D2   1     M         gr26=gr5,gr12
    0| 00134C mulld    7F26F1D2   1     M         gr25=gr6,gr30
    0| 001350 ld       EB020000   1     L8        gr24=.+CONSTANT_AREA(gr2,0)
    0| 001354 subf     7FA3E850   1     S         gr29=gr29,gr3
    0| 001358 add      7F7ADA14   1     A         gr27=gr26,gr27
    0| 00135C add      7FBCEA14   1     A         gr29=gr28,gr29
    0| 001360 add      7F99DA14   1     A         gr28=gr25,gr27
    0| 001364 cmpdi    2C2A0000   1     C8        cr0=gr10,0
    0| 001368 add      7F7CEA14   1     A         gr27=gr28,gr29
    0| 00136C lfs      C018006C   1     LFS       fp0=+CONSTANT_AREA(gr24,108)
  104|                              CL.181:
  104| 001370 addi     3BA00000   1     LI        gr29=0
    0| 001374 bc       4081002C   1     BF        CL.185,cr0,0x2/gt,taken=20%(20,80)
    0| 001378 or       7F7ADB78   1     LR        gr26=gr27
  104|                              CL.182:
  104| 00137C or       7F5CD378   1     LR        gr28=gr26
    0| 001380 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 001384 ori      60210000   1     XNOP      
  104|                              CL.183:
  104| 001388 stfdux   7C1C1DEE   1     STFDU     gr28,v3(gr28,gr3,0)=fp0
  104| 00138C bc       4200FFFC   1     BCT       ctr=CL.183,taken=100%(100,0)
  104| 001390 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 001394 add      7F5AF214   1     A         gr26=gr26,gr30
  104| 001398 cmpld    7CBD0040   1     CL8       cr1=gr29,gr0
  104| 00139C bc       4184FFE0   1     BT        CL.182,cr1,0x8/llt,taken=80%(80,20)
  104|                              CL.185:
  104| 0013A0 addi     39290001   1     AI        gr9=gr9,1
    0| 0013A4 add      7F6CDA14   1     A         gr27=gr12,gr27
  104| 0013A8 cmpd     7CA84800   1     C8        cr1=gr8,gr9
  104| 0013AC bc       4185FFC4   1     BT        CL.181,cr1,0x2/gt,taken=80%(80,20)
  104|                              CL.186:
  104| 0013B0 cmpd     7CAB4000   1     C8        cr1=gr11,gr8
  104| 0013B4 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  104| 0013B8 bc       40810100   1     BF        CL.155,cr0,0x2/gt,taken=50%(0,0)
  104| 0013BC ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  104| 0013C0 addi     3884FFFF   1     AI        gr4=gr4,-1
    0| 0013C4 cmpdi    2C200000   1     C8        cr0=gr0,0
  104| 0013C8 sradi    7C841674   1     SRA8CA    gr4,ca=gr4,2
  104| 0013CC ld       E93D02A0   1     L8        gr9=<s35:d672:l8>(gr29,672)
  104| 0013D0 ld       E87D02D0   1     L8        gr3=<s35:d720:l8>(gr29,720)
  104| 0013D4 ld       E97D02B8   1     L8        gr11=<s35:d696:l8>(gr29,696)
  104| 0013D8 ld       E99D0270   1     L8        gr12=<s35:d624:l8>(gr29,624)
  104| 0013DC ld       EBDD0288   1     L8        gr30=<s35:d648:l8>(gr29,648)
    0| 0013E0 mulld    7CA549D2   1     M         gr5=gr5,gr9
    0| 0013E4 mulld    7CE719D2   1     M         gr7=gr7,gr3
    0| 0013E8 mulld    7CC659D2   1     M         gr6=gr6,gr11
    0| 0013EC subf     7D836050   1     S         gr12=gr12,gr3
    0| 0013F0 add      7CA53A14   1     A         gr5=gr5,gr7
    0| 0013F4 mulld    7CE849D2   1     M         gr7=gr8,gr9
    0| 0013F8 add      7D0CF214   1     A         gr8=gr12,gr30
    0| 0013FC add      7CA53214   1     A         gr5=gr5,gr6
    0| 001400 rldicr   792C1764   1     SLL8      gr12=gr9,2
    0| 001404 add      7CC54214   1     A         gr6=gr5,gr8
  104| 001408 addze    7CA40194   1     ADDE      gr5,ca=gr4,0,ca
    0| 00140C add      7FC63A14   1     A         gr30=gr6,gr7
    0| 001410 subf     7CE96050   1     S         gr7=gr12,gr9
    0| 001414 rldicr   79260FA4   1     SLL8      gr6=gr9,1
  104| 001418 addi     38800000   1     LI        gr4=0
    0| 00141C bc       4081009C   1     BF        CL.155,cr0,0x2/gt,taken=20%(20,80)
    0| 001420 addi     3B450001   1     AI        gr26=gr5,1
    0| 001424 ld       E8A20000   1     L8        gr5=.+CONSTANT_AREA(gr2,0)
    0| 001428 add      7FA9F214   1     A         gr29=gr9,gr30
    0| 00142C add      7F87F214   1     A         gr28=gr7,gr30
    0| 001430 add      7F66F214   1     A         gr27=gr6,gr30
    0| 001434 cmpdi    2C2A0000   1     C8        cr0=gr10,0
    0| 001438 lfs      C005006C   1     LFS       fp0=+CONSTANT_AREA(gr5,108)
  104|                              CL.156:
  104| 00143C addi     38A00000   1     LI        gr5=0
    0| 001440 bc       4081005C   1     BF        CL.157,cr0,0x2/gt,taken=20%(20,80)
    0| 001444 or       7FD9F378   1     LR        gr25=gr30
    0| 001448 or       7F98E378   1     LR        gr24=gr28
    0| 00144C or       7F77DB78   1     LR        gr23=gr27
    0| 001450 or       7FB6EB78   1     LR        gr22=gr29
  104|                              CL.158:
  104| 001454 or       7F06C378   1     LR        gr6=gr24
  104| 001458 or       7EE7BB78   1     LR        gr7=gr23
  104| 00145C or       7EC8B378   1     LR        gr8=gr22
  104| 001460 or       7F29CB78   1     LR        gr9=gr25
    0| 001464 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 001468 ori      60210000   1     XNOP      
    0|                              CL.1149:
  104| 00146C stfdux   7C091DEE   1     STFDU     gr9,v3(gr9,gr3,0)=fp0
  104| 001470 stfdux   7C081DEE   1     STFDU     gr8,v3(gr8,gr3,0)=fp0
  104| 001474 stfdux   7C071DEE   1     STFDU     gr7,v3(gr7,gr3,0)=fp0
  104| 001478 stfdux   7C061DEE   1     STFDU     gr6,v3(gr6,gr3,0)=fp0
    0| 00147C bc       4200FFF0   1     BCT       ctr=CL.1149,taken=100%(100,0)
  104| 001480 addi     38A50001   1     AI        gr5=gr5,1
    0| 001484 add      7F2BCA14   1     A         gr25=gr11,gr25
  104| 001488 cmpld    7CA50040   1     CL8       cr1=gr5,gr0
    0| 00148C add      7F0BC214   1     A         gr24=gr11,gr24
    0| 001490 add      7EEBBA14   1     A         gr23=gr11,gr23
    0| 001494 add      7ECBB214   1     A         gr22=gr11,gr22
  104| 001498 bc       4184FFBC   1     BT        CL.158,cr1,0x8/llt,taken=80%(80,20)
  104|                              CL.157:
  104| 00149C addi     38840001   1     AI        gr4=gr4,1
    0| 0014A0 add      7FACEA14   1     A         gr29=gr12,gr29
  104| 0014A4 cmpld    7CA4D040   1     CL8       cr1=gr4,gr26
    0| 0014A8 add      7F8CE214   1     A         gr28=gr12,gr28
    0| 0014AC add      7F6CDA14   1     A         gr27=gr12,gr27
    0| 0014B0 add      7FCCF214   1     A         gr30=gr12,gr30
  104| 0014B4 bc       4184FF88   1     BT        CL.156,cr1,0x8/llt,taken=80%(80,20)
  104|                              CL.155:
  106| 0014B8 ld       E80102A8   1     L8        gr0=#SPILL2(gr1,680)
  106| 0014BC cmpdi    2C200000   1     C8        cr0=gr0,0
  106| 0014C0 bc       41820010   1     BT        CL.79,cr0,0x4/eq,taken=60%(60,40)
  106| 0014C4 or       7C030378   1     LR        gr3=gr0
  106| 0014C8 bl       48000001   1     CALL      free,1,pert",gr3,free",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  106| 0014CC ori      60000000   1
  106|                              CL.79:
  107| 0014D0 ld       E8620000   1     L8        gr3=.&&N&&mpipar(gr2,0)
  107| 0014D4 lwz      80030008   1     L4Z       gr0=<s119:d8:l4>(gr3,8)
  107| 0014D8 cmpdi    2C200000   1     C8        cr0=gr0,0
  107| 0014DC bc       418200B0   1     BT        CL.1549,cr0,0x4/eq,taken=40%(40,60)
  111|                              CL.116:
  111| 0014E0 ld       E8010408   1     L8        gr0=#SPILL46(gr1,1032)
  111| 0014E4 cmpdi    2C200000   1     C8        cr0=gr0,0
  111| 0014E8 bc       40820094   1     BF        CL.1550,cr0,0x4/eq,taken=40%(40,60)
   59|                              CL.248:
  112| 0014EC ld       E8010510   1     L8        gr0=#stack(gr1,1296)
  112| 0014F0 lwa      E981050A   1     L4A       gr12=#stack(gr1,1288)
  112| 0014F4 lfd      CBE104F8   1     LFL       fp31=#stack(gr1,1272)
  112| 0014F8 lfd      CBC104F0   1     LFL       fp30=#stack(gr1,1264)
  112| 0014FC lfd      CBA104E8   1     LFL       fp29=#stack(gr1,1256)
  112| 001500 lfd      CB8104E0   1     LFL       fp28=#stack(gr1,1248)
  112| 001504 lfd      CB6104D8   1     LFL       fp27=#stack(gr1,1240)
  112| 001508 lfd      CB4104D0   1     LFL       fp26=#stack(gr1,1232)
  112| 00150C lfd      CB2104C8   1     LFL       fp25=#stack(gr1,1224)
  112| 001510 lfd      CB0104C0   1     LFL       fp24=#stack(gr1,1216)
  112| 001514 lfd      CAE104B8   1     LFL       fp23=#stack(gr1,1208)
  112| 001518 lfd      CAC104B0   1     LFL       fp22=#stack(gr1,1200)
  112| 00151C addi     38210500   1     AI        gr1=gr1,1280
  112| 001520 mtspr    7C0803A6   1     LLR       lr=gr0
  112| 001524 mtcrf    7D820120   1     MTCRF     cr2=gr12
  112| 001528 mtcrf    7D810120   1     MTCRF     cr3=gr12
  112| 00152C mtcrf    7D808120   1     MTCRF     cr4=gr12
  112| 001530 ld       E9C1FF20   1     L8        gr14=#stack(gr1,-224)
  112| 001534 ld       E9E1FF28   1     L8        gr15=#stack(gr1,-216)
  112| 001538 ld       EA01FF30   1     L8        gr16=#stack(gr1,-208)
  112| 00153C ld       EA21FF38   1     L8        gr17=#stack(gr1,-200)
  112| 001540 ld       EA41FF40   1     L8        gr18=#stack(gr1,-192)
  112| 001544 ld       EA61FF48   1     L8        gr19=#stack(gr1,-184)
  112| 001548 ld       EA81FF50   1     L8        gr20=#stack(gr1,-176)
  112| 00154C ld       EAA1FF58   1     L8        gr21=#stack(gr1,-168)
  112| 001550 ld       EAC1FF60   1     L8        gr22=#stack(gr1,-160)
  112| 001554 ld       EAE1FF68   1     L8        gr23=#stack(gr1,-152)
  112| 001558 ld       EB01FF70   1     L8        gr24=#stack(gr1,-144)
  112| 00155C ld       EB21FF78   1     L8        gr25=#stack(gr1,-136)
  112| 001560 ld       EB41FF80   1     L8        gr26=#stack(gr1,-128)
  112| 001564 ld       EB61FF88   1     L8        gr27=#stack(gr1,-120)
  112| 001568 ld       EB81FF90   1     L8        gr28=#stack(gr1,-112)
  112| 00156C ld       EBA1FF98   1     L8        gr29=#stack(gr1,-104)
  112| 001570 ld       EBC1FFA0   1     L8        gr30=#stack(gr1,-96)
  112| 001574 ld       EBE1FFA8   1     L8        gr31=#stack(gr1,-88)
  112| 001578 bclr     4E800020   1     BA        lr
    0|                              CL.1550:
  111| 00157C or       7C030378   1     LR        gr3=gr0
  111| 001580 bl       48000001   1     CALL      free,1,rannum",gr3,free",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  111| 001584 ori      60000000   1
    0| 001588 b        4BFFFF64   1     B         CL.248,-1
    0|                              CL.1549:
  108| 00158C ld       EB220000   1     L8        gr25=.$STATIC(gr2,0)
  108| 001590 addi     3B400000   1     LI        gr26=0
  108| 001594 addi     38600006   1     LI        gr3=6
  108| 001598 ori      635E8000   1     OIL       gr30=gr26,0x8000
  108| 00159C addi     38800101   1     LI        gr4=257
  108| 0015A0 or       7FC6F378   1     LR        gr6=gr30
  108| 0015A4 addi     38B900E0   1     AI        gr5=gr25,224
  108| 0015A8 addi     38E00000   1     LI        gr7=0
  108| 0015AC addi     39000000   1     LI        gr8=0
  108| 0015B0 addi     39200000   1     LI        gr9=0
  108| 0015B4 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#7",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  108| 0015B8 ori      60000000   1
  108| 0015BC ld       EB020000   1     L8        gr24=.+CONSTANT_AREA(gr2,0)
  108| 0015C0 or       7C7D1B78   1     LR        gr29=gr3
  108| 0015C4 addi     38A00011   1     LI        gr5=17
  108| 0015C8 addi     38C00001   1     LI        gr6=1
  108| 0015CC addi     38980070   1     AI        gr4=gr24,112
  108| 0015D0 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  108| 0015D4 ori      60000000   1
  108| 0015D8 or       7FA3EB78   1     LR        gr3=gr29
  108| 0015DC bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  108| 0015E0 ori      60000000   1
  109| 0015E4 addi     38B90120   1     AI        gr5=gr25,288
  109| 0015E8 addi     38600006   1     LI        gr3=6
  109| 0015EC addi     38800101   1     LI        gr4=257
  109| 0015F0 or       7FC6F378   1     LR        gr6=gr30
  109| 0015F4 addi     38E00000   1     LI        gr7=0
  109| 0015F8 addi     39000000   1     LI        gr8=0
  109| 0015FC addi     39200000   1     LI        gr9=0
  109| 001600 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#9",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 001604 ori      60000000   1
  109| 001608 ld       EAE20000   1     L8        gr23=.&&N&field(gr2,0)
  109| 00160C lfd      C8180088   1     LFL       fp0=+CONSTANT_AREA(gr24,136)
  109| 001610 or       7C7D1B78   1     LR        gr29=gr3
  109| 001614 ld       EBD70028   1     L8        gr30=<s35:d40:l8>(gr23,40)
  109| 001618 ld       EB970040   1     L8        gr28=<s35:d64:l8>(gr23,64)
  109| 00161C ld       EB770058   1     L8        gr27=<s35:d88:l8>(gr23,88)
  109| 001620 stfd     D80100D0   1     STFL      T_40(gr1,208)=fp0
  109| 001624 cmpdi    2C3E0000   1     C8        cr0=gr30,0
  109| 001628 bc       4081021C   1     BF        CL.408,cr0,0x2/gt,taken=40%(40,60)
  109| 00162C or       7EE4BB78   1     LR        gr4=gr23
    0| 001630 cmpdi    2C3C0000   1     C8        cr0=gr28,0
  109| 001634 ld       EB570030   1     L8        gr26=<s35:d48:l8>(gr23,48)
  109| 001638 ld       EB370048   1     L8        gr25=<s35:d72:l8>(gr23,72)
  109| 00163C ld       EB170060   1     L8        gr24=<s35:d96:l8>(gr23,96)
  109| 001640 ld       EAF70000   1     L8        gr23=<s35:d0:l8>(gr23,0)
  109| 001644 ld       EAC40018   1     L8        gr22=<s35:d24:l8>(gr4,24)
  109| 001648 addi     38800000   1     LI        gr4=0
    0| 00164C bc       408101D4   1     BF        CL.1083,cr0,0x2/gt,taken=40%(40,60)
    0| 001650 ld       E9010298   1     L8        gr8=#SPILL0(gr1,664)
    0| 001654 ld       E92102C8   1     L8        gr9=#SPILL6(gr1,712)
    0| 001658 mulld    7CB8F9D2   1     M         gr5=gr24,gr31
    0| 00165C mulld    7CE8D1D2   1     M         gr7=gr8,gr26
    0| 001660 mulld    7CC9C9D2   1     M         gr6=gr9,gr25
    0| 001664 add      7C16BA14   1     A         gr0=gr22,gr23
    0| 001668 add      7CA53214   1     A         gr5=gr5,gr6
    0| 00166C add      7CC03A14   1     A         gr6=gr0,gr7
    0| 001670 cmpdi    2C3B0000   1     C8        cr0=gr27,0
    0| 001674 subf     7CD83050   1     S         gr6=gr6,gr24
    0| 001678 or       7FEAFB78   1     LR        gr10=gr31
    0| 00167C add      7D653214   1     A         gr11=gr5,gr6
    0| 001680 bc       40820008   1     BF        CL.873,cr0,0x4/eq,taken=50%(0,0)
    0| 001684 addi     39400001   1     LI        gr10=1
    0|                              CL.873:
    0| 001688 mulld    7D89C9D2   1     M         gr12=gr9,gr25
    0| 00168C or       7CF53B78   1     LR        gr21=gr7
  109|                              CL.162:
  109| 001690 addi     38A00000   1     LI        gr5=0
    0| 001694 bc       4081007C   1     BF        CL.163,cr0,0x2/gt,taken=20%(20,80)
    0| 001698 or       7C140378   1     LR        gr20=gr0
    0| 00169C or       7D735B78   1     LR        gr19=gr11
    0| 0016A0 ori      60210000   1     XNOP      
    0| 0016A4 ori      60210000   1     XNOP      
  109|                              CL.164:
  109| 0016A8 or       7E669B78   1     LR        gr6=gr19
    0| 0016AC mtspr    7F6903A6   1     LCTR      ctr=gr27
  109| 0016B0 lfdux    7C26C4EE   1     LFDU      fp1,gr6=d(gr6,gr24,0)
  109| 0016B4 or       7D525378   1     LR        gr18=gr10
    0| 0016B8 add      7CECA214   1     A         gr7=gr12,gr20
  109| 0016BC addi     39000001   1     LI        gr8=1
    0| 0016C0 bc       42400028   1     BCF       ctr=CL.1242,taken=0%(0,100)
  109|                              CL.166:
  109| 0016C4 mulld    7D32C1D2   1     M         gr9=gr18,gr24
  109| 0016C8 fcmpu    FC810000   1     CFL       cr1=fp1,fp0
  109| 0016CC add      7E485214   1     A         gr18=gr8,gr10
  109| 0016D0 add      7D29AA14   1     A         gr9=gr9,gr21
  109| 0016D4 lfdux    7C26C4EE   1     LFDU      fp1,gr6=d(gr6,gr24,0)
  109| 0016D8 addi     39080001   1     AI        gr8=gr8,1
  109| 0016DC bc       40850008   1     BF        CL.84,cr1,0x40/fgt,taken=50%(0,0)
  109| 0016E0 lfdx     7C074CAE   1     LFL       fp0=d(gr7,gr9,0)
  109|                              CL.84:
    0| 0016E4 bc       4200FFE0   1     BCT       ctr=CL.166,taken=100%(100,0)
    0|                              CL.1242:
  109| 0016E8 mulld    7CD2C1D2   1     M         gr6=gr18,gr24
  109| 0016EC fcmpu    FC810000   1     CFL       cr1=fp1,fp0
  109| 0016F0 addi     38A50001   1     AI        gr5=gr5,1
  109| 0016F4 add      7CC6AA14   1     A         gr6=gr6,gr21
  109| 0016F8 bc       40850008   1     BF        CL.1190,cr1,0x40/fgt,taken=50%(0,0)
  109| 0016FC lfdx     7C0734AE   1     LFL       fp0=d(gr7,gr6,0)
    0|                              CL.1190:
  109| 001700 cmpld    7CA5E040   1     CL8       cr1=gr5,gr28
    0| 001704 add      7E94CA14   1     A         gr20=gr20,gr25
    0| 001708 add      7E73CA14   1     A         gr19=gr19,gr25
  109| 00170C bc       4184FF9C   1     BT        CL.164,cr1,0x8/llt,taken=80%(80,20)
  109|                              CL.163:
  109| 001710 addi     38840001   1     AI        gr4=gr4,1
    0| 001714 add      7C00D214   1     A         gr0=gr0,gr26
  109| 001718 cmpld    7CA4F040   1     CL8       cr1=gr4,gr30
    0| 00171C add      7D6BD214   1     A         gr11=gr11,gr26
  109| 001720 bc       4184FF70   1     BT        CL.162,cr1,0x8/llt,taken=80%(80,20)
  109| 001724 stfd     D80100D0   1     STFL      T_40(gr1,208)=fp0
  109| 001728 addi     388100D0   1     AI        gr4=gr1,208
  109| 00172C addi     38A00008   1     LI        gr5=8
  109| 001730 addi     38C00008   1     LI        gr6=8
  109| 001734 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,T_40,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 001738 ori      60000000   1
    0| 00173C ld       E8E10298   1     L8        gr7=#SPILL0(gr1,664)
    0| 001740 ld       E90102C8   1     L8        gr8=#SPILL6(gr1,712)
    0| 001744 mulld    7C98F9D2   1     M         gr4=gr24,gr31
    0| 001748 mulld    7CC7D1D2   1     M         gr6=gr7,gr26
    0| 00174C mulld    7CA8C9D2   1     M         gr5=gr8,gr25
    0| 001750 add      7C16BA14   1     A         gr0=gr22,gr23
  109| 001754 ld       EAA20000   1     L8        gr21=.+CONSTANT_AREA(gr2,0)
    0| 001758 add      7C842A14   1     A         gr4=gr4,gr5
    0| 00175C add      7CA03214   1     A         gr5=gr0,gr6
    0| 001760 cmpdi    2C3B0000   1     C8        cr0=gr27,0
    0| 001764 subf     7CB82850   1     S         gr5=gr5,gr24
  109| 001768 addi     38600000   1     LI        gr3=0
  109| 00176C lfd      C8150090   1     LFL       fp0=+CONSTANT_AREA(gr21,144)
    0| 001770 add      7C842A14   1     A         gr4=gr4,gr5
    0| 001774 bc       40820008   1     BF        CL.876,cr0,0x4/eq,taken=50%(0,0)
    0| 001778 addi     3BE00001   1     LI        gr31=1
    0|                              CL.876:
    0| 00177C mulld    7CA8C9D2   1     M         gr5=gr8,gr25
    0| 001780 or       7CCB3378   1     LR        gr11=gr6
  109|                              CL.168:
  109| 001784 addi     38C00000   1     LI        gr6=0
    0| 001788 bc       40810080   1     BF        CL.169,cr0,0x2/gt,taken=20%(20,80)
    0| 00178C or       7C0C0378   1     LR        gr12=gr0
    0| 001790 or       7C972378   1     LR        gr23=gr4
  109|                              CL.170:
  109| 001794 or       7EE7BB78   1     LR        gr7=gr23
    0| 001798 mtspr    7F6903A6   1     LCTR      ctr=gr27
  109| 00179C lfdux    7C27C4EE   1     LFDU      fp1,gr7=d(gr7,gr24,0)
  109| 0017A0 or       7FF6FB78   1     LR        gr22=gr31
    0| 0017A4 add      7D056214   1     A         gr8=gr5,gr12
  109| 0017A8 addi     39200001   1     LI        gr9=1
    0| 0017AC bc       42400034   1     BCF       ctr=CL.1244,taken=0%(0,100)
    0| 0017B0 ori      60210000   1     XNOP      
    0| 0017B4 ori      60210000   1     XNOP      
    0| 0017B8 ori      60210000   1     XNOP      
  109|                              CL.172:
  109| 0017BC mulld    7D56C1D2   1     M         gr10=gr22,gr24
  109| 0017C0 fcmpu    FC810000   1     CFL       cr1=fp1,fp0
  109| 0017C4 add      7EC9FA14   1     A         gr22=gr9,gr31
  109| 0017C8 add      7D4A5A14   1     A         gr10=gr10,gr11
  109| 0017CC lfdux    7C27C4EE   1     LFDU      fp1,gr7=d(gr7,gr24,0)
  109| 0017D0 addi     39290001   1     AI        gr9=gr9,1
  109| 0017D4 bc       40840008   1     BF        CL.97,cr1,0x20/flt,taken=50%(0,0)
  109| 0017D8 lfdx     7C0854AE   1     LFL       fp0=d(gr8,gr10,0)
  109|                              CL.97:
    0| 0017DC bc       4200FFE0   1     BCT       ctr=CL.172,taken=100%(100,0)
    0|                              CL.1244:
  109| 0017E0 mulld    7CF6C1D2   1     M         gr7=gr22,gr24
  109| 0017E4 fcmpu    FC810000   1     CFL       cr1=fp1,fp0
  109| 0017E8 addi     38C60001   1     AI        gr6=gr6,1
  109| 0017EC add      7CE75A14   1     A         gr7=gr7,gr11
  109| 0017F0 bc       40840008   1     BF        CL.1194,cr1,0x20/flt,taken=50%(0,0)
  109| 0017F4 lfdx     7C083CAE   1     LFL       fp0=d(gr8,gr7,0)
    0|                              CL.1194:
  109| 0017F8 cmpld    7CA6E040   1     CL8       cr1=gr6,gr28
    0| 0017FC add      7D8CCA14   1     A         gr12=gr12,gr25
    0| 001800 add      7EF7CA14   1     A         gr23=gr23,gr25
  109| 001804 bc       4184FF90   1     BT        CL.170,cr1,0x8/llt,taken=80%(80,20)
  109|                              CL.169:
  109| 001808 addi     38630001   1     AI        gr3=gr3,1
    0| 00180C add      7C00D214   1     A         gr0=gr0,gr26
  109| 001810 cmpld    7CA3F040   1     CL8       cr1=gr3,gr30
    0| 001814 add      7C84D214   1     A         gr4=gr4,gr26
  109| 001818 bc       4184FF6C   1     BT        CL.168,cr1,0x8/llt,taken=80%(80,20)
    0| 00181C b        48000020   1     B         CL.500,-1
    0|                              CL.1083:
  109| 001820 addi     388100D0   1     AI        gr4=gr1,208
  109| 001824 addi     38A00008   1     LI        gr5=8
  109| 001828 addi     38C00008   1     LI        gr6=8
  109| 00182C bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,T_40,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 001830 ori      60000000   1
  109| 001834 ld       EBE20000   1     L8        gr31=.+CONSTANT_AREA(gr2,0)
  109| 001838 lfd      C81F0090   1     LFL       fp0=+CONSTANT_AREA(gr31,144)
    0|                              CL.500:
  109| 00183C stfd     D80100D8   1     STFL      T_44(gr1,216)=fp0
  109| 001840 b        48000020   1     B         CL.167,-1
  109|                              CL.408:
  109| 001844 addi     388100D0   1     AI        gr4=gr1,208
  109| 001848 addi     38A00008   1     LI        gr5=8
  109| 00184C addi     38C00008   1     LI        gr6=8
  109| 001850 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,T_40,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 001854 ori      60000000   1
  109| 001858 lfd      C8180090   1     LFL       fp0=+CONSTANT_AREA(gr24,144)
  109| 00185C stfd     D80100D8   1     STFL      T_44(gr1,216)=fp0
  109|                              CL.167:
  109| 001860 or       7FA3EB78   1     LR        gr3=gr29
  109| 001864 addi     388100D8   1     AI        gr4=gr1,216
  109| 001868 addi     38A00008   1     LI        gr5=8
  109| 00186C addi     38C00008   1     LI        gr6=8
  109| 001870 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,T_44,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 001874 ori      60000000   1
  109| 001878 or       7FA3EB78   1     LR        gr3=gr29
  109| 00187C bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 001880 ori      60000000   1
    0| 001884 b        4BFFFC5C   1     B         CL.116,-1
  104|                              CL.763:
    0| 001888 mtspr    7D0903A6   1     LCTR      ctr=gr8
  104|                              CL.546:
  104| 00188C addi     39290001   1     AI        gr9=gr9,1
  104| 001890 cmpd     7CA94000   1     C8        cr1=gr9,gr8
  104| 001894 bc       4104FFF8   1     BCTT      ctr=CL.546,cr1,0x1/lt,taken=80%(80,20)
    0| 001898 b        4BFFFB18   1     B         CL.186,-1
  103|                              CL.753:
    0| 00189C mtspr    7D2903A6   1     LCTR      ctr=gr9
  103|                              CL.576:
  103| 0018A0 addi     394A0001   1     AI        gr10=gr10,1
  103| 0018A4 cmpd     7CAA4800   1     C8        cr1=gr10,gr9
  103| 0018A8 bc       4104FFF8   1     BCTT      ctr=CL.576,cr1,0x1/lt,taken=80%(80,20)
    0| 0018AC b        4BFFF7EC   1     B         CL.198,-1
    0|                              CL.744:
   97| 0018B0 lfdu     CC4C0008   1     LFDU      fp2,gr12=pert(gr12,8)
   97| 0018B4 lfdu     CC7F0008   1     LFDU      fp3,gr31=pert(gr31,8)
   96| 0018B8 stfdux   7F9E1DEE   1     STFDU     gr30,d(gr30,gr3,0)=fp28
   96| 0018BC or       7EC8B378   1     LR        gr8=gr22
   99| 0018C0 or       7E298B78   1     LR        gr9=gr17
   96| 0018C4 stfdux   7F881DEE   1     STFDU     gr8,d(gr8,gr3,0)=fp28
   97| 0018C8 or       7E6A9B78   1     LR        gr10=gr19
   97| 0018CC fadd     FC5E102A   1     AFL       fp2=fp30,fp2,fcr
   99| 0018D0 or       7E0B8378   2     LR        gr11=gr16
   97| 0018D4 fadd     FC7E182A   1     AFL       fp3=fp30,fp3,fcr
   97| 0018D8 or       7E5D9378   2     LR        gr29=gr18
    0| 0018DC bc       42400044   1     BCF       ctr=CL.1143,taken=0%(0,100)
    0| 0018E0 ori      60210000   1     XNOP      
    0| 0018E4 ori      60210000   1     XNOP      
    0| 0018E8 ori      60210000   1     XNOP      
    0|                              CL.1144:
   97| 0018EC lfdu     CC8C0008   1     LFDU      fp4,gr12=pert(gr12,8)
   97| 0018F0 lfdu     CCBF0008   1     LFDU      fp5,gr31=pert(gr31,8)
   97| 0018F4 fmul     FCDC00B2   1     MFL       fp6=fp28,fp2,fcr
   97| 0018F8 fmul     FCFC00F2   2     MFL       fp7=fp28,fp3,fcr
   96| 0018FC stfdux   7F9E1DEE   2     STFDU     gr30,d(gr30,gr3,0)=fp28
   99| 001900 stfdux   7D0925EE   1     STFDU     gr9,e(gr9,gr4,0)=fp8
   99| 001904 stfdux   7D0B25EE   1     STFDU     gr11,e(gr11,gr4,0)=fp8
   97| 001908 fadd     FC5E202A   1     AFL       fp2=fp30,fp4,fcr
   97| 00190C fadd     FC7E282A   2     AFL       fp3=fp30,fp5,fcr
   97| 001910 stfdux   7CCA2DEE   1     STFDU     gr10,v1(gr10,gr5,0)=fp6
   97| 001914 stfdux   7CFD2DEE   1     STFDU     gr29,v1(gr29,gr5,0)=fp7
   96| 001918 stfdux   7F881DEE   1     STFDU     gr8,d(gr8,gr3,0)=fp28
    0| 00191C bc       4200FFD0   1     BCT       ctr=CL.1144,taken=100%(100,0)
    0|                              CL.1143:
   97| 001920 fmul     FC5C00B2   1     MFL       fp2=fp28,fp2,fcr
   99| 001924 stfdux   7D0925EE   1     STFDU     gr9,e(gr9,gr4,0)=fp8
   97| 001928 fmul     FC7C00F2   1     MFL       fp3=fp28,fp3,fcr
   99| 00192C stfdux   7D0B25EE   1     STFDU     gr11,e(gr11,gr4,0)=fp8
   97| 001930 stfdux   7C4A2DEE   1     STFDU     gr10,v1(gr10,gr5,0)=fp2
   97| 001934 stfdux   7C7D2DEE   1     STFDU     gr29,v1(gr29,gr5,0)=fp3
    0| 001938 b        4BFFF550   1     B         CL.147,-1
    0|                              CL.738:
   97| 00193C lfdu     CC4B0008   1     LFDU      fp2,gr11=pert(gr11,8)
   96| 001940 stfdux   7F8C2DEE   1     STFDU     gr12,d(gr12,gr5,0)=fp28
   97| 001944 fadd     FC5E102A   1     AFL       fp2=fp30,fp2,fcr
    0| 001948 bc       42400028   1     BCF       ctr=CL.1139,taken=0%(0,100)
    0| 00194C ori      60210000   1     XNOP      
    0| 001950 ori      60210000   1     XNOP      
    0|                              CL.1140:
   97| 001954 lfdu     CC6B0008   1     LFDU      fp3,gr11=pert(gr11,8)
   97| 001958 fmul     FC9C00B2   1     MFL       fp4=fp28,fp2,fcr
   99| 00195C stfdux   7CBF35EE   1     STFDU     gr31,e(gr31,gr6,0)=fp5
   97| 001960 fadd     FC5E182A   1     AFL       fp2=fp30,fp3,fcr
   97| 001964 stfdux   7C9E3DEE   1     STFDU     gr30,v1(gr30,gr7,0)=fp4
   96| 001968 stfdux   7F8C2DEE   1     STFDU     gr12,d(gr12,gr5,0)=fp28
    0| 00196C bc       4200FFE8   1     BCT       ctr=CL.1140,taken=100%(100,0)
    0|                              CL.1139:
   99| 001970 stfdux   7CBF35EE   1     STFDU     gr31,e(gr31,gr6,0)=fp5
   97| 001974 fmul     FC5C00B2   1     MFL       fp2=fp28,fp2,fcr
   97| 001978 stfdux   7C5E3DEE   2     STFDU     gr30,v1(gr30,gr7,0)=fp2
    0| 00197C b        4BFFF280   1     B         CL.214,-1
  101|                              CL.742:
    0| 001980 mtspr    7C6903A6   1     LCTR      ctr=gr3
  101|                              CL.623:
  102| 001984 addi     38840001   1     AI        gr4=gr4,1
  102| 001988 cmpd     7CA41800   1     C8        cr1=gr4,gr3
  102| 00198C bc       4104FFF8   1     BCTT      ctr=CL.623,cr1,0x1/lt,taken=80%(80,20)
    0| 001990 b        4BFFF2AC   1     B         CL.216,-1
   84|                              CL.731:
    0| 001994 mtspr    7D0903A6   1     LCTR      ctr=gr8
   84|                              CL.660:
   84| 001998 addi     39290001   1     AI        gr9=gr9,1
   84| 00199C cmpd     7CA94000   1     C8        cr1=gr9,gr8
   84| 0019A0 bc       4104FFF8   1     BCTT      ctr=CL.660,cr1,0x1/lt,taken=80%(80,20)
    0| 0019A4 b        4BFFEF9C   1     B         CL.231,-1
   70|                              CL.716:
    0| 0019A8 mtspr    7F6903A6   1     LCTR      ctr=gr27
   70|                              CL.705:
   71| 0019AC addi     3BBD0001   1     AI        gr29=gr29,1
   71| 0019B0 cmpd     7C3DD800   1     C8        cr0=gr29,gr27
   71| 0019B4 bc       4100FFF8   1     BCTT      ctr=CL.705,cr0,0x1/lt,taken=80%(80,20)
    0| 0019B8 b        4BFFE948   1     B         CL.243,-1
   60|                              CL.11:
   60| 0019BC or       7CA32B78   1     LR        gr3=gr5
   60| 0019C0 addi     38800020   1     LI        gr4=32
   60| 0019C4 bl       48000001   1     CALL      gr3=__xlf_malloc,2,gr3,gr4,rannum",__xlf_malloc",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   60| 0019C8 ori      60000000   1
   60| 0019CC or       7C7B1B79   1     LR_R      gr27,cr0=gr3
   60| 0019D0 std      FB610408   1     ST8       #SPILL46(gr1,1032)=gr27
   60| 0019D4 bc       4082E7F8   1     BF        CL.12,cr0,0x4/eq,taken=90%(90,10)
   60| 0019D8 addi     3BC00000   1     LI        gr30=0
   60| 0019DC std      FBA100E8   1     ST8       <a1:d232:l8>(gr1,232)=gr29
   60| 0019E0 ld       EBE20000   1     L8        gr31=.+CONSTANT_AREA(gr2,0)
   60| 0019E4 addi     392100E0   1     AI        gr9=gr1,224
   60| 0019E8 std      FBC10070   1     ST8       #MX_TEMP1(gr1,112)=gr30
   60| 0019EC addi     38600000   1     LI        gr3=0
   60| 0019F0 addi     38800003   1     LI        gr4=3
   60| 0019F4 addi     38A0006C   1     LI        gr5=108
   60| 0019F8 std      FBE100E0   1     ST8       <a1:d224:l8>(gr1,224)=gr31
   60| 0019FC addi     38C00000   1     LI        gr6=0
   60| 001A00 addi     38E00000   1     LI        gr7=0
   60| 001A04 addi     39000000   1     LI        gr8=0
   60| 001A08 addi     3940003C   1     LI        gr10=60
   60| 001A0C bl       48000001   1     CALL      _xlfErrorExitWithLoc,9,gr3-gr6,ShadowTemp_1.rns3.,gr7,ShadowTemp_1.rns3.,gr8,filename_3,gr9,gr10,ShadowTemp_1.rns3.,_xlfErrorExitWithLoc",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   60| 001A10 ori      60000000   1
   60| 001A14 tw       7C8E7008   1     TRAP      9
   59|                              CL.6:
   59| 001A18 addi     38800020   1     LI        gr4=32
   59| 001A1C bl       48000001   1     CALL      gr3=__xlf_malloc,2,gr3,gr4,pert",__xlf_malloc",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   59| 001A20 ori      60000000   1
   59| 001A24 or       7C7B1B79   1     LR_R      gr27,cr0=gr3
   59| 001A28 std      FB6102A8   1     ST8       #SPILL2(gr1,680)=gr27
   59| 001A2C bc       4082E770   1     BF        CL.9,cr0,0x4/eq,taken=90%(90,10)
   59| 001A30 addi     3BC00000   1     LI        gr30=0
   59| 001A34 std      FBA10108   1     ST8       <a1:d264:l8>(gr1,264)=gr29
   59| 001A38 ld       EBE20000   1     L8        gr31=.+CONSTANT_AREA(gr2,0)
   59| 001A3C addi     39210100   1     AI        gr9=gr1,256
   59| 001A40 addi     38600000   1     LI        gr3=0
   59| 001A44 std      FBC10070   1     ST8       #MX_TEMP1(gr1,112)=gr30
   59| 001A48 addi     38800003   1     LI        gr4=3
   59| 001A4C addi     38A0006C   1     LI        gr5=108
   59| 001A50 std      FBE10100   1     ST8       <a1:d256:l8>(gr1,256)=gr31
   59| 001A54 addi     38C00000   1     LI        gr6=0
   59| 001A58 addi     38E00000   1     LI        gr7=0
   59| 001A5C addi     39000000   1     LI        gr8=0
   59| 001A60 addi     3940003B   1     LI        gr10=59
   59| 001A64 bl       48000001   1     CALL      _xlfErrorExitWithLoc,9,gr3-gr6,ShadowTemp_1.rns3.,gr7,ShadowTemp_1.rns3.,gr8,filename_1,gr9,gr10,ShadowTemp_1.rns3.,_xlfErrorExitWithLoc",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   59| 001A68 ori      60000000   1
   59| 001A6C tw       7C8E7008   1     TRAP      9
    0|                              CL.1547:
    0| 001A70 addi     38000081   1     LI        gr0=129
    0| 001A74 ld       EB620000   1     L8        gr27=.+CONSTANT_AREA(gr2,0)
    0| 001A78 stw      90010120   1     ST4Z      <a1:d288:l4>(gr1,288)=gr0
    0| 001A7C addi     38000004   1     LI        gr0=4
    0| 001A80 addi     38800008   1     LI        gr4=8
    0| 001A84 std      F8010130   1     ST8       <a1:d304:l8>(gr1,304)=gr0
    0| 001A88 std      F8010140   1     ST8       <a1:d320:l8>(gr1,320)=gr0
    0| 001A8C std      F8810148   1     ST8       <a1:d328:l8>(gr1,328)=gr4
    0| 001A90 std      F8010170   1     ST8       <a1:d368:l8>(gr1,368)=gr0
    0| 001A94 std      F8810178   1     ST8       <a1:d376:l8>(gr1,376)=gr4
    0| 001A98 std      F80101A0   1     ST8       <a1:d416:l8>(gr1,416)=gr0
    0| 001A9C std      F88101A8   1     ST8       <a1:d424:l8>(gr1,424)=gr4
    0| 001AA0 std      F80101D0   1     ST8       <a1:d464:l8>(gr1,464)=gr0
    0| 001AA4 std      F88101D8   1     ST8       <a1:d472:l8>(gr1,472)=gr4
    0| 001AA8 std      F8010200   1     ST8       <a1:d512:l8>(gr1,512)=gr0
    0| 001AAC std      F8810208   1     ST8       <a1:d520:l8>(gr1,520)=gr4
    0| 001AB0 std      F8010220   1     ST8       <a1:d544:l8>(gr1,544)=gr0
    0| 001AB4 std      F8010230   1     ST8       <a1:d560:l8>(gr1,560)=gr0
    0| 001AB8 std      F8810238   1     ST8       <a1:d568:l8>(gr1,568)=gr4
    0| 001ABC addi     3809000C   1     AI        gr0=gr9,12
   33| 001AC0 ld       EBE20000   1     L8        gr31=.$STATIC(gr2,0)
    0| 001AC4 std      F8010158   1     ST8       <a1:d344:l8>(gr1,344)=gr0
    0| 001AC8 addi     380100C0   1     AI        gr0=gr1,192
    0| 001ACC addi     38690008   1     AI        gr3=gr9,8
    0| 001AD0 std      F8010258   1     ST8       <a1:d600:l8>(gr1,600)=gr0
    0| 001AD4 addi     38010098   1     AI        gr0=gr1,152
    0| 001AD8 std      F8810150   1     ST8       <a1:d336:l8>(gr1,336)=gr4
    0| 001ADC std      F8010168   1     ST8       <a1:d360:l8>(gr1,360)=gr0
    0| 001AE0 addi     380100A0   1     AI        gr0=gr1,160
    0| 001AE4 std      F8810180   1     ST8       <a1:d384:l8>(gr1,384)=gr4
    0| 001AE8 std      F88101B0   1     ST8       <a1:d432:l8>(gr1,432)=gr4
    0| 001AEC std      F88101E0   1     ST8       <a1:d480:l8>(gr1,480)=gr4
    0| 001AF0 std      F8810210   1     ST8       <a1:d528:l8>(gr1,528)=gr4
    0| 001AF4 std      F8810240   1     ST8       <a1:d576:l8>(gr1,576)=gr4
    0| 001AF8 std      F8810250   1     ST8       <a1:d592:l8>(gr1,592)=gr4
    0| 001AFC addi     38800002   1     LI        gr4=2
    0| 001B00 addi     38E90010   1     AI        gr7=gr9,16
    0| 001B04 std      F8010198   1     ST8       <a1:d408:l8>(gr1,408)=gr0
    0| 001B08 addi     38000007   1     LI        gr0=7
   33| 001B0C addi     3BC00000   1     LI        gr30=0
    0| 001B10 std      F8610128   1     ST8       <a1:d296:l8>(gr1,296)=gr3
    0| 001B14 addi     38600006   1     LI        gr3=6
    0| 001B18 std      F8810160   1     ST8       <a1:d352:l8>(gr1,352)=gr4
    0| 001B1C std      F8810190   1     ST8       <a1:d400:l8>(gr1,400)=gr4
    0| 001B20 addi     38C100A8   1     AI        gr6=gr1,168
    0| 001B24 addi     38890016   1     AI        gr4=gr9,22
    0| 001B28 std      F8E101B8   1     ST8       <a1:d440:l8>(gr1,440)=gr7
    0| 001B2C addi     38E100B0   1     AI        gr7=gr1,176
    0| 001B30 addi     3909001E   1     AI        gr8=gr9,30
    0| 001B34 addi     392100B8   1     AI        gr9=gr1,184
    0| 001B38 addi     395B0022   1     AI        gr10=gr27,34
    0| 001B3C std      F80101F0   1     ST8       <a1:d496:l8>(gr1,496)=gr0
    0| 001B40 std      F8E101F8   1     ST8       <a1:d504:l8>(gr1,504)=gr7
    0| 001B44 addi     381B000E   1     AI        gr0=gr27,14
   33| 001B48 ori      63DC8000   1     OIL       gr28=gr30,0x8000
    0| 001B4C std      F8610138   1     ST8       <a1:d312:l8>(gr1,312)=gr3
    0| 001B50 std      F86101C0   1     ST8       <a1:d448:l8>(gr1,448)=gr3
    0| 001B54 std      F8C101C8   1     ST8       <a1:d456:l8>(gr1,456)=gr6
   33| 001B58 addi     38BF0020   1     AI        gr5=gr31,32
    0| 001B5C std      F88101E8   1     ST8       <a1:d488:l8>(gr1,488)=gr4
   33| 001B60 addi     38800101   1     LI        gr4=257
   33| 001B64 or       7F86E378   1     LR        gr6=gr28
    0| 001B68 std      F9010218   1     ST8       <a1:d536:l8>(gr1,536)=gr8
    0| 001B6C std      F8010188   1     ST8       <a1:d392:l8>(gr1,392)=gr0
    0| 001B70 std      F9210228   1     ST8       <a1:d552:l8>(gr1,552)=gr9
    0| 001B74 std      F9410248   1     ST8       <a1:d584:l8>(gr1,584)=gr10
   33| 001B78 addi     38E00000   1     LI        gr7=0
   33| 001B7C addi     39000000   1     LI        gr8=0
   33| 001B80 addi     39200000   1     LI        gr9=0
   33| 001B84 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#1",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   33| 001B88 ori      60000000   1
   33| 001B8C or       7C7D1B78   1     LR        gr29=gr3
   33| 001B90 addi     389B0048   1     AI        gr4=gr27,72
   33| 001B94 addi     38A00012   1     LI        gr5=18
   33| 001B98 addi     38C00001   1     LI        gr6=1
   33| 001B9C bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   33| 001BA0 ori      60000000   1
   33| 001BA4 or       7FA3EB78   1     LR        gr3=gr29
   33| 001BA8 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   33| 001BAC ori      60000000   1
   39| 001BB0 stw      93C10124   1     ST4Z      <a1:d292:l4>(gr1,292)=gr30
   39| 001BB4 addi     38BF0060   1     AI        gr5=gr31,96
   39| 001BB8 addi     38600001   1     LI        gr3=1
   39| 001BBC addi     38800002   1     LI        gr4=2
   39| 001BC0 or       7F86E378   1     LR        gr6=gr28
   39| 001BC4 addi     38E00000   1     LI        gr7=0
   39| 001BC8 addi     39000000   1     LI        gr8=0
   39| 001BCC addi     39210120   1     AI        gr9=gr1,288
   39| 001BD0 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#3",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|pgen,gr9,#def_xlfBeginIO11",_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   39| 001BD4 ori      60000000   1
   39| 001BD8 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   39| 001BDC ori      60000000   1
   40| 001BE0 addi     38BF00A0   1     AI        gr5=gr31,160
   40| 001BE4 addi     38600002   1     LI        gr3=2
   40| 001BE8 addi     38800102   1     LI        gr4=258
   40| 001BEC or       7F86E378   1     LR        gr6=gr28
   40| 001BF0 addi     38E00000   1     LI        gr7=0
   40| 001BF4 addi     39000000   1     LI        gr8=0
   40| 001BF8 addi     39210120   1     AI        gr9=gr1,288
   40| 001BFC bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#5",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|pgen,gr9,#use_xlfBeginIO21,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   40| 001C00 ori      60000000   1
   40| 001C04 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   40| 001C08 ori      60000000   1
   41| 001C0C lfd      CBC10098   1     LFL       fp30=v0(gr1,152)
   42| 001C10 lfd      CBA100A0   1     LFL       fp29=p0(gr1,160)
   43| 001C14 lfd      CB8100A8   1     LFL       fp28=rho_in(gr1,168)
   44| 001C18 lfd      CB6100B0   1     LFL       fp27=rho_out(gr1,176)
   45| 001C1C lfd      CBE100B8   1     LFL       fp31=ampl(gr1,184)
   46| 001C20 lfd      CAC100C0   1     LFL       fp22=line_val(gr1,192)
   41| 001C24 ld       E9020000   1     L8        gr8=.&&N&&mpipar(gr2,0)
   41| 001C28 stfd     DBC806D0   1     STFL      <s119:d1744:l8>(gr8,1744)=fp30
   42| 001C2C stfd     DBA806D8   1     STFL      <s119:d1752:l8>(gr8,1752)=fp29
   43| 001C30 stfd     DB8806E0   1     STFL      <s119:d1760:l8>(gr8,1760)=fp28
   44| 001C34 stfd     DB6806E8   1     STFL      <s119:d1768:l8>(gr8,1768)=fp27
   45| 001C38 stfd     DBE806F0   1     STFL      <s119:d1776:l8>(gr8,1776)=fp31
   46| 001C3C stfd     DAC806F8   1     STFL      <s119:d1784:l8>(gr8,1784)=fp22
    0| 001C40 b        4BFFE4B0   1     B         CL.2,-1
     |               Tag Table
     | 001C44        00000000 00012203 8A120000 00001C44
     |               Instruction count         1809
     |               Straight-line exec time   1847
     |               Constant Area
     | 000000        6B682E66 39300000 7067656E 76307030 72686F5F 696E7268
     | 000018        6F5F6F75 7449616D 706C6C69 6E655F76 616C6B68 2E663930
     | 000030        3F000000 40200000 40000000 3F800000 3C23D70A 3E800000
     | 000048        4B2D4820 70726F62 6C656D20 73657475 702E4942 40C90FDB
     | 000060        2B617F7D 4ED8C33E BF800000 00000000 4B482053 45545550
     | 000078        2046494E 49534845 4449424D 2049424D FFEFFFFF FFFFFFFF
     | 000090        7FEFFFFF FFFFFFFF

 
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    kh.f90                      07/08/15   15:48:57
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................     112
1501-510  Compilation successful for file kh.f90.
1501-543  Object file created.
