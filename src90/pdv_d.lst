IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- pdv_d.f90 07/08/15 15:48:29
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** pdv_d   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at pdv_d.f90 <line 83> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at pdv_d.f90 <line 84> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 85> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 2ll][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 2ll][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 85> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + (($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks)][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[1ll + (($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks)][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 85> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 85> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 1ll][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 1ll][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 2ll) + (#7)*(($$CIV1 + (long long) js) - 2ll) + (8ll)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 2ll) + (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + (($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks)) + (#7)*(($$CIV1 + (long long) js) - 2ll) + (8ll)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) + (#7)*(($$CIV1 + (long long) js) - 2ll) + (8ll)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) + (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 1ll) + (#7)*(($$CIV1 + (long long) js) - 2ll) + (8ll)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) + (long long) ks) - 1ll) + (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at pdv_d.f90 <line 86> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 4) at pdv_d.f90 <line 103> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at pdv_d.f90 <line 104> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 105> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][3ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[3ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 105> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 105> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][2ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[2ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 105> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[1ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(3ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*(($$CIV4 + (long long) js) - 2ll) + (8ll)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV4 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (#7)*(($$CIV4 + (long long) js) - 2ll) + (8ll)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-d%bounds%mult[].off72)*(($$CIV4 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(2ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*(($$CIV4 + (long long) js) - 2ll) + (8ll)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV4 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*(($$CIV4 + (long long) js) - 2ll) + (8ll)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV18 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV4 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at pdv_d.f90 <line 106> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 7) at pdv_d.f90 <line 134> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 8) at pdv_d.f90 <line 135> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 136> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 136> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][2ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[2ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 136> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][3ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[3ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 136> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[1ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (#7)*(1ll + ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 + (long long) je)) + (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(2ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*(1ll + ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 + (long long) je)) + (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(3ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*(1ll + ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 + (long long) je)) + (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*(1ll + ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV19 * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 + (long long) je)) + (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at pdv_d.f90 <line 137> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 10) at pdv_d.f90 <line 141> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 11) at pdv_d.f90 <line 142> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVA) + (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVA) + (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVA) + (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVA) + (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at pdv_d.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 13) at pdv_d.f90 <line 172> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 14) at pdv_d.f90 <line 173> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[1ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][2ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[2ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][4ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[4ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][3ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[3ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + (8ll)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(2ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + (8ll)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(4ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + (8ll)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(4ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(3ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + (8ll)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV1B * 4ll + (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) % 4ll) + (long long) ke)) + (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at pdv_d.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 16) at pdv_d.f90 <line 179> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 17) at pdv_d.f90 <line 180> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 181> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 181> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[1ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 181> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][2ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[2ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 181> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][3ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[3ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable strides.
1586-554 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable strides.
1586-554 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(2ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable strides.
1586-554 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(3ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(3ll + (($$CIV1C * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable strides.
1586-554 (I) Loop (loop index 18) at pdv_d.f90 <line 182> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 19) at pdv_d.f90 <line 217> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 20) at pdv_d.f90 <line 218> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(long long) ks][($$CIV15 + (long long) js) - 2ll][($$CIV14 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns8.[(long long) ks][($$CIV15 + (long long) js) - 2ll][($$CIV14 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 20) at pdv_d.f90 <line 219> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((long long) ks) + (#7)*(($$CIV15 + (long long) js) - 2ll) + (8ll)*(($$CIV14 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at pdv_d.f90 <line 219> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*(($$CIV15 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV14 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at pdv_d.f90 <line 219> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 21) at pdv_d.f90 <line 280> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 22) at pdv_d.f90 <line 281> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(long long) ks][(long long) js + $$CIV13][($$CIV12 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns7.[(long long) ks][(long long) js + $$CIV13][($$CIV12 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 22) at pdv_d.f90 <line 282> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((long long) ks) + (#7)*((long long) js + $$CIV13) + (8ll)*(($$CIV12 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 22) at pdv_d.f90 <line 282> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV13) + (d-d%bounds%mult[].off96)*(($$CIV12 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 22) at pdv_d.f90 <line 282> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-538 (I) Loop (loop index 23) at pdv_d.f90 <line 309> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 23) at pdv_d.f90 <line 309> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 27) at pdv_d.f90 <line 179> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 28) at pdv_d.f90 <line 180> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 29) at pdv_d.f90 <line 181> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(long long) ks + $$CIV11][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[(long long) ks + $$CIV11][(long long) js + $$CIV10][1ll + ($$CIVF + (long long) ie)]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 29) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((long long) ks + $$CIV11) + (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 29) at pdv_d.f90 <line 182> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks + $$CIV11) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF + (long long) ie))) with non-vectorizable strides.
1586-554 (I) Loop (loop index 29) at pdv_d.f90 <line 182> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 33) at pdv_d.f90 <line 172> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 34) at pdv_d.f90 <line 173> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 35) at pdv_d.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][1ll + ($$CIVE + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns6.[1ll + ($$CIVE + (long long) ke)][($$CIVD + (long long) js) - 2ll][($$CIVC + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 35) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(1ll + ($$CIVE + (long long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + (8ll)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 35) at pdv_d.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + ($$CIVE + (long long) ke)) + (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 35) at pdv_d.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 39) at pdv_d.f90 <line 141> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 40) at pdv_d.f90 <line 142> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 41) at pdv_d.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(long long) ks + $$CIVB][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[(long long) ks + $$CIVB][(long long) js + $$CIVA][($$CIV9 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 41) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((long long) ks + $$CIVB) + (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 41) at pdv_d.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks + $$CIVB) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVA) + (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 41) at pdv_d.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 45) at pdv_d.f90 <line 134> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 46) at pdv_d.f90 <line 135> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 47) at pdv_d.f90 <line 136> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(long long) ks + $$CIV8][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns4.[(long long) ks + $$CIV8][1ll + ($$CIV7 + (long long) je)][($$CIV6 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 47) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((long long) ks + $$CIV8) + (#7)*(1ll + ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 47) at pdv_d.f90 <line 137> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks + $$CIV8) + (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 + (long long) je)) + (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 47) at pdv_d.f90 <line 137> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 51) at pdv_d.f90 <line 103> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 52) at pdv_d.f90 <line 104> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 53) at pdv_d.f90 <line 105> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][(long long) ks + $$CIV5][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV5][($$CIV4 + (long long) js) - 2ll][($$CIV3 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 53) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*((long long) ks + $$CIV5) + (#7)*(($$CIV4 + (long long) js) - 2ll) + (8ll)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 53) at pdv_d.f90 <line 106> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks + $$CIV5) + (d-d%bounds%mult[].off72)*(($$CIV4 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 53) at pdv_d.f90 <line 106> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 57) at pdv_d.f90 <line 83> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 58) at pdv_d.f90 <line 84> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 59) at pdv_d.f90 <line 85> was not SIMD vectorized because it contains memory references ((double *)((char *).dcopy  + -8ll - (#8 + #7)))->dcopy[][($$CIV2 + (long long) ks) - 2ll][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll] = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[($$CIV2 + (long long) ks) - 2ll][($$CIV1 + (long long) js) - 2ll][($$CIV0 + (long long) is) - 2ll]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 59) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *).dcopy  + -8ll - (#8 + #7) + (#8)*(($$CIV2 + (long long) ks) - 2ll) + (#7)*(($$CIV1 + (long long) js) - 2ll) + (8ll)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 59) at pdv_d.f90 <line 86> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(($$CIV2 + (long long) ks) - 2ll) + (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long long) is) - 2ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 59) at pdv_d.f90 <line 86> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"15">. Total number of the innermost loops SIMD vectorized <"0">.


    11|         SUBROUTINE pdv_d (dcopy, eod)
    11|           #7 = max(int(in),0) * 8
                  #8 = 8 * (max(int(jn),0) * max(int(in),0))
    61|           IF (ldimen == 2) GOTO lab_2
    62|           IF (ldimen == 1) GOTO lab_4
    68|           k1 = int((real((1 + (ke - ks))) *  3.33333343E-01)) + ks
    69|           k2 = int((real((1 + (ke - ks))) *  3.33333343E-01)) * 2 + ks
    75|           nreq = 0
    76|           nsub = nsub + 1
    77|           T_2 = 0
                  T_3 = 1
                  T_4 = 0
                  T_5 = 0
                  T_6 = 0
                  T_7 = 0
                  CALL bvalv1(T_2,T_3,T_4,T_5,T_6,T_7,d-v1%addr)
    83|           IF ((MOD((1 + (int((ks - 1)) - int((ks - 2)))), 4) > 0  .AND. &
                &    1 + (int((ks - 1)) - int((ks - 2))) > 0)) THEN
                    $$CIV2 = 0
       Id=57        DO $$CIV2 = $$CIV2, MOD((1 + (int((ks - 1)) - int((ks - 2)&
                &       ))), int(4))-1
    84|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIV1 = 0
       Id=58            DO $$CIV1 = $$CIV1, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
    85|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV0 = 0
       Id=59                DO $$CIV0 = $$CIV0, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
    86|                       dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,($$CIV2 + int(ks)) - 2) = d-d%addr%d(($$CIV0 + &
                &               int(is)) - 2,($$CIV1 + int(js)) - 2,($$CIV2 + int(&
                &               ks)) - 2)
    87|                     ENDDO
                          ENDIF
    88|                 ENDDO
                      ENDIF
    89|             ENDDO
                  ENDIF
    83|           IF ((1 + (int((ks - 1)) - int((ks - 2))) > 0  .AND.  1 + (int(&
                &   (ks - 1)) - int((ks - 2))) > MOD((1 + (int((ks - 1)) - int((&
                &   ks - 2)))), 4))) THEN
                    $$CIV17 = int(0)
       Id=1         DO $$CIV17 = $$CIV17, int(((int((ks - 1)) - (MOD((1 + (&
                &       int((ks - 1)) - int((ks - 2)))), 4) + int((ks - 2)))) / 4 &
                &       + 1))-1
    84|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
    85|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV0 = 0
       Id=3                 DO $$CIV0 = $$CIV0, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
    86|                       dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,(($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((&
                &               ks - 2)))), 4)) + int(ks)) - 2) = d-d%addr%d((&
                &               $$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - 2,((&
                &               $$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((ks - &
                &               2)))), 4)) + int(ks)) - 2)
                              dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,(($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((&
                &               ks - 2)))), 4)) + int(ks)) - 1) = d-d%addr%d((&
                &               $$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - 2,((&
                &               $$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((ks - &
                &               2)))), 4)) + int(ks)) - 1)
                              dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((&
                &               ks - 2)))), 4)) + int(ks)) = d-d%addr%d(($$CIV0 + &
                &               int(is)) - 2,($$CIV1 + int(js)) - 2,($$CIV17 * 4 &
                &               + MOD((1 + (int((ks - 1)) - int((ks - 2)))), 4)) &
                &               + int(ks))
                              dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,1 + (($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - &
                &               int((ks - 2)))), 4)) + int(ks))) = d-d%addr%d((&
                &               $$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - 2,1 + (&
                &               ($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((ks &
                &               - 2)))), 4)) + int(ks)))
    87|                     ENDDO
                          ENDIF
    88|                 ENDDO
                      ENDIF
    89|             ENDDO
                  ENDIF
    91|           T_8 = ie - 1
                  T_9 = je - 1
                  CALL pdv(is,T_8,js,T_9,ks,k1,dcopy,eod)
   103|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV5 = 0
       Id=51        DO $$CIV5 = $$CIV5, MOD((1 + (int(ke) - int(ks))), int(4))&
                &       -1
   104|               IF ((1 + (int((js - 1)) - int((js - 2))) > 0)) THEN
                        $$CIV4 = 0
       Id=52            DO $$CIV4 = $$CIV4, int((1 + (int((js - 1)) - int((js &
                &           - 2)))))-1
   105|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV3 = 0
       Id=53                DO $$CIV3 = $$CIV3, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   106|                       dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,int(ks) + $$CIV5) = d-d%addr%d(($$CIV3 + int(is)&
                &               ) - 2,($$CIV4 + int(js)) - 2,int(ks) + $$CIV5)
   107|                     ENDDO
                          ENDIF
   108|                 ENDDO
                      ENDIF
   109|             ENDDO
                  ENDIF
   103|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV18 = int(0)
       Id=4         DO $$CIV18 = $$CIV18, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   104|               IF ((1 + (int((js - 1)) - int((js - 2))) > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int((1 + (int((js - 1)) - int((js &
                &           - 2)))))-1
   105|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   106|                       dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))), 4)&
                &               ) + int(ks)) = d-d%addr%d(($$CIV3 + int(is)) - 2,(&
                &               $$CIV4 + int(js)) - 2,($$CIV18 * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks))
                              dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,1 + (($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV3 + int(is))&
                &                - 2,($$CIV4 + int(js)) - 2,1 + (($$CIV18 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,2 + (($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV3 + int(is))&
                &                - 2,($$CIV4 + int(js)) - 2,2 + (($$CIV18 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,3 + (($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV3 + int(is))&
                &                - 2,($$CIV4 + int(js)) - 2,3 + (($$CIV18 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
   107|                     ENDDO
                          ENDIF
   108|                 ENDDO
                      ENDIF
   109|             ENDDO
                  ENDIF
   113|           IF ((nreq <> 0)) THEN
   114|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   121|           nreq = 0
   122|           nsub = nsub + 1
   123|           T_10 = 0
                  T_11 = 0
                  T_12 = 0
                  T_13 = 1
                  T_14 = 0
                  T_15 = 0
                  CALL bvalv2(T_10,T_11,T_12,T_13,T_14,T_15,d-v2%addr)
   127|           T_16 = je - 1
                  CALL pdv(ie,ie,js,T_16,ks,k1,dcopy,eod)
   129|           T_17 = je - 1
                  T_18 = k1 + 1
                  CALL pdv(is,ie,js,T_17,T_18,k2,dcopy,eod)
   134|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV8 = 0
       Id=45        DO $$CIV8 = $$CIV8, MOD((1 + (int(ke) - int(ks))), int(4))&
                &       -1
   135|               IF ((1 + (int((je + 2)) - int((je + 1))) > 0)) THEN
                        $$CIV7 = 0
       Id=46            DO $$CIV7 = $$CIV7, int((1 + (int((je + 2)) - int((je &
                &           + 1)))))-1
   136|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV6 = 0
       Id=47                DO $$CIV6 = $$CIV6, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   137|                       dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),int(ks) + $$CIV8) = d-d%addr%d(($$CIV6 + int(is)&
                &               ) - 2,1 + ($$CIV7 + int(je)),int(ks) + $$CIV8)
   138|                     ENDDO
                          ENDIF
   139|                 ENDDO
                      ENDIF
   140|             ENDDO
                  ENDIF
   134|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV19 = int(0)
       Id=7         DO $$CIV19 = $$CIV19, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   135|               IF ((1 + (int((je + 2)) - int((je + 1))) > 0)) THEN
                        $$CIV7 = 0
       Id=8             DO $$CIV7 = $$CIV7, int((1 + (int((je + 2)) - int((je &
                &           + 1)))))-1
   136|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV6 = 0
       Id=9                 DO $$CIV6 = $$CIV6, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   137|                       dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))), 4)&
                &               ) + int(ks)) = d-d%addr%d(($$CIV6 + int(is)) - 2,&
                &               1 + ($$CIV7 + int(je)),($$CIV19 * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks))
                              dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),1 + (($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV6 + int(is))&
                &                - 2,1 + ($$CIV7 + int(je)),1 + (($$CIV19 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),2 + (($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV6 + int(is))&
                &                - 2,1 + ($$CIV7 + int(je)),2 + (($$CIV19 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),3 + (($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV6 + int(is))&
                &                - 2,1 + ($$CIV7 + int(je)),3 + (($$CIV19 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
   138|                     ENDDO
                          ENDIF
   139|                 ENDDO
                      ENDIF
   140|             ENDDO
                  ENDIF
   141|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIVB = 0
       Id=39        DO $$CIVB = $$CIVB, MOD((1 + (int(ke) - int(ks))), int(4))&
                &       -1
   142|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVA = 0
       Id=40            DO $$CIVA = $$CIVA, int((1 + (int(je) - int(js))))-1
   143|                   IF ((1 + (int((is - 1)) - int((is - 2))) > 0)) THEN
                            $$CIV9 = 0
       Id=41                DO $$CIV9 = $$CIV9, int((1 + (int((is - 1)) - int(&
                &               (is - 2)))))-1
   144|                       dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,int(&
                &               ks) + $$CIVB) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,int(ks) + $$CIVB)
   145|                     ENDDO
                          ENDIF
   146|                 ENDDO
                      ENDIF
   147|             ENDDO
                  ENDIF
   141|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV1A = int(0)
       Id=10        DO $$CIV1A = $$CIV1A, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   142|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVA = 0
       Id=11            DO $$CIVA = $$CIVA, int((1 + (int(je) - int(js))))-1
   143|                   IF ((1 + (int((is - 1)) - int((is - 2))) > 0)) THEN
                            $$CIV9 = 0
       Id=12                DO $$CIV9 = $$CIV9, int((1 + (int((is - 1)) - int(&
                &               (is - 2)))))-1
   144|                       dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,(&
                &               $$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4)) &
                &               + int(ks)) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,($$CIV1A * 4 + MOD((1 + (int(ke) &
                &               - int(ks))), 4)) + int(ks))
                              dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,1 + &
                &               (($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4))&
                &                + int(ks))) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,1 + (($$CIV1A * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,2 + &
                &               (($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4))&
                &                + int(ks))) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,2 + (($$CIV1A * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,3 + &
                &               (($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4))&
                &                + int(ks))) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,3 + (($$CIV1A * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
   145|                     ENDDO
                          ENDIF
   146|                 ENDDO
                      ENDIF
   147|             ENDDO
                  ENDIF
   151|           IF ((nreq <> 0)) THEN
   152|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   159|           nreq = 0
   160|           nsub = nsub + 1
   161|           T_19 = 0
                  T_20 = 0
                  T_21 = 0
                  T_22 = 0
                  T_23 = 0
                  T_24 = 1
                  CALL bvalv3(T_19,T_20,T_21,T_22,T_23,T_24,d-v3%addr)
   165|           CALL pdv(is,ie,je,je,ks,k2,dcopy,eod)
   167|           T_25 = k2 + 1
                  T_26 = ke - 1
                  CALL pdv(is,ie,js,je,T_25,T_26,dcopy,eod)
   172|           IF ((MOD((1 + (int((ke + 2)) - int((ke + 1)))), 4) > 0  .AND. &
                &    1 + (int((ke + 2)) - int((ke + 1))) > 0)) THEN
                    $$CIVE = 0
       Id=33        DO $$CIVE = $$CIVE, MOD((1 + (int((ke + 2)) - int((ke + 1)&
                &       ))), int(4))-1
   173|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIVD = 0
       Id=34            DO $$CIVD = $$CIVD, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
   174|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIVC = 0
       Id=35                DO $$CIVC = $$CIVC, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   175|                       dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,1 + ($$CIVE + int(ke))) = d-d%addr%d(($$CIVC + &
                &               int(is)) - 2,($$CIVD + int(js)) - 2,1 + ($$CIVE + &
                &               int(ke)))
   176|                     ENDDO
                          ENDIF
   177|                 ENDDO
                      ENDIF
   178|             ENDDO
                  ENDIF
   172|           IF ((1 + (int((ke + 2)) - int((ke + 1))) > 0  .AND.  1 + (int(&
                &   (ke + 2)) - int((ke + 1))) > MOD((1 + (int((ke + 2)) - int((&
                &   ke + 1)))), 4))) THEN
                    $$CIV1B = int(0)
       Id=13        DO $$CIV1B = $$CIV1B, int(((int((ke + 2)) - (MOD((1 + (&
                &       int((ke + 2)) - int((ke + 1)))), 4) + int((ke + 1)))) / 4 &
                &       + 1))-1
   173|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIVD = 0
       Id=14            DO $$CIVD = $$CIVD, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
   174|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIVC = 0
       Id=15                DO $$CIVC = $$CIVC, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   175|                       dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,1 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,1 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
                              dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,2 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,2 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
                              dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,3 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,3 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
                              dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,4 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,4 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
   176|                     ENDDO
                          ENDIF
   177|                 ENDDO
                      ENDIF
   178|             ENDDO
                  ENDIF
   179|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV11 = 0
       Id=27        DO $$CIV11 = $$CIV11, MOD((1 + (int(ke) - int(ks))), int(&
                &       4))-1
   180|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV10 = 0
       Id=28            DO $$CIV10 = $$CIV10, int((1 + (int(je) - int(js))))&
                &           -1
   181|                   IF ((1 + (int((ie + 2)) - int((ie + 1))) > 0)) THEN
                            $$CIVF = 0
       Id=29                DO $$CIVF = $$CIVF, int((1 + (int((ie + 2)) - int(&
                &               (ie + 1)))))-1
   182|                       dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,&
                &               int(ks) + $$CIV11) = d-d%addr%d(1 + ($$CIVF + int(&
                &               ie)),int(js) + $$CIV10,int(ks) + $$CIV11)
   183|                     ENDDO
                          ENDIF
   184|                 ENDDO
                      ENDIF
   185|             ENDDO
                  ENDIF
   179|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV1C = int(0)
       Id=16        DO $$CIV1C = $$CIV1C, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   180|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV10 = 0
       Id=17            DO $$CIV10 = $$CIV10, int((1 + (int(je) - int(js))))&
                &           -1
   181|                   IF ((1 + (int((ie + 2)) - int((ie + 1))) > 0)) THEN
                            $$CIVF = 0
       Id=18                DO $$CIVF = $$CIVF, int((1 + (int((ie + 2)) - int(&
                &               (ie + 1)))))-1
   182|                       dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,(&
                &               $$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), 4)) &
                &               + int(ks)) = d-d%addr%d(1 + ($$CIVF + int(ie)),&
                &               int(js) + $$CIV10,($$CIV1C * 4 + MOD((1 + (int(ke)&
                &                - int(ks))), 4)) + int(ks))
                              dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,1 &
                &               + (($$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), &
                &               4)) + int(ks))) = d-d%addr%d(1 + ($$CIVF + int(ie)&
                &               ),int(js) + $$CIV10,1 + (($$CIV1C * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,2 &
                &               + (($$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), &
                &               4)) + int(ks))) = d-d%addr%d(1 + ($$CIVF + int(ie)&
                &               ),int(js) + $$CIV10,2 + (($$CIV1C * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,3 &
                &               + (($$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), &
                &               4)) + int(ks))) = d-d%addr%d(1 + ($$CIVF + int(ie)&
                &               ),int(js) + $$CIV10,3 + (($$CIV1C * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
   183|                     ENDDO
                          ENDIF
   184|                 ENDDO
                      ENDIF
   185|             ENDDO
                  ENDIF
   189|           IF ((nreq <> 0)) THEN
   190|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   195|           CALL pdv(is,ie,js,je,ke,ke,dcopy,eod)
   198|           GOTO lab_80
   266|           lab_4
   272|           nreq = 0
   273|           nsub = nsub + 1
   274|           T_42 = 0
                  T_43 = 1
                  T_44 = 0
                  T_45 = 0
                  T_46 = 0
                  T_47 = 0
                  CALL bvalv1(T_42,T_43,T_44,T_45,T_46,T_47,d-v1%addr)
   280|           IF (.FALSE.) GOTO lab_145
                  $$CIV13 = 0
       Id=21      DO $$CIV13 = $$CIV13, 0
   281|             IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                      $$CIV12 = 0
       Id=22          DO $$CIV12 = $$CIV12, int((1 + (int((ie + 2)) - int((is &
                &         - 2)))))-1
   282|                 dcopy(($$CIV12 + int(is)) - 2,int(js) + $$CIV13,int(ks))&
                &          = d-d%addr%d(($$CIV12 + int(is)) - 2,int(js) + $$CIV13,&
                &         int(ks))
   283|               ENDDO
                    ENDIF
   284|           ENDDO
                  lab_145
   286|           T_48 = ie - 1
                  CALL pdv(is,T_48,js,js,ks,ks,dcopy,eod)
   297|           IF ((nreq <> 0)) THEN
   298|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   303|           CALL pdv(ie,ie,js,js,ks,ks,dcopy,eod)
   203|           lab_2
   209|           nreq = 0
   210|           nsub = nsub + 1
   211|           T_27 = 0
                  T_28 = 1
                  T_29 = 0
                  T_30 = 0
                  T_31 = 0
                  T_32 = 0
                  CALL bvalv1(T_27,T_28,T_29,T_30,T_31,T_32,d-v1%addr)
   217|           IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                    $$CIV15 = 0
       Id=19        DO $$CIV15 = $$CIV15, int((1 + (int((je + 2)) - int((js - &
                &       2)))))-1
   218|               IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                        $$CIV14 = 0
       Id=20            DO $$CIV14 = $$CIV14, int((1 + (int((ie + 2)) - int((&
                &           is - 2)))))-1
   219|                   dcopy(($$CIV14 + int(is)) - 2,($$CIV15 + int(js)) - 2,&
                &           int(ks)) = d-d%addr%d(($$CIV14 + int(is)) - 2,(&
                &           $$CIV15 + int(js)) - 2,int(ks))
   220|                 ENDDO
                      ENDIF
   221|             ENDDO
                  ENDIF
   223|           T_33 = ie - 1
                  T_34 = je - 1
                  CALL pdv(is,T_33,js,T_34,ks,ks,dcopy,eod)
   234|           IF ((nreq <> 0)) THEN
   235|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   242|           nreq = 0
   243|           nsub = nsub + 1
   244|           T_35 = 0
                  T_36 = 0
                  T_37 = 0
                  T_38 = 1
                  T_39 = 0
                  T_40 = 0
                  CALL bvalv2(T_35,T_36,T_37,T_38,T_39,T_40,d-v2%addr)
   246|           T_41 = je - 1
                  CALL pdv(ie,ie,js,T_41,ks,ks,dcopy,eod)
   251|           IF ((nreq <> 0)) THEN
   252|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   256|           CALL pdv(is,ie,je,je,ks,ks,dcopy,eod)
   261|           GOTO lab_80
   308|           lab_80
   309|           IF (.FALSE.) GOTO lab_149
                  $$CIV16 = 0
       Id=23      DO $$CIV16 = $$CIV16, 5
   310|             bvstat($$CIV16 + 1,2) = 0
   311|             IF ((leos <> 1)) THEN
                      bvstat($$CIV16 + 1,8) = 0
                    ENDIF
   312|           ENDDO
                  lab_149
   315|           RETURN
                END SUBROUTINE pdv_d


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            83            57    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84            58    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(($$CIV2 + (long long) ks) - 2ll) + 
                                          (#7)*(($$CIV1 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV0 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV2 + (long long) ks) 
                                          - 2ll) + (d-d%bounds%mult[].off72)*(($$CIV1 + (long 
                                          long) js) - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 
                                          + (long long) is) - 2ll)) with  non-vectorizable 
                                          strides.
         0            86                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            83             1    Outer loop has been unrolled 4 time(s).
         0            83             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((($$CIV17 * 4ll + (1ll + ((long long) (ks - 
                                          1) - (long long) (ks - 2))) % 4ll) + (long long) ks) 
                                          - 2ll) + (#7)*(($$CIV1 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV0 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((($$CIV17 * 4ll + (1ll + 
                                          ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) 
                                          + (long long) ks) - 2ll) + 
                                          (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0            86                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((($$CIV17 * 4ll + (1ll + ((long long) (ks - 
                                          1) - (long long) (ks - 2))) % 4ll) + (long long) ks) 
                                          - 1ll) + (#7)*(($$CIV1 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV0 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((($$CIV17 * 4ll + (1ll + 
                                          ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) 
                                          + (long long) ks) - 1ll) + 
                                          (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0            86                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(($$CIV17 * 4ll + (1ll + ((long long) (ks - 1) 
                                          - (long long) (ks - 2))) % 4ll) + (long long) ks) + 
                                          (#7)*(($$CIV1 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV0 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV17 * 4ll + (1ll + 
                                          ((long long) (ks - 1) - (long long) (ks - 2))) % 4ll) 
                                          + (long long) ks) + 
                                          (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0            86                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + (($$CIV17 * 4ll + (1ll + ((long long) 
                                          (ks - 1) - (long long) (ks - 2))) % 4ll) + (long 
                                          long) ks)) + (#7)*(($$CIV1 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV0 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0            86                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV17 * 4ll + 
                                          (1ll + ((long long) (ks - 1) - (long long) (ks - 2))) 
                                          % 4ll) + (long long) ks)) + 
                                          (d-d%bounds%mult[].off72)*(($$CIV1 + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV0 + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0            86                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           103            51    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           104            52    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((long long) ks + $$CIV5) + (#7)*(($$CIV4 + 
                                          (long long) js) - 2ll) + (8ll)*(($$CIV3 + (long long) 
                                          is) - 2ll))  with non-vectorizable alignment.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks + $$CIV5) + 
                                          (d-d%bounds%mult[].off72)*(($$CIV4 + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV3 + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0           106                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           103             4    Outer loop has been unrolled 4 time(s).
         0           103             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           104             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(($$CIV18 * 4ll + (1ll + ((long long) ke - 
                                          (long long) ks)) % 4ll) + (long long) ks) + 
                                          (#7)*(($$CIV4 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV3 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV18 * 4ll + (1ll + 
                                          ((long long) ke - (long long) ks)) % 4ll) + (long 
                                          long) ks) + (d-d%bounds%mult[].off72)*(($$CIV4 + 
                                          (long long) js) - 2ll) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           106                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + (($$CIV18 * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*(($$CIV4 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV3 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV18 * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV4 
                                          + (long long) js) - 2ll) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           106                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(2ll + (($$CIV18 * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*(($$CIV4 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV3 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV18 * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV4 
                                          + (long long) js) - 2ll) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           106                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(3ll + (($$CIV18 * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*(($$CIV4 + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIV3 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           106                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV18 * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*(($$CIV4 
                                          + (long long) js) - 2ll) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV3 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           106                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           134            45    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           135            46    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((long long) ks + $$CIV8) + (#7)*(1ll + 
                                          ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long 
                                          long) is) - 2ll))  with non-vectorizable alignment.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks + $$CIV8) + 
                                          (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 + (long 
                                          long) je)) + (d-d%bounds%mult[].off96)*(($$CIV6 + 
                                          (long long) is) - 2ll)) with  non-vectorizable 
                                          strides.
         0           137                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           134             7    Outer loop has been unrolled 4 time(s).
         0           134             7    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           135             8    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(($$CIV19 * 4ll + (1ll + ((long long) ke - 
                                          (long long) ks)) % 4ll) + (long long) ks) + (#7)*(1ll 
                                          + ($$CIV7 + (long long) je)) + (8ll)*(($$CIV6 + (long 
                                          long) is) - 2ll))  with non-vectorizable alignment.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV19 * 4ll + (1ll + 
                                          ((long long) ke - (long long) ks)) % 4ll) + (long 
                                          long) ks) + (d-d%bounds%mult[].off72)*(1ll + ($$CIV7 
                                          + (long long) je)) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           137                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + (($$CIV19 * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*(1ll + ($$CIV7 + (long long) je)) + 
                                          (8ll)*(($$CIV6 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV19 * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*(1ll + 
                                          ($$CIV7 + (long long) je)) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           137                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(2ll + (($$CIV19 * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*(1ll + ($$CIV7 + (long long) je)) + 
                                          (8ll)*(($$CIV6 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV19 * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*(1ll + 
                                          ($$CIV7 + (long long) je)) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           137                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(3ll + (($$CIV19 * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*(1ll + ($$CIV7 + (long long) je)) + 
                                          (8ll)*(($$CIV6 + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           137                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV19 * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*(1ll + 
                                          ($$CIV7 + (long long) je)) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV6 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           137                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           141            39    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           142            40    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((long long) ks + $$CIVB) + (#7)*((long long) 
                                          js + $$CIVA) + (8ll)*(($$CIV9 + (long long) is) - 
                                          2ll))  with non-vectorizable alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks + $$CIVB) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIVA) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           141            10    Outer loop has been unrolled 4 time(s).
         0           141            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           142            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(($$CIV1A * 4ll + (1ll + ((long long) ke - 
                                          (long long) ks)) % 4ll) + (long long) ks) + 
                                          (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + 
                                          (long long) is) - 2ll))  with non-vectorizable 
                                          alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV1A * 4ll + (1ll + 
                                          ((long long) ke - (long long) ks)) % 4ll) + (long 
                                          long) ks) + (d-d%bounds%mult[].off72)*((long long) js 
                                          + $$CIVA) + (d-d%bounds%mult[].off96)*(($$CIV9 + 
                                          (long long) is) - 2ll)) with  non-vectorizable 
                                          strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + (($$CIV1A * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + 
                                          (long long) is) - 2ll))  with non-vectorizable 
                                          alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV1A * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*((long 
                                          long) js + $$CIVA) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(2ll + (($$CIV1A * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + 
                                          (long long) is) - 2ll))  with non-vectorizable 
                                          alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV1A * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*((long 
                                          long) js + $$CIVA) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(3ll + (($$CIV1A * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*((long long) js + $$CIVA) + (8ll)*(($$CIV9 + 
                                          (long long) is) - 2ll))  with non-vectorizable 
                                          alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV1A * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*((long 
                                          long) js + $$CIVA) + 
                                          (d-d%bounds%mult[].off96)*(($$CIV9 + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           172            33    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           173            34    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + ($$CIVE + (long long) ke)) + 
                                          (#7)*(($$CIVD + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIVC + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + ($$CIVE + (long 
                                          long) ke)) + (d-d%bounds%mult[].off72)*(($$CIVD + 
                                          (long long) js) - 2ll) + 
                                          (d-d%bounds%mult[].off96)*(($$CIVC + (long long) is) 
                                          - 2ll)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           172            13    Outer loop has been unrolled 4 time(s).
         0           172            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           173            14    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + (($$CIV1B * 4ll + (1ll + ((long long) 
                                          (ke + 2) - (long long) (ke + 1))) % 4ll) + (long 
                                          long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIVC + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV1B * 4ll + 
                                          (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) 
                                          % 4ll) + (long long) ke)) + 
                                          (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(2ll + (($$CIV1B * 4ll + (1ll + ((long long) 
                                          (ke + 2) - (long long) (ke + 1))) % 4ll) + (long 
                                          long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIVC + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV1B * 4ll + 
                                          (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) 
                                          % 4ll) + (long long) ke)) + 
                                          (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(3ll + (($$CIV1B * 4ll + (1ll + ((long long) 
                                          (ke + 2) - (long long) (ke + 1))) % 4ll) + (long 
                                          long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIVC + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV1B * 4ll + 
                                          (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) 
                                          % 4ll) + (long long) ke)) + 
                                          (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(4ll + (($$CIV1B * 4ll + (1ll + ((long long) 
                                          (ke + 2) - (long long) (ke + 1))) % 4ll) + (long 
                                          long) ke)) + (#7)*(($$CIVD + (long long) js) - 2ll) + 
                                          (8ll)*(($$CIVC + (long long) is) - 2ll))  with 
                                          non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(4ll + (($$CIV1B * 4ll + 
                                          (1ll + ((long long) (ke + 2) - (long long) (ke + 1))) 
                                          % 4ll) + (long long) ke)) + 
                                          (d-d%bounds%mult[].off72)*(($$CIVD + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIVC + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           179            27    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           180            28    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((long long) ks + $$CIV11) + (#7)*((long long) 
                                          js + $$CIV10) + (8ll)*(1ll + ($$CIVF + (long long) 
                                          ie)))  with non-vectorizable alignment.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks + $$CIV11) 
                                          + (d-d%bounds%mult[].off72)*((long long) js + 
                                          $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF + 
                                          (long long) ie))) with  non-vectorizable strides.
         0           182                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           179            16    Outer loop has been unrolled 4 time(s).
         0           179            16    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           180            17    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(($$CIV1C * 4ll + (1ll + ((long long) ke - 
                                          (long long) ks)) % 4ll) + (long long) ks) + 
                                          (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + 
                                          ($$CIVF + (long long) ie)))  with non-vectorizable 
                                          alignment.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(($$CIV1C * 4ll + (1ll + 
                                          ((long long) ke - (long long) ks)) % 4ll) + (long 
                                          long) ks) + (d-d%bounds%mult[].off72)*((long long) js 
                                          + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll + ($$CIVF 
                                          + (long long) ie))) with  non-vectorizable strides.
         0           182                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(1ll + (($$CIV1C * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + 
                                          ($$CIVF + (long long) ie)))  with non-vectorizable 
                                          alignment.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + (($$CIV1C * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*((long 
                                          long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll 
                                          + ($$CIVF + (long long) ie))) with  non-vectorizable 
                                          strides.
         0           182                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(2ll + (($$CIV1C * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + 
                                          ($$CIVF + (long long) ie)))  with non-vectorizable 
                                          alignment.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + (($$CIV1C * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*((long 
                                          long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll 
                                          + ($$CIVF + (long long) ie))) with  non-vectorizable 
                                          strides.
         0           182                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*(3ll + (($$CIV1C * 4ll + (1ll + ((long long) 
                                          ke - (long long) ks)) % 4ll) + (long long) ks)) + 
                                          (#7)*((long long) js + $$CIV10) + (8ll)*(1ll + 
                                          ($$CIVF + (long long) ie)))  with non-vectorizable 
                                          alignment.
         0           182                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(3ll + (($$CIV1C * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks)) + (d-d%bounds%mult[].off72)*((long 
                                          long) js + $$CIV10) + (d-d%bounds%mult[].off96)*(1ll 
                                          + ($$CIVF + (long long) ie))) with  non-vectorizable 
                                          strides.
         0           182                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           280            21    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           282                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((long long) ks) + (#7)*((long long) js + 
                                          $$CIV13) + (8ll)*(($$CIV12 + (long long) is) - 2ll))  
                                          with non-vectorizable alignment.
         0           282                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIV13) 
                                          + (d-d%bounds%mult[].off96)*(($$CIV12 + (long long) 
                                          is) - 2ll)) with  non-vectorizable strides.
         0           282                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           309            23    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0           309            23    Loop was not SIMD vectorized because it contains 
                                          control flow.
         0           217            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           219                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).dcopy  + -8ll - (#8 + #7) 
                                          + (#8)*((long long) ks) + (#7)*(($$CIV15 + (long 
                                          long) js) - 2ll) + (8ll)*(($$CIV14 + (long long) is) 
                                          - 2ll))  with non-vectorizable alignment.
         0           219                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks) + 
                                          (d-d%bounds%mult[].off72)*(($$CIV15 + (long long) js) 
                                          - 2ll) + (d-d%bounds%mult[].off96)*(($$CIV14 + (long 
                                          long) is) - 2ll)) with  non-vectorizable strides.
         0           219                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.


    11|         SUBROUTINE pdv_d (dcopy, eod)
    11|           #7 = max(int(in),0) * 8
                  #8 = 8 * (max(int(jn),0) * max(int(in),0))
    61|           IF (ldimen == 2) GOTO lab_2
    62|           IF (ldimen == 1) GOTO lab_4
    68|           k1 = int((real((1 + (ke - ks))) *  3.33333343E-01)) + ks
    69|           k2 = int((real((1 + (ke - ks))) *  3.33333343E-01)) * 2 + ks
    75|           nreq = 0
    76|           nsub = nsub + 1
    77|           T_2 = 0
                  T_3 = 1
                  T_4 = 0
                  T_5 = 0
                  T_6 = 0
                  T_7 = 0
                  CALL bvalv1(T_2,T_3,T_4,T_5,T_6,T_7,d-v1%addr)
    83|           IF ((MOD((1 + (int((ks - 1)) - int((ks - 2)))), 4) > 0  .AND. &
                &    1 + (int((ks - 1)) - int((ks - 2))) > 0)) THEN
                    $$CIV2 = 0
       Id=57        DO $$CIV2 = $$CIV2, MOD((1 + (int((ks - 1)) - int((ks - 2)&
                &       ))), int(4))-1
    84|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIV1 = 0
       Id=58            DO $$CIV1 = $$CIV1, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
    85|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV0 = 0
       Id=59                DO $$CIV0 = $$CIV0, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
    86|                       dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,($$CIV2 + int(ks)) - 2) = d-d%addr%d(($$CIV0 + &
                &               int(is)) - 2,($$CIV1 + int(js)) - 2,($$CIV2 + int(&
                &               ks)) - 2)
    87|                     ENDDO
                          ENDIF
    88|                 ENDDO
                      ENDIF
    89|             ENDDO
                  ENDIF
    83|           IF ((1 + (int((ks - 1)) - int((ks - 2))) > 0  .AND.  1 + (int(&
                &   (ks - 1)) - int((ks - 2))) > MOD((1 + (int((ks - 1)) - int((&
                &   ks - 2)))), 4))) THEN
                    $$CIV17 = int(0)
       Id=1         DO $$CIV17 = $$CIV17, int(((int((ks - 1)) - (MOD((1 + (&
                &       int((ks - 1)) - int((ks - 2)))), 4) + int((ks - 2)))) / 4 &
                &       + 1))-1
    84|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
    85|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV0 = 0
       Id=3                 DO $$CIV0 = $$CIV0, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
    86|                       dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,(($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((&
                &               ks - 2)))), 4)) + int(ks)) - 2) = d-d%addr%d((&
                &               $$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - 2,((&
                &               $$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((ks - &
                &               2)))), 4)) + int(ks)) - 2)
                              dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,(($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((&
                &               ks - 2)))), 4)) + int(ks)) - 1) = d-d%addr%d((&
                &               $$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - 2,((&
                &               $$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((ks - &
                &               2)))), 4)) + int(ks)) - 1)
                              dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((&
                &               ks - 2)))), 4)) + int(ks)) = d-d%addr%d(($$CIV0 + &
                &               int(is)) - 2,($$CIV1 + int(js)) - 2,($$CIV17 * 4 &
                &               + MOD((1 + (int((ks - 1)) - int((ks - 2)))), 4)) &
                &               + int(ks))
                              dcopy(($$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - &
                &               2,1 + (($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - &
                &               int((ks - 2)))), 4)) + int(ks))) = d-d%addr%d((&
                &               $$CIV0 + int(is)) - 2,($$CIV1 + int(js)) - 2,1 + (&
                &               ($$CIV17 * 4 + MOD((1 + (int((ks - 1)) - int((ks &
                &               - 2)))), 4)) + int(ks)))
    87|                     ENDDO
                          ENDIF
    88|                 ENDDO
                      ENDIF
    89|             ENDDO
                  ENDIF
    91|           T_8 = ie - 1
                  T_9 = je - 1
                  CALL pdv(is,T_8,js,T_9,ks,k1,dcopy,eod)
   103|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV5 = 0
       Id=51        DO $$CIV5 = $$CIV5, MOD((1 + (int(ke) - int(ks))), int(4))&
                &       -1
   104|               IF ((1 + (int((js - 1)) - int((js - 2))) > 0)) THEN
                        $$CIV4 = 0
       Id=52            DO $$CIV4 = $$CIV4, int((1 + (int((js - 1)) - int((js &
                &           - 2)))))-1
   105|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV3 = 0
       Id=53                DO $$CIV3 = $$CIV3, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   106|                       dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,int(ks) + $$CIV5) = d-d%addr%d(($$CIV3 + int(is)&
                &               ) - 2,($$CIV4 + int(js)) - 2,int(ks) + $$CIV5)
   107|                     ENDDO
                          ENDIF
   108|                 ENDDO
                      ENDIF
   109|             ENDDO
                  ENDIF
   103|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV18 = int(0)
       Id=4         DO $$CIV18 = $$CIV18, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   104|               IF ((1 + (int((js - 1)) - int((js - 2))) > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int((1 + (int((js - 1)) - int((js &
                &           - 2)))))-1
   105|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   106|                       dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))), 4)&
                &               ) + int(ks)) = d-d%addr%d(($$CIV3 + int(is)) - 2,(&
                &               $$CIV4 + int(js)) - 2,($$CIV18 * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks))
                              dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,1 + (($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV3 + int(is))&
                &                - 2,($$CIV4 + int(js)) - 2,1 + (($$CIV18 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,2 + (($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV3 + int(is))&
                &                - 2,($$CIV4 + int(js)) - 2,2 + (($$CIV18 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV3 + int(is)) - 2,($$CIV4 + int(js)) - &
                &               2,3 + (($$CIV18 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV3 + int(is))&
                &                - 2,($$CIV4 + int(js)) - 2,3 + (($$CIV18 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
   107|                     ENDDO
                          ENDIF
   108|                 ENDDO
                      ENDIF
   109|             ENDDO
                  ENDIF
   113|           IF ((nreq <> 0)) THEN
   114|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   121|           nreq = 0
   122|           nsub = nsub + 1
   123|           T_10 = 0
                  T_11 = 0
                  T_12 = 0
                  T_13 = 1
                  T_14 = 0
                  T_15 = 0
                  CALL bvalv2(T_10,T_11,T_12,T_13,T_14,T_15,d-v2%addr)
   127|           T_16 = je - 1
                  CALL pdv(ie,ie,js,T_16,ks,k1,dcopy,eod)
   129|           T_17 = je - 1
                  T_18 = k1 + 1
                  CALL pdv(is,ie,js,T_17,T_18,k2,dcopy,eod)
   134|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV8 = 0
       Id=45        DO $$CIV8 = $$CIV8, MOD((1 + (int(ke) - int(ks))), int(4))&
                &       -1
   135|               IF ((1 + (int((je + 2)) - int((je + 1))) > 0)) THEN
                        $$CIV7 = 0
       Id=46            DO $$CIV7 = $$CIV7, int((1 + (int((je + 2)) - int((je &
                &           + 1)))))-1
   136|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV6 = 0
       Id=47                DO $$CIV6 = $$CIV6, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   137|                       dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),int(ks) + $$CIV8) = d-d%addr%d(($$CIV6 + int(is)&
                &               ) - 2,1 + ($$CIV7 + int(je)),int(ks) + $$CIV8)
   138|                     ENDDO
                          ENDIF
   139|                 ENDDO
                      ENDIF
   140|             ENDDO
                  ENDIF
   134|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV19 = int(0)
       Id=7         DO $$CIV19 = $$CIV19, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   135|               IF ((1 + (int((je + 2)) - int((je + 1))) > 0)) THEN
                        $$CIV7 = 0
       Id=8             DO $$CIV7 = $$CIV7, int((1 + (int((je + 2)) - int((je &
                &           + 1)))))-1
   136|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIV6 = 0
       Id=9                 DO $$CIV6 = $$CIV6, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   137|                       dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))), 4)&
                &               ) + int(ks)) = d-d%addr%d(($$CIV6 + int(is)) - 2,&
                &               1 + ($$CIV7 + int(je)),($$CIV19 * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks))
                              dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),1 + (($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV6 + int(is))&
                &                - 2,1 + ($$CIV7 + int(je)),1 + (($$CIV19 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),2 + (($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV6 + int(is))&
                &                - 2,1 + ($$CIV7 + int(je)),2 + (($$CIV19 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV6 + int(is)) - 2,1 + ($$CIV7 + int(je)&
                &               ),3 + (($$CIV19 * 4 + MOD((1 + (int(ke) - int(ks))&
                &               ), 4)) + int(ks))) = d-d%addr%d(($$CIV6 + int(is))&
                &                - 2,1 + ($$CIV7 + int(je)),3 + (($$CIV19 * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
   138|                     ENDDO
                          ENDIF
   139|                 ENDDO
                      ENDIF
   140|             ENDDO
                  ENDIF
   141|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIVB = 0
       Id=39        DO $$CIVB = $$CIVB, MOD((1 + (int(ke) - int(ks))), int(4))&
                &       -1
   142|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVA = 0
       Id=40            DO $$CIVA = $$CIVA, int((1 + (int(je) - int(js))))-1
   143|                   IF ((1 + (int((is - 1)) - int((is - 2))) > 0)) THEN
                            $$CIV9 = 0
       Id=41                DO $$CIV9 = $$CIV9, int((1 + (int((is - 1)) - int(&
                &               (is - 2)))))-1
   144|                       dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,int(&
                &               ks) + $$CIVB) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,int(ks) + $$CIVB)
   145|                     ENDDO
                          ENDIF
   146|                 ENDDO
                      ENDIF
   147|             ENDDO
                  ENDIF
   141|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV1A = int(0)
       Id=10        DO $$CIV1A = $$CIV1A, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   142|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVA = 0
       Id=11            DO $$CIVA = $$CIVA, int((1 + (int(je) - int(js))))-1
   143|                   IF ((1 + (int((is - 1)) - int((is - 2))) > 0)) THEN
                            $$CIV9 = 0
       Id=12                DO $$CIV9 = $$CIV9, int((1 + (int((is - 1)) - int(&
                &               (is - 2)))))-1
   144|                       dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,(&
                &               $$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4)) &
                &               + int(ks)) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,($$CIV1A * 4 + MOD((1 + (int(ke) &
                &               - int(ks))), 4)) + int(ks))
                              dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,1 + &
                &               (($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4))&
                &                + int(ks))) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,1 + (($$CIV1A * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,2 + &
                &               (($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4))&
                &                + int(ks))) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,2 + (($$CIV1A * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(($$CIV9 + int(is)) - 2,int(js) + $$CIVA,3 + &
                &               (($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks))), 4))&
                &                + int(ks))) = d-d%addr%d(($$CIV9 + int(is)) - 2,&
                &               int(js) + $$CIVA,3 + (($$CIV1A * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
   145|                     ENDDO
                          ENDIF
   146|                 ENDDO
                      ENDIF
   147|             ENDDO
                  ENDIF
   151|           IF ((nreq <> 0)) THEN
   152|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   159|           nreq = 0
   160|           nsub = nsub + 1
   161|           T_19 = 0
                  T_20 = 0
                  T_21 = 0
                  T_22 = 0
                  T_23 = 0
                  T_24 = 1
                  CALL bvalv3(T_19,T_20,T_21,T_22,T_23,T_24,d-v3%addr)
   165|           CALL pdv(is,ie,je,je,ks,k2,dcopy,eod)
   167|           T_25 = k2 + 1
                  T_26 = ke - 1
                  CALL pdv(is,ie,js,je,T_25,T_26,dcopy,eod)
   172|           IF ((MOD((1 + (int((ke + 2)) - int((ke + 1)))), 4) > 0  .AND. &
                &    1 + (int((ke + 2)) - int((ke + 1))) > 0)) THEN
                    $$CIVE = 0
       Id=33        DO $$CIVE = $$CIVE, MOD((1 + (int((ke + 2)) - int((ke + 1)&
                &       ))), int(4))-1
   173|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIVD = 0
       Id=34            DO $$CIVD = $$CIVD, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
   174|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIVC = 0
       Id=35                DO $$CIVC = $$CIVC, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   175|                       dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,1 + ($$CIVE + int(ke))) = d-d%addr%d(($$CIVC + &
                &               int(is)) - 2,($$CIVD + int(js)) - 2,1 + ($$CIVE + &
                &               int(ke)))
   176|                     ENDDO
                          ENDIF
   177|                 ENDDO
                      ENDIF
   178|             ENDDO
                  ENDIF
   172|           IF ((1 + (int((ke + 2)) - int((ke + 1))) > 0  .AND.  1 + (int(&
                &   (ke + 2)) - int((ke + 1))) > MOD((1 + (int((ke + 2)) - int((&
                &   ke + 1)))), 4))) THEN
                    $$CIV1B = int(0)
       Id=13        DO $$CIV1B = $$CIV1B, int(((int((ke + 2)) - (MOD((1 + (&
                &       int((ke + 2)) - int((ke + 1)))), 4) + int((ke + 1)))) / 4 &
                &       + 1))-1
   173|               IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                        $$CIVD = 0
       Id=14            DO $$CIVD = $$CIVD, int((1 + (int((je + 2)) - int((js &
                &           - 2)))))-1
   174|                   IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                            $$CIVC = 0
       Id=15                DO $$CIVC = $$CIVC, int((1 + (int((ie + 2)) - int(&
                &               (is - 2)))))-1
   175|                       dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,1 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,1 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
                              dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,2 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,2 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
                              dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,3 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,3 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
                              dcopy(($$CIVC + int(is)) - 2,($$CIVD + int(js)) - &
                &               2,4 + (($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - &
                &               int((ke + 1)))), 4)) + int(ke))) = d-d%addr%d((&
                &               $$CIVC + int(is)) - 2,($$CIVD + int(js)) - 2,4 + (&
                &               ($$CIV1B * 4 + MOD((1 + (int((ke + 2)) - int((ke &
                &               + 1)))), 4)) + int(ke)))
   176|                     ENDDO
                          ENDIF
   177|                 ENDDO
                      ENDIF
   178|             ENDDO
                  ENDIF
   179|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV11 = 0
       Id=27        DO $$CIV11 = $$CIV11, MOD((1 + (int(ke) - int(ks))), int(&
                &       4))-1
   180|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV10 = 0
       Id=28            DO $$CIV10 = $$CIV10, int((1 + (int(je) - int(js))))&
                &           -1
   181|                   IF ((1 + (int((ie + 2)) - int((ie + 1))) > 0)) THEN
                            $$CIVF = 0
       Id=29                DO $$CIVF = $$CIVF, int((1 + (int((ie + 2)) - int(&
                &               (ie + 1)))))-1
   182|                       dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,&
                &               int(ks) + $$CIV11) = d-d%addr%d(1 + ($$CIVF + int(&
                &               ie)),int(js) + $$CIV10,int(ks) + $$CIV11)
   183|                     ENDDO
                          ENDIF
   184|                 ENDDO
                      ENDIF
   185|             ENDDO
                  ENDIF
   179|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV1C = int(0)
       Id=16        DO $$CIV1C = $$CIV1C, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   180|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV10 = 0
       Id=17            DO $$CIV10 = $$CIV10, int((1 + (int(je) - int(js))))&
                &           -1
   181|                   IF ((1 + (int((ie + 2)) - int((ie + 1))) > 0)) THEN
                            $$CIVF = 0
       Id=18                DO $$CIVF = $$CIVF, int((1 + (int((ie + 2)) - int(&
                &               (ie + 1)))))-1
   182|                       dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,(&
                &               $$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), 4)) &
                &               + int(ks)) = d-d%addr%d(1 + ($$CIVF + int(ie)),&
                &               int(js) + $$CIV10,($$CIV1C * 4 + MOD((1 + (int(ke)&
                &                - int(ks))), 4)) + int(ks))
                              dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,1 &
                &               + (($$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), &
                &               4)) + int(ks))) = d-d%addr%d(1 + ($$CIVF + int(ie)&
                &               ),int(js) + $$CIV10,1 + (($$CIV1C * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,2 &
                &               + (($$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), &
                &               4)) + int(ks))) = d-d%addr%d(1 + ($$CIVF + int(ie)&
                &               ),int(js) + $$CIV10,2 + (($$CIV1C * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
                              dcopy(1 + ($$CIVF + int(ie)),int(js) + $$CIV10,3 &
                &               + (($$CIV1C * 4 + MOD((1 + (int(ke) - int(ks))), &
                &               4)) + int(ks))) = d-d%addr%d(1 + ($$CIVF + int(ie)&
                &               ),int(js) + $$CIV10,3 + (($$CIV1C * 4 + MOD((1 + (&
                &               int(ke) - int(ks))), 4)) + int(ks)))
   183|                     ENDDO
                          ENDIF
   184|                 ENDDO
                      ENDIF
   185|             ENDDO
                  ENDIF
   189|           IF ((nreq <> 0)) THEN
   190|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   195|           CALL pdv(is,ie,js,je,ke,ke,dcopy,eod)
   198|           GOTO lab_80
   266|           lab_4
   272|           nreq = 0
   273|           nsub = nsub + 1
   274|           T_42 = 0
                  T_43 = 1
                  T_44 = 0
                  T_45 = 0
                  T_46 = 0
                  T_47 = 0
                  CALL bvalv1(T_42,T_43,T_44,T_45,T_46,T_47,d-v1%addr)
   280|           IF (.FALSE.) GOTO lab_145
                  $$CIV13 = 0
       Id=21      DO $$CIV13 = $$CIV13, 0
   281|             IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                      $$CIV12 = 0
       Id=22          DO $$CIV12 = $$CIV12, int((1 + (int((ie + 2)) - int((is &
                &         - 2)))))-1
   282|                 dcopy(($$CIV12 + int(is)) - 2,int(js) + $$CIV13,int(ks))&
                &          = d-d%addr%d(($$CIV12 + int(is)) - 2,int(js) + $$CIV13,&
                &         int(ks))
   283|               ENDDO
                    ENDIF
   284|           ENDDO
                  lab_145
   286|           T_48 = ie - 1
                  CALL pdv(is,T_48,js,js,ks,ks,dcopy,eod)
   297|           IF ((nreq <> 0)) THEN
   298|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   303|           CALL pdv(ie,ie,js,js,ks,ks,dcopy,eod)
   203|           lab_2
   209|           nreq = 0
   210|           nsub = nsub + 1
   211|           T_27 = 0
                  T_28 = 1
                  T_29 = 0
                  T_30 = 0
                  T_31 = 0
                  T_32 = 0
                  CALL bvalv1(T_27,T_28,T_29,T_30,T_31,T_32,d-v1%addr)
   217|           IF ((1 + (int((je + 2)) - int((js - 2))) > 0)) THEN
                    $$CIV15 = 0
       Id=19        DO $$CIV15 = $$CIV15, int((1 + (int((je + 2)) - int((js - &
                &       2)))))-1
   218|               IF ((1 + (int((ie + 2)) - int((is - 2))) > 0)) THEN
                        $$CIV14 = 0
       Id=20            DO $$CIV14 = $$CIV14, int((1 + (int((ie + 2)) - int((&
                &           is - 2)))))-1
   219|                   dcopy(($$CIV14 + int(is)) - 2,($$CIV15 + int(js)) - 2,&
                &           int(ks)) = d-d%addr%d(($$CIV14 + int(is)) - 2,(&
                &           $$CIV15 + int(js)) - 2,int(ks))
   220|                 ENDDO
                      ENDIF
   221|             ENDDO
                  ENDIF
   223|           T_33 = ie - 1
                  T_34 = je - 1
                  CALL pdv(is,T_33,js,T_34,ks,ks,dcopy,eod)
   234|           IF ((nreq <> 0)) THEN
   235|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   242|           nreq = 0
   243|           nsub = nsub + 1
   244|           T_35 = 0
                  T_36 = 0
                  T_37 = 0
                  T_38 = 1
                  T_39 = 0
                  T_40 = 0
                  CALL bvalv2(T_35,T_36,T_37,T_38,T_39,T_40,d-v2%addr)
   246|           T_41 = je - 1
                  CALL pdv(ie,ie,js,T_41,ks,ks,dcopy,eod)
   251|           IF ((nreq <> 0)) THEN
   252|             CALL mpi_waitall(nreq,req,stat,ierr)
                  ENDIF
   256|           CALL pdv(is,ie,je,je,ks,ks,dcopy,eod)
   261|           GOTO lab_80
   308|           lab_80
   309|           IF (.FALSE.) GOTO lab_149
                  $$CIV16 = 0
       Id=23      DO $$CIV16 = $$CIV16, 5
   310|             bvstat($$CIV16 + 1,2) = 0
   311|             IF ((leos <> 1)) THEN
                      bvstat($$CIV16 + 1,8) = 0
                    ENDIF
   312|           ENDDO
                  lab_149
   315|           RETURN
                END SUBROUTINE pdv_d

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- ---- ----
 CCR's set/used:   sss- ssss
     | 000000                           PDEF     pdv_d
   11|                                  PROC      .dcopy,.eod,gr3,gr4
    0| 000000 std      FBE1FFF8   1     ST8       #stack(gr1,-8)=gr31
    0| 000004 std      FBC1FFF0   1     ST8       #stack(gr1,-16)=gr30
    0| 000008 std      FBA1FFE8   1     ST8       #stack(gr1,-24)=gr29
    0| 00000C std      FB81FFE0   1     ST8       #stack(gr1,-32)=gr28
    0| 000010 std      FB61FFD8   1     ST8       #stack(gr1,-40)=gr27
    0| 000014 std      FB41FFD0   1     ST8       #stack(gr1,-48)=gr26
    0| 000018 std      FB21FFC8   1     ST8       #stack(gr1,-56)=gr25
    0| 00001C std      FB01FFC0   1     ST8       #stack(gr1,-64)=gr24
    0| 000020 std      FAE1FFB8   1     ST8       #stack(gr1,-72)=gr23
    0| 000024 std      FAC1FFB0   1     ST8       #stack(gr1,-80)=gr22
    0| 000028 std      FAA1FFA8   1     ST8       #stack(gr1,-88)=gr21
    0| 00002C std      FA81FFA0   1     ST8       #stack(gr1,-96)=gr20
    0| 000030 std      FA61FF98   1     ST8       #stack(gr1,-104)=gr19
    0| 000034 std      FA41FF90   1     ST8       #stack(gr1,-112)=gr18
    0| 000038 std      FA21FF88   1     ST8       #stack(gr1,-120)=gr17
    0| 00003C std      FA01FF80   1     ST8       #stack(gr1,-128)=gr16
    0| 000040 std      F9E1FF78   1     ST8       #stack(gr1,-136)=gr15
    0| 000044 std      F9C1FF70   1     ST8       #stack(gr1,-144)=gr14
    0| 000048 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 00004C mfcr     7D800026   1     LFCR      gr12=cr[24],2
    0| 000050 stw      91810008   1     ST4A      #stack(gr1,8)=gr12
    0| 000054 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000058 stdu     F821FCE1   1     ST8U      gr1,#stack(gr1,-800)=gr1
    0| 00005C std      F8810178   1     ST8       #SPILL1(gr1,376)=gr4
    0| 000060 ld       E8A20000   1     L8        gr5=.&&N&&param(gr2,0)
   61| 000064 ld       E9020000   1     L8        gr8=.&&N&&config(gr2,0)
    0| 000068 ld       E9420000   1     L8        gr10=.&&N&&mpipar(gr2,0)
    0| 00006C std      F8610170   1     ST8       #SPILL0(gr1,368)=gr3
    0| 000070 ld       E9620000   1     L8        gr11=.&&N&field(gr2,0)
    0| 000074 ld       E9820000   1     L8        gr12=.&&N&&grid(gr2,0)
    0| 000078 lwa      E8C50002   1     L4A       gr6=<s11:d0:l4>(gr5,0)
    0| 00007C lwa      E8050006   1     L4A       gr0=<s11:d4:l4>(gr5,4)
   61| 000080 lwz      80680004   1     L4Z       gr3=<s61:d4:l4>(gr8,4)
    0| 000084 lwz      80AA001C   1     L4Z       gr5=<s70:d28:l4>(gr10,28)
    0| 000088 ld       E92B01A0   1     L8        gr9=<s24:d416:l8>(gr11,416)
    0| 00008C addi     3B600000   1     LI        gr27=0
    0| 000090 sradi    7CC7FE76   1     SRA8      gr7=gr6,63,ca"
    0| 000094 sradi    7C04FE76   1     SRA8      gr4=gr0,63,ca"
    0| 000098 andc     7CDE3878   1     ANDC      gr30=gr6,gr7
    0| 00009C std      FBC10180   1     ST8       #SPILL2(gr1,384)=gr30
    0| 0000A0 andc     7C002078   1     ANDC      gr0=gr0,gr4
    0| 0000A4 mulld    7FA0F1D2   1     M         gr29=gr0,gr30
   61| 0000A8 cmpwi    2C030002   1     C4        cr0=gr3,2
    0| 0000AC std      FBA10188   1     ST8       #SPILL3(gr1,392)=gr29
    0| 0000B0 rldicr   7BBC1F24   1     SLL8      gr28=gr29,3
    0| 0000B4 rldicr   7BDF1F24   1     SLL8      gr31=gr30,3
    0| 0000B8 std      FB810190   1     ST8       #SPILL4(gr1,400)=gr28
    0| 0000BC addi     38050001   1     AI        gr0=gr5,1
   61| 0000C0 bc       41821C68   1     BT        CL.2,cr0,0x4/eq,taken=40%(40,60)
   62| 0000C4 cmpwi    2C030001   1     C4        cr0=gr3,1
   62| 0000C8 bc       41821A68   1     BT        CL.4,cr0,0x4/eq,taken=50%(0,0)
   76| 0000CC stw      900A001C   1     ST4Z      <s70:d28:l4>(gr10,28)=gr0
   68| 0000D0 lwa      E86C0016   1     L4A       gr3=<s64:d20:l4>(gr12,20)
   77| 0000D4 stw      93610098   1     ST4Z      T_6(gr1,152)=gr27
   68| 0000D8 lwz      800C0010   1     L4Z       gr0=<s64:d16:l4>(gr12,16)
   68| 0000DC ld       E8820000   1     L8        gr4=.+CONSTANT_AREA(gr2,0)
   75| 0000E0 stw      936A0018   1     ST4Z      <s70:d24:l4>(gr10,24)=gr27
   77| 0000E4 addi     3B400001   1     LI        gr26=1
   77| 0000E8 stw      93610088   1     ST4Z      T_2(gr1,136)=gr27
   77| 0000EC stw      93610090   1     ST4Z      T_4(gr1,144)=gr27
   68| 0000F0 subf     7C601850   1     S         gr3=gr3,gr0
   68| 0000F4 lfs      C0240000   1     LFS       fp1=+CONSTANT_AREA(gr4,0)
   68| 0000F8 addi     38630001   1     AI        gr3=gr3,1
   77| 0000FC stw      93610094   1     ST4Z      T_5(gr1,148)=gr27
   68| 000100 extsw    7C6307B4   1     EXTS4     gr3=gr3
   77| 000104 stw      9341008C   1     ST4Z      T_3(gr1,140)=gr26
   68| 000108 std      F8610148   1     ST8       #MX_CONVF1_0(gr1,328)=gr3
   77| 00010C stw      9361009C   1     ST4Z      T_7(gr1,156)=gr27
   77| 000110 addi     3901009C   1     AI        gr8=gr1,156
   77| 000114 addi     38E10098   1     AI        gr7=gr1,152
   77| 000118 addi     38C10094   1     AI        gr6=gr1,148
   77| 00011C addi     38A10090   1     AI        gr5=gr1,144
   77| 000120 addi     3881008C   1     AI        gr4=gr1,140
   77| 000124 addi     38610088   1     AI        gr3=gr1,136
   68| 000128 lfd      C8010148   1     LFL       fp0=#MX_CONVF1_0(gr1,328)
   68| 00012C fcfid    FC00069C   1     FCFID     fp0=fp0,fcr
   68| 000130 frsp     FC000018   2     CVLS      fp0=fp0,fcr
   68| 000134 fmuls    EC000072   1     MFS       fp0=fp0,fp1,fcr
   68| 000138 fctiwz   FC00001E   1     FCTIWZ    fp0=fp0
   68| 00013C stfd     D8010158   1     STFL      #MX_CONVF1_1(gr1,344)=fp0
   68| 000140 lwa      E941015E   1     L4A       gr10=#MX_CONVF1_1(gr1,348)
   69| 000144 rldicr   794B0FA4   1     SLL8      gr11=gr10,1
   68| 000148 add      7D405214   1     A         gr10=gr0,gr10
   69| 00014C add      7C005A14   1     A         gr0=gr0,gr11
   68| 000150 stw      91410080   1     ST4Z      k1(gr1,128)=gr10
   69| 000154 stw      90010084   1     ST4Z      k2(gr1,132)=gr0
   77| 000158 bl       48000001   1     CALL      bvalv1,7,T_2",gr3,T_3",gr4,T_4",gr5,T_5",gr6,T_6",gr7,T_7",gr8,v1",gr9,#ProcAlias",bvalv1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   77| 00015C ori      60000000   1
   83| 000160 ld       E9820000   1     L8        gr12=.&&N&&grid(gr2,0)
   83| 000164 lwz      808C0010   1     L4Z       gr4=<s64:d16:l4>(gr12,16)
    0| 000168 lwz      832C000C   1     L4Z       gr25=<s64:d12:l4>(gr12,12)
    0| 00016C lwz      830C0004   1     L4Z       gr24=<s64:d4:l4>(gr12,4)
   83| 000170 addi     3804FFFF   1     AI        gr0=gr4,-1
   83| 000174 addi     3864FFFE   1     AI        gr3=gr4,-2
   83| 000178 extsw    7C0007B4   1     EXTS4     gr0=gr0
   83| 00017C extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 000180 std      FB210198   1     ST8       #SPILL5(gr1,408)=gr25
   83| 000184 subf     7C630050   1     S         gr3=gr0,gr3
    0| 000188 std      FB0101A0   1     ST8       #SPILL6(gr1,416)=gr24
   83| 00018C addi     38030001   1     AI        gr0=gr3,1
   83| 000190 sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
   83| 000194 cmpdi    2E200000   1     C8        cr4=gr0,0
   83| 000198 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
   83| 00019C rldicr   78661764   1     SLL8      gr6=gr3,2
   83| 0001A0 subf     7CA60051   1     S_R       gr5,cr0=gr0,gr6
   83| 0001A4 crand    4C310A02   1     CR_N      cr0=cr[40],0x2/gt,0x2/gt,0x2/gt,cr0
   83| 0001A8 bc       40810158   1     BF        CL.234,cr0,0x2/gt,taken=50%(0,0)
   84| 0001AC lwz      83CC0008   1     L4Z       gr30=<s64:d8:l4>(gr12,8)
   84| 0001B0 addi     38F90002   1     AI        gr7=gr25,2
   85| 0001B4 lwa      E86C0002   1     L4A       gr3=<s64:d0:l4>(gr12,0)
   84| 0001B8 extsw    7CEA07B4   1     EXTS4     gr10=gr7
   86| 0001BC ld       EAE20000   1     L8        gr23=.&&N&field(gr2,0)
   83| 0001C0 addi     38E00000   1     LI        gr7=0
   84| 0001C4 addi     391EFFFE   1     AI        gr8=gr30,-2
   84| 0001C8 extsw    7D0807B4   1     EXTS4     gr8=gr8
   84| 0001CC subf     7D085050   1     S         gr8=gr10,gr8
   86| 0001D0 ld       EB570000   1     L8        gr26=<s24:d0:l8>(gr23,0)
   84| 0001D4 addic.   35680001   1     AI_R      gr11,cr0=gr8,1,ca"
   86| 0001D8 ld       EB370018   1     L8        gr25=<s24:d24:l8>(gr23,24)
   86| 0001DC ld       E9370030   1     L8        gr9=<s24:d48:l8>(gr23,48)
   86| 0001E0 ld       E9570048   1     L8        gr10=<s24:d72:l8>(gr23,72)
   86| 0001E4 ld       E9170060   1     L8        gr8=<s24:d96:l8>(gr23,96)
    0| 0001E8 bc       40811934   1     BF        CL.687,cr0,0x2/gt,taken=50%(0,0)
    0| 0001EC extsw    7F0C07B4   1     EXTS4     gr12=gr24
    0| 0001F0 subfic   23830005   1     SFI       gr28=5,gr3,ca"
    0| 0001F4 ld       EA810190   1     L8        gr20=#SPILL4(gr1,400)
    0| 0001F8 extsw    7C9807B4   1     EXTS4     gr24=gr4
    0| 0001FC add      7D8CE214   1     A         gr12=gr12,gr28
    0| 000200 mulld    7F8341D2   1     M         gr28=gr3,gr8
    0| 000204 ld       EA610170   1     L8        gr19=#SPILL0(gr1,368)
    0| 000208 extsw    7FDE07B4   1     EXTS4     gr30=gr30
    0| 00020C add      7F5ACA14   1     A         gr26=gr26,gr25
    0| 000210 rldicr   791B1764   1     SLL8      gr27=gr8,2
    0| 000214 ld       EAA10180   1     L8        gr21=#SPILL2(gr1,384)
    0| 000218 mulld    7F34C1D2   1     M         gr25=gr20,gr24
    0| 00021C mulld    7F1849D2   1     M         gr24=gr24,gr9
    0| 000220 ld       EA4101A0   1     L8        gr18=#SPILL6(gr1,416)
    0| 000224 rldicr   78771F24   1     SLL8      gr23=gr3,3
    0| 000228 addi     3AD3FFE0   1     AI        gr22=gr19,-32
    0| 00022C subf     7F7B4050   1     S         gr27=gr8,gr27
    0| 000230 add      7F9CD214   1     A         gr28=gr28,gr26
    0| 000234 mulld    7F5EF9D2   1     M         gr26=gr30,gr31
    0| 000238 mulld    7FDE51D2   1     M         gr30=gr30,gr10
    0| 00023C ld       EA210188   1     L8        gr17=#SPILL3(gr1,392)
    0| 000240 add      7EF7B214   1     A         gr23=gr23,gr22
    0| 000244 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 000248 rldicr   7ABD2EA4   1     SLL8      gr29=gr21,5
    0| 00024C add      7F77CA14   1     A         gr27=gr23,gr25
    0| 000250 add      7F98E214   1     A         gr28=gr24,gr28
    0| 000254 addi     3863FFFE   1     AI        gr3=gr3,-2
    0| 000258 addi     3B320002   1     AI        gr25=gr18,2
    0| 00025C subf     7FBDF850   1     S         gr29=gr31,gr29
    0| 000260 add      7F7ADA14   1     A         gr27=gr26,gr27
    0| 000264 rldicr   7A3A2EA4   1     SLL8      gr26=gr17,5
    0| 000268 rldicr   79580FA4   1     SLL8      gr24=gr10,1
    0| 00026C add      7FDEE214   1     A         gr30=gr30,gr28
    0| 000270 extsw    7F3C07B4   1     EXTS4     gr28=gr25
    0| 000274 extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 000278 add      7FBDDA14   1     A         gr29=gr29,gr27
    0| 00027C subf     7F7AA050   1     S         gr27=gr20,gr26
    0| 000280 subf     7FD8F050   1     S         gr30=gr30,gr24
    0| 000284 rldicr   793A0FA4   1     SLL8      gr26=gr9,1
    0| 000288 subf     7C63E050   1     S         gr3=gr28,gr3
    0| 00028C add      7FBDDA14   1     A         gr29=gr29,gr27
    0| 000290 subf     7FDAF050   1     S         gr30=gr30,gr26
    0| 000294 addic.   34630001   1     AI_R      gr3,cr0=gr3,1,ca"
   83|                              CL.229:
   84| 000298 addi     38600000   1     LI        gr3=0
    0| 00029C bc       4081004C   1     BF        CL.233,cr0,0x2/gt,taken=20%(20,80)
    0| 0002A0 or       7FDAF378   1     LR        gr26=gr30
    0| 0002A4 or       7FB9EB78   1     LR        gr25=gr29
   84|                              CL.230:
   86| 0002A8 or       7F5CD378   1     LR        gr28=gr26
    0| 0002AC mtspr    7D8903A6   1     LCTR      ctr=gr12
   86| 0002B0 lfdux    7C1C44EE   1     LFDU      fp0,gr28=d(gr28,gr8,0)
   86| 0002B4 or       7F3BCB78   1     LR        gr27=gr25
    0| 0002B8 bc       42400018   1     BCF       ctr=CL.1094,taken=0%(0,100)
    0| 0002BC ori      60210000   1     XNOP      
    0|                              CL.1095:
   86| 0002C0 lfdux    7C3C44EE   1     LFDU      fp1,gr28=d(gr28,gr8,0)
   86| 0002C4 stfdu    DC1B0008   1     STFDU     gr27,dcopy[](gr27,8)=fp0
    0| 0002C8 fmr      FC000890   1     LRFL      fp0=fp1
    0| 0002CC bc       4200FFF4   1     BCT       ctr=CL.1095,taken=100%(100,0)
    0|                              CL.1094:
   88| 0002D0 addi     38630001   1     AI        gr3=gr3,1
   86| 0002D4 stfdu    DC1B0008   1     STFDU     gr27,dcopy[](gr27,8)=fp0
   88| 0002D8 cmpld    7CA35840   1     CL8       cr1=gr3,gr11
    0| 0002DC add      7F4AD214   1     A         gr26=gr10,gr26
    0| 0002E0 add      7F39FA14   1     A         gr25=gr25,gr31
   88| 0002E4 bc       4184FFC4   1     BT        CL.230,cr1,0x8/llt,taken=80%(80,20)
   88|                              CL.233:
    0| 0002E8 ld       E8610190   1     L8        gr3=#SPILL4(gr1,400)
   89| 0002EC addi     38E70001   1     AI        gr7=gr7,1
    0| 0002F0 add      7FC9F214   1     A         gr30=gr9,gr30
   89| 0002F4 cmpd     7CA53800   1     C8        cr1=gr5,gr7
    0| 0002F8 add      7FA3EA14   1     A         gr29=gr3,gr29
   89| 0002FC bc       4185FF9C   1     BT        CL.229,cr1,0x2/gt,taken=80%(80,20)
   89|                              CL.234:
   83| 000300 cmpd     7D202800   1     C8        cr2=gr0,gr5
   83| 000304 crand    4C314A02   1     CR_N      cr0=cr[42],0x2/gt,0x2/gt,0x2/gt,cr0
   83| 000308 bc       4081028C   1     BF        CL.105,cr0,0x2/gt,taken=50%(0,0)
   84| 00030C ld       E9620000   1     L8        gr11=.&&N&&grid(gr2,0)
   84| 000310 ld       EB810198   1     L8        gr28=#SPILL5(gr1,408)
   89| 000314 addi     3946FFFF   1     AI        gr10=gr6,-1
   86| 000318 ld       EB620000   1     L8        gr27=.&&N&field(gr2,0)
   83| 00031C addi     3B400000   1     LI        gr26=0
   83| 000320 std      FB4101A8   1     ST8       #SPILL7(gr1,424)=gr26
   84| 000324 lwz      812B0008   1     L4Z       gr9=<s64:d8:l4>(gr11,8)
   84| 000328 addi     381C0002   1     AI        gr0=gr28,2
   85| 00032C lwa      E90B0002   1     L4A       gr8=<s64:d0:l4>(gr11,0)
   84| 000330 extsw    7C0007B4   1     EXTS4     gr0=gr0
   86| 000334 ld       E87B0060   1     L8        gr3=<s24:d96:l8>(gr27,96)
   86| 000338 ld       E8FB0030   1     L8        gr7=<s24:d48:l8>(gr27,48)
   84| 00033C addi     38C9FFFE   1     AI        gr6=gr9,-2
   86| 000340 ld       E99B0000   1     L8        gr12=<s24:d0:l8>(gr27,0)
   84| 000344 extsw    7CC607B4   1     EXTS4     gr6=gr6
   86| 000348 ld       EBBB0018   1     L8        gr29=<s24:d24:l8>(gr27,24)
   84| 00034C subf     7CC60050   1     S         gr6=gr0,gr6
   86| 000350 ld       EBDB0048   1     L8        gr30=<s24:d72:l8>(gr27,72)
   84| 000354 addic.   34060001   1     AI_R      gr0,cr0=gr6,1,ca"
   89| 000358 sradi    7D461674   1     SRA8CA    gr6,ca=gr10,2
   89| 00035C addze    7CC60194   1     ADDE      gr6,ca=gr6,0,ca
    0| 000360 bc       40810234   1     BF        CL.105,cr0,0x2/gt,taken=50%(0,0)
    0| 000364 ld       EB010190   1     L8        gr24=#SPILL4(gr1,400)
    0| 000368 extsw    7C8407B4   1     EXTS4     gr4=gr4
    0| 00036C mulld    7D4341D2   1     M         gr10=gr3,gr8
    0| 000370 ld       EAE10170   1     L8        gr23=#SPILL0(gr1,368)
    0| 000374 extsw    7D2B07B4   1     EXTS4     gr11=gr9
    0| 000378 rldicr   787C1764   1     SLL8      gr28=gr3,2
    0| 00037C add      7F6CEA14   1     A         gr27=gr12,gr29
    0| 000380 mulld    7D24C1D2   1     M         gr9=gr4,gr24
    0| 000384 mulld    7C8439D2   1     M         gr4=gr4,gr7
    0| 000388 ld       EAC10180   1     L8        gr22=#SPILL2(gr1,384)
    0| 00038C rldicr   790C1F24   1     SLL8      gr12=gr8,3
    0| 000390 addi     3BB7FFE0   1     AI        gr29=gr23,-32
    0| 000394 subf     7F9C1850   1     S         gr28=gr3,gr28
    0| 000398 add      7F6ADA14   1     A         gr27=gr10,gr27
    0| 00039C mulld    7D4BF9D2   1     M         gr10=gr11,gr31
    0| 0003A0 mulld    7D6BF1D2   1     M         gr11=gr11,gr30
    0| 0003A4 ld       EAA101A0   1     L8        gr21=#SPILL6(gr1,416)
    0| 0003A8 add      7FACEA14   1     A         gr29=gr12,gr29
    0| 0003AC add      7F9CDA14   1     A         gr28=gr28,gr27
    0| 0003B0 rldicr   7ACC2EA4   1     SLL8      gr12=gr22,5
    0| 0003B4 add      7FA9EA14   1     A         gr29=gr9,gr29
    0| 0003B8 add      7F84E214   1     A         gr28=gr4,gr28
    0| 0003BC mulld    7D25C1D2   1     M         gr9=gr5,gr24
    0| 0003C0 mulld    7CA539D2   1     M         gr5=gr5,gr7
    0| 0003C4 ld       EA810188   1     L8        gr20=#SPILL3(gr1,392)
    0| 0003C8 addi     3B68FFFE   1     AI        gr27=gr8,-2
    0| 0003CC addi     3B550002   1     AI        gr26=gr21,2
    0| 0003D0 subfic   20880005   1     SFI       gr4=5,gr8,ca"
    0| 0003D4 subf     7D0CF850   1     S         gr8=gr31,gr12
    0| 0003D8 add      7D4AEA14   1     A         gr10=gr10,gr29
    0| 0003DC rldicr   7BCC0FA4   1     SLL8      gr12=gr30,1
    0| 0003E0 add      7D6BE214   1     A         gr11=gr11,gr28
    0| 0003E4 extsw    7EB907B4   1     EXTS4     gr25=gr21
    0| 0003E8 extsw    7F5D07B4   1     EXTS4     gr29=gr26
    0| 0003EC extsw    7F7C07B4   1     EXTS4     gr28=gr27
    0| 0003F0 add      7D085214   1     A         gr8=gr8,gr10
    0| 0003F4 subf     7D6C5850   1     S         gr11=gr11,gr12
    0| 0003F8 add      7F64CA14   1     A         gr27=gr4,gr25
    0| 0003FC rldicr   7A932EA4   1     SLL8      gr19=gr20,5
    0| 000400 subf     7D5CE850   1     S         gr10=gr29,gr28
    0| 000404 std      FA6101B0   1     ST8       #SPILL8(gr1,432)=gr19
    0| 000408 add      7FA84A14   1     A         gr29=gr8,gr9
    0| 00040C add      7F855A14   1     A         gr28=gr5,gr11
    0| 000410 subf     7D33C050   1     S         gr9=gr24,gr19
    0| 000414 rldicr   7A8826E4   1     SLL8      gr8=gr20,4
    0| 000418 rldicr   78E50FA4   1     SLL8      gr5=gr7,1
    0| 00041C addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
    0| 000420 rldicl   7B7BF842   1     SRL8      gr27=gr27,1
    0| 000424 add      7C84AA14   1     A         gr4=gr4,gr21
    0| 000428 subf     7E58E850   1     S         gr18=gr29,gr24
    0| 00042C rldicr   78F11764   1     SLL8      gr17=gr7,2
    0| 000430 std      FA4101B8   1     ST8       #SPILL9(gr1,440)=gr18
    0| 000434 std      FA2101C0   1     ST8       #SPILL10(gr1,448)=gr17
    0| 000438 subf     7F07E050   1     S         gr24=gr28,gr7
    0| 00043C add      7EC7E214   1     A         gr22=gr7,gr28
    0| 000440 addi     38E60001   1     AI        gr7=gr6,1
    0| 000444 add      7F49EA14   1     A         gr26=gr9,gr29
    0| 000448 std      F8E101C8   1     ST8       #SPILL11(gr1,456)=gr7
    0| 00044C subf     7F28E850   1     S         gr25=gr29,gr8
    0| 000450 subf     7EE5E050   1     S         gr23=gr28,gr5
    0| 000454 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000458 andi.    70840001   1     RN4_R     gr4,cr0=gr4,0,0x1
    0| 00045C cmpdi    2FBB0000   1     C8        cr7=gr27,0
   83|                              CL.106:
   84| 000460 addi     38800000   1     LI        gr4=0
    0| 000464 bc       408500E8   1     BF        CL.107,cr1,0x2/gt,taken=20%(20,80)
    0| 000468 or       7EF5BB78   1     LR        gr21=gr23
    0| 00046C or       7F94E378   1     LR        gr20=gr28
    0| 000470 or       7ED3B378   1     LR        gr19=gr22
    0| 000474 or       7F12C378   1     LR        gr18=gr24
    0| 000478 ld       EA2101B8   1     L8        gr17=#SPILL9(gr1,440)
    0| 00047C or       7F50D378   1     LR        gr16=gr26
    0| 000480 or       7F2FCB78   1     LR        gr15=gr25
    0| 000484 or       7FAEEB78   1     LR        gr14=gr29
   84|                              CL.108:
   86| 000488 or       7EA5AB78   1     LR        gr5=gr21
   86| 00048C or       7E469378   1     LR        gr6=gr18
   86| 000490 or       7E87A378   1     LR        gr7=gr20
   86| 000494 or       7E689B78   1     LR        gr8=gr19
   86| 000498 or       7E098378   1     LR        gr9=gr16
   86| 00049C or       7DEA7B78   1     LR        gr10=gr15
   86| 0004A0 or       7E2B8B78   1     LR        gr11=gr17
   86| 0004A4 or       7DCC7378   1     LR        gr12=gr14
    0| 0004A8 mtspr    7F6903A6   1     LCTR      ctr=gr27
    0| 0004AC bc       41820030   1     BT        CL.1049,cr0,0x4/eq,taken=50%(0,0)
   86| 0004B0 lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
   86| 0004B4 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
   86| 0004B8 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
   86| 0004BC lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
   86| 0004C0 stfdu    DC090008   1     STFDU     gr9,dcopy[](gr9,8)=fp0
   86| 0004C4 stfdu    DC2A0008   1     STFDU     gr10,dcopy[](gr10,8)=fp1
   86| 0004C8 stfdu    DC4B0008   1     STFDU     gr11,dcopy[](gr11,8)=fp2
   86| 0004CC stfdu    DC6C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp3
    0| 0004D0 bc       419E0050   1     BT        CL.931,cr7,0x4/eq,taken=20%(20,80)
    0| 0004D4 ori      60210000   1     XNOP      
    0| 0004D8 ori      60210000   1     XNOP      
    0|                              CL.1049:
   86| 0004DC lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
   86| 0004E0 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
   86| 0004E4 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
   86| 0004E8 lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
   86| 0004EC lfdux    7C851CEE   1     LFDU      fp4,gr5=d(gr5,gr3,0)
   86| 0004F0 lfdux    7CA61CEE   1     LFDU      fp5,gr6=d(gr6,gr3,0)
   86| 0004F4 lfdux    7CC71CEE   1     LFDU      fp6,gr7=d(gr7,gr3,0)
   86| 0004F8 lfdux    7CE81CEE   1     LFDU      fp7,gr8=d(gr8,gr3,0)
   86| 0004FC stfd     D8090008   1     STFL      dcopy[](gr9,8)=fp0
   86| 000500 stfd     D82A0008   1     STFL      dcopy[](gr10,8)=fp1
   86| 000504 stfd     D84B0008   1     STFL      dcopy[](gr11,8)=fp2
   86| 000508 stfd     D86C0008   1     STFL      dcopy[](gr12,8)=fp3
   86| 00050C stfdu    DC890010   1     STFDU     gr9,dcopy[](gr9,16)=fp4
   86| 000510 stfdu    DCAA0010   1     STFDU     gr10,dcopy[](gr10,16)=fp5
   86| 000514 stfdu    DCCB0010   1     STFDU     gr11,dcopy[](gr11,16)=fp6
   86| 000518 stfdu    DCEC0010   1     STFDU     gr12,dcopy[](gr12,16)=fp7
    0| 00051C bc       4200FFC0   1     BCT       ctr=CL.1049,taken=100%(100,0)
    0|                              CL.931:
   88| 000520 addi     38840001   1     AI        gr4=gr4,1
    0| 000524 add      7EB5F214   1     A         gr21=gr21,gr30
   88| 000528 cmpld    7F240040   1     CL8       cr6=gr4,gr0
    0| 00052C add      7E94F214   1     A         gr20=gr20,gr30
    0| 000530 add      7E73F214   1     A         gr19=gr19,gr30
    0| 000534 add      7E52F214   1     A         gr18=gr18,gr30
    0| 000538 add      7E31FA14   1     A         gr17=gr17,gr31
    0| 00053C add      7E10FA14   1     A         gr16=gr16,gr31
    0| 000540 add      7DEFFA14   1     A         gr15=gr15,gr31
    0| 000544 add      7DCEFA14   1     A         gr14=gr14,gr31
   88| 000548 bc       4198FF40   1     BT        CL.108,cr6,0x8/llt,taken=80%(80,20)
   88|                              CL.107:
   89| 00054C ld       E88101A8   1     L8        gr4=#SPILL7(gr1,424)
    0| 000550 ld       E8A101B0   1     L8        gr5=#SPILL8(gr1,432)
   89| 000554 ld       E8C101C8   1     L8        gr6=#SPILL11(gr1,456)
    0| 000558 ld       E8E101B8   1     L8        gr7=#SPILL9(gr1,440)
    0| 00055C ld       E90101C0   1     L8        gr8=#SPILL10(gr1,448)
   89| 000560 addi     38840001   1     AI        gr4=gr4,1
    0| 000564 add      7FA5EA14   1     A         gr29=gr5,gr29
   89| 000568 cmpld    7F243040   1     CL8       cr6=gr4,gr6
    0| 00056C add      7CE53A14   1     A         gr7=gr5,gr7
   89| 000570 std      F88101A8   1     ST8       #SPILL7(gr1,424)=gr4
    0| 000574 std      F8E101B8   1     ST8       #SPILL9(gr1,440)=gr7
    0| 000578 add      7F45D214   1     A         gr26=gr5,gr26
    0| 00057C add      7F25CA14   1     A         gr25=gr5,gr25
    0| 000580 add      7F08C214   1     A         gr24=gr8,gr24
    0| 000584 add      7EE8BA14   1     A         gr23=gr8,gr23
    0| 000588 add      7EC8B214   1     A         gr22=gr8,gr22
    0| 00058C add      7F88E214   1     A         gr28=gr8,gr28
   89| 000590 bc       4198FED0   1     BT        CL.106,cr6,0x8/llt,taken=80%(80,20)
   89|                              CL.105:
   91| 000594 ld       EBA10198   1     L8        gr29=#SPILL5(gr1,408)
   91| 000598 ld       EBC101A0   1     L8        gr30=#SPILL6(gr1,416)
   91| 00059C addi     388100A0   1     AI        gr4=gr1,160
   91| 0005A0 addi     38C100A4   1     AI        gr6=gr1,164
   91| 0005A4 addi     39010080   1     AI        gr8=gr1,128
   91| 0005A8 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
   91| 0005AC addi     387DFFFF   1     AI        gr3=gr29,-1
   91| 0005B0 addi     381EFFFF   1     AI        gr0=gr30,-1
   91| 0005B4 stw      906100A4   1     ST4Z      T_9(gr1,164)=gr3
   91| 0005B8 ld       E8620000   1     L8        gr3=.&&N&&grid(gr2,0)
   91| 0005BC stw      900100A0   1     ST4Z      T_8(gr1,160)=gr0
   91| 0005C0 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
   91| 0005C4 or       7C7C1B78   1     LR        gr28=gr3
   91| 0005C8 addi     38BC0008   1     AI        gr5=gr28,8
   91| 0005CC addi     38FC0010   1     AI        gr7=gr28,16
   91| 0005D0 bl       48000001   1     CALL      pdv,8,is",gr3,T_8",gr4,js",gr5,T_9",gr6,ks",gr7,k1",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   91| 0005D4 ori      60000000   1
  103| 0005D8 lwa      E81C0016   1     L4A       gr0=<s64:d20:l4>(gr28,20)
  103| 0005DC lwa      E89C0012   1     L4A       gr4=<s64:d16:l4>(gr28,16)
  103| 0005E0 subf     7C640050   1     S         gr3=gr0,gr4
  103| 0005E4 addi     38030001   1     AI        gr0=gr3,1
  103| 0005E8 sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
  103| 0005EC cmpdi    2F200000   1     C8        cr6=gr0,0
  103| 0005F0 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  103| 0005F4 rldicr   78661764   1     SLL8      gr6=gr3,2
  103| 0005F8 subf     7CA60051   1     S_R       gr5,cr0=gr0,gr6
  103| 0005FC crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  103| 000600 bc       40810140   1     BF        CL.222,cr0,0x2/gt,taken=50%(0,0)
  104| 000604 lwz      83DC0008   1     L4Z       gr30=<s64:d8:l4>(gr28,8)
  106| 000608 ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  105| 00060C lwa      E87C0002   1     L4A       gr3=<s64:d0:l4>(gr28,0)
  104| 000610 addi     393EFFFF   1     AI        gr9=gr30,-1
  104| 000614 addi     391EFFFE   1     AI        gr8=gr30,-2
  104| 000618 extsw    7D2907B4   1     EXTS4     gr9=gr9
  104| 00061C extsw    7D0807B4   1     EXTS4     gr8=gr8
  106| 000620 ld       E8FD0060   1     L8        gr7=<s24:d96:l8>(gr29,96)
  104| 000624 subf     7D284850   1     S         gr9=gr9,gr8
  106| 000628 ld       E95D0030   1     L8        gr10=<s24:d48:l8>(gr29,48)
  104| 00062C addic.   35890001   1     AI_R      gr12,cr0=gr9,1,ca"
  106| 000630 ld       EB1D0000   1     L8        gr24=<s24:d0:l8>(gr29,0)
  106| 000634 ld       EAFD0018   1     L8        gr23=<s24:d24:l8>(gr29,24)
  106| 000638 ld       E97D0048   1     L8        gr11=<s24:d72:l8>(gr29,72)
  103| 00063C addi     39000000   1     LI        gr8=0
  105| 000640 lwa      E93C0006   1     L4A       gr9=<s64:d4:l4>(gr28,4)
    0| 000644 bc       408114C4   1     BF        CL.697,cr0,0x2/gt,taken=50%(0,0)
    0| 000648 ld       EAA10190   1     L8        gr21=#SPILL4(gr1,400)
    0| 00064C ld       EA810170   1     L8        gr20=#SPILL0(gr1,368)
    0| 000650 extsw    7FDE07B4   1     EXTS4     gr30=gr30
    0| 000654 mulld    7FA339D2   1     M         gr29=gr3,gr7
    0| 000658 mulld    7F84A9D2   1     M         gr28=gr4,gr21
    0| 00065C ld       EA610180   1     L8        gr19=#SPILL2(gr1,384)
    0| 000660 rldicr   787B1F24   1     SLL8      gr27=gr3,3
    0| 000664 addi     3B54FFE0   1     AI        gr26=gr20,-32
    0| 000668 rldicr   78F91764   1     SLL8      gr25=gr7,2
    0| 00066C add      7F18BA14   1     A         gr24=gr24,gr23
    0| 000670 mulld    7EFEF9D2   1     M         gr23=gr30,gr31
    0| 000674 mulld    7EC451D2   1     M         gr22=gr4,gr10
    0| 000678 add      7F7BD214   1     A         gr27=gr27,gr26
    0| 00067C subf     7F593850   1     S         gr26=gr7,gr25
    0| 000680 add      7FBDC214   1     A         gr29=gr29,gr24
    0| 000684 mulld    7FDE59D2   1     M         gr30=gr30,gr11
    0| 000688 rldicr   7A792EA4   1     SLL8      gr25=gr19,5
    0| 00068C add      7F9CDA14   1     A         gr28=gr28,gr27
    0| 000690 add      7FBAEA14   1     A         gr29=gr26,gr29
    0| 000694 addi     3B690002   1     AI        gr27=gr9,2
    0| 000698 addi     3B43FFFE   1     AI        gr26=gr3,-2
    0| 00069C subf     7F39F850   1     S         gr25=gr31,gr25
    0| 0006A0 add      7F97E214   1     A         gr28=gr23,gr28
    0| 0006A4 add      7FB6EA14   1     A         gr29=gr22,gr29
    0| 0006A8 extsw    7F7B07B4   1     EXTS4     gr27=gr27
    0| 0006AC extsw    7F5A07B4   1     EXTS4     gr26=gr26
    0| 0006B0 subfic   20630005   1     SFI       gr3=5,gr3,ca"
    0| 0006B4 add      7F99E214   1     A         gr28=gr25,gr28
    0| 0006B8 rldicr   79790FA4   1     SLL8      gr25=gr11,1
    0| 0006BC add      7FDEEA14   1     A         gr30=gr30,gr29
    0| 0006C0 subf     7FBAD850   1     S         gr29=gr27,gr26
    0| 0006C4 add      7C634A14   1     A         gr3=gr3,gr9
    0| 0006C8 subf     7D35E050   1     S         gr9=gr28,gr21
    0| 0006CC subf     7FD9F050   1     S         gr30=gr30,gr25
    0| 0006D0 addic.   37BD0001   1     AI_R      gr29,cr0=gr29,1,ca"
  103|                              CL.217:
  104| 0006D4 addi     3BA00000   1     LI        gr29=0
    0| 0006D8 bc       40810050   1     BF        CL.221,cr0,0x2/gt,taken=20%(20,80)
    0| 0006DC or       7FDAF378   1     LR        gr26=gr30
    0| 0006E0 or       7D394B78   1     LR        gr25=gr9
  104|                              CL.218:
  106| 0006E4 or       7F5CD378   1     LR        gr28=gr26
    0| 0006E8 mtspr    7C6903A6   1     LCTR      ctr=gr3
  106| 0006EC lfdux    7C1C3CEE   1     LFDU      fp0,gr28=d(gr28,gr7,0)
  106| 0006F0 or       7F3BCB78   1     LR        gr27=gr25
    0| 0006F4 bc       4240001C   1     BCF       ctr=CL.1097,taken=0%(0,100)
    0| 0006F8 ori      60210000   1     XNOP      
    0| 0006FC ori      60210000   1     XNOP      
    0|                              CL.1098:
  106| 000700 lfdux    7C3C3CEE   1     LFDU      fp1,gr28=d(gr28,gr7,0)
  106| 000704 stfdu    DC1B0008   1     STFDU     gr27,dcopy[](gr27,8)=fp0
    0| 000708 fmr      FC000890   1     LRFL      fp0=fp1
    0| 00070C bc       4200FFF4   1     BCT       ctr=CL.1098,taken=100%(100,0)
    0|                              CL.1097:
  108| 000710 addi     3BBD0001   1     AI        gr29=gr29,1
  106| 000714 stfdu    DC1B0008   1     STFDU     gr27,dcopy[](gr27,8)=fp0
  108| 000718 cmpld    7CBD6040   1     CL8       cr1=gr29,gr12
    0| 00071C add      7F4BD214   1     A         gr26=gr11,gr26
    0| 000720 add      7F39FA14   1     A         gr25=gr25,gr31
  108| 000724 bc       4184FFC0   1     BT        CL.218,cr1,0x8/llt,taken=80%(80,20)
  108|                              CL.221:
    0| 000728 ld       EBA10190   1     L8        gr29=#SPILL4(gr1,400)
  109| 00072C addi     39080001   1     AI        gr8=gr8,1
    0| 000730 add      7FCAF214   1     A         gr30=gr10,gr30
  109| 000734 cmpd     7CA54000   1     C8        cr1=gr5,gr8
    0| 000738 add      7D29EA14   1     A         gr9=gr9,gr29
  109| 00073C bc       4185FF98   1     BT        CL.217,cr1,0x2/gt,taken=80%(80,20)
  109|                              CL.222:
  103| 000740 cmpd     7CA02800   1     C8        cr1=gr0,gr5
  103| 000744 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  103| 000748 bc       4081027C   1     BF        CL.111,cr0,0x2/gt,taken=50%(0,0)
  104| 00074C ld       E9420000   1     L8        gr10=.&&N&&grid(gr2,0)
  106| 000750 ld       E9820000   1     L8        gr12=.&&N&field(gr2,0)
  109| 000754 addi     3926FFFF   1     AI        gr9=gr6,-1
  103| 000758 addi     3B600000   1     LI        gr27=0
  103| 00075C std      FB6101D0   1     ST8       #SPILL12(gr1,464)=gr27
  104| 000760 lwz      816A0008   1     L4Z       gr11=<s64:d8:l4>(gr10,8)
  105| 000764 lwa      E90A0002   1     L4A       gr8=<s64:d0:l4>(gr10,0)
  106| 000768 ld       E86C0060   1     L8        gr3=<s24:d96:l8>(gr12,96)
  106| 00076C ld       E8EC0030   1     L8        gr7=<s24:d48:l8>(gr12,48)
  106| 000770 ld       EBAC0000   1     L8        gr29=<s24:d0:l8>(gr12,0)
  106| 000774 ld       EB8C0018   1     L8        gr28=<s24:d24:l8>(gr12,24)
  104| 000778 addi     38CBFFFF   1     AI        gr6=gr11,-1
  104| 00077C addi     380BFFFE   1     AI        gr0=gr11,-2
  104| 000780 extsw    7CC607B4   1     EXTS4     gr6=gr6
  104| 000784 extsw    7C0007B4   1     EXTS4     gr0=gr0
  104| 000788 subf     7CC03050   1     S         gr6=gr6,gr0
  106| 00078C ld       E80C0048   1     L8        gr0=<s24:d72:l8>(gr12,72)
  104| 000790 addic.   37C60001   1     AI_R      gr30,cr0=gr6,1,ca"
  109| 000794 sradi    7D261674   1     SRA8CA    gr6,ca=gr9,2
  105| 000798 lwa      E92A0006   1     L4A       gr9=<s64:d4:l4>(gr10,4)
  109| 00079C addze    7CC60194   1     ADDE      gr6,ca=gr6,0,ca
    0| 0007A0 bc       40810224   1     BF        CL.111,cr0,0x2/gt,taken=50%(0,0)
    0| 0007A4 ld       EB210190   1     L8        gr25=#SPILL4(gr1,400)
    0| 0007A8 mulld    7D4341D2   1     M         gr10=gr3,gr8
    0| 0007AC ld       EB010170   1     L8        gr24=#SPILL0(gr1,368)
    0| 0007B0 extsw    7D6B07B4   1     EXTS4     gr11=gr11
    0| 0007B4 rldicr   786C1764   1     SLL8      gr12=gr3,2
    0| 0007B8 add      7FBDE214   1     A         gr29=gr29,gr28
    0| 0007BC mulld    7F84C9D2   1     M         gr28=gr4,gr25
    0| 0007C0 mulld    7C8439D2   1     M         gr4=gr4,gr7
    0| 0007C4 subf     7D8C1850   1     S         gr12=gr3,gr12
    0| 0007C8 add      7D4AEA14   1     A         gr10=gr10,gr29
    0| 0007CC ld       EAE10180   1     L8        gr23=#SPILL2(gr1,384)
    0| 0007D0 rldicr   791B1F24   1     SLL8      gr27=gr8,3
    0| 0007D4 addi     3B58FFE0   1     AI        gr26=gr24,-32
    0| 0007D8 mulld    7FABF9D2   1     M         gr29=gr11,gr31
    0| 0007DC mulld    7D6B01D2   1     M         gr11=gr11,gr0
    0| 0007E0 add      7D8C5214   1     A         gr12=gr12,gr10
    0| 0007E4 add      7F7BD214   1     A         gr27=gr27,gr26
    0| 0007E8 rldicr   7AEA2EA4   1     SLL8      gr10=gr23,5
    0| 0007EC add      7F9CDA14   1     A         gr28=gr28,gr27
    0| 0007F0 add      7C846214   1     A         gr4=gr4,gr12
    0| 0007F4 mulld    7D85C9D2   1     M         gr12=gr5,gr25
    0| 0007F8 mulld    7CA539D2   1     M         gr5=gr5,gr7
    0| 0007FC addi     3B690002   1     AI        gr27=gr9,2
    0| 000800 addi     3B48FFFE   1     AI        gr26=gr8,-2
    0| 000804 subfic   21080005   1     SFI       gr8=5,gr8,ca"
    0| 000808 subf     7D4AF850   1     S         gr10=gr31,gr10
    0| 00080C add      7F9CEA14   1     A         gr28=gr28,gr29
    0| 000810 rldicr   781D0FA4   1     SLL8      gr29=gr0,1
    0| 000814 add      7D6B2214   1     A         gr11=gr11,gr4
    0| 000818 ld       EA810188   1     L8        gr20=#SPILL3(gr1,392)
    0| 00081C extsw    7F7B07B4   1     EXTS4     gr27=gr27
    0| 000820 extsw    7F5A07B4   1     EXTS4     gr26=gr26
    0| 000824 add      7C884A14   1     A         gr4=gr8,gr9
    0| 000828 add      7D0AE214   1     A         gr8=gr10,gr28
    0| 00082C subf     7D7D5850   1     S         gr11=gr11,gr29
    0| 000830 rldicr   78F51764   1     SLL8      gr21=gr7,2
    0| 000834 subf     7D5AD850   1     S         gr10=gr27,gr26
    0| 000838 std      FAA101D8   1     ST8       #SPILL13(gr1,472)=gr21
    0| 00083C add      7FA86214   1     A         gr29=gr8,gr12
    0| 000840 add      7F855A14   1     A         gr28=gr5,gr11
    0| 000844 rldicr   7A8926E4   1     SLL8      gr9=gr20,4
    0| 000848 rldicr   78E80FA4   1     SLL8      gr8=gr7,1
    0| 00084C subf     7CA7A850   1     S         gr5=gr21,gr7
    0| 000850 addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
    0| 000854 rldicl   789BF842   1     SRL8      gr27=gr4,1
    0| 000858 rldicr   7A932EA4   1     SLL8      gr19=gr20,5
    0| 00085C add      7E59EA14   1     A         gr18=gr25,gr29
    0| 000860 std      FA6101E0   1     ST8       #SPILL14(gr1,480)=gr19
    0| 000864 std      FA4101E8   1     ST8       #SPILL15(gr1,488)=gr18
    0| 000868 add      7EC7E214   1     A         gr22=gr7,gr28
    0| 00086C addi     38E60001   1     AI        gr7=gr6,1
    0| 000870 add      7F49EA14   1     A         gr26=gr9,gr29
    0| 000874 std      F8E101F0   1     ST8       #SPILL16(gr1,496)=gr7
    0| 000878 subf     7F39E850   1     S         gr25=gr29,gr25
    0| 00087C add      7F08E214   1     A         gr24=gr8,gr28
    0| 000880 add      7EE5E214   1     A         gr23=gr5,gr28
    0| 000884 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000888 andi.    70840001   1     RN4_R     gr4,cr0=gr4,0,0x1
    0| 00088C cmpdi    2FBB0000   1     C8        cr7=gr27,0
  103|                              CL.112:
  104| 000890 addi     38800000   1     LI        gr4=0
    0| 000894 bc       408500E8   1     BF        CL.113,cr1,0x2/gt,taken=20%(20,80)
    0| 000898 or       7EF5BB78   1     LR        gr21=gr23
    0| 00089C or       7F94E378   1     LR        gr20=gr28
    0| 0008A0 or       7ED3B378   1     LR        gr19=gr22
    0| 0008A4 or       7F12C378   1     LR        gr18=gr24
    0| 0008A8 or       7FB1EB78   1     LR        gr17=gr29
    0| 0008AC or       7F50D378   1     LR        gr16=gr26
    0| 0008B0 or       7F2FCB78   1     LR        gr15=gr25
    0| 0008B4 ld       E9C101E8   1     L8        gr14=#SPILL15(gr1,488)
  104|                              CL.114:
  106| 0008B8 or       7E85A378   1     LR        gr5=gr20
  106| 0008BC or       7E669B78   1     LR        gr6=gr19
  106| 0008C0 or       7E479378   1     LR        gr7=gr18
  106| 0008C4 or       7EA8AB78   1     LR        gr8=gr21
  106| 0008C8 or       7DE97B78   1     LR        gr9=gr15
  106| 0008CC or       7E2A8B78   1     LR        gr10=gr17
  106| 0008D0 or       7DCB7378   1     LR        gr11=gr14
  106| 0008D4 or       7E0C8378   1     LR        gr12=gr16
    0| 0008D8 mtspr    7F6903A6   1     LCTR      ctr=gr27
    0| 0008DC bc       41820030   1     BT        CL.1057,cr0,0x4/eq,taken=50%(0,0)
  106| 0008E0 lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
  106| 0008E4 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
  106| 0008E8 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
  106| 0008EC lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
  106| 0008F0 stfdu    DC090008   1     STFDU     gr9,dcopy[](gr9,8)=fp0
  106| 0008F4 stfdu    DC2A0008   1     STFDU     gr10,dcopy[](gr10,8)=fp1
  106| 0008F8 stfdu    DC4B0008   1     STFDU     gr11,dcopy[](gr11,8)=fp2
  106| 0008FC stfdu    DC6C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp3
    0| 000900 bc       419E0050   1     BT        CL.937,cr7,0x4/eq,taken=20%(20,80)
    0| 000904 ori      60210000   1     XNOP      
    0| 000908 ori      60210000   1     XNOP      
    0|                              CL.1057:
  106| 00090C lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
  106| 000910 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
  106| 000914 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
  106| 000918 lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
  106| 00091C lfdux    7C851CEE   1     LFDU      fp4,gr5=d(gr5,gr3,0)
  106| 000920 lfdux    7CA61CEE   1     LFDU      fp5,gr6=d(gr6,gr3,0)
  106| 000924 lfdux    7CC71CEE   1     LFDU      fp6,gr7=d(gr7,gr3,0)
  106| 000928 lfdux    7CE81CEE   1     LFDU      fp7,gr8=d(gr8,gr3,0)
  106| 00092C stfd     D8090008   1     STFL      dcopy[](gr9,8)=fp0
  106| 000930 stfd     D82A0008   1     STFL      dcopy[](gr10,8)=fp1
  106| 000934 stfd     D84B0008   1     STFL      dcopy[](gr11,8)=fp2
  106| 000938 stfd     D86C0008   1     STFL      dcopy[](gr12,8)=fp3
  106| 00093C stfdu    DC890010   1     STFDU     gr9,dcopy[](gr9,16)=fp4
  106| 000940 stfdu    DCAA0010   1     STFDU     gr10,dcopy[](gr10,16)=fp5
  106| 000944 stfdu    DCCB0010   1     STFDU     gr11,dcopy[](gr11,16)=fp6
  106| 000948 stfdu    DCEC0010   1     STFDU     gr12,dcopy[](gr12,16)=fp7
    0| 00094C bc       4200FFC0   1     BCT       ctr=CL.1057,taken=100%(100,0)
    0|                              CL.937:
  108| 000950 addi     38840001   1     AI        gr4=gr4,1
    0| 000954 add      7EA0AA14   1     A         gr21=gr0,gr21
  108| 000958 cmpld    7F24F040   1     CL8       cr6=gr4,gr30
    0| 00095C add      7E80A214   1     A         gr20=gr0,gr20
    0| 000960 add      7E609A14   1     A         gr19=gr0,gr19
    0| 000964 add      7E409214   1     A         gr18=gr0,gr18
    0| 000968 add      7E31FA14   1     A         gr17=gr17,gr31
    0| 00096C add      7E10FA14   1     A         gr16=gr16,gr31
    0| 000970 add      7DEFFA14   1     A         gr15=gr15,gr31
    0| 000974 add      7DCEFA14   1     A         gr14=gr14,gr31
  108| 000978 bc       4198FF40   1     BT        CL.114,cr6,0x8/llt,taken=80%(80,20)
  108|                              CL.113:
  109| 00097C ld       E88101D0   1     L8        gr4=#SPILL12(gr1,464)
    0| 000980 ld       E8A101E0   1     L8        gr5=#SPILL14(gr1,480)
    0| 000984 ld       E8C101E8   1     L8        gr6=#SPILL15(gr1,488)
  109| 000988 ld       E8E101F0   1     L8        gr7=#SPILL16(gr1,496)
    0| 00098C ld       E90101D8   1     L8        gr8=#SPILL13(gr1,472)
  109| 000990 addi     38840001   1     AI        gr4=gr4,1
    0| 000994 add      7F45D214   1     A         gr26=gr5,gr26
    0| 000998 add      7CC53214   1     A         gr6=gr5,gr6
  109| 00099C cmpld    7F243840   1     CL8       cr6=gr4,gr7
  109| 0009A0 std      F88101D0   1     ST8       #SPILL12(gr1,464)=gr4
    0| 0009A4 std      F8C101E8   1     ST8       #SPILL15(gr1,488)=gr6
    0| 0009A8 add      7F25CA14   1     A         gr25=gr5,gr25
    0| 0009AC add      7FA5EA14   1     A         gr29=gr5,gr29
    0| 0009B0 add      7F08C214   1     A         gr24=gr8,gr24
    0| 0009B4 add      7EE8BA14   1     A         gr23=gr8,gr23
    0| 0009B8 add      7EC8B214   1     A         gr22=gr8,gr22
    0| 0009BC add      7F88E214   1     A         gr28=gr8,gr28
  109| 0009C0 bc       4198FED0   1     BT        CL.112,cr6,0x8/llt,taken=80%(80,20)
  109|                              CL.111:
  113| 0009C4 ld       E8620000   1     L8        gr3=.&&N&&mpipar(gr2,0)
  113| 0009C8 lwz      80030018   1     L4Z       gr0=<s70:d24:l4>(gr3,24)
  113| 0009CC cmpdi    2C200000   1     C8        cr0=gr0,0
  113| 0009D0 bc       4182001C   1     BT        CL.29,cr0,0x4/eq,taken=60%(60,40)
  114| 0009D4 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  114| 0009D8 addi     38C30014   1     AI        gr6=gr3,20
  114| 0009DC addi     38630018   1     AI        gr3=gr3,24
  114| 0009E0 addi     38851770   1     AI        gr4=gr5,6000
  114| 0009E4 bl       48000001   1     CALL      mpi_waitall,4,nreq",gr3,req[]",gr4,stat[]",gr5,ierr",gr6,mpi_waitall",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  114| 0009E8 ori      60000000   1
  114|                              CL.29:
  122| 0009EC ld       EBA20000   1     L8        gr29=.&&N&&mpipar(gr2,0)
  123| 0009F0 ld       EB620000   1     L8        gr27=.&&N&field(gr2,0)
  123| 0009F4 addi     3BC00000   1     LI        gr30=0
  123| 0009F8 addi     3B800001   1     LI        gr28=1
  123| 0009FC stw      93C100A8   1     ST4Z      T_10(gr1,168)=gr30
  123| 000A00 stw      93C100AC   1     ST4Z      T_11(gr1,172)=gr30
  122| 000A04 lwz      807D001C   1     L4Z       gr3=<s70:d28:l4>(gr29,28)
  121| 000A08 stw      93DD0018   1     ST4Z      <s70:d24:l4>(gr29,24)=gr30
  123| 000A0C stw      93C100B0   1     ST4Z      T_12(gr1,176)=gr30
  123| 000A10 stw      938100B4   1     ST4Z      T_13(gr1,180)=gr28
  123| 000A14 stw      93C100B8   1     ST4Z      T_14(gr1,184)=gr30
  123| 000A18 stw      93C100BC   1     ST4Z      T_15(gr1,188)=gr30
  122| 000A1C addi     38030001   1     AI        gr0=gr3,1
  123| 000A20 ld       E93B0208   1     L8        gr9=<s24:d520:l8>(gr27,520)
  122| 000A24 stw      901D001C   1     ST4Z      <s70:d28:l4>(gr29,28)=gr0
  123| 000A28 addi     390100BC   1     AI        gr8=gr1,188
  123| 000A2C addi     38E100B8   1     AI        gr7=gr1,184
  123| 000A30 addi     38C100B4   1     AI        gr6=gr1,180
  123| 000A34 addi     38A100B0   1     AI        gr5=gr1,176
  123| 000A38 addi     388100AC   1     AI        gr4=gr1,172
  123| 000A3C addi     386100A8   1     AI        gr3=gr1,168
  123| 000A40 bl       48000001   1     CALL      bvalv2,7,T_10",gr3,T_11",gr4,T_12",gr5,T_13",gr6,T_14",gr7,T_15",gr8,v2",gr9,bvalv2",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  123| 000A44 ori      60000000   1
  127| 000A48 ld       EB420000   1     L8        gr26=.&&N&&grid(gr2,0)
  127| 000A4C addi     38C100C0   1     AI        gr6=gr1,192
  127| 000A50 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  127| 000A54 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  127| 000A58 lwz      811A000C   1     L4Z       gr8=<s64:d12:l4>(gr26,12)
  127| 000A5C addi     38BA0008   1     AI        gr5=gr26,8
  127| 000A60 addi     387A0004   1     AI        gr3=gr26,4
  127| 000A64 addi     389A0004   1     AI        gr4=gr26,4
  127| 000A68 addi     38FA0010   1     AI        gr7=gr26,16
  127| 000A6C addi     3808FFFF   1     AI        gr0=gr8,-1
  127| 000A70 addi     39010080   1     AI        gr8=gr1,128
  127| 000A74 stw      900100C0   1     ST4Z      T_16(gr1,192)=gr0
  127| 000A78 bl       48000001   1     CALL      pdv,8,ie",gr3,ie",gr4,js",gr5,T_16",gr6,ks",gr7,k1",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  127| 000A7C ori      60000000   1
  129| 000A80 lwz      80BA000C   1     L4Z       gr5=<s64:d12:l4>(gr26,12)
  129| 000A84 lwz      81010080   1     L4Z       gr8=k1(gr1,128)
  129| 000A88 addi     38E100C8   1     AI        gr7=gr1,200
  129| 000A8C addi     38C100C4   1     AI        gr6=gr1,196
  129| 000A90 or       7F43D378   1     LR        gr3=gr26
  129| 000A94 addi     389A0004   1     AI        gr4=gr26,4
  129| 000A98 addi     3805FFFF   1     AI        gr0=gr5,-1
  129| 000A9C addi     38A80001   1     AI        gr5=gr8,1
  129| 000AA0 stw      900100C4   1     ST4Z      T_17(gr1,196)=gr0
  129| 000AA4 stw      90A100C8   1     ST4Z      T_18(gr1,200)=gr5
  129| 000AA8 addi     38BA0008   1     AI        gr5=gr26,8
  129| 000AAC addi     39010084   1     AI        gr8=gr1,132
  129| 000AB0 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  129| 000AB4 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  129| 000AB8 bl       48000001   1     CALL      pdv,8,is",gr3,ie",gr4,js",gr5,T_17",gr6,T_18",gr7,k2",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  129| 000ABC ori      60000000   1
  134| 000AC0 lwa      E81A0016   1     L4A       gr0=<s64:d20:l4>(gr26,20)
  134| 000AC4 lwa      EB3A0012   1     L4A       gr25=<s64:d16:l4>(gr26,16)
  134| 000AC8 subf     7C790050   1     S         gr3=gr0,gr25
  134| 000ACC std      FB2101F8   1     ST8       #SPILL17(gr1,504)=gr25
  134| 000AD0 addi     38C30001   1     AI        gr6=gr3,1
  134| 000AD4 sradi    7CC01674   1     SRA8CA    gr0,ca=gr6,2
  134| 000AD8 cmpdi    2F260000   1     C8        cr6=gr6,0
  134| 000ADC addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
  134| 000AE0 rldicr   78181764   1     SLL8      gr24=gr0,2
  134| 000AE4 subf     7C183051   1     S_R       gr0,cr0=gr6,gr24
  134| 000AE8 std      FB010200   1     ST8       #SPILL18(gr1,512)=gr24
  134| 000AEC crand    4E390A02   1     CR_N      cr4=cr[60],0x2/gt,0x2/gt,0x2/gt,cr4
  134| 000AF0 bc       40910128   1     BF        CL.210,cr4,0x2/gt,taken=50%(0,0)
  135| 000AF4 lwz      815A000C   1     L4Z       gr10=<s64:d12:l4>(gr26,12)
  136| 000AF8 lwa      EBDA0002   1     L4A       gr30=<s64:d0:l4>(gr26,0)
  137| 000AFC ld       E87B0060   1     L8        gr3=<s24:d96:l8>(gr27,96)
  137| 000B00 ld       E97B0000   1     L8        gr11=<s24:d0:l8>(gr27,0)
  137| 000B04 ld       E99B0018   1     L8        gr12=<s24:d24:l8>(gr27,24)
  137| 000B08 ld       E8FB0030   1     L8        gr7=<s24:d48:l8>(gr27,48)
  135| 000B0C addi     38AA0002   1     AI        gr5=gr10,2
  135| 000B10 addi     388A0001   1     AI        gr4=gr10,1
  135| 000B14 extsw    7CA507B4   1     EXTS4     gr5=gr5
  135| 000B18 extsw    7C8407B4   1     EXTS4     gr4=gr4
  137| 000B1C ld       E91B0048   1     L8        gr8=<s24:d72:l8>(gr27,72)
  135| 000B20 subf     7CA42850   1     S         gr5=gr5,gr4
  134| 000B24 addi     38800000   1     LI        gr4=0
  135| 000B28 addic.   35250001   1     AI_R      gr9,cr0=gr5,1,ca"
  136| 000B2C lwa      E8BA0006   1     L4A       gr5=<s64:d4:l4>(gr26,4)
    0| 000B30 bc       40810FC4   1     BF        CL.707,cr0,0x2/gt,taken=50%(0,0)
    0| 000B34 add      7FAB6214   1     A         gr29=gr11,gr12
    0| 000B38 mulld    7D83F1D2   1     M         gr12=gr3,gr30
    0| 000B3C addi     3B650002   1     AI        gr27=gr5,2
    0| 000B40 add      7F8CEA14   1     A         gr28=gr12,gr29
    0| 000B44 rldicr   7BCC1F24   1     SLL8      gr12=gr30,3
    0| 000B48 addi     3BBEFFFE   1     AI        gr29=gr30,-2
    0| 000B4C subfic   23DE0005   1     SFI       gr30=5,gr30,ca"
    0| 000B50 ld       EAE10190   1     L8        gr23=#SPILL4(gr1,400)
    0| 000B54 add      7FC5F214   1     A         gr30=gr5,gr30
    0| 000B58 extsw    7F6507B4   1     EXTS4     gr5=gr27
    0| 000B5C extsw    7FBB07B4   1     EXTS4     gr27=gr29
    0| 000B60 rldicr   787D1764   1     SLL8      gr29=gr3,2
    0| 000B64 extsw    7D4B07B4   1     EXTS4     gr11=gr10
    0| 000B68 ld       EAC10170   1     L8        gr22=#SPILL0(gr1,368)
    0| 000B6C subf     7CBB2850   1     S         gr5=gr5,gr27
    0| 000B70 subf     7F7D1850   1     S         gr27=gr3,gr29
    0| 000B74 mulld    7D4BF9D2   1     M         gr10=gr11,gr31
    0| 000B78 mulld    7D6B41D2   1     M         gr11=gr11,gr8
    0| 000B7C mulld    7FB7C9D2   1     M         gr29=gr23,gr25
    0| 000B80 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 000B84 mulld    7F67C9D2   1     M         gr27=gr7,gr25
    0| 000B88 addi     3B56FFE0   1     AI        gr26=gr22,-32
    0| 000B8C add      7D8CD214   1     A         gr12=gr12,gr26
    0| 000B90 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 000B94 add      7D8CEA14   1     A         gr12=gr12,gr29
    0| 000B98 add      7D6BE214   1     A         gr11=gr11,gr28
    0| 000B9C add      7D4A6214   1     A         gr10=gr10,gr12
    0| 000BA0 add      7D6B4214   1     A         gr11=gr11,gr8
    0| 000BA4 subf     7D575050   1     S         gr10=gr10,gr23
    0| 000BA8 addic.   34A50001   1     AI_R      gr5,cr0=gr5,1,ca"
  134|                              CL.205:
  135| 000BAC addi     38A00000   1     LI        gr5=0
    0| 000BB0 bc       40810050   1     BF        CL.209,cr0,0x2/gt,taken=20%(20,80)
    0| 000BB4 or       7D7C5B78   1     LR        gr28=gr11
    0| 000BB8 or       7D5B5378   1     LR        gr27=gr10
    0| 000BBC ori      60210000   1     XNOP      
    0| 000BC0 ori      60210000   1     XNOP      
  135|                              CL.206:
  137| 000BC4 or       7F8CE378   1     LR        gr12=gr28
    0| 000BC8 mtspr    7FC903A6   1     LCTR      ctr=gr30
  137| 000BCC lfdux    7C0C1CEE   1     LFDU      fp0,gr12=d(gr12,gr3,0)
  137| 000BD0 or       7F7DDB78   1     LR        gr29=gr27
    0| 000BD4 bc       42400014   1     BCF       ctr=CL.1100,taken=0%(0,100)
    0|                              CL.1101:
  137| 000BD8 lfdux    7C2C1CEE   1     LFDU      fp1,gr12=d(gr12,gr3,0)
  137| 000BDC stfdu    DC1D0008   1     STFDU     gr29,dcopy[](gr29,8)=fp0
    0| 000BE0 fmr      FC000890   1     LRFL      fp0=fp1
    0| 000BE4 bc       4200FFF4   1     BCT       ctr=CL.1101,taken=100%(100,0)
    0|                              CL.1100:
  139| 000BE8 addi     38A50001   1     AI        gr5=gr5,1
  137| 000BEC stfdu    DC1D0008   1     STFDU     gr29,dcopy[](gr29,8)=fp0
  139| 000BF0 cmpld    7CA54840   1     CL8       cr1=gr5,gr9
    0| 000BF4 add      7F88E214   1     A         gr28=gr8,gr28
    0| 000BF8 add      7F7BFA14   1     A         gr27=gr27,gr31
  139| 000BFC bc       4184FFC8   1     BT        CL.206,cr1,0x8/llt,taken=80%(80,20)
  139|                              CL.209:
    0| 000C00 ld       E8A10190   1     L8        gr5=#SPILL4(gr1,400)
  140| 000C04 addi     38840001   1     AI        gr4=gr4,1
    0| 000C08 add      7D675A14   1     A         gr11=gr7,gr11
  140| 000C0C cmpd     7CA02000   1     C8        cr1=gr0,gr4
    0| 000C10 add      7D455214   1     A         gr10=gr5,gr10
  140| 000C14 bc       4185FF98   1     BT        CL.205,cr1,0x2/gt,taken=80%(80,20)
  140|                              CL.210:
  134| 000C18 cmpd     7C260000   1     C8        cr0=gr6,gr0
  134| 000C1C crand    4D390A02   1     CR_N      cr2=cr[60],0x2/gt,0x2/gt,0x2/gt,cr2
  134| 000C20 bc       4089027C   1     BF        CL.117,cr2,0x2/gt,taken=50%(0,0)
  135| 000C24 ld       E9620000   1     L8        gr11=.&&N&&grid(gr2,0)
  137| 000C28 ld       EB820000   1     L8        gr28=.&&N&field(gr2,0)
  140| 000C2C or       7F1BC378   1     LR        gr27=gr24
  134| 000C30 addi     3B400000   1     LI        gr26=0
  140| 000C34 addi     393BFFFF   1     AI        gr9=gr27,-1
  134| 000C38 std      FB410208   1     ST8       #SPILL19(gr1,520)=gr26
  135| 000C3C lwz      80EB000C   1     L4Z       gr7=<s64:d12:l4>(gr11,12)
  136| 000C40 lwa      E8AB0002   1     L4A       gr5=<s64:d0:l4>(gr11,0)
  137| 000C44 ld       E87C0060   1     L8        gr3=<s24:d96:l8>(gr28,96)
  137| 000C48 ld       E91C0030   1     L8        gr8=<s24:d48:l8>(gr28,48)
  137| 000C4C ld       E8DC0000   1     L8        gr6=<s24:d0:l8>(gr28,0)
  137| 000C50 ld       E99C0018   1     L8        gr12=<s24:d24:l8>(gr28,24)
  135| 000C54 addi     39470002   1     AI        gr10=gr7,2
  135| 000C58 addi     38870001   1     AI        gr4=gr7,1
  135| 000C5C extsw    7D4A07B4   1     EXTS4     gr10=gr10
  135| 000C60 extsw    7C8407B4   1     EXTS4     gr4=gr4
  137| 000C64 ld       EBDC0048   1     L8        gr30=<s24:d72:l8>(gr28,72)
  135| 000C68 subf     7C845050   1     S         gr4=gr10,gr4
  135| 000C6C addic.   37A40001   1     AI_R      gr29,cr0=gr4,1,ca"
  140| 000C70 sradi    7D241674   1     SRA8CA    gr4,ca=gr9,2
  136| 000C74 lwa      E92B0006   1     L4A       gr9=<s64:d4:l4>(gr11,4)
  140| 000C78 addze    7C840194   1     ADDE      gr4,ca=gr4,0,ca
    0| 000C7C bc       40810220   1     BF        CL.117,cr0,0x2/gt,taken=50%(0,0)
    0| 000C80 ld       EAE10190   1     L8        gr23=#SPILL4(gr1,400)
    0| 000C84 mulld    7D6329D2   1     M         gr11=gr3,gr5
    0| 000C88 extsw    7CEA07B4   1     EXTS4     gr10=gr7
    0| 000C8C rldicr   78671764   1     SLL8      gr7=gr3,2
    0| 000C90 add      7D866214   1     A         gr12=gr6,gr12
    0| 000C94 ld       EAC10170   1     L8        gr22=#SPILL0(gr1,368)
    0| 000C98 mulld    7CC8C9D2   1     M         gr6=gr8,gr25
    0| 000C9C subf     7CE71850   1     S         gr7=gr3,gr7
    0| 000CA0 add      7F4B6214   1     A         gr26=gr11,gr12
    0| 000CA4 mulld    7D77C9D2   1     M         gr11=gr23,gr25
    0| 000CA8 mulld    7D8AF1D2   1     M         gr12=gr10,gr30
    0| 000CAC add      7CE7D214   1     A         gr7=gr7,gr26
    0| 000CB0 rldicr   78BC1F24   1     SLL8      gr28=gr5,3
    0| 000CB4 addi     3B76FFE0   1     AI        gr27=gr22,-32
    0| 000CB8 mulld    7D4AF9D2   1     M         gr10=gr10,gr31
    0| 000CBC add      7F9CDA14   1     A         gr28=gr28,gr27
    0| 000CC0 add      7F663A14   1     A         gr27=gr6,gr7
    0| 000CC4 mulld    7CE0B9D2   1     M         gr7=gr0,gr23
    0| 000CC8 mulld    7CC041D2   1     M         gr6=gr0,gr8
    0| 000CCC ld       EA810188   1     L8        gr20=#SPILL3(gr1,392)
    0| 000CD0 addi     3B490002   1     AI        gr26=gr9,2
    0| 000CD4 addi     3B25FFFE   1     AI        gr25=gr5,-2
    0| 000CD8 subfic   20A50005   1     SFI       gr5=5,gr5,ca"
    0| 000CDC add      7D6BE214   1     A         gr11=gr11,gr28
    0| 000CE0 add      7D8CDA14   1     A         gr12=gr12,gr27
    0| 000CE4 extsw    7F5C07B4   1     EXTS4     gr28=gr26
    0| 000CE8 extsw    7F3B07B4   1     EXTS4     gr27=gr25
    0| 000CEC add      7CA54A14   1     A         gr5=gr5,gr9
    0| 000CF0 add      7D2A5A14   1     A         gr9=gr10,gr11
    0| 000CF4 add      7D6CF214   1     A         gr11=gr12,gr30
    0| 000CF8 rldicr   79151764   1     SLL8      gr21=gr8,2
    0| 000CFC subf     7D5BE050   1     S         gr10=gr28,gr27
    0| 000D00 std      FAA10210   1     ST8       #SPILL20(gr1,528)=gr21
    0| 000D04 add      7F874A14   1     A         gr28=gr7,gr9
    0| 000D08 rldicr   7A8926E4   1     SLL8      gr9=gr20,4
    0| 000D0C add      7F665A14   1     A         gr27=gr6,gr11
    0| 000D10 subf     7CE8A850   1     S         gr7=gr21,gr8
    0| 000D14 rldicr   79060FA4   1     SLL8      gr6=gr8,1
    0| 000D18 addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
    0| 000D1C rldicl   78BAF842   1     SRL8      gr26=gr5,1
    0| 000D20 rldicr   7A932EA4   1     SLL8      gr19=gr20,5
    0| 000D24 add      7E57E214   1     A         gr18=gr23,gr28
    0| 000D28 std      FA610218   1     ST8       #SPILL21(gr1,536)=gr19
    0| 000D2C std      FA410220   1     ST8       #SPILL22(gr1,544)=gr18
    0| 000D30 add      7E29E214   1     A         gr17=gr9,gr28
    0| 000D34 add      7F08DA14   1     A         gr24=gr8,gr27
    0| 000D38 std      FA210228   1     ST8       #SPILL23(gr1,552)=gr17
    0| 000D3C addi     39040001   1     AI        gr8=gr4,1
    0| 000D40 subf     7F37E050   1     S         gr25=gr28,gr23
    0| 000D44 std      F9010230   1     ST8       #SPILL24(gr1,560)=gr8
    0| 000D48 add      7EE7DA14   1     A         gr23=gr7,gr27
    0| 000D4C add      7EC6DA14   1     A         gr22=gr6,gr27
    0| 000D50 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000D54 andi.    70A40001   1     RN4_R     gr4,cr0=gr5,0,0x1
    0| 000D58 cmpdi    2FBA0000   1     C8        cr7=gr26,0
  134|                              CL.118:
  135| 000D5C addi     38800000   1     LI        gr4=0
    0| 000D60 bc       408500EC   1     BF        CL.119,cr1,0x2/gt,taken=20%(20,80)
    0| 000D64 or       7EF5BB78   1     LR        gr21=gr23
    0| 000D68 or       7F74DB78   1     LR        gr20=gr27
    0| 000D6C or       7ED3B378   1     LR        gr19=gr22
    0| 000D70 or       7F12C378   1     LR        gr18=gr24
    0| 000D74 or       7F91E378   1     LR        gr17=gr28
    0| 000D78 ld       EA010228   1     L8        gr16=#SPILL23(gr1,552)
    0| 000D7C or       7F2FCB78   1     LR        gr15=gr25
    0| 000D80 ld       E9C10220   1     L8        gr14=#SPILL22(gr1,544)
  135|                              CL.120:
  137| 000D84 or       7E85A378   1     LR        gr5=gr20
  137| 000D88 or       7E469378   1     LR        gr6=gr18
  137| 000D8C or       7E679B78   1     LR        gr7=gr19
  137| 000D90 or       7EA8AB78   1     LR        gr8=gr21
  137| 000D94 or       7DE97B78   1     LR        gr9=gr15
  137| 000D98 or       7E2A8B78   1     LR        gr10=gr17
  137| 000D9C or       7DCB7378   1     LR        gr11=gr14
  137| 000DA0 or       7E0C8378   1     LR        gr12=gr16
    0| 000DA4 mtspr    7F4903A6   1     LCTR      ctr=gr26
    0| 000DA8 bc       41820034   1     BT        CL.1065,cr0,0x4/eq,taken=50%(0,0)
  137| 000DAC lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
  137| 000DB0 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
  137| 000DB4 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
  137| 000DB8 lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
  137| 000DBC stfdu    DC090008   1     STFDU     gr9,dcopy[](gr9,8)=fp0
  137| 000DC0 stfdu    DC2A0008   1     STFDU     gr10,dcopy[](gr10,8)=fp1
  137| 000DC4 stfdu    DC4B0008   1     STFDU     gr11,dcopy[](gr11,8)=fp2
  137| 000DC8 stfdu    DC6C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp3
    0| 000DCC bc       419E0054   1     BT        CL.943,cr7,0x4/eq,taken=20%(20,80)
    0| 000DD0 ori      60210000   1     XNOP      
    0| 000DD4 ori      60210000   1     XNOP      
    0| 000DD8 ori      60210000   1     XNOP      
    0|                              CL.1065:
  137| 000DDC lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
  137| 000DE0 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
  137| 000DE4 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
  137| 000DE8 lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
  137| 000DEC lfdux    7C851CEE   1     LFDU      fp4,gr5=d(gr5,gr3,0)
  137| 000DF0 lfdux    7CA61CEE   1     LFDU      fp5,gr6=d(gr6,gr3,0)
  137| 000DF4 lfdux    7CC71CEE   1     LFDU      fp6,gr7=d(gr7,gr3,0)
  137| 000DF8 lfdux    7CE81CEE   1     LFDU      fp7,gr8=d(gr8,gr3,0)
  137| 000DFC stfd     D8090008   1     STFL      dcopy[](gr9,8)=fp0
  137| 000E00 stfd     D82A0008   1     STFL      dcopy[](gr10,8)=fp1
  137| 000E04 stfd     D84B0008   1     STFL      dcopy[](gr11,8)=fp2
  137| 000E08 stfd     D86C0008   1     STFL      dcopy[](gr12,8)=fp3
  137| 000E0C stfdu    DC890010   1     STFDU     gr9,dcopy[](gr9,16)=fp4
  137| 000E10 stfdu    DCAA0010   1     STFDU     gr10,dcopy[](gr10,16)=fp5
  137| 000E14 stfdu    DCCB0010   1     STFDU     gr11,dcopy[](gr11,16)=fp6
  137| 000E18 stfdu    DCEC0010   1     STFDU     gr12,dcopy[](gr12,16)=fp7
    0| 000E1C bc       4200FFC0   1     BCT       ctr=CL.1065,taken=100%(100,0)
    0|                              CL.943:
  139| 000E20 addi     38840001   1     AI        gr4=gr4,1
    0| 000E24 add      7EB5F214   1     A         gr21=gr21,gr30
  139| 000E28 cmpld    7F24E840   1     CL8       cr6=gr4,gr29
    0| 000E2C add      7E94F214   1     A         gr20=gr20,gr30
    0| 000E30 add      7E73F214   1     A         gr19=gr19,gr30
    0| 000E34 add      7E52F214   1     A         gr18=gr18,gr30
    0| 000E38 add      7E31FA14   1     A         gr17=gr17,gr31
    0| 000E3C add      7E10FA14   1     A         gr16=gr16,gr31
    0| 000E40 add      7DEFFA14   1     A         gr15=gr15,gr31
    0| 000E44 add      7DCEFA14   1     A         gr14=gr14,gr31
  139| 000E48 bc       4198FF3C   1     BT        CL.120,cr6,0x8/llt,taken=80%(80,20)
  139|                              CL.119:
  140| 000E4C ld       E8810208   1     L8        gr4=#SPILL19(gr1,520)
    0| 000E50 ld       E8A10218   1     L8        gr5=#SPILL21(gr1,536)
    0| 000E54 ld       E8C10220   1     L8        gr6=#SPILL22(gr1,544)
  140| 000E58 ld       E8E10230   1     L8        gr7=#SPILL24(gr1,560)
    0| 000E5C ld       E9010228   1     L8        gr8=#SPILL23(gr1,552)
    0| 000E60 ld       E9210210   1     L8        gr9=#SPILL20(gr1,528)
  140| 000E64 addi     38840001   1     AI        gr4=gr4,1
    0| 000E68 add      7F25CA14   1     A         gr25=gr5,gr25
  140| 000E6C std      F8810208   1     ST8       #SPILL19(gr1,520)=gr4
    0| 000E70 add      7CC53214   1     A         gr6=gr5,gr6
  140| 000E74 cmpld    7F243840   1     CL8       cr6=gr4,gr7
    0| 000E78 add      7D054214   1     A         gr8=gr5,gr8
    0| 000E7C std      F8C10220   1     ST8       #SPILL22(gr1,544)=gr6
    0| 000E80 std      F9010228   1     ST8       #SPILL23(gr1,552)=gr8
    0| 000E84 add      7F85E214   1     A         gr28=gr5,gr28
    0| 000E88 add      7F09C214   1     A         gr24=gr9,gr24
    0| 000E8C add      7EE9BA14   1     A         gr23=gr9,gr23
    0| 000E90 add      7EC9B214   1     A         gr22=gr9,gr22
    0| 000E94 add      7F69DA14   1     A         gr27=gr9,gr27
  140| 000E98 bc       4198FEC4   1     BT        CL.118,cr6,0x8/llt,taken=80%(80,20)
  140|                              CL.117:
  141| 000E9C bc       40910110   1     BF        CL.198,cr4,0x2/gt,taken=50%(0,0)
  142| 000EA0 ld       E9420000   1     L8        gr10=.&&N&&grid(gr2,0)
  144| 000EA4 ld       E9620000   1     L8        gr11=.&&N&field(gr2,0)
  142| 000EA8 lwa      E8CA000A   1     L4A       gr6=<s64:d8:l4>(gr10,8)
  142| 000EAC lwa      E8AA000E   1     L4A       gr5=<s64:d12:l4>(gr10,12)
  143| 000EB0 lwa      E86A0002   1     L4A       gr3=<s64:d0:l4>(gr10,0)
  144| 000EB4 ld       E88B0060   1     L8        gr4=<s24:d96:l8>(gr11,96)
  144| 000EB8 ld       E8EB0030   1     L8        gr7=<s24:d48:l8>(gr11,48)
  144| 000EBC ld       EB8B0000   1     L8        gr28=<s24:d0:l8>(gr11,0)
  144| 000EC0 ld       EB6B0018   1     L8        gr27=<s24:d24:l8>(gr11,24)
  142| 000EC4 subf     7CA62850   1     S         gr5=gr5,gr6
  144| 000EC8 ld       E90B0048   1     L8        gr8=<s24:d72:l8>(gr11,72)
  142| 000ECC addic.   35250001   1     AI_R      gr9,cr0=gr5,1,ca"
  141| 000ED0 addi     38A00000   1     LI        gr5=0
    0| 000ED4 bc       40810C0C   1     BF        CL.717,cr0,0x2/gt,taken=50%(0,0)
    0| 000ED8 ld       EB2101F8   1     L8        gr25=#SPILL17(gr1,504)
    0| 000EDC ld       EB010190   1     L8        gr24=#SPILL4(gr1,400)
    0| 000EE0 ld       EAE10170   1     L8        gr23=#SPILL0(gr1,368)
    0| 000EE4 mulld    7D4321D2   1     M         gr10=gr3,gr4
    0| 000EE8 mulld    7D78C9D2   1     M         gr11=gr24,gr25
    0| 000EEC rldicr   786C1F24   1     SLL8      gr12=gr3,3
    0| 000EF0 addi     3BD7FFE0   1     AI        gr30=gr23,-32
    0| 000EF4 rldicr   789D1764   1     SLL8      gr29=gr4,2
    0| 000EF8 add      7F9CDA14   1     A         gr28=gr28,gr27
    0| 000EFC mulld    7F66F9D2   1     M         gr27=gr6,gr31
    0| 000F00 mulld    7F47C9D2   1     M         gr26=gr7,gr25
    0| 000F04 add      7D8CF214   1     A         gr12=gr12,gr30
    0| 000F08 subf     7FDD2050   1     S         gr30=gr4,gr29
    0| 000F0C add      7D4AE214   1     A         gr10=gr10,gr28
    0| 000F10 addi     3BA3FFFE   1     AI        gr29=gr3,-2
    0| 000F14 addi     3863FFFF   1     AI        gr3=gr3,-1
    0| 000F18 mulld    7CC641D2   1     M         gr6=gr6,gr8
    0| 000F1C add      7D6B6214   1     A         gr11=gr11,gr12
    0| 000F20 add      7D4AF214   1     A         gr10=gr10,gr30
    0| 000F24 extsw    7FAC07B4   1     EXTS4     gr12=gr29
    0| 000F28 extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 000F2C add      7D6BDA14   1     A         gr11=gr11,gr27
    0| 000F30 add      7FD8FA14   1     A         gr30=gr24,gr31
    0| 000F34 add      7D4AD214   1     A         gr10=gr10,gr26
    0| 000F38 subf     7C6C1850   1     S         gr3=gr3,gr12
    0| 000F3C subf     7D7E5850   1     S         gr11=gr11,gr30
    0| 000F40 add      7CC65214   1     A         gr6=gr6,gr10
    0| 000F44 addic.   34630001   1     AI_R      gr3,cr0=gr3,1,ca"
    0| 000F48 addi     3BC00002   1     LI        gr30=2
  141|                              CL.193:
  142| 000F4C addi     38600000   1     LI        gr3=0
    0| 000F50 bc       40810044   1     BF        CL.197,cr0,0x2/gt,taken=20%(20,80)
    0| 000F54 or       7CDD3378   1     LR        gr29=gr6
    0| 000F58 or       7D7C5B78   1     LR        gr28=gr11
  142|                              CL.194:
  144| 000F5C or       7F8AE378   1     LR        gr10=gr28
  144| 000F60 or       7FACEB78   1     LR        gr12=gr29
    0| 000F64 mtspr    7FC903A6   1     LCTR      ctr=gr30
    0| 000F68 ori      60210000   1     XNOP      
    0| 000F6C ori      60210000   1     XNOP      
    0| 000F70 ori      60210000   1     XNOP      
  143|                              CL.195:
  144| 000F74 lfdux    7C0C24EE   1     LFDU      fp0,gr12=d(gr12,gr4,0)
  144| 000F78 stfdu    DC0A0008   1     STFDU     gr10,dcopy[](gr10,8)=fp0
  145| 000F7C bc       4200FFF8   1     BCT       ctr=CL.195,taken=100%(100,0)
  146| 000F80 addi     38630001   1     AI        gr3=gr3,1
    0| 000F84 add      7FA8EA14   1     A         gr29=gr8,gr29
  146| 000F88 cmpld    7CA34840   1     CL8       cr1=gr3,gr9
    0| 000F8C add      7F9CFA14   1     A         gr28=gr28,gr31
  146| 000F90 bc       4184FFCC   1     BT        CL.194,cr1,0x8/llt,taken=80%(80,20)
  146|                              CL.197:
    0| 000F94 ld       E8610190   1     L8        gr3=#SPILL4(gr1,400)
  147| 000F98 addi     38A50001   1     AI        gr5=gr5,1
    0| 000F9C add      7CC63A14   1     A         gr6=gr6,gr7
  147| 000FA0 cmpd     7CA02800   1     C8        cr1=gr0,gr5
    0| 000FA4 add      7D635A14   1     A         gr11=gr3,gr11
  147| 000FA8 bc       4185FFA4   1     BT        CL.193,cr1,0x2/gt,taken=80%(80,20)
  147|                              CL.198:
  141| 000FAC bc       408901E0   1     BF        CL.123,cr2,0x2/gt,taken=50%(0,0)
  142| 000FB0 ld       EBC20000   1     L8        gr30=.&&N&&grid(gr2,0)
  144| 000FB4 ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  147| 000FB8 ld       EB810200   1     L8        gr28=#SPILL18(gr1,512)
  142| 000FBC lwa      E95E000A   1     L4A       gr10=<s64:d8:l4>(gr30,8)
  142| 000FC0 lwa      E89E000E   1     L4A       gr4=<s64:d12:l4>(gr30,12)
  143| 000FC4 lwa      E91E0002   1     L4A       gr8=<s64:d0:l4>(gr30,0)
  144| 000FC8 ld       E87D0060   1     L8        gr3=<s24:d96:l8>(gr29,96)
  147| 000FCC addi     38DCFFFF   1     AI        gr6=gr28,-1
  144| 000FD0 ld       E93D0030   1     L8        gr9=<s24:d48:l8>(gr29,48)
  144| 000FD4 ld       E97D0000   1     L8        gr11=<s24:d0:l8>(gr29,0)
  142| 000FD8 subf     7C8A2050   1     S         gr4=gr4,gr10
  144| 000FDC ld       E99D0018   1     L8        gr12=<s24:d24:l8>(gr29,24)
  142| 000FE0 addic.   34A40001   1     AI_R      gr5,cr0=gr4,1,ca"
  147| 000FE4 sradi    7CC71674   1     SRA8CA    gr7,ca=gr6,2
  144| 000FE8 ld       E8DD0048   1     L8        gr6=<s24:d72:l8>(gr29,72)
  141| 000FEC addi     38800000   1     LI        gr4=0
  147| 000FF0 addze    7CE70194   1     ADDE      gr7,ca=gr7,0,ca
    0| 000FF4 bc       40810198   1     BF        CL.123,cr0,0x2/gt,taken=50%(0,0)
    0| 000FF8 ld       EAC101F8   1     L8        gr22=#SPILL17(gr1,504)
    0| 000FFC ld       EAA10190   1     L8        gr21=#SPILL4(gr1,400)
    0| 001000 ld       EA610170   1     L8        gr19=#SPILL0(gr1,368)
    0| 001004 mulld    7F6341D2   1     M         gr27=gr3,gr8
    0| 001008 add      7F4B6214   1     A         gr26=gr11,gr12
    0| 00100C rldicr   787C1764   1     SLL8      gr28=gr3,2
    0| 001010 mulld    7D75B1D2   1     M         gr11=gr21,gr22
    0| 001014 mulld    7D89B1D2   1     M         gr12=gr9,gr22
    0| 001018 addi     3BB3FFE0   1     AI        gr29=gr19,-32
    0| 00101C rldicr   791E1F24   1     SLL8      gr30=gr8,3
    0| 001020 subf     7F9C1850   1     S         gr28=gr3,gr28
    0| 001024 add      7F7BD214   1     A         gr27=gr27,gr26
    0| 001028 mulld    7F40A9D2   1     M         gr26=gr0,gr21
    0| 00102C mulld    7F2AF9D2   1     M         gr25=gr10,gr31
    0| 001030 mulld    7D4A31D2   1     M         gr10=gr10,gr6
    0| 001034 add      7FDEEA14   1     A         gr30=gr30,gr29
    0| 001038 add      7F9CDA14   1     A         gr28=gr28,gr27
    0| 00103C mulld    7FA049D2   1     M         gr29=gr0,gr9
    0| 001040 ld       EA410188   1     L8        gr18=#SPILL3(gr1,392)
    0| 001044 add      7C0BF214   1     A         gr0=gr11,gr30
    0| 001048 add      7D6CE214   1     A         gr11=gr12,gr28
    0| 00104C addi     3988FFFE   1     AI        gr12=gr8,-2
    0| 001050 addi     3908FFFF   1     AI        gr8=gr8,-1
    0| 001054 subf     7FDFD050   1     S         gr30=gr26,gr31
    0| 001058 add      7F60CA14   1     A         gr27=gr0,gr25
    0| 00105C rldicr   79201764   1     SLL8      gr0=gr9,2
    0| 001060 add      7D4A5A14   1     A         gr10=gr10,gr11
    0| 001064 extsw    7D9C07B4   1     EXTS4     gr28=gr12
    0| 001068 extsw    7D0807B4   1     EXTS4     gr8=gr8
    0| 00106C add      7F7BF214   1     A         gr27=gr27,gr30
    0| 001070 rldicr   7A4C26E4   1     SLL8      gr12=gr18,4
    0| 001074 add      7F4AEA14   1     A         gr26=gr10,gr29
    0| 001078 subf     7D690050   1     S         gr11=gr0,gr9
    0| 00107C rldicr   792A0FA4   1     SLL8      gr10=gr9,1
    0| 001080 subf     7D1C4050   1     S         gr8=gr8,gr28
    0| 001084 rldicr   7A592EA4   1     SLL8      gr25=gr18,5
    0| 001088 add      7F15DA14   1     A         gr24=gr21,gr27
    0| 00108C add      7EECDA14   1     A         gr23=gr12,gr27
    0| 001090 subf     7ED5D850   1     S         gr22=gr27,gr21
    0| 001094 add      7EA9D214   1     A         gr21=gr9,gr26
    0| 001098 add      7E8BD214   1     A         gr20=gr11,gr26
    0| 00109C add      7E6AD214   1     A         gr19=gr10,gr26
    0| 0010A0 addi     39C70001   1     AI        gr14=gr7,1
    0| 0010A4 addic.   34E80001   1     AI_R      gr7,cr0=gr8,1,ca"
  141|                              CL.124:
  142| 0010A8 addi     38E00000   1     LI        gr7=0
    0| 0010AC bc       408100B4   1     BF        CL.125,cr0,0x2/gt,taken=20%(20,80)
    0| 0010B0 or       7F52D378   1     LR        gr18=gr26
    0| 0010B4 or       7E91A378   1     LR        gr17=gr20
    0| 0010B8 or       7E709B78   1     LR        gr16=gr19
    0| 0010BC or       7EAFAB78   1     LR        gr15=gr21
    0| 0010C0 or       7F68DB78   1     LR        gr8=gr27
    0| 0010C4 or       7EE9BB78   1     LR        gr9=gr23
    0| 0010C8 or       7ECAB378   1     LR        gr10=gr22
    0| 0010CC or       7F0BC378   1     LR        gr11=gr24
    0| 0010D0 ori      60210000   1     XNOP      
  142|                              CL.126:
  144| 0010D4 or       7E4C9378   1     LR        gr12=gr18
  144| 0010D8 or       7DFE7B78   1     LR        gr30=gr15
  144| 0010DC lfdux    7C0C1CEE   1     LFDU      fp0,gr12=d(gr12,gr3,0)
  144| 0010E0 or       7E1D8378   1     LR        gr29=gr16
  144| 0010E4 or       7E3C8B78   1     LR        gr28=gr17
  144| 0010E8 lfdux    7C3E1CEE   1     LFDU      fp1,gr30=d(gr30,gr3,0)
  144| 0010EC lfdux    7C5D1CEE   1     LFDU      fp2,gr29=d(gr29,gr3,0)
  144| 0010F0 lfdux    7C7C1CEE   1     LFDU      fp3,gr28=d(gr28,gr3,0)
  146| 0010F4 addi     38E70001   1     AI        gr7=gr7,1
    0| 0010F8 add      7E523214   1     A         gr18=gr18,gr6
  144| 0010FC stfd     D80A0008   1     STFL      dcopy[](gr10,8)=fp0
  144| 001100 lfdux    7C0C1CEE   1     LFDU      fp0,gr12=d(gr12,gr3,0)
  144| 001104 or       7D4C5378   1     LR        gr12=gr10
  144| 001108 stfd     D8280008   1     STFL      dcopy[](gr8,8)=fp1
  144| 00110C stfd     D84B0008   1     STFL      dcopy[](gr11,8)=fp2
  144| 001110 stfd     D8690008   1     STFL      dcopy[](gr9,8)=fp3
  144| 001114 lfdux    7C3E1CEE   1     LFDU      fp1,gr30=d(gr30,gr3,0)
  144| 001118 lfdux    7C5D1CEE   1     LFDU      fp2,gr29=d(gr29,gr3,0)
  144| 00111C lfdux    7C7C1CEE   1     LFDU      fp3,gr28=d(gr28,gr3,0)
  144| 001120 stfdu    DC0C0010   1     STFDU     gr12,dcopy[](gr12,16)=fp0
  146| 001124 cmpld    7CA72840   1     CL8       cr1=gr7,gr5
  144| 001128 or       7D1E4378   1     LR        gr30=gr8
  144| 00112C or       7D7D5B78   1     LR        gr29=gr11
  144| 001130 or       7D2C4B78   1     LR        gr12=gr9
  144| 001134 stfdu    DC3E0010   1     STFDU     gr30,dcopy[](gr30,16)=fp1
  144| 001138 stfdu    DC5D0010   1     STFDU     gr29,dcopy[](gr29,16)=fp2
  144| 00113C stfdu    DC6C0010   1     STFDU     gr12,dcopy[](gr12,16)=fp3
    0| 001140 add      7E313214   1     A         gr17=gr17,gr6
    0| 001144 add      7E103214   1     A         gr16=gr16,gr6
    0| 001148 add      7DEF3214   1     A         gr15=gr15,gr6
    0| 00114C add      7D08FA14   1     A         gr8=gr8,gr31
    0| 001150 add      7D29FA14   1     A         gr9=gr9,gr31
    0| 001154 add      7D4AFA14   1     A         gr10=gr10,gr31
    0| 001158 add      7D6BFA14   1     A         gr11=gr11,gr31
  146| 00115C bc       4184FF78   1     BT        CL.126,cr1,0x8/llt,taken=80%(80,20)
  146|                              CL.125:
  147| 001160 addi     38840001   1     AI        gr4=gr4,1
    0| 001164 add      7F18CA14   1     A         gr24=gr24,gr25
  147| 001168 cmpld    7CA47040   1     CL8       cr1=gr4,gr14
    0| 00116C add      7EF7CA14   1     A         gr23=gr23,gr25
    0| 001170 add      7ED6CA14   1     A         gr22=gr22,gr25
    0| 001174 add      7F79DA14   1     A         gr27=gr25,gr27
    0| 001178 add      7EA0AA14   1     A         gr21=gr0,gr21
    0| 00117C add      7E80A214   1     A         gr20=gr0,gr20
    0| 001180 add      7E609A14   1     A         gr19=gr0,gr19
    0| 001184 add      7F40D214   1     A         gr26=gr0,gr26
  147| 001188 bc       4184FF20   1     BT        CL.124,cr1,0x8/llt,taken=80%(80,20)
  147|                              CL.123:
  151| 00118C ld       E8620000   1     L8        gr3=.&&N&&mpipar(gr2,0)
  151| 001190 lwz      80030018   1     L4Z       gr0=<s70:d24:l4>(gr3,24)
  151| 001194 cmpdi    2C200000   1     C8        cr0=gr0,0
  151| 001198 bc       4182001C   1     BT        CL.54,cr0,0x4/eq,taken=60%(60,40)
  152| 00119C ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  152| 0011A0 addi     38C30014   1     AI        gr6=gr3,20
  152| 0011A4 addi     38630018   1     AI        gr3=gr3,24
  152| 0011A8 addi     38851770   1     AI        gr4=gr5,6000
  152| 0011AC bl       48000001   1     CALL      mpi_waitall,4,nreq",gr3,req[]",gr4,stat[]",gr5,ierr",gr6,mpi_waitall",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  152| 0011B0 ori      60000000   1
  152|                              CL.54:
  160| 0011B4 ld       EBA20000   1     L8        gr29=.&&N&&mpipar(gr2,0)
  161| 0011B8 ld       EB620000   1     L8        gr27=.&&N&field(gr2,0)
  161| 0011BC addi     3BC00000   1     LI        gr30=0
  161| 0011C0 addi     3B800001   1     LI        gr28=1
  161| 0011C4 stw      93C100CC   1     ST4Z      T_19(gr1,204)=gr30
  161| 0011C8 stw      93C100D0   1     ST4Z      T_20(gr1,208)=gr30
  160| 0011CC lwz      807D001C   1     L4Z       gr3=<s70:d28:l4>(gr29,28)
  159| 0011D0 stw      93DD0018   1     ST4Z      <s70:d24:l4>(gr29,24)=gr30
  161| 0011D4 stw      93C100D4   1     ST4Z      T_21(gr1,212)=gr30
  161| 0011D8 stw      93C100D8   1     ST4Z      T_22(gr1,216)=gr30
  161| 0011DC stw      93C100DC   1     ST4Z      T_23(gr1,220)=gr30
  161| 0011E0 stw      938100E0   1     ST4Z      T_24(gr1,224)=gr28
  160| 0011E4 addi     38030001   1     AI        gr0=gr3,1
  161| 0011E8 addi     386100CC   1     AI        gr3=gr1,204
  160| 0011EC stw      901D001C   1     ST4Z      <s70:d28:l4>(gr29,28)=gr0
  161| 0011F0 ld       E93B0270   1     L8        gr9=<s24:d624:l8>(gr27,624)
  161| 0011F4 addi     390100E0   1     AI        gr8=gr1,224
  161| 0011F8 addi     38E100DC   1     AI        gr7=gr1,220
  161| 0011FC addi     38C100D8   1     AI        gr6=gr1,216
  161| 001200 addi     38A100D4   1     AI        gr5=gr1,212
  161| 001204 addi     388100D0   1     AI        gr4=gr1,208
  161| 001208 bl       48000001   1     CALL      bvalv3,7,T_19",gr3,T_20",gr4,T_21",gr5,T_22",gr6,T_23",gr7,T_24",gr8,v3",gr9,bvalv3",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  161| 00120C ori      60000000   1
  165| 001210 ld       E8620000   1     L8        gr3=.&&N&&grid(gr2,0)
  165| 001214 addi     39010084   1     AI        gr8=gr1,132
  165| 001218 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  165| 00121C ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  165| 001220 or       7C7A1B78   1     LR        gr26=gr3
  165| 001224 addi     38BA000C   1     AI        gr5=gr26,12
  165| 001228 addi     38DA000C   1     AI        gr6=gr26,12
  165| 00122C addi     389A0004   1     AI        gr4=gr26,4
  165| 001230 addi     38FA0010   1     AI        gr7=gr26,16
  165| 001234 bl       48000001   1     CALL      pdv,8,is",gr3,ie",gr4,je",gr5,je",gr6,ks",gr7,k2",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  165| 001238 ori      60000000   1
  167| 00123C lwz      80A10084   1     L4Z       gr5=k2(gr1,132)
  167| 001240 lwz      80DA0014   1     L4Z       gr6=<s64:d20:l4>(gr26,20)
  167| 001244 ld       E8620000   1     L8        gr3=.&&N&&grid(gr2,0)
  167| 001248 addi     390100E8   1     AI        gr8=gr1,232
  167| 00124C addi     38E100E4   1     AI        gr7=gr1,228
  167| 001250 addi     389A0004   1     AI        gr4=gr26,4
  167| 001254 addi     38050001   1     AI        gr0=gr5,1
  167| 001258 addi     38A6FFFF   1     AI        gr5=gr6,-1
  167| 00125C stw      900100E4   1     ST4Z      T_25(gr1,228)=gr0
  167| 001260 stw      90A100E8   1     ST4Z      T_26(gr1,232)=gr5
  167| 001264 addi     38BA0008   1     AI        gr5=gr26,8
  167| 001268 addi     38DA000C   1     AI        gr6=gr26,12
  167| 00126C ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  167| 001270 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  167| 001274 bl       48000001   1     CALL      pdv,8,is",gr3,ie",gr4,js",gr5,je",gr6,T_25",gr7,T_26",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  167| 001278 ori      60000000   1
  172| 00127C lwz      807A0014   1     L4Z       gr3=<s64:d20:l4>(gr26,20)
  172| 001280 addi     38830002   1     AI        gr4=gr3,2
  172| 001284 addi     38030001   1     AI        gr0=gr3,1
  172| 001288 extsw    7C8407B4   1     EXTS4     gr4=gr4
  172| 00128C extsw    7C0007B4   1     EXTS4     gr0=gr0
  172| 001290 extsw    7C7907B4   1     EXTS4     gr25=gr3
  172| 001294 subf     7C602050   1     S         gr3=gr4,gr0
  172| 001298 std      FB210238   1     ST8       #SPILL25(gr1,568)=gr25
  172| 00129C addi     38030001   1     AI        gr0=gr3,1
  172| 0012A0 sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
  172| 0012A4 cmpdi    2F200000   1     C8        cr6=gr0,0
  172| 0012A8 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  172| 0012AC rldicr   78651764   1     SLL8      gr5=gr3,2
  172| 0012B0 subf     7C850051   1     S_R       gr4,cr0=gr0,gr5
  172| 0012B4 crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  172| 0012B8 bc       40810148   1     BF        CL.186,cr0,0x2/gt,taken=50%(0,0)
  173| 0012BC lwz      83BA0008   1     L4Z       gr29=<s64:d8:l4>(gr26,8)
  173| 0012C0 lwz      811A000C   1     L4Z       gr8=<s64:d12:l4>(gr26,12)
  174| 0012C4 lwa      E87A0002   1     L4A       gr3=<s64:d0:l4>(gr26,0)
  175| 0012C8 ld       E8DB0060   1     L8        gr6=<s24:d96:l8>(gr27,96)
  175| 0012CC ld       E93B0048   1     L8        gr9=<s24:d72:l8>(gr27,72)
  172| 0012D0 addi     38E00000   1     LI        gr7=0
  173| 0012D4 addi     395DFFFE   1     AI        gr10=gr29,-2
  173| 0012D8 addi     39080002   1     AI        gr8=gr8,2
  173| 0012DC extsw    7D4A07B4   1     EXTS4     gr10=gr10
  173| 0012E0 extsw    7D0807B4   1     EXTS4     gr8=gr8
  175| 0012E4 ld       E99B0000   1     L8        gr12=<s24:d0:l8>(gr27,0)
  173| 0012E8 subf     7D4A4050   1     S         gr10=gr8,gr10
  174| 0012EC lwa      E91A0006   1     L4A       gr8=<s64:d4:l4>(gr26,4)
  173| 0012F0 addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
  175| 0012F4 ld       EBDB0018   1     L8        gr30=<s24:d24:l8>(gr27,24)
  175| 0012F8 ld       E97B0030   1     L8        gr11=<s24:d48:l8>(gr27,48)
    0| 0012FC bc       408107D0   1     BF        CL.727,cr0,0x2/gt,taken=50%(0,0)
    0| 001300 ld       EAE10190   1     L8        gr23=#SPILL4(gr1,400)
    0| 001304 mulld    7F8331D2   1     M         gr28=gr3,gr6
    0| 001308 ld       EAC10170   1     L8        gr22=#SPILL0(gr1,368)
    0| 00130C add      7FCCF214   1     A         gr30=gr12,gr30
    0| 001310 extsw    7FBD07B4   1     EXTS4     gr29=gr29
    0| 001314 mulld    7F77C9D2   1     M         gr27=gr23,gr25
    0| 001318 ld       EAA10180   1     L8        gr21=#SPILL2(gr1,384)
    0| 00131C add      7FDCF214   1     A         gr30=gr28,gr30
    0| 001320 rldicr   787C1F24   1     SLL8      gr28=gr3,3
    0| 001324 addi     3B36FFE0   1     AI        gr25=gr22,-32
    0| 001328 rldicr   78D81764   1     SLL8      gr24=gr6,2
    0| 00132C subfic   23430005   1     SFI       gr26=5,gr3,ca"
    0| 001330 add      7F9CCA14   1     A         gr28=gr28,gr25
    0| 001334 subf     7F383050   1     S         gr25=gr6,gr24
    0| 001338 mulld    7D89E9D2   1     M         gr12=gr9,gr29
    0| 00133C mulld    7FBDF9D2   1     M         gr29=gr29,gr31
    0| 001340 add      7F48D214   1     A         gr26=gr8,gr26
    0| 001344 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 001348 add      7FD9F214   1     A         gr30=gr25,gr30
    0| 00134C rldicr   7ABB2EA4   1     SLL8      gr27=gr21,5
    0| 001350 rldicr   79390FA4   1     SLL8      gr25=gr9,1
    0| 001354 ld       EA810238   1     L8        gr20=#SPILL25(gr1,568)
    0| 001358 subf     7D996050   1     S         gr12=gr12,gr25
    0| 00135C addi     39080002   1     AI        gr8=gr8,2
    0| 001360 addi     3863FFFE   1     AI        gr3=gr3,-2
    0| 001364 subf     7F7BF850   1     S         gr27=gr31,gr27
    0| 001368 add      7D8CF214   1     A         gr12=gr12,gr30
    0| 00136C mulld    7F2BA1D2   1     M         gr25=gr11,gr20
    0| 001370 extsw    7D0807B4   1     EXTS4     gr8=gr8
    0| 001374 extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 001378 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 00137C add      7D8CCA14   1     A         gr12=gr12,gr25
    0| 001380 subf     7C634050   1     S         gr3=gr8,gr3
    0| 001384 add      7FDCEA14   1     A         gr30=gr28,gr29
    0| 001388 add      7F6B6214   1     A         gr27=gr11,gr12
    0| 00138C addic.   34630001   1     AI_R      gr3,cr0=gr3,1,ca"
  172|                              CL.181:
  173| 001390 addi     38600000   1     LI        gr3=0
    0| 001394 bc       40810054   1     BF        CL.185,cr0,0x2/gt,taken=20%(20,80)
    0| 001398 or       7F7DDB78   1     LR        gr29=gr27
    0| 00139C or       7FDCF378   1     LR        gr28=gr30
  173|                              CL.182:
  175| 0013A0 or       7FA8EB78   1     LR        gr8=gr29
    0| 0013A4 mtspr    7F4903A6   1     LCTR      ctr=gr26
  175| 0013A8 lfdux    7C0834EE   1     LFDU      fp0,gr8=d(gr8,gr6,0)
  175| 0013AC or       7F8CE378   1     LR        gr12=gr28
    0| 0013B0 bc       42400020   1     BCF       ctr=CL.1103,taken=0%(0,100)
    0| 0013B4 ori      60210000   1     XNOP      
    0| 0013B8 ori      60210000   1     XNOP      
    0| 0013BC ori      60210000   1     XNOP      
    0|                              CL.1104:
  175| 0013C0 lfdux    7C2834EE   1     LFDU      fp1,gr8=d(gr8,gr6,0)
  175| 0013C4 stfdu    DC0C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp0
    0| 0013C8 fmr      FC000890   1     LRFL      fp0=fp1
    0| 0013CC bc       4200FFF4   1     BCT       ctr=CL.1104,taken=100%(100,0)
    0|                              CL.1103:
  177| 0013D0 addi     38630001   1     AI        gr3=gr3,1
  175| 0013D4 stfdu    DC0C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp0
  177| 0013D8 cmpld    7CA35040   1     CL8       cr1=gr3,gr10
    0| 0013DC add      7FA9EA14   1     A         gr29=gr9,gr29
    0| 0013E0 add      7F9CFA14   1     A         gr28=gr28,gr31
  177| 0013E4 bc       4184FFBC   1     BT        CL.182,cr1,0x8/llt,taken=80%(80,20)
  177|                              CL.185:
    0| 0013E8 ld       E8610190   1     L8        gr3=#SPILL4(gr1,400)
  178| 0013EC addi     38E70001   1     AI        gr7=gr7,1
    0| 0013F0 add      7F6BDA14   1     A         gr27=gr11,gr27
  178| 0013F4 cmpd     7CA43800   1     C8        cr1=gr4,gr7
    0| 0013F8 add      7FC3F214   1     A         gr30=gr3,gr30
  178| 0013FC bc       4185FF94   1     BT        CL.181,cr1,0x2/gt,taken=80%(80,20)
  178|                              CL.186:
  172| 001400 cmpd     7CA02000   1     C8        cr1=gr0,gr4
  172| 001404 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  172| 001408 bc       4081028C   1     BF        CL.129,cr0,0x2/gt,taken=50%(0,0)
  173| 00140C ld       E9820000   1     L8        gr12=.&&N&&grid(gr2,0)
  178| 001410 addi     38C5FFFF   1     AI        gr6=gr5,-1
  175| 001414 ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  172| 001418 addi     3B800000   1     LI        gr28=0
  172| 00141C std      FB810240   1     ST8       #SPILL26(gr1,576)=gr28
  173| 001420 lwz      816C0008   1     L4Z       gr11=<s64:d8:l4>(gr12,8)
  173| 001424 lwz      810C000C   1     L4Z       gr8=<s64:d12:l4>(gr12,12)
  174| 001428 lwa      E8EC0002   1     L4A       gr7=<s64:d0:l4>(gr12,0)
  175| 00142C ld       E87D0060   1     L8        gr3=<s24:d96:l8>(gr29,96)
  175| 001430 ld       E81D0048   1     L8        gr0=<s24:d72:l8>(gr29,72)
  175| 001434 ld       E93D0000   1     L8        gr9=<s24:d0:l8>(gr29,0)
  173| 001438 addi     38ABFFFE   1     AI        gr5=gr11,-2
  173| 00143C addi     39080002   1     AI        gr8=gr8,2
  173| 001440 extsw    7CA507B4   1     EXTS4     gr5=gr5
  173| 001444 extsw    7D0807B4   1     EXTS4     gr8=gr8
  175| 001448 ld       E95D0018   1     L8        gr10=<s24:d24:l8>(gr29,24)
  173| 00144C subf     7CA54050   1     S         gr5=gr8,gr5
  174| 001450 lwa      E90C0006   1     L4A       gr8=<s64:d4:l4>(gr12,4)
  173| 001454 addic.   37C50001   1     AI_R      gr30,cr0=gr5,1,ca"
  178| 001458 sradi    7CC51674   1     SRA8CA    gr5,ca=gr6,2
  175| 00145C ld       E8DD0030   1     L8        gr6=<s24:d48:l8>(gr29,48)
  178| 001460 addze    7CA50194   1     ADDE      gr5,ca=gr5,0,ca
    0| 001464 bc       40810230   1     BF        CL.129,cr0,0x2/gt,taken=50%(0,0)
    0| 001468 ld       EAE10238   1     L8        gr23=#SPILL25(gr1,568)
    0| 00146C ld       EAC10190   1     L8        gr22=#SPILL4(gr1,400)
    0| 001470 extsw    7D6B07B4   1     EXTS4     gr11=gr11
    0| 001474 ld       EAA10170   1     L8        gr21=#SPILL0(gr1,368)
    0| 001478 mulld    7D8339D2   1     M         gr12=gr3,gr7
    0| 00147C ld       EA810180   1     L8        gr20=#SPILL2(gr1,384)
    0| 001480 mulld    7FB6B9D2   1     M         gr29=gr22,gr23
    0| 001484 mulld    7F8059D2   1     M         gr28=gr0,gr11
    0| 001488 add      7D295214   1     A         gr9=gr9,gr10
    0| 00148C rldicr   78FB1F24   1     SLL8      gr27=gr7,3
    0| 001490 addi     3B55FFE0   1     AI        gr26=gr21,-32
    0| 001494 rldicr   78791764   1     SLL8      gr25=gr3,2
    0| 001498 add      7D8C4A14   1     A         gr12=gr12,gr9
    0| 00149C rldicr   7A8A2EA4   1     SLL8      gr10=gr20,5
    0| 0014A0 add      7F7BD214   1     A         gr27=gr27,gr26
    0| 0014A4 subf     7F591850   1     S         gr26=gr3,gr25
    0| 0014A8 rldicr   78190FA4   1     SLL8      gr25=gr0,1
    0| 0014AC mulld    7D6BF9D2   1     M         gr11=gr11,gr31
    0| 0014B0 mulld    7D26B9D2   1     M         gr9=gr6,gr23
    0| 0014B4 subf     7D4AF850   1     S         gr10=gr31,gr10
    0| 0014B8 add      7FBDDA14   1     A         gr29=gr29,gr27
    0| 0014BC add      7F6CD214   1     A         gr27=gr12,gr26
    0| 0014C0 subf     7F59E050   1     S         gr26=gr28,gr25
    0| 0014C4 mulld    7D84B1D2   1     M         gr12=gr4,gr22
    0| 0014C8 mulld    7F8431D2   1     M         gr28=gr4,gr6
    0| 0014CC ld       EA610188   1     L8        gr19=#SPILL3(gr1,392)
    0| 0014D0 addi     3B280002   1     AI        gr25=gr8,2
    0| 0014D4 addi     3B07FFFE   1     AI        gr24=gr7,-2
    0| 0014D8 subfic   20870005   1     SFI       gr4=5,gr7,ca"
    0| 0014DC add      7CEAEA14   1     A         gr7=gr10,gr29
    0| 0014E0 add      7D5ADA14   1     A         gr10=gr26,gr27
    0| 0014E4 extsw    7F3D07B4   1     EXTS4     gr29=gr25
    0| 0014E8 extsw    7F1B07B4   1     EXTS4     gr27=gr24
    0| 0014EC add      7C844214   1     A         gr4=gr4,gr8
    0| 0014F0 add      7D075A14   1     A         gr8=gr7,gr11
    0| 0014F4 add      7CE95214   1     A         gr7=gr9,gr10
    0| 0014F8 rldicr   7A722EA4   1     SLL8      gr18=gr19,5
    0| 0014FC rldicr   78D11764   1     SLL8      gr17=gr6,2
    0| 001500 std      FA410248   1     ST8       #SPILL27(gr1,584)=gr18
    0| 001504 std      FA210250   1     ST8       #SPILL28(gr1,592)=gr17
    0| 001508 subf     7D7BE850   1     S         gr11=gr29,gr27
    0| 00150C add      7FA86214   1     A         gr29=gr8,gr12
    0| 001510 add      7CE7E214   1     A         gr7=gr7,gr28
    0| 001514 subf     7D969050   1     S         gr12=gr18,gr22
    0| 001518 rldicr   7A6A26E4   1     SLL8      gr10=gr19,4
    0| 00151C rldicr   78C90FA4   1     SLL8      gr9=gr6,1
    0| 001520 subf     7D068850   1     S         gr8=gr17,gr6
    0| 001524 addic.   356B0001   1     AI_R      gr11,cr0=gr11,1,ca"
    0| 001528 rldicl   789CF842   1     SRL8      gr28=gr4,1
    0| 00152C add      7E16EA14   1     A         gr16=gr22,gr29
    0| 001530 add      7F063A14   1     A         gr24=gr6,gr7
    0| 001534 std      FA010258   1     ST8       #SPILL29(gr1,600)=gr16
    0| 001538 addi     38C50001   1     AI        gr6=gr5,1
    0| 00153C add      7F6CEA14   1     A         gr27=gr12,gr29
    0| 001540 std      F8C10260   1     ST8       #SPILL30(gr1,608)=gr6
    0| 001544 add      7F4AEA14   1     A         gr26=gr10,gr29
    0| 001548 add      7F278A14   1     A         gr25=gr7,gr17
    0| 00154C add      7EE74A14   1     A         gr23=gr7,gr9
    0| 001550 add      7EC74214   1     A         gr22=gr7,gr8
    0| 001554 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 001558 andi.    70840001   1     RN4_R     gr4,cr0=gr4,0,0x1
    0| 00155C cmpdi    2FBC0000   1     C8        cr7=gr28,0
  172|                              CL.130:
  173| 001560 addi     38800000   1     LI        gr4=0
    0| 001564 bc       408500E8   1     BF        CL.131,cr1,0x2/gt,taken=20%(20,80)
    0| 001568 or       7F15C378   1     LR        gr21=gr24
    0| 00156C or       7EF4BB78   1     LR        gr20=gr23
    0| 001570 or       7ED3B378   1     LR        gr19=gr22
    0| 001574 or       7F32CB78   1     LR        gr18=gr25
    0| 001578 or       7FB1EB78   1     LR        gr17=gr29
    0| 00157C or       7F70DB78   1     LR        gr16=gr27
    0| 001580 or       7F4FD378   1     LR        gr15=gr26
    0| 001584 ld       E9C10258   1     L8        gr14=#SPILL29(gr1,600)
  173|                              CL.132:
  175| 001588 or       7EA5AB78   1     LR        gr5=gr21
  175| 00158C or       7E86A378   1     LR        gr6=gr20
  175| 001590 or       7E679B78   1     LR        gr7=gr19
  175| 001594 or       7E489378   1     LR        gr8=gr18
  175| 001598 or       7E298B78   1     LR        gr9=gr17
  175| 00159C or       7DCA7378   1     LR        gr10=gr14
  175| 0015A0 or       7DEB7B78   1     LR        gr11=gr15
  175| 0015A4 or       7E0C8378   1     LR        gr12=gr16
    0| 0015A8 mtspr    7F8903A6   1     LCTR      ctr=gr28
    0| 0015AC bc       41820030   1     BT        CL.1079,cr0,0x4/eq,taken=50%(0,0)
  175| 0015B0 lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
  175| 0015B4 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
  175| 0015B8 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
  175| 0015BC lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
  175| 0015C0 stfdu    DC090008   1     STFDU     gr9,dcopy[](gr9,8)=fp0
  175| 0015C4 stfdu    DC2A0008   1     STFDU     gr10,dcopy[](gr10,8)=fp1
  175| 0015C8 stfdu    DC4B0008   1     STFDU     gr11,dcopy[](gr11,8)=fp2
  175| 0015CC stfdu    DC6C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp3
    0| 0015D0 bc       419E0050   1     BT        CL.949,cr7,0x4/eq,taken=20%(20,80)
    0| 0015D4 ori      60210000   1     XNOP      
    0| 0015D8 ori      60210000   1     XNOP      
    0|                              CL.1079:
  175| 0015DC lfdux    7C051CEE   1     LFDU      fp0,gr5=d(gr5,gr3,0)
  175| 0015E0 lfdux    7C261CEE   1     LFDU      fp1,gr6=d(gr6,gr3,0)
  175| 0015E4 lfdux    7C471CEE   1     LFDU      fp2,gr7=d(gr7,gr3,0)
  175| 0015E8 lfdux    7C681CEE   1     LFDU      fp3,gr8=d(gr8,gr3,0)
  175| 0015EC lfdux    7C851CEE   1     LFDU      fp4,gr5=d(gr5,gr3,0)
  175| 0015F0 lfdux    7CA61CEE   1     LFDU      fp5,gr6=d(gr6,gr3,0)
  175| 0015F4 lfdux    7CC71CEE   1     LFDU      fp6,gr7=d(gr7,gr3,0)
  175| 0015F8 lfdux    7CE81CEE   1     LFDU      fp7,gr8=d(gr8,gr3,0)
  175| 0015FC stfd     D8090008   1     STFL      dcopy[](gr9,8)=fp0
  175| 001600 stfd     D82A0008   1     STFL      dcopy[](gr10,8)=fp1
  175| 001604 stfd     D84B0008   1     STFL      dcopy[](gr11,8)=fp2
  175| 001608 stfd     D86C0008   1     STFL      dcopy[](gr12,8)=fp3
  175| 00160C stfdu    DC890010   1     STFDU     gr9,dcopy[](gr9,16)=fp4
  175| 001610 stfdu    DCAA0010   1     STFDU     gr10,dcopy[](gr10,16)=fp5
  175| 001614 stfdu    DCCB0010   1     STFDU     gr11,dcopy[](gr11,16)=fp6
  175| 001618 stfdu    DCEC0010   1     STFDU     gr12,dcopy[](gr12,16)=fp7
    0| 00161C bc       4200FFC0   1     BCT       ctr=CL.1079,taken=100%(100,0)
    0|                              CL.949:
  177| 001620 addi     38840001   1     AI        gr4=gr4,1
    0| 001624 add      7EA0AA14   1     A         gr21=gr0,gr21
  177| 001628 cmpld    7F24F040   1     CL8       cr6=gr4,gr30
    0| 00162C add      7E80A214   1     A         gr20=gr0,gr20
    0| 001630 add      7E609A14   1     A         gr19=gr0,gr19
    0| 001634 add      7E409214   1     A         gr18=gr0,gr18
    0| 001638 add      7E31FA14   1     A         gr17=gr17,gr31
    0| 00163C add      7E10FA14   1     A         gr16=gr16,gr31
    0| 001640 add      7DEFFA14   1     A         gr15=gr15,gr31
    0| 001644 add      7DCEFA14   1     A         gr14=gr14,gr31
  177| 001648 bc       4198FF40   1     BT        CL.132,cr6,0x8/llt,taken=80%(80,20)
  177|                              CL.131:
  178| 00164C ld       E8810240   1     L8        gr4=#SPILL26(gr1,576)
    0| 001650 ld       E8A10248   1     L8        gr5=#SPILL27(gr1,584)
    0| 001654 ld       E8C10258   1     L8        gr6=#SPILL29(gr1,600)
  178| 001658 ld       E8E10260   1     L8        gr7=#SPILL30(gr1,608)
    0| 00165C ld       E9010250   1     L8        gr8=#SPILL28(gr1,592)
  178| 001660 addi     38840001   1     AI        gr4=gr4,1
    0| 001664 add      7F65DA14   1     A         gr27=gr5,gr27
    0| 001668 add      7CC53214   1     A         gr6=gr5,gr6
  178| 00166C cmpld    7F243840   1     CL8       cr6=gr4,gr7
  178| 001670 std      F8810240   1     ST8       #SPILL26(gr1,576)=gr4
    0| 001674 std      F8C10258   1     ST8       #SPILL29(gr1,600)=gr6
    0| 001678 add      7F45D214   1     A         gr26=gr5,gr26
    0| 00167C add      7FA5EA14   1     A         gr29=gr5,gr29
    0| 001680 add      7F28CA14   1     A         gr25=gr8,gr25
    0| 001684 add      7F08C214   1     A         gr24=gr8,gr24
    0| 001688 add      7EE8BA14   1     A         gr23=gr8,gr23
    0| 00168C add      7EC8B214   1     A         gr22=gr8,gr22
  178| 001690 bc       4198FED0   1     BT        CL.130,cr6,0x8/llt,taken=80%(80,20)
  178|                              CL.129:
  179| 001694 ld       E8E20000   1     L8        gr7=.&&N&&grid(gr2,0)
  179| 001698 ld       E9010238   1     L8        gr8=#SPILL25(gr1,568)
  179| 00169C lwa      E8870012   1     L4A       gr4=<s64:d16:l4>(gr7,16)
  179| 0016A0 subf     7C644050   1     S         gr3=gr8,gr4
  179| 0016A4 addi     38030001   1     AI        gr0=gr3,1
  179| 0016A8 sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
  179| 0016AC cmpdi    2F200000   1     C8        cr6=gr0,0
  179| 0016B0 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  179| 0016B4 rldicr   78651764   1     SLL8      gr5=gr3,2
  179| 0016B8 subf     7CC50051   1     S_R       gr6,cr0=gr0,gr5
  179| 0016BC crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  179| 0016C0 bc       408100FC   1     BF        CL.174,cr0,0x2/gt,taken=50%(0,0)
  180| 0016C4 lwa      E927000A   1     L4A       gr9=<s64:d8:l4>(gr7,8)
  180| 0016C8 lwa      E987000E   1     L4A       gr12=<s64:d12:l4>(gr7,12)
  182| 0016CC ld       EB820000   1     L8        gr28=.&&N&field(gr2,0)
  181| 0016D0 lwa      E8670006   1     L4A       gr3=<s64:d4:l4>(gr7,4)
  179| 0016D4 addi     39000000   1     LI        gr8=0
  180| 0016D8 subf     7FC96050   1     S         gr30=gr12,gr9
  182| 0016DC ld       E8FC0060   1     L8        gr7=<s24:d96:l8>(gr28,96)
  180| 0016E0 addic.   37DE0001   1     AI_R      gr30,cr0=gr30,1,ca"
  182| 0016E4 ld       E95C0030   1     L8        gr10=<s24:d48:l8>(gr28,48)
  182| 0016E8 ld       E97C0000   1     L8        gr11=<s24:d0:l8>(gr28,0)
  182| 0016EC ld       E99C0018   1     L8        gr12=<s24:d24:l8>(gr28,24)
  182| 0016F0 ld       EBBC0048   1     L8        gr29=<s24:d72:l8>(gr28,72)
    0| 0016F4 bc       408103C4   1     BF        CL.737,cr0,0x2/gt,taken=50%(0,0)
    0| 0016F8 ld       EAE10190   1     L8        gr23=#SPILL4(gr1,400)
    0| 0016FC ld       EAC10170   1     L8        gr22=#SPILL0(gr1,368)
    0| 001700 mulld    7F89F9D2   1     M         gr28=gr9,gr31
    0| 001704 mulld    7F44B9D2   1     M         gr26=gr4,gr23
    0| 001708 mulld    7F6339D2   1     M         gr27=gr3,gr7
    0| 00170C add      7F56D214   1     A         gr26=gr22,gr26
    0| 001710 mulld    7F2451D2   1     M         gr25=gr4,gr10
    0| 001714 rldicr   78781F24   1     SLL8      gr24=gr3,3
    0| 001718 add      7F9CD214   1     A         gr28=gr28,gr26
    0| 00171C add      7D6B6214   1     A         gr11=gr11,gr12
    0| 001720 addi     39830002   1     AI        gr12=gr3,2
    0| 001724 addi     38630001   1     AI        gr3=gr3,1
    0| 001728 mulld    7D29E9D2   1     M         gr9=gr9,gr29
    0| 00172C add      7F98E214   1     A         gr28=gr24,gr28
    0| 001730 add      7D6BDA14   1     A         gr11=gr11,gr27
    0| 001734 extsw    7D8C07B4   1     EXTS4     gr12=gr12
    0| 001738 extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 00173C addi     3B9CFFF8   1     AI        gr28=gr28,-8
    0| 001740 add      7F77FA14   1     A         gr27=gr23,gr31
    0| 001744 add      7D6BCA14   1     A         gr11=gr11,gr25
    0| 001748 subf     7C636050   1     S         gr3=gr12,gr3
    0| 00174C subf     7D9BE050   1     S         gr12=gr28,gr27
    0| 001750 add      7D295A14   1     A         gr9=gr9,gr11
    0| 001754 addic.   34630001   1     AI_R      gr3,cr0=gr3,1,ca"
    0| 001758 addi     3B600002   1     LI        gr27=2
  179|                              CL.169:
  180| 00175C addi     38600000   1     LI        gr3=0
    0| 001760 bc       40810044   1     BF        CL.173,cr0,0x2/gt,taken=20%(20,80)
    0| 001764 or       7D3A4B78   1     LR        gr26=gr9
    0| 001768 or       7D996378   1     LR        gr25=gr12
  180|                              CL.170:
  182| 00176C or       7F2BCB78   1     LR        gr11=gr25
  182| 001770 or       7F5CD378   1     LR        gr28=gr26
    0| 001774 mtspr    7F6903A6   1     LCTR      ctr=gr27
    0| 001778 ori      60210000   1     XNOP      
    0| 00177C ori      60210000   1     XNOP      
    0| 001780 ori      60210000   1     XNOP      
  181|                              CL.171:
  182| 001784 lfdux    7C1C3CEE   1     LFDU      fp0,gr28=d(gr28,gr7,0)
  182| 001788 stfdu    DC0B0008   1     STFDU     gr11,dcopy[](gr11,8)=fp0
  183| 00178C bc       4200FFF8   1     BCT       ctr=CL.171,taken=100%(100,0)
  184| 001790 addi     38630001   1     AI        gr3=gr3,1
    0| 001794 add      7F5AEA14   1     A         gr26=gr26,gr29
  184| 001798 cmpld    7CA3F040   1     CL8       cr1=gr3,gr30
    0| 00179C add      7F39FA14   1     A         gr25=gr25,gr31
  184| 0017A0 bc       4184FFCC   1     BT        CL.170,cr1,0x8/llt,taken=80%(80,20)
  184|                              CL.173:
    0| 0017A4 ld       E8610190   1     L8        gr3=#SPILL4(gr1,400)
  185| 0017A8 addi     39080001   1     AI        gr8=gr8,1
    0| 0017AC add      7D295214   1     A         gr9=gr9,gr10
  185| 0017B0 cmpd     7CA64000   1     C8        cr1=gr6,gr8
    0| 0017B4 add      7D836214   1     A         gr12=gr3,gr12
  185| 0017B8 bc       4185FFA4   1     BT        CL.169,cr1,0x2/gt,taken=80%(80,20)
  185|                              CL.174:
  179| 0017BC cmpd     7CA03000   1     C8        cr1=gr0,gr6
  179| 0017C0 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  179| 0017C4 bc       408101CC   1     BF        CL.135,cr0,0x2/gt,taken=50%(0,0)
  180| 0017C8 ld       EB220000   1     L8        gr25=.&&N&&grid(gr2,0)
  182| 0017CC ld       EB020000   1     L8        gr24=.&&N&field(gr2,0)
    0| 0017D0 ld       EAE10190   1     L8        gr23=#SPILL4(gr1,400)
    0| 0017D4 ld       EAC10170   1     L8        gr22=#SPILL0(gr1,368)
    0| 0017D8 ld       EAA10188   1     L8        gr21=#SPILL3(gr1,392)
  180| 0017DC lwa      E919000A   1     L4A       gr8=<s64:d8:l4>(gr25,8)
  181| 0017E0 lwa      E8F90006   1     L4A       gr7=<s64:d4:l4>(gr25,4)
  182| 0017E4 ld       E8780060   1     L8        gr3=<s24:d96:l8>(gr24,96)
  182| 0017E8 ld       EBD80030   1     L8        gr30=<s24:d48:l8>(gr24,48)
  182| 0017EC ld       E8180048   1     L8        gr0=<s24:d72:l8>(gr24,72)
  182| 0017F0 ld       E9780000   1     L8        gr11=<s24:d0:l8>(gr24,0)
  182| 0017F4 ld       EBB80018   1     L8        gr29=<s24:d24:l8>(gr24,24)
    0| 0017F8 mulld    7F64B9D2   1     M         gr27=gr4,gr23
    0| 0017FC mulld    7F88F9D2   1     M         gr28=gr8,gr31
    0| 001800 mulld    7D4339D2   1     M         gr10=gr3,gr7
  180| 001804 lwa      E999000E   1     L4A       gr12=<s64:d12:l4>(gr25,12)
    0| 001808 mulld    7D24F1D2   1     M         gr9=gr4,gr30
    0| 00180C add      7F76DA14   1     A         gr27=gr22,gr27
    0| 001810 add      7F4BEA14   1     A         gr26=gr11,gr29
    0| 001814 mulld    7D66B9D2   1     M         gr11=gr6,gr23
    0| 001818 mulld    7C8041D2   1     M         gr4=gr0,gr8
    0| 00181C rldicr   78FD1F24   1     SLL8      gr29=gr7,3
    0| 001820 add      7F7BE214   1     A         gr27=gr27,gr28
    0| 001824 add      7D4AD214   1     A         gr10=gr10,gr26
  180| 001828 subf     7D886050   1     S         gr12=gr12,gr8
    0| 00182C mulld    7D06F1D2   1     M         gr8=gr6,gr30
  185| 001830 addi     3B85FFFF   1     AI        gr28=gr5,-1
    0| 001834 add      7CDBEA14   1     A         gr6=gr27,gr29
    0| 001838 add      7D495214   1     A         gr10=gr9,gr10
    0| 00183C addi     3BA70002   1     AI        gr29=gr7,2
    0| 001840 addi     39270001   1     AI        gr9=gr7,1
  180| 001844 addic.   34AC0001   1     AI_R      gr5,cr0=gr12,1,ca"
  185| 001848 sradi    7F8C1674   1     SRA8CA    gr12,ca=gr28,2
    0| 00184C subf     7CFF5850   1     S         gr7=gr11,gr31
    0| 001850 addi     3966FFF8   1     AI        gr11=gr6,-8
    0| 001854 rldicr   7BC61764   1     SLL8      gr6=gr30,2
    0| 001858 add      7D445214   1     A         gr10=gr4,gr10
    0| 00185C extsw    7FA407B4   1     EXTS4     gr4=gr29
    0| 001860 extsw    7D2907B4   1     EXTS4     gr9=gr9
  185| 001864 addze    7FAC0194   1     ADDE      gr29,ca=gr12,0,ca
    0| 001868 add      7CE75A14   1     A         gr7=gr7,gr11
    0| 00186C rldicr   7AAB26E4   1     SLL8      gr11=gr21,4
    0| 001870 add      7D085214   1     A         gr8=gr8,gr10
    0| 001874 subf     7F7E3050   1     S         gr27=gr6,gr30
    0| 001878 rldicr   7BDC0FA4   1     SLL8      gr28=gr30,1
    0| 00187C subf     7D892050   1     S         gr12=gr4,gr9
  179| 001880 addi     38800000   1     LI        gr4=0
    0| 001884 bc       4081010C   1     BF        CL.135,cr0,0x2/gt,taken=20%(20,80)
    0| 001888 rldicr   7AA92EA4   1     SLL8      gr9=gr21,5
    0| 00188C add      7D47BA14   1     A         gr10=gr7,gr23
    0| 001890 add      7D675A14   1     A         gr11=gr7,gr11
    0| 001894 subf     7ED73850   1     S         gr22=gr7,gr23
    0| 001898 add      7EA8F214   1     A         gr21=gr8,gr30
    0| 00189C add      7E88DA14   1     A         gr20=gr8,gr27
    0| 0018A0 add      7E68E214   1     A         gr19=gr8,gr28
    0| 0018A4 addi     3A5D0001   1     AI        gr18=gr29,1
    0| 0018A8 addic.   358C0001   1     AI_R      gr12,cr0=gr12,1,ca"
    0| 0018AC ori      60210000   1     XNOP      
  179|                              CL.136:
  180| 0018B0 addi     39800000   1     LI        gr12=0
    0| 0018B4 bc       408100B0   1     BF        CL.137,cr0,0x2/gt,taken=20%(20,80)
    0| 0018B8 or       7D114378   1     LR        gr17=gr8
    0| 0018BC or       7E90A378   1     LR        gr16=gr20
    0| 0018C0 or       7E6F9B78   1     LR        gr15=gr19
    0| 0018C4 or       7EAEAB78   1     LR        gr14=gr21
    0| 0018C8 or       7CFE3B78   1     LR        gr30=gr7
    0| 0018CC or       7D7D5B78   1     LR        gr29=gr11
    0| 0018D0 or       7EDCB378   1     LR        gr28=gr22
    0| 0018D4 or       7D5B5378   1     LR        gr27=gr10
  180|                              CL.138:
  182| 0018D8 or       7E3A8B78   1     LR        gr26=gr17
  182| 0018DC or       7DD97378   1     LR        gr25=gr14
  182| 0018E0 lfdux    7C1A1CEE   1     LFDU      fp0,gr26=d(gr26,gr3,0)
  182| 0018E4 or       7DF87B78   1     LR        gr24=gr15
  182| 0018E8 or       7E178378   1     LR        gr23=gr16
  182| 0018EC lfdux    7C391CEE   1     LFDU      fp1,gr25=d(gr25,gr3,0)
  182| 0018F0 lfdux    7C581CEE   1     LFDU      fp2,gr24=d(gr24,gr3,0)
  182| 0018F4 lfdux    7C771CEE   1     LFDU      fp3,gr23=d(gr23,gr3,0)
  184| 0018F8 addi     398C0001   1     AI        gr12=gr12,1
    0| 0018FC add      7E310214   1     A         gr17=gr17,gr0
  182| 001900 stfd     D81C0008   1     STFL      dcopy[](gr28,8)=fp0
  182| 001904 lfdux    7C1A1CEE   1     LFDU      fp0,gr26=d(gr26,gr3,0)
  182| 001908 or       7F9AE378   1     LR        gr26=gr28
  182| 00190C stfd     D83E0008   1     STFL      dcopy[](gr30,8)=fp1
  182| 001910 stfd     D85B0008   1     STFL      dcopy[](gr27,8)=fp2
  182| 001914 stfd     D87D0008   1     STFL      dcopy[](gr29,8)=fp3
  182| 001918 lfdux    7C391CEE   1     LFDU      fp1,gr25=d(gr25,gr3,0)
  182| 00191C lfdux    7C581CEE   1     LFDU      fp2,gr24=d(gr24,gr3,0)
  182| 001920 lfdux    7C771CEE   1     LFDU      fp3,gr23=d(gr23,gr3,0)
  182| 001924 stfdu    DC1A0010   1     STFDU     gr26,dcopy[](gr26,16)=fp0
  184| 001928 cmpld    7CAC2840   1     CL8       cr1=gr12,gr5
  182| 00192C or       7FD9F378   1     LR        gr25=gr30
  182| 001930 or       7F78DB78   1     LR        gr24=gr27
  182| 001934 or       7FBAEB78   1     LR        gr26=gr29
  182| 001938 stfdu    DC390010   1     STFDU     gr25,dcopy[](gr25,16)=fp1
  182| 00193C stfdu    DC580010   1     STFDU     gr24,dcopy[](gr24,16)=fp2
  182| 001940 stfdu    DC7A0010   1     STFDU     gr26,dcopy[](gr26,16)=fp3
    0| 001944 add      7E100214   1     A         gr16=gr16,gr0
    0| 001948 add      7DEF0214   1     A         gr15=gr15,gr0
    0| 00194C add      7DCE0214   1     A         gr14=gr14,gr0
    0| 001950 add      7FDEFA14   1     A         gr30=gr30,gr31
    0| 001954 add      7FBDFA14   1     A         gr29=gr29,gr31
    0| 001958 add      7F9CFA14   1     A         gr28=gr28,gr31
    0| 00195C add      7F7BFA14   1     A         gr27=gr27,gr31
  184| 001960 bc       4184FF78   1     BT        CL.138,cr1,0x8/llt,taken=80%(80,20)
  184|                              CL.137:
  185| 001964 addi     38840001   1     AI        gr4=gr4,1
    0| 001968 add      7D495214   1     A         gr10=gr9,gr10
  185| 00196C cmpld    7CA49040   1     CL8       cr1=gr4,gr18
    0| 001970 add      7D695A14   1     A         gr11=gr9,gr11
    0| 001974 add      7EC9B214   1     A         gr22=gr9,gr22
    0| 001978 add      7CE74A14   1     A         gr7=gr7,gr9
    0| 00197C add      7EA6AA14   1     A         gr21=gr6,gr21
    0| 001980 add      7E86A214   1     A         gr20=gr6,gr20
    0| 001984 add      7E669A14   1     A         gr19=gr6,gr19
    0| 001988 add      7D064214   1     A         gr8=gr6,gr8
  185| 00198C bc       4184FF24   1     BT        CL.136,cr1,0x8/llt,taken=80%(80,20)
  185|                              CL.135:
  189| 001990 ld       E8620000   1     L8        gr3=.&&N&&mpipar(gr2,0)
  189| 001994 lwz      80030018   1     L4Z       gr0=<s70:d24:l4>(gr3,24)
  189| 001998 cmpdi    2C200000   1     C8        cr0=gr0,0
  189| 00199C bc       4182001C   1     BT        CL.79,cr0,0x4/eq,taken=60%(60,40)
  190| 0019A0 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  190| 0019A4 addi     38C30014   1     AI        gr6=gr3,20
  190| 0019A8 addi     38630018   1     AI        gr3=gr3,24
  190| 0019AC addi     38851770   1     AI        gr4=gr5,6000
  190| 0019B0 bl       48000001   1     CALL      mpi_waitall,4,nreq",gr3,req[]",gr4,stat[]",gr5,ierr",gr6,mpi_waitall",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  190| 0019B4 ori      60000000   1
  190|                              CL.79:
  195| 0019B8 ld       EBE20000   1     L8        gr31=.&&N&&grid(gr2,0)
  195| 0019BC ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  195| 0019C0 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  195| 0019C4 addi     38FF0014   1     AI        gr7=gr31,20
  195| 0019C8 or       7FE3FB78   1     LR        gr3=gr31
  195| 0019CC addi     389F0004   1     AI        gr4=gr31,4
  195| 0019D0 addi     38BF0008   1     AI        gr5=gr31,8
  195| 0019D4 addi     38DF000C   1     AI        gr6=gr31,12
  195| 0019D8 or       7CE83B78   1     LR        gr8=gr7
  195| 0019DC bl       48000001   1     CALL      pdv,8,is",gr3,ie",gr4,js",gr5,je",gr6,ke",gr7,ke",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  195| 0019E0 ori      60000000   1
  308|                              CL.80:
  311| 0019E4 ld       E8820000   1     L8        gr4=.&&N&&config(gr2,0)
    0| 0019E8 ld       E8620000   1     L8        gr3=.&&N&&bndry(gr2,0)
  311| 0019EC lwz      80040020   1     L4Z       gr0=<s61:d32:l4>(gr4,32)
    0| 0019F0 cmpwi    2C000001   1     C4        cr0=gr0,1
    0| 0019F4 bc       418200A0   1     BT        CL.748,cr0,0x4/eq,taken=50%(0,0)
  310| 0019F8 addi     38000000   1     LI        gr0=0
  310| 0019FC stw      90030308   1     ST4Z      bvstat[](gr3,776)=gr0
  311| 001A00 stw      900303C8   1     ST4Z      bvstat[](gr3,968)=gr0
  310| 001A04 stw      9003030C   1     ST4Z      bvstat[](gr3,780)=gr0
  311| 001A08 stw      900303CC   1     ST4Z      bvstat[](gr3,972)=gr0
  310| 001A0C stw      90030310   1     ST4Z      bvstat[](gr3,784)=gr0
  311| 001A10 stw      900303D0   1     ST4Z      bvstat[](gr3,976)=gr0
  310| 001A14 stw      90030314   1     ST4Z      bvstat[](gr3,788)=gr0
  311| 001A18 stw      900303D4   1     ST4Z      bvstat[](gr3,980)=gr0
  310| 001A1C stw      90030318   1     ST4Z      bvstat[](gr3,792)=gr0
  311| 001A20 stw      900303D8   1     ST4Z      bvstat[](gr3,984)=gr0
  310| 001A24 stw      9003031C   1     ST4Z      bvstat[](gr3,796)=gr0
  311| 001A28 addi     386303C4   1     AI        gr3=gr3,964
  311| 001A2C stwu     94030018   1     ST4U      gr3,bvstat[](gr3,24)=gr0
  261|                              CL.247:
  315| 001A30 ld       E8010330   1     L8        gr0=#stack(gr1,816)
  315| 001A34 lwa      E981032A   1     L4A       gr12=#stack(gr1,808)
  315| 001A38 addi     38210320   1     AI        gr1=gr1,800
  315| 001A3C ld       E9C1FF70   1     L8        gr14=#stack(gr1,-144)
  315| 001A40 ld       E9E1FF78   1     L8        gr15=#stack(gr1,-136)
  315| 001A44 mtspr    7C0803A6   1     LLR       lr=gr0
  315| 001A48 mtcrf    7D820120   1     MTCRF     cr2=gr12
  315| 001A4C mtcrf    7D808120   1     MTCRF     cr4=gr12
  315| 001A50 ld       EA01FF80   1     L8        gr16=#stack(gr1,-128)
  315| 001A54 ld       EA21FF88   1     L8        gr17=#stack(gr1,-120)
  315| 001A58 ld       EA41FF90   1     L8        gr18=#stack(gr1,-112)
  315| 001A5C ld       EA61FF98   1     L8        gr19=#stack(gr1,-104)
  315| 001A60 ld       EA81FFA0   1     L8        gr20=#stack(gr1,-96)
  315| 001A64 ld       EAA1FFA8   1     L8        gr21=#stack(gr1,-88)
  315| 001A68 ld       EAC1FFB0   1     L8        gr22=#stack(gr1,-80)
  315| 001A6C ld       EAE1FFB8   1     L8        gr23=#stack(gr1,-72)
  315| 001A70 ld       EB01FFC0   1     L8        gr24=#stack(gr1,-64)
  315| 001A74 ld       EB21FFC8   1     L8        gr25=#stack(gr1,-56)
  315| 001A78 ld       EB41FFD0   1     L8        gr26=#stack(gr1,-48)
  315| 001A7C ld       EB61FFD8   1     L8        gr27=#stack(gr1,-40)
  315| 001A80 ld       EB81FFE0   1     L8        gr28=#stack(gr1,-32)
  315| 001A84 ld       EBA1FFE8   1     L8        gr29=#stack(gr1,-24)
  315| 001A88 ld       EBC1FFF0   1     L8        gr30=#stack(gr1,-16)
  315| 001A8C ld       EBE1FFF8   1     L8        gr31=#stack(gr1,-8)
  315| 001A90 bclr     4E800020   1     BA        lr
    0|                              CL.748:
  310| 001A94 addi     38000000   1     LI        gr0=0
  310| 001A98 stw      90030308   1     ST4Z      bvstat[](gr3,776)=gr0
  310| 001A9C stw      9003030C   1     ST4Z      bvstat[](gr3,780)=gr0
  310| 001AA0 stw      90030310   1     ST4Z      bvstat[](gr3,784)=gr0
  310| 001AA4 stw      90030314   1     ST4Z      bvstat[](gr3,788)=gr0
  310| 001AA8 stw      90030318   1     ST4Z      bvstat[](gr3,792)=gr0
  310| 001AAC addi     38630304   1     AI        gr3=gr3,772
  310| 001AB0 stwu     94030018   1     ST4U      gr3,bvstat[](gr3,24)=gr0
  315| 001AB4 b        4BFFFF7C   1     B         CL.247,-1
  184|                              CL.737:
    0| 001AB8 mtspr    7CC903A6   1     LCTR      ctr=gr6
  184|                              CL.526:
  185| 001ABC addi     39080001   1     AI        gr8=gr8,1
  185| 001AC0 cmpd     7CA83000   1     C8        cr1=gr8,gr6
  185| 001AC4 bc       4104FFF8   1     BCTT      ctr=CL.526,cr1,0x1/lt,taken=80%(80,20)
    0| 001AC8 b        4BFFFCF4   1     B         CL.174,-1
  177|                              CL.727:
    0| 001ACC mtspr    7C8903A6   1     LCTR      ctr=gr4
  177|                              CL.556:
  178| 001AD0 addi     38E70001   1     AI        gr7=gr7,1
  178| 001AD4 cmpd     7CA72000   1     C8        cr1=gr7,gr4
  178| 001AD8 bc       4104FFF8   1     BCTT      ctr=CL.556,cr1,0x1/lt,taken=80%(80,20)
    0| 001ADC b        4BFFF924   1     B         CL.186,-1
  146|                              CL.717:
    0| 001AE0 mtspr    7C0903A6   1     LCTR      ctr=gr0
  146|                              CL.586:
  147| 001AE4 addi     38A50001   1     AI        gr5=gr5,1
  147| 001AE8 cmpd     7C250000   1     C8        cr0=gr5,gr0
  147| 001AEC bc       4100FFF8   1     BCTT      ctr=CL.586,cr0,0x1/lt,taken=80%(80,20)
    0| 001AF0 b        4BFFF4BC   1     B         CL.198,-1
  139|                              CL.707:
    0| 001AF4 mtspr    7C0903A6   1     LCTR      ctr=gr0
  139|                              CL.616:
  140| 001AF8 addi     38840001   1     AI        gr4=gr4,1
  140| 001AFC cmpd     7C240000   1     C8        cr0=gr4,gr0
  140| 001B00 bc       4100FFF8   1     BCTT      ctr=CL.616,cr0,0x1/lt,taken=80%(80,20)
    0| 001B04 b        4BFFF114   1     B         CL.210,-1
  108|                              CL.697:
    0| 001B08 mtspr    7CA903A6   1     LCTR      ctr=gr5
  108|                              CL.646:
  109| 001B0C addi     39080001   1     AI        gr8=gr8,1
  109| 001B10 cmpd     7CA82800   1     C8        cr1=gr8,gr5
  109| 001B14 bc       4104FFF8   1     BCTT      ctr=CL.646,cr1,0x1/lt,taken=80%(80,20)
    0| 001B18 b        4BFFEC28   1     B         CL.222,-1
   88|                              CL.687:
    0| 001B1C mtspr    7CA903A6   1     LCTR      ctr=gr5
   88|                              CL.676:
   89| 001B20 addi     38E70001   1     AI        gr7=gr7,1
   89| 001B24 cmpd     7CA72800   1     C8        cr1=gr7,gr5
   89| 001B28 bc       4104FFF8   1     BCTT      ctr=CL.676,cr1,0x1/lt,taken=80%(80,20)
    0| 001B2C b        4BFFE7D4   1     B         CL.234,-1
  266|                              CL.4:
  272| 001B30 stw      936A0018   1     ST4Z      <s70:d24:l4>(gr10,24)=gr27
  274| 001B34 addi     38600001   1     LI        gr3=1
  274| 001B38 stw      936100F4   1     ST4Z      T_44(gr1,244)=gr27
  273| 001B3C stw      900A001C   1     ST4Z      <s70:d28:l4>(gr10,28)=gr0
  274| 001B40 stw      936100EC   1     ST4Z      T_42(gr1,236)=gr27
  274| 001B44 stw      906100F0   1     ST4Z      T_43(gr1,240)=gr3
  274| 001B48 stw      936100F8   1     ST4Z      T_45(gr1,248)=gr27
  274| 001B4C stw      936100FC   1     ST4Z      T_46(gr1,252)=gr27
  274| 001B50 stw      93610100   1     ST4Z      T_47(gr1,256)=gr27
  274| 001B54 addi     39010100   1     AI        gr8=gr1,256
  274| 001B58 addi     38E100FC   1     AI        gr7=gr1,252
  274| 001B5C addi     38C100F8   1     AI        gr6=gr1,248
  274| 001B60 addi     38A100F4   1     AI        gr5=gr1,244
  274| 001B64 addi     388100F0   1     AI        gr4=gr1,240
  274| 001B68 addi     386100EC   1     AI        gr3=gr1,236
  274| 001B6C bl       48000001   1     CALL      bvalv1,7,T_42",gr3,T_43",gr4,T_44",gr5,T_45",gr6,T_46",gr7,T_47",gr8,v1",gr9,#ProcAlias",bvalv1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  274| 001B70 ori      60000000   1
  281| 001B74 ld       E9820000   1     L8        gr12=.&&N&&grid(gr2,0)
  281| 001B78 lwz      806C0000   1     L4Z       gr3=<s64:d0:l4>(gr12,0)
  281| 001B7C lwz      808C0004   1     L4Z       gr4=<s64:d4:l4>(gr12,4)
  282| 001B80 lwa      E80C000A   1     L4A       gr0=<s64:d8:l4>(gr12,8)
    0| 001B84 addi     38C3FFFE   1     AI        gr6=gr3,-2
    0| 001B88 addi     38A40002   1     AI        gr5=gr4,2
    0| 001B8C extsw    7CC607B4   1     EXTS4     gr6=gr6
    0| 001B90 extsw    7CA707B4   1     EXTS4     gr7=gr5
    0| 001B94 subf     7CA63850   1     S         gr5=gr7,gr6
    0| 001B98 addic.   34A50001   1     AI_R      gr5,cr0=gr5,1,ca"
  281| 001B9C bc       40810104   1     BF        CL.147,cr0,0x2/gt,taken=50%(0,0)
  282| 001BA0 ld       EB220000   1     L8        gr25=.&&N&field(gr2,0)
  282| 001BA4 lwa      E90C0012   1     L4A       gr8=<s64:d16:l4>(gr12,16)
  282| 001BA8 ld       EB010190   1     L8        gr24=#SPILL4(gr1,400)
    0| 001BAC extsw    7C7C07B4   1     EXTS4     gr28=gr3
  282| 001BB0 ld       EAE10170   1     L8        gr23=#SPILL0(gr1,368)
    0| 001BB4 subfic   20C60001   1     SFI       gr6=1,gr6,ca"
  282| 001BB8 ld       E8B90060   1     L8        gr5=<s24:d96:l8>(gr25,96)
  282| 001BBC ld       E9390030   1     L8        gr9=<s24:d48:l8>(gr25,48)
  282| 001BC0 ld       E9590000   1     L8        gr10=<s24:d0:l8>(gr25,0)
  282| 001BC4 ld       E9790018   1     L8        gr11=<s24:d24:l8>(gr25,24)
  282| 001BC8 ld       E9990048   1     L8        gr12=<s24:d72:l8>(gr25,72)
  282| 001BCC mulld    7FA8C1D2   1     M         gr29=gr8,gr24
  282| 001BD0 mulld    7FC5E1D2   1     M         gr30=gr5,gr28
  282| 001BD4 rldicr   7B9C1F24   1     SLL8      gr28=gr28,3
  282| 001BD8 addi     3B77FFE0   1     AI        gr27=gr23,-32
  282| 001BDC rldicr   78BA1764   1     SLL8      gr26=gr5,2
  282| 001BE0 add      7D4A5A14   1     A         gr10=gr10,gr11
  282| 001BE4 mulld    7D60F9D2   1     M         gr11=gr0,gr31
  282| 001BE8 mulld    7D0849D2   1     M         gr8=gr8,gr9
  282| 001BEC add      7D3BE214   1     A         gr9=gr27,gr28
  282| 001BF0 subf     7F9A2850   1     S         gr28=gr5,gr26
  282| 001BF4 add      7D4AF214   1     A         gr10=gr10,gr30
    0| 001BF8 addi     3BC40005   1     AI        gr30=gr4,5
    0| 001BFC add      7CC63A14   1     A         gr6=gr6,gr7
  282| 001C00 mulld    7C0061D2   1     M         gr0=gr0,gr12
  282| 001C04 add      7CE9EA14   1     A         gr7=gr9,gr29
  282| 001C08 add      7D2AE214   1     A         gr9=gr10,gr28
    0| 001C0C subf     7C63F050   1     S         gr3=gr30,gr3
    0| 001C10 rldicl   78C6E8C2   1     SRL8      gr6=gr6,3
  282| 001C14 add      7CE75A14   1     A         gr7=gr7,gr11
  282| 001C18 add      7D58FA14   1     A         gr10=gr24,gr31
  282| 001C1C add      7D084A14   1     A         gr8=gr8,gr9
    0| 001C20 andi.    70630007   1     RN4_R     gr3,cr0=gr3,0,0x7
    0| 001C24 cmpdi    2CA60000   1     C8        cr1=gr6,0
  282| 001C28 subf     7CEA3850   1     S         gr7=gr7,gr10
  282| 001C2C add      7D004214   1     A         gr8=gr0,gr8
    0| 001C30 bc       41820028   1     BT        CL.951,cr0,0x4/eq,taken=50%(0,0)
    0| 001C34 mtspr    7C6903A6   1     LCTR      ctr=gr3
  282| 001C38 lfdux    7C082CEE   1     LFDU      fp0,gr8=d(gr8,gr5,0)
    0| 001C3C bc       42400014   1     BCF       ctr=CL.1106,taken=0%(0,100)
    0|                              CL.1107:
  282| 001C40 lfdux    7C282CEE   1     LFDU      fp1,gr8=d(gr8,gr5,0)
  282| 001C44 stfdu    DC070008   1     STFDU     gr7,dcopy[](gr7,8)=fp0
    0| 001C48 fmr      FC000890   1     LRFL      fp0=fp1
    0| 001C4C bc       4200FFF4   1     BCT       ctr=CL.1107,taken=100%(100,0)
    0|                              CL.1106:
  282| 001C50 stfdu    DC070008   1     STFDU     gr7,dcopy[](gr7,8)=fp0
    0| 001C54 bc       4186004C   1     BT        CL.147,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.951:
    0| 001C58 mtspr    7CC903A6   1     LCTR      ctr=gr6
    0|                              CL.1108:
  282| 001C5C lfdux    7C082CEE   1     LFDU      fp0,gr8=d(gr8,gr5,0)
  282| 001C60 lfdux    7C282CEE   1     LFDU      fp1,gr8=d(gr8,gr5,0)
  282| 001C64 stfd     D8070008   1     STFL      dcopy[](gr7,8)=fp0
  282| 001C68 lfdux    7C082CEE   1     LFDU      fp0,gr8=d(gr8,gr5,0)
  282| 001C6C stfd     D8270010   1     STFL      dcopy[](gr7,16)=fp1
  282| 001C70 lfdux    7C282CEE   1     LFDU      fp1,gr8=d(gr8,gr5,0)
  282| 001C74 stfd     D8070018   1     STFL      dcopy[](gr7,24)=fp0
  282| 001C78 lfdux    7C082CEE   1     LFDU      fp0,gr8=d(gr8,gr5,0)
  282| 001C7C stfd     D8270020   1     STFL      dcopy[](gr7,32)=fp1
  282| 001C80 lfdux    7C282CEE   1     LFDU      fp1,gr8=d(gr8,gr5,0)
  282| 001C84 stfd     D8070028   1     STFL      dcopy[](gr7,40)=fp0
  282| 001C88 lfdux    7C082CEE   1     LFDU      fp0,gr8=d(gr8,gr5,0)
  282| 001C8C stfd     D8270030   1     STFL      dcopy[](gr7,48)=fp1
  282| 001C90 lfdux    7C282CEE   1     LFDU      fp1,gr8=d(gr8,gr5,0)
  282| 001C94 stfd     D8070038   1     STFL      dcopy[](gr7,56)=fp0
  282| 001C98 stfdu    DC270040   1     STFDU     gr7,dcopy[](gr7,64)=fp1
    0| 001C9C bc       4200FFC0   1     BCT       ctr=CL.1108,taken=100%(100,0)
  283|                              CL.147:
  286| 001CA0 ld       E8620000   1     L8        gr3=.&&N&&grid(gr2,0)
  286| 001CA4 addi     3804FFFF   1     AI        gr0=gr4,-1
  286| 001CA8 addi     38810104   1     AI        gr4=gr1,260
  286| 001CAC stw      90010104   1     ST4Z      T_48(gr1,260)=gr0
  286| 001CB0 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  286| 001CB4 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  286| 001CB8 or       7C7F1B78   1     LR        gr31=gr3
  286| 001CBC addi     38BF0008   1     AI        gr5=gr31,8
  286| 001CC0 addi     38DF0008   1     AI        gr6=gr31,8
  286| 001CC4 addi     38FF0010   1     AI        gr7=gr31,16
  286| 001CC8 addi     391F0010   1     AI        gr8=gr31,16
  286| 001CCC bl       48000001   1     CALL      pdv,8,is",gr3,T_48",gr4,js",gr5,js",gr6,ks",gr7,ks",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  286| 001CD0 ori      60000000   1
  297| 001CD4 ld       EBC20000   1     L8        gr30=.&&N&&mpipar(gr2,0)
  297| 001CD8 lwz      801E0018   1     L4Z       gr0=<s70:d24:l4>(gr30,24)
  297| 001CDC cmpdi    2C200000   1     C8        cr0=gr0,0
  297| 001CE0 bc       4182001C   1     BT        CL.99,cr0,0x4/eq,taken=60%(60,40)
  298| 001CE4 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  298| 001CE8 addi     38DE0014   1     AI        gr6=gr30,20
  298| 001CEC addi     387E0018   1     AI        gr3=gr30,24
  298| 001CF0 addi     38851770   1     AI        gr4=gr5,6000
  298| 001CF4 bl       48000001   1     CALL      mpi_waitall,4,nreq",gr3,req[]",gr4,stat[]",gr5,ierr",gr6,mpi_waitall",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  298| 001CF8 ori      60000000   1
  298|                              CL.99:
  303| 001CFC addi     387F0004   1     AI        gr3=gr31,4
  303| 001D00 addi     389F0004   1     AI        gr4=gr31,4
  303| 001D04 addi     38BF0008   1     AI        gr5=gr31,8
  303| 001D08 addi     38DF0008   1     AI        gr6=gr31,8
  303| 001D0C addi     38FF0010   1     AI        gr7=gr31,16
  303| 001D10 addi     391F0010   1     AI        gr8=gr31,16
  303| 001D14 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  303| 001D18 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  303| 001D1C bl       48000001   1     CALL      pdv,8,ie",gr3,ie",gr4,js",gr5,js",gr6,ks",gr7,ks",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  303| 001D20 ori      60000000   1
    0| 001D24 b        4BFFFCC0   1     B         CL.80,-1
  203|                              CL.2:
  209| 001D28 stw      936A0018   1     ST4Z      <s70:d24:l4>(gr10,24)=gr27
  211| 001D2C addi     3BC00001   1     LI        gr30=1
  211| 001D30 stw      93610110   1     ST4Z      T_29(gr1,272)=gr27
  210| 001D34 stw      900A001C   1     ST4Z      <s70:d28:l4>(gr10,28)=gr0
  211| 001D38 stw      93610108   1     ST4Z      T_27(gr1,264)=gr27
  211| 001D3C stw      93C1010C   1     ST4Z      T_28(gr1,268)=gr30
  211| 001D40 stw      93610114   1     ST4Z      T_30(gr1,276)=gr27
  211| 001D44 stw      93610118   1     ST4Z      T_31(gr1,280)=gr27
  211| 001D48 stw      9361011C   1     ST4Z      T_32(gr1,284)=gr27
  211| 001D4C addi     3901011C   1     AI        gr8=gr1,284
  211| 001D50 addi     38E10118   1     AI        gr7=gr1,280
  211| 001D54 addi     38C10114   1     AI        gr6=gr1,276
  211| 001D58 addi     38A10110   1     AI        gr5=gr1,272
  211| 001D5C addi     3881010C   1     AI        gr4=gr1,268
  211| 001D60 addi     38610108   1     AI        gr3=gr1,264
  211| 001D64 bl       48000001   1     CALL      bvalv1,7,T_27",gr3,T_28",gr4,T_29",gr5,T_30",gr6,T_31",gr7,T_32",gr8,v1",gr9,#ProcAlias",bvalv1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  211| 001D68 ori      60000000   1
  217| 001D6C ld       E9820000   1     L8        gr12=.&&N&&grid(gr2,0)
  217| 001D70 lwz      806C000C   1     L4Z       gr3=<s64:d12:l4>(gr12,12)
  217| 001D74 lwz      80EC0008   1     L4Z       gr7=<s64:d8:l4>(gr12,8)
    0| 001D78 lwz      808C0004   1     L4Z       gr4=<s64:d4:l4>(gr12,4)
  217| 001D7C addi     38030002   1     AI        gr0=gr3,2
  217| 001D80 addi     38A7FFFE   1     AI        gr5=gr7,-2
  217| 001D84 extsw    7C0007B4   1     EXTS4     gr0=gr0
  217| 001D88 extsw    7CA507B4   1     EXTS4     gr5=gr5
  217| 001D8C subf     7CA50050   1     S         gr5=gr0,gr5
  217| 001D90 addic.   34050001   1     AI_R      gr0,cr0=gr5,1,ca"
  217| 001D94 bc       40810160   1     BF        CL.141,cr0,0x2/gt,taken=50%(0,0)
  219| 001D98 ld       EB020000   1     L8        gr24=.&&N&field(gr2,0)
  218| 001D9C lwz      836C0000   1     L4Z       gr27=<s64:d0:l4>(gr12,0)
  219| 001DA0 lwa      EBAC0012   1     L4A       gr29=<s64:d16:l4>(gr12,16)
    0| 001DA4 ld       EAE10190   1     L8        gr23=#SPILL4(gr1,400)
    0| 001DA8 ld       EAC10170   1     L8        gr22=#SPILL0(gr1,368)
    0| 001DAC extsw    7CE707B4   1     EXTS4     gr7=gr7
  219| 001DB0 ld       E8B80060   1     L8        gr5=<s24:d96:l8>(gr24,96)
    0| 001DB4 extsw    7F6B07B4   1     EXTS4     gr11=gr27
  219| 001DB8 ld       EB980030   1     L8        gr28=<s24:d48:l8>(gr24,48)
  219| 001DBC ld       E9580000   1     L8        gr10=<s24:d0:l8>(gr24,0)
  219| 001DC0 ld       E9980018   1     L8        gr12=<s24:d24:l8>(gr24,24)
  219| 001DC4 ld       E8D80048   1     L8        gr6=<s24:d72:l8>(gr24,72)
    0| 001DC8 mulld    7D0559D2   1     M         gr8=gr5,gr11
    0| 001DCC mulld    7D37E9D2   1     M         gr9=gr23,gr29
    0| 001DD0 ld       EAA10180   1     L8        gr21=#SPILL2(gr1,384)
  218| 001DD4 addi     3B3BFFFE   1     AI        gr25=gr27,-2
    0| 001DD8 rldicr   78BA1764   1     SLL8      gr26=gr5,2
    0| 001DDC add      7D4A6214   1     A         gr10=gr10,gr12
    0| 001DE0 rldicr   796B1F24   1     SLL8      gr11=gr11,3
    0| 001DE4 addi     3996FFE0   1     AI        gr12=gr22,-32
  218| 001DE8 addi     3B640002   1     AI        gr27=gr4,2
    0| 001DEC mulld    7FBDE1D2   1     M         gr29=gr29,gr28
    0| 001DF0 mulld    7F87F9D2   1     M         gr28=gr7,gr31
  218| 001DF4 extsw    7F3907B4   1     EXTS4     gr25=gr25
    0| 001DF8 subf     7F5A2850   1     S         gr26=gr5,gr26
    0| 001DFC add      7D085214   1     A         gr8=gr8,gr10
    0| 001E00 add      7D4B6214   1     A         gr10=gr11,gr12
  218| 001E04 extsw    7F6B07B4   1     EXTS4     gr11=gr27
    0| 001E08 mulld    7CE731D2   1     M         gr7=gr7,gr6
    0| 001E0C add      7D08D214   1     A         gr8=gr8,gr26
    0| 001E10 rldicr   7AAC2EA4   1     SLL8      gr12=gr21,5
    0| 001E14 add      7D495214   1     A         gr10=gr9,gr10
  218| 001E18 subf     7D395850   1     S         gr9=gr11,gr25
    0| 001E1C add      7D08EA14   1     A         gr8=gr8,gr29
    0| 001E20 subf     7D8CF850   1     S         gr12=gr31,gr12
    0| 001E24 add      7D4AE214   1     A         gr10=gr10,gr28
  218| 001E28 addic.   35290001   1     AI_R      gr9,cr0=gr9,1,ca"
    0| 001E2C rldicr   78C90FA4   1     SLL8      gr9=gr6,1
    0| 001E30 add      7CE74214   1     A         gr7=gr7,gr8
    0| 001E34 add      7D4A6214   1     A         gr10=gr10,gr12
  217| 001E38 addi     39000000   1     LI        gr8=0
    0| 001E3C bc       408100B8   1     BF        CL.141,cr0,0x2/gt,taken=20%(20,80)
    0| 001E40 subfic   21990001   1     SFI       gr12=1,gr25,ca"
    0| 001E44 subfic   23B90003   1     SFI       gr29=3,gr25,ca"
    0| 001E48 add      7D6B6214   1     A         gr11=gr11,gr12
    0| 001E4C add      7D84EA14   1     A         gr12=gr4,gr29
    0| 001E50 rldicl   796BE8C2   1     SRL8      gr11=gr11,3
    0| 001E54 subf     7CE93850   1     S         gr7=gr7,gr9
    0| 001E58 subf     7D375050   1     S         gr9=gr10,gr23
    0| 001E5C andi.    719D0007   1     RN4_R     gr29,cr0=gr12,0,0x7
    0| 001E60 cmpdi    2CAB0000   1     C8        cr1=gr11,0
  217|                              CL.142:
  219| 001E64 or       7CEA3B78   1     LR        gr10=gr7
  219| 001E68 or       7D2C4B78   1     LR        gr12=gr9
    0| 001E6C bc       4182002C   1     BT        CL.954,cr0,0x4/eq,taken=50%(0,0)
    0| 001E70 mtspr    7FA903A6   1     LCTR      ctr=gr29
  219| 001E74 lfdux    7C0A2CEE   1     LFDU      fp0,gr10=d(gr10,gr5,0)
    0| 001E78 bc       42400018   1     BCF       ctr=CL.1109,taken=0%(0,100)
    0| 001E7C ori      60210000   1     XNOP      
    0|                              CL.1110:
  219| 001E80 lfdux    7C2A2CEE   1     LFDU      fp1,gr10=d(gr10,gr5,0)
  219| 001E84 stfdu    DC0C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp0
    0| 001E88 fmr      FC000890   1     LRFL      fp0=fp1
    0| 001E8C bc       4200FFF4   1     BCT       ctr=CL.1110,taken=100%(100,0)
    0|                              CL.1109:
  219| 001E90 stfdu    DC0C0008   1     STFDU     gr12,dcopy[](gr12,8)=fp0
    0| 001E94 bc       4186004C   1     BT        CL.955,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.954:
    0| 001E98 mtspr    7D6903A6   1     LCTR      ctr=gr11
    0|                              CL.1111:
  219| 001E9C lfdux    7C0A2CEE   1     LFDU      fp0,gr10=d(gr10,gr5,0)
  219| 001EA0 lfdux    7C2A2CEE   1     LFDU      fp1,gr10=d(gr10,gr5,0)
  219| 001EA4 stfd     D80C0008   1     STFL      dcopy[](gr12,8)=fp0
  219| 001EA8 lfdux    7C0A2CEE   1     LFDU      fp0,gr10=d(gr10,gr5,0)
  219| 001EAC stfd     D82C0010   1     STFL      dcopy[](gr12,16)=fp1
  219| 001EB0 lfdux    7C2A2CEE   1     LFDU      fp1,gr10=d(gr10,gr5,0)
  219| 001EB4 stfd     D80C0018   1     STFL      dcopy[](gr12,24)=fp0
  219| 001EB8 lfdux    7C0A2CEE   1     LFDU      fp0,gr10=d(gr10,gr5,0)
  219| 001EBC stfd     D82C0020   1     STFL      dcopy[](gr12,32)=fp1
  219| 001EC0 lfdux    7C2A2CEE   1     LFDU      fp1,gr10=d(gr10,gr5,0)
  219| 001EC4 stfd     D80C0028   1     STFL      dcopy[](gr12,40)=fp0
  219| 001EC8 lfdux    7C0A2CEE   1     LFDU      fp0,gr10=d(gr10,gr5,0)
  219| 001ECC stfd     D82C0030   1     STFL      dcopy[](gr12,48)=fp1
  219| 001ED0 lfdux    7C2A2CEE   1     LFDU      fp1,gr10=d(gr10,gr5,0)
  219| 001ED4 stfd     D80C0038   1     STFL      dcopy[](gr12,56)=fp0
  219| 001ED8 stfdu    DC2C0040   1     STFDU     gr12,dcopy[](gr12,64)=fp1
    0| 001EDC bc       4200FFC0   1     BCT       ctr=CL.1111,taken=100%(100,0)
    0|                              CL.955:
  221| 001EE0 addi     39080001   1     AI        gr8=gr8,1
    0| 001EE4 add      7CE63A14   1     A         gr7=gr6,gr7
  221| 001EE8 cmpld    7F280040   1     CL8       cr6=gr8,gr0
    0| 001EEC add      7D29FA14   1     A         gr9=gr9,gr31
  221| 001EF0 bc       4198FF74   1     BT        CL.142,cr6,0x8/llt,taken=80%(80,20)
  221|                              CL.141:
  223| 001EF4 addi     3863FFFF   1     AI        gr3=gr3,-1
  223| 001EF8 addi     3804FFFF   1     AI        gr0=gr4,-1
  223| 001EFC stw      90610124   1     ST4Z      T_34(gr1,292)=gr3
  223| 001F00 ld       E8620000   1     L8        gr3=.&&N&&grid(gr2,0)
  223| 001F04 stw      90010120   1     ST4Z      T_33(gr1,288)=gr0
  223| 001F08 addi     38C10124   1     AI        gr6=gr1,292
  223| 001F0C addi     38810120   1     AI        gr4=gr1,288
  223| 001F10 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  223| 001F14 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  223| 001F18 or       7C7F1B78   1     LR        gr31=gr3
  223| 001F1C addi     38BF0008   1     AI        gr5=gr31,8
  223| 001F20 addi     38FF0010   1     AI        gr7=gr31,16
  223| 001F24 addi     391F0010   1     AI        gr8=gr31,16
  223| 001F28 bl       48000001   1     CALL      pdv,8,is",gr3,T_33",gr4,js",gr5,T_34",gr6,ks",gr7,ks",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  223| 001F2C ori      60000000   1
  234| 001F30 ld       EBA20000   1     L8        gr29=.&&N&&mpipar(gr2,0)
  234| 001F34 lwz      801D0018   1     L4Z       gr0=<s70:d24:l4>(gr29,24)
  234| 001F38 cmpdi    2C200000   1     C8        cr0=gr0,0
  234| 001F3C bc       4182001C   1     BT        CL.89,cr0,0x4/eq,taken=60%(60,40)
  235| 001F40 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  235| 001F44 addi     38DD0014   1     AI        gr6=gr29,20
  235| 001F48 addi     387D0018   1     AI        gr3=gr29,24
  235| 001F4C addi     38851770   1     AI        gr4=gr5,6000
  235| 001F50 bl       48000001   1     CALL      mpi_waitall,4,nreq",gr3,req[]",gr4,stat[]",gr5,ierr",gr6,mpi_waitall",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  235| 001F54 ori      60000000   1
  235|                              CL.89:
  244| 001F58 stw      93C10134   1     ST4Z      T_38(gr1,308)=gr30
  243| 001F5C lwz      807D001C   1     L4Z       gr3=<s70:d28:l4>(gr29,28)
  244| 001F60 ld       EBC20000   1     L8        gr30=.&&N&field(gr2,0)
  244| 001F64 addi     3B800000   1     LI        gr28=0
  244| 001F68 addi     3901013C   1     AI        gr8=gr1,316
  244| 001F6C stw      93810128   1     ST4Z      T_35(gr1,296)=gr28
  242| 001F70 stw      939D0018   1     ST4Z      <s70:d24:l4>(gr29,24)=gr28
  243| 001F74 addi     38030001   1     AI        gr0=gr3,1
  244| 001F78 stw      9381012C   1     ST4Z      T_36(gr1,300)=gr28
  244| 001F7C stw      93810130   1     ST4Z      T_37(gr1,304)=gr28
  244| 001F80 stw      93810138   1     ST4Z      T_39(gr1,312)=gr28
  244| 001F84 stw      9381013C   1     ST4Z      T_40(gr1,316)=gr28
  243| 001F88 stw      901D001C   1     ST4Z      <s70:d28:l4>(gr29,28)=gr0
  244| 001F8C ld       E93E0208   1     L8        gr9=<s24:d520:l8>(gr30,520)
  244| 001F90 addi     38E10138   1     AI        gr7=gr1,312
  244| 001F94 addi     38C10134   1     AI        gr6=gr1,308
  244| 001F98 addi     38A10130   1     AI        gr5=gr1,304
  244| 001F9C addi     3881012C   1     AI        gr4=gr1,300
  244| 001FA0 addi     38610128   1     AI        gr3=gr1,296
  244| 001FA4 bl       48000001   1     CALL      bvalv2,7,T_35",gr3,T_36",gr4,T_37",gr5,T_38",gr6,T_39",gr7,T_40",gr8,v2",gr9,bvalv2",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  244| 001FA8 ori      60000000   1
  246| 001FAC lwz      811F000C   1     L4Z       gr8=<s64:d12:l4>(gr31,12)
  246| 001FB0 addi     38C10140   1     AI        gr6=gr1,320
  246| 001FB4 addi     387F0004   1     AI        gr3=gr31,4
  246| 001FB8 addi     389F0004   1     AI        gr4=gr31,4
  246| 001FBC addi     38BF0008   1     AI        gr5=gr31,8
  246| 001FC0 addi     38FF0010   1     AI        gr7=gr31,16
  246| 001FC4 addi     3808FFFF   1     AI        gr0=gr8,-1
  246| 001FC8 addi     391F0010   1     AI        gr8=gr31,16
  246| 001FCC stw      90010140   1     ST4Z      T_41(gr1,320)=gr0
  246| 001FD0 ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  246| 001FD4 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  246| 001FD8 bl       48000001   1     CALL      pdv,8,ie",gr3,ie",gr4,js",gr5,T_41",gr6,ks",gr7,ks",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  246| 001FDC ori      60000000   1
  251| 001FE0 lwz      801D0018   1     L4Z       gr0=<s70:d24:l4>(gr29,24)
  251| 001FE4 cmpdi    2C200000   1     C8        cr0=gr0,0
  251| 001FE8 bc       4182001C   1     BT        CL.90,cr0,0x4/eq,taken=60%(60,40)
  252| 001FEC ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  252| 001FF0 addi     38DD0014   1     AI        gr6=gr29,20
  252| 001FF4 addi     387D0018   1     AI        gr3=gr29,24
  252| 001FF8 addi     38851770   1     AI        gr4=gr5,6000
  252| 001FFC bl       48000001   1     CALL      mpi_waitall,4,nreq",gr3,req[]",gr4,stat[]",gr5,ierr",gr6,mpi_waitall",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  252| 002000 ori      60000000   1
  252|                              CL.90:
  256| 002004 addi     38BF000C   1     AI        gr5=gr31,12
  256| 002008 ld       E8620000   1     L8        gr3=.&&N&&grid(gr2,0)
  256| 00200C addi     389F0004   1     AI        gr4=gr31,4
  256| 002010 or       7CA62B78   1     LR        gr6=gr5
  256| 002014 addi     38FF0010   1     AI        gr7=gr31,16
  256| 002018 addi     391F0010   1     AI        gr8=gr31,16
  256| 00201C ld       E9210170   1     L8        gr9=#SPILL0(gr1,368)
  256| 002020 ld       E9410178   1     L8        gr10=#SPILL1(gr1,376)
  256| 002024 bl       48000001   1     CALL      pdv,8,is",gr3,ie",gr4,je",gr5,je",gr6,ks",gr7,ks",gr8,dcopy[]",gr9,eod[]",gr10,pdv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  256| 002028 ori      60000000   1
  261| 00202C b        4BFFF9B8   1     B         CL.80,-1
     |               Tag Table
     | 002030        00000000 00012203 80120000 00002030
     |               Instruction count         2060
     |               Straight-line exec time   2061
     |               Constant Area
     | 000000        3EAAAAAB

 
 
>>>>> COMPILATION UNIT EPILOGUE SECTION <<<<<
 
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    pdv_d.f90                   07/08/15   15:48:29
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................     322
1501-510  Compilation successful for file pdv_d.f90.
1501-543  Object file created.
