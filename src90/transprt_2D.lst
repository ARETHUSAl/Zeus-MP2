IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- transprt_2D.f90 07/08/15 15:48:39
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 "transprt_2D.f90", line 109.17: 1513-029 (W) The number of arguments to "advx1" differs from the number of arguments in a previous reference. You should use the OPTIONAL attribute and an explicit interface to define a procedure with optional arguments.
 "transprt_2D.f90", line 110.17: 1513-029 (W) The number of arguments to "advx2" differs from the number of arguments in a previous reference. You should use the OPTIONAL attribute and an explicit interface to define a procedure with optional arguments.
 "transprt_2D.f90", line 157.17: 1513-029 (W) The number of arguments to "advx2" differs from the number of arguments in a previous reference. You should use the OPTIONAL attribute and an explicit interface to define a procedure with optional arguments.
 "transprt_2D.f90", line 158.17: 1513-029 (W) The number of arguments to "advx1" differs from the number of arguments in a previous reference. You should use the OPTIONAL attribute and an explicit interface to define a procedure with optional arguments.
** transprt_2d   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at transprt_2D.f90 <line 81> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at transprt_2D.f90 <line 82> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 3) at transprt_2D.f90 <line 83> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3db%addr  + d-w3db%rvo))->w3db[].rns3.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] = ((((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns5.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][($$CIV2 + (long long) js) - 1ll][(long long) is + $$CIV1] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1])) * ((double *)((char *)d-g2b%addr  + d-g2b%rvo))->g2b[].rns4.[(long long) is + $$CIV1]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at transprt_2D.f90 <line 83> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dc%addr  + d-w3dc%rvo))->w3dc[].rns6.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] = (((((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns9.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1])) * ((double *)((char *)d-g31b%addr  + d-g31b%rvo))->g31b[].rns8.[(long long) is + $$CIV1]) * ((double *)((char *)d-g32b%addr  + d-g32b%rvo))->g32b[].rns7.[(long long) js + $$CIV2]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at transprt_2D.f90 <line 83> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3da%addr  + d-w3da%rvo))->w3da[].rns0.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] = (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][($$CIV1 + (long long) is) - 1ll] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1]); with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at transprt_2D.f90 <line 85> was not SIMD vectorized because it contains memory references ((char *)d-w3db%addr  + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long long) ks + $$CIV3) + (d-w3db%bounds%mult[].off1352)*((long long) js + $$CIV2) + (d-w3db%bounds%mult[].off1376)*((long long) is + $$CIV1)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at transprt_2D.f90 <line 85> was not SIMD vectorized because it contains operation in ((((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns5.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][($$CIV2 + (long long) js) - 1ll][(long long) is + $$CIV1] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1])) * ((double *)((char *)d-g2b%addr  + d-g2b%rvo))->g2b[].rns4.[(long long) is + $$CIV1] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at transprt_2D.f90 <line 85> was not SIMD vectorized because it contains memory references ((char *)d-w3db%addr  + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long long) ks + $$CIV3) + (d-w3db%bounds%mult[].off1352)*((long long) js + $$CIV2) + (d-w3db%bounds%mult[].off1376)*((long long) is + $$CIV1)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at transprt_2D.f90 <line 85> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at transprt_2D.f90 <line 85> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3db%addr  + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long long) ks + $$CIV3) + (d-w3db%bounds%mult[].off1352)*((long long) js + $$CIV2) + (d-w3db%bounds%mult[].off1376)*((long long) is + $$CIV1)).
1586-536 (I) Loop (loop index 3) at transprt_2D.f90 <line 87> was not SIMD vectorized because it contains memory references ((char *)d-w3dc%addr  + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long long) ks + $$CIV3) + (d-w3dc%bounds%mult[].off1456)*((long long) js + $$CIV2) + (d-w3dc%bounds%mult[].off1480)*((long long) is + $$CIV1)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at transprt_2D.f90 <line 87> was not SIMD vectorized because it contains operation in (((((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns9.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1])) * ((double *)((char *)d-g31b%addr  + d-g31b%rvo))->g31b[].rns8.[(long long) is + $$CIV1]) * ((double *)((char *)d-g32b%addr  + d-g32b%rvo))->g32b[].rns7.[(long long) js + $$CIV2] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at transprt_2D.f90 <line 87> was not SIMD vectorized because it contains memory references ((char *)d-w3dc%addr  + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long long) ks + $$CIV3) + (d-w3dc%bounds%mult[].off1456)*((long long) js + $$CIV2) + (d-w3dc%bounds%mult[].off1480)*((long long) is + $$CIV1)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at transprt_2D.f90 <line 87> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at transprt_2D.f90 <line 87> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dc%addr  + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long long) ks + $$CIV3) + (d-w3dc%bounds%mult[].off1456)*((long long) js + $$CIV2) + (d-w3dc%bounds%mult[].off1480)*((long long) is + $$CIV1)).
1586-536 (I) Loop (loop index 3) at transprt_2D.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-w3da%addr  + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long long) ks + $$CIV3) + (d-w3da%bounds%mult[].off1248)*((long long) js + $$CIV2) + (d-w3da%bounds%mult[].off1272)*((long long) is + $$CIV1)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at transprt_2D.f90 <line 84> was not SIMD vectorized because it contains operation in (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][($$CIV1 + (long long) is) - 1ll] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long long) js + $$CIV2][(long long) is + $$CIV1]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at transprt_2D.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-w3da%addr  + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long long) ks + $$CIV3) + (d-w3da%bounds%mult[].off1248)*((long long) js + $$CIV2) + (d-w3da%bounds%mult[].off1272)*((long long) is + $$CIV1)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at transprt_2D.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at transprt_2D.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3da%addr  + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long long) ks + $$CIV3) + (d-w3da%bounds%mult[].off1248)*((long long) js + $$CIV2) + (d-w3da%bounds%mult[].off1272)*((long long) is + $$CIV1)).
1586-534 (I) Loop (loop index 4) at transprt_2D.f90 <line 117> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 5) at transprt_2D.f90 <line 118> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns30.[(long long) ks][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-w3dd%addr  + d-w3dd%rvo))->w3dd[].rns31.[(long long) ks][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 5) at transprt_2D.f90 <line 118> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dg%addr  + d-w3dg%rvo))->w3dg[].rns32.[(long long) ks][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-w3de%addr  + d-w3de%rvo))->w3de[].rns33.[(long long) ks][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 5) at transprt_2D.f90 <line 119> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV5) + (d-d%bounds%mult[].off96)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 5) at transprt_2D.f90 <line 119> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV5) + (d-d%bounds%mult[].off96)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 5) at transprt_2D.f90 <line 119> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 5) at transprt_2D.f90 <line 119> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV5) + (d-d%bounds%mult[].off96)*((long long) is + $$CIV4)).
1586-536 (I) Loop (loop index 5) at transprt_2D.f90 <line 120> was not SIMD vectorized because it contains memory references ((char *)d-w3dg%addr  + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + (d-w3dg%bounds%mult[].off1872)*((long long) js + $$CIV5) + (d-w3dg%bounds%mult[].off1896)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 5) at transprt_2D.f90 <line 120> was not SIMD vectorized because it contains memory references ((char *)d-w3dg%addr  + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + (d-w3dg%bounds%mult[].off1872)*((long long) js + $$CIV5) + (d-w3dg%bounds%mult[].off1896)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 5) at transprt_2D.f90 <line 120> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 5) at transprt_2D.f90 <line 120> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dg%addr  + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + (d-w3dg%bounds%mult[].off1872)*((long long) js + $$CIV5) + (d-w3dg%bounds%mult[].off1896)*((long long) is + $$CIV4)).
1586-534 (I) Loop (loop index 6) at transprt_2D.f90 <line 124> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 7) at transprt_2D.f90 <line 125> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 126> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns34.[2ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns35.[2ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 126> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns34.[3ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns35.[3ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 126> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns34.[4ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns35.[4ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 126> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns34.[1ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns35.[1ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)).
1586-536 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)).
1586-536 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)).
1586-536 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 8) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV17 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)).
1586-534 (I) Loop (loop index 9) at transprt_2D.f90 <line 133> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 10) at transprt_2D.f90 <line 134> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns36.[(long long) ks][(long long) js + $$CIVA][(long long) is + $$CIV9] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns37.[(long long) ks][(long long) js + $$CIVA][(long long) is + $$CIV9]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 10) at transprt_2D.f90 <line 135> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIVA) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV9)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 10) at transprt_2D.f90 <line 135> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIVA) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 10) at transprt_2D.f90 <line 135> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 10) at transprt_2D.f90 <line 135> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIVA) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV9)).
1586-534 (I) Loop (loop index 11) at transprt_2D.f90 <line 165> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 12) at transprt_2D.f90 <line 166> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns58.[(long long) ks][(long long) js + $$CIVC][(long long) is + $$CIVB] = ((double *)((char *)d-w3dd%addr  + d-w3dd%rvo))->w3dd[].rns59.[(long long) ks][(long long) js + $$CIVC][(long long) is + $$CIVB]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at transprt_2D.f90 <line 166> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dg%addr  + d-w3dg%rvo))->w3dg[].rns60.[(long long) ks][(long long) js + $$CIVC][(long long) is + $$CIVB] = ((double *)((char *)d-w3de%addr  + d-w3de%rvo))->w3de[].rns61.[(long long) ks][(long long) js + $$CIVC][(long long) is + $$CIVB]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 12) at transprt_2D.f90 <line 167> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVC) + (d-d%bounds%mult[].off96)*((long long) is + $$CIVB)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at transprt_2D.f90 <line 167> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVC) + (d-d%bounds%mult[].off96)*((long long) is + $$CIVB)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at transprt_2D.f90 <line 167> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at transprt_2D.f90 <line 167> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) + (d-d%bounds%mult[].off72)*((long long) js + $$CIVC) + (d-d%bounds%mult[].off96)*((long long) is + $$CIVB)).
1586-536 (I) Loop (loop index 12) at transprt_2D.f90 <line 168> was not SIMD vectorized because it contains memory references ((char *)d-w3dg%addr  + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + (d-w3dg%bounds%mult[].off1872)*((long long) js + $$CIVC) + (d-w3dg%bounds%mult[].off1896)*((long long) is + $$CIVB)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at transprt_2D.f90 <line 168> was not SIMD vectorized because it contains memory references ((char *)d-w3dg%addr  + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + (d-w3dg%bounds%mult[].off1872)*((long long) js + $$CIVC) + (d-w3dg%bounds%mult[].off1896)*((long long) is + $$CIVB)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at transprt_2D.f90 <line 168> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at transprt_2D.f90 <line 168> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dg%addr  + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + (d-w3dg%bounds%mult[].off1872)*((long long) js + $$CIVC) + (d-w3dg%bounds%mult[].off1896)*((long long) is + $$CIVB)).
1586-534 (I) Loop (loop index 13) at transprt_2D.f90 <line 172> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 14) at transprt_2D.f90 <line 173> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns62.[2ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns63.[2ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns62.[3ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns63.[3ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns62.[1ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns63.[1ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns62.[4ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns63.[4ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)).
1586-536 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)).
1586-536 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)).
1586-536 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 15) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV18 * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)).
1586-534 (I) Loop (loop index 16) at transprt_2D.f90 <line 181> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 17) at transprt_2D.f90 <line 182> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns64.[(long long) ks][(long long) js + $$CIV11][(long long) is + $$CIV10] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns65.[(long long) ks][(long long) js + $$CIV11][(long long) is + $$CIV10]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 17) at transprt_2D.f90 <line 183> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV11) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV10)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 17) at transprt_2D.f90 <line 183> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV11) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV10)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 17) at transprt_2D.f90 <line 183> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 17) at transprt_2D.f90 <line 183> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV11) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV10)).
1586-534 (I) Loop (loop index 18) at transprt_2D.f90 <line 196> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 19) at transprt_2D.f90 <line 212> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 20) at transprt_2D.f90 <line 213> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 214> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns74.[3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV14][(long long) is + $$CIV13] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns75.[3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV14][(long long) is + $$CIV13]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 214> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns74.[($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIV14][(long long) is + $$CIV13] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns75.[($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIV14][(long long) is + $$CIV13]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 214> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns74.[2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV14][(long long) is + $$CIV13] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns75.[2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV14][(long long) is + $$CIV13]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 214> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns74.[1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV14][(long long) is + $$CIV13] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns75.[1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV14][(long long) is + $$CIV13]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)).
1586-536 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)).
1586-536 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)).
1586-536 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 21) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIV1A * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)).
1586-550 (I) Loop (loop index 22) at transprt_2D.f90 <line 220> was not SIMD vectorized because it is not profitable to vectorize.
1586-551 (I) Loop (loop index 22) at transprt_2D.f90 <line 221> was not SIMD vectorized because it contains unsupported vector data types.
1586-550 (I) Loop (loop index 23) at transprt_2D.f90 <line 196> was not SIMD vectorized because it is not profitable to vectorize.
1586-551 (I) Loop (loop index 23) at transprt_2D.f90 <line 197> was not SIMD vectorized because it contains unsupported vector data types.
1586-551 (I) Loop (loop index 23) at transprt_2D.f90 <line 197> was not SIMD vectorized because it contains unsupported vector data types.
1586-551 (I) Loop (loop index 23) at transprt_2D.f90 <line 197> was not SIMD vectorized because it contains unsupported vector data types.
1586-534 (I) Loop (loop index 27) at transprt_2D.f90 <line 212> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 28) at transprt_2D.f90 <line 213> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 29) at transprt_2D.f90 <line 214> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns74.[(long long) ks + $$CIV15][(long long) js + $$CIV14][(long long) is + $$CIV13] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns75.[(long long) ks + $$CIV15][(long long) js + $$CIV14][(long long) is + $$CIV13]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 29) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks + $$CIV15) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 29) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks + $$CIV15) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 29) at transprt_2D.f90 <line 215> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 29) at transprt_2D.f90 <line 215> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks + $$CIV15) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV13)).
1586-534 (I) Loop (loop index 37) at transprt_2D.f90 <line 172> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 38) at transprt_2D.f90 <line 173> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 39) at transprt_2D.f90 <line 174> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns62.[$$CIVF + 1ll][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns63.[$$CIVF + 1ll][(long long) ks][(long long) js + $$CIVE][(long long) is + $$CIVD]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 39) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVF + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 39) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVF + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 39) at transprt_2D.f90 <line 175> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 39) at transprt_2D.f90 <line 175> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVF + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIVD)).
1586-534 (I) Loop (loop index 43) at transprt_2D.f90 <line 124> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 44) at transprt_2D.f90 <line 125> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 45) at transprt_2D.f90 <line 126> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns34.[$$CIV8 + 1ll][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns35.[$$CIV8 + 1ll][(long long) ks][(long long) js + $$CIV7][(long long) is + $$CIV6]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 45) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIV8 + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 45) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIV8 + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 45) at transprt_2D.f90 <line 127> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 45) at transprt_2D.f90 <line 127> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIV8 + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV6)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"13">. Total number of the innermost loops SIMD vectorized <"0">.


    11|         SUBROUTINE transprt_2d ()
    65|           IF (.NOT.0 <> ((xhydro  .XOR.  1)  .AND.  1)) THEN
    67|             IF ((0 <> (xmhd  .AND.  1))) THEN
    71|               CALL ct_2d()
    72|             ENDIF
    81|             IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                      $$CIV3 = 0
       Id=1           DO $$CIV3 = $$CIV3, int((1 + (int(ke) - int(ks))))-1
    82|                 IF ((1 + (int(je) - int(js)) > 0)) THEN
                          $$CIV2 = 0
       Id=2               DO $$CIV2 = $$CIV2, int((1 + (int(je) - int(js))))&
                &             -1
    83|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV1 = 0
                              $$PRC0 = d-d%addr%d(int(is) - 1,$$CIV2 + int(js),&
                &               $$CIV3 + int(ks))
       Id=3                   DO $$CIV1 = $$CIV1, int((1 + (int(ie) - int(is))&
                &                 ))-1
                                $$PRC1 = d-d%addr%d(int(is) + $$CIV1,$$CIV2 + &
                &                 int(js),$$CIV3 + int(ks))
    84|                         d-w3da%addr%w3da(int(is) + $$CIV1,int(js) + &
                &                 $$CIV2,int(ks) + $$CIV3) = (d-v1%addr%v1(int(is)&
                &                  + $$CIV1,int(js) + $$CIV2,int(ks) + $$CIV3) *  &
                &                 5.0000000000000000E-001) * ($$PRC0 + $$PRC1)
    85|                         d-w3db%addr%w3db(int(is) + $$CIV1,int(js) + &
                &                 $$CIV2,int(ks) + $$CIV3) = ((d-v2%addr%v2(int(&
                &                 is) + $$CIV1,int(js) + $$CIV2,int(ks) + $$CIV3) &
                &                 *  5.0000000000000000E-001) * (d-d%addr%d(int(&
                &                 is) + $$CIV1,($$CIV2 + int(js)) - 1,int(ks) + &
                &                 $$CIV3) + $$PRC1)) * d-g2b%addr%g2b(int(is) + &
                &                 $$CIV1)
    87|                         d-w3dc%addr%w3dc(int(is) + $$CIV1,int(js) + &
                &                 $$CIV2,int(ks) + $$CIV3) = (((d-v3%addr%v3(int(&
                &                 is) + $$CIV1,int(js) + $$CIV2,int(ks) + $$CIV3) &
                &                 *  5.0000000000000000E-001) * ($$PRC1 + $$PRC1))&
                &                  * d-g31b%addr%g31b(int(is) + $$CIV1)) * &
                &                 d-g32b%addr%g32b(int(js) + $$CIV2)
                                $$PRC0 = $$PRC1
    89|                       ENDDO
                            ENDIF
    90|                   ENDDO
                        ENDIF
    91|               ENDDO
                    ENDIF
    95|             nseq = 0
    96|             IF ((ix1x2x3 == 1)) THEN
   103|               IF ((lrad > 0)) THEN
   104|                 CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr,&
                &         d-er%addr,d-w3dh%addr,d-abun%addr,d-w4da%addr)
   106|                 CALL advx2(d-d%addr,d-w3dd%addr,d-w3dg%addr,d-w3de%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr,&
                &         d-w3dh%addr,d-er%addr,d-w4da%addr,d-abun%addr)
   108|               ELSE
   109|                 CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr)
   110|                 CALL advx2(d-d%addr,d-w3dd%addr,d-w3dg%addr,d-w3de%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr)
   111|               ENDIF
   117|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV5 = 0
       Id=4             DO $$CIV5 = $$CIV5, int((1 + (int(je) - int(js))))-1
   118|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV4 = 0
       Id=5                 DO $$CIV4 = $$CIV4, int((1 + (int(ie) - int(is))))&
                &               -1
   119|                       d-d%addr%d(int(is) + $$CIV4,int(js) + $$CIV5,int(&
                &               ks)) = d-w3dd%addr%w3dd(int(is) + $$CIV4,int(js) &
                &               + $$CIV5,int(ks))
   120|                       d-w3dg%addr%w3dg(int(is) + $$CIV4,int(js) + &
                &               $$CIV5,int(ks)) = d-w3de%addr%w3de(int(is) + &
                &               $$CIV4,int(js) + $$CIV5,int(ks))
   121|                     ENDDO
                          ENDIF
   122|                 ENDDO
                      ENDIF
   123|               IF ((nspec > 1)) THEN
   124|                 IF ((MOD(int(nspec), 4) > 0  .AND.  int(nspec) > 0)) &
                &         THEN
                          $$CIV8 = 0
       Id=43              DO $$CIV8 = $$CIV8, MOD(int(nspec), int(4))-1
   125|                     IF ((1 + (int(je) - int(js)) > 0)) THEN
                              $$CIV7 = 0
       Id=44                  DO $$CIV7 = $$CIV7, int((1 + (int(je) - int(js))&
                &                 ))-1
   126|                         IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                  $$CIV6 = 0
       Id=45                      DO $$CIV6 = $$CIV6, int((1 + (int(ie) - int(&
                &                     is))))-1
   127|                             d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                     $$CIV7,int(ks),$$CIV8 + 1) = &
                &                     d-abun%addr%abun(int(is) + $$CIV6,int(js) + &
                &                     $$CIV7,int(ks),$$CIV8 + 1)
   128|                           ENDDO
                                ENDIF
   129|                       ENDDO
                            ENDIF
   130|                   ENDDO
                        ENDIF
   124|                 IF (.NOT.(int(nspec) > 0  .AND.  int(nspec) > MOD(int(&
                &         nspec), 4))) GOTO lab_116
                        $$CIV17 = int(0)
       Id=6             DO $$CIV17 = $$CIV17, int((((int(nspec) - MOD(int(&
                &           nspec), 4)) - 1) / 4 + 1))-1
   125|                   IF ((1 + (int(je) - int(js)) > 0)) THEN
                            $$CIV7 = 0
       Id=7                 DO $$CIV7 = $$CIV7, int((1 + (int(je) - int(js))))&
                &               -1
   126|                       IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                $$CIV6 = 0
       Id=8                     DO $$CIV6 = $$CIV6, int((1 + (int(ie) - int(&
                &                   is))))-1
   127|                           d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),1 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),1 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
                                  d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),2 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),2 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
                                  d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),3 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),3 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
                                  d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),4 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),4 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
   128|                         ENDDO
                              ENDIF
   129|                     ENDDO
                          ENDIF
   130|                 ENDDO
                        lab_116
   131|                 lab_27
   132|                 IF ((lrad <> 0)) THEN
   133|                   IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_122
                          $$CIVA = 0
       Id=9               DO $$CIVA = $$CIVA, int((1 + (int(je) - int(js))))&
                &             -1
   134|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV9 = 0
       Id=10                  DO $$CIV9 = $$CIV9, int((1 + (int(ie) - int(is))&
                &                 ))-1
   135|                         d-w3dh%addr%w3dh(int(is) + $$CIV9,int(js) + &
                &                 $$CIVA,int(ks)) = d-er%addr%er(int(is) + $$CIV9,&
                &                 int(js) + $$CIVA,int(ks))
   136|                       ENDDO
                            ENDIF
   137|                   ENDDO
                          lab_122
   138|                   lab_40
   140|                   ix1x2x3 = 2
   141|                 ELSE
   145|                   lab_16
                          IF (.NOT.(ix1x2x3 == 2)) GOTO lab_51
   151|                   IF (.NOT.(lrad <> 0)) GOTO lab_52
   152|                   CALL advx2(d-w3dd%addr,d-d%addr,d-w3de%addr,&
                &           d-w3dg%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr,d-er%addr,d-w3dh%addr,d-abun%addr,&
                &           d-w4da%addr)
   154|                   CALL advx1(d-d%addr,d-w3dd%addr,d-w3dg%addr,&
                &           d-w3de%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr,d-w3dh%addr,d-er%addr,d-w4da%addr,&
                &           d-abun%addr)
   156|                   GOTO lab_53
   157|                   CALL advx2(d-w3dd%addr,d-d%addr,d-w3de%addr,&
                &           d-w3dg%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr)
   158|                   CALL advx1(d-d%addr,d-w3dd%addr,d-w3dg%addr,&
                &           d-w3de%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr)
   159|                 ENDIF
   165|                 IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_126
                        $$CIVC = 0
       Id=11            DO $$CIVC = $$CIVC, int((1 + (int(je) - int(js))))-1
   166|                   IF (.NOT.(1 + (int(ie) - int(is)) > 0)) GOTO lab_128
                          $$CIVB = 0
       Id=12              DO $$CIVB = $$CIVB, int((1 + (int(ie) - int(is))))&
                &             -1
   167|                     d-d%addr%d(int(is) + $$CIVB,int(js) + $$CIVC,int(ks)&
                &             ) = d-w3dd%addr%w3dd(int(is) + $$CIVB,int(js) + &
                &             $$CIVC,int(ks))
   168|                     d-w3dg%addr%w3dg(int(is) + $$CIVB,int(js) + $$CIVC,&
                &             int(ks)) = d-w3de%addr%w3de(int(is) + $$CIVB,int(js)&
                &              + $$CIVC,int(ks))
   169|                   ENDDO
                        ENDIF
   170|               ENDDO
                    ENDIF
   171|             IF (.NOT.(nspec > 1)) GOTO lab_62
   172|             IF (.NOT.(MOD(int(nspec), 4) > 0  .AND.  int(nspec) > 0)) &
                &     GOTO lab_184
                    $$CIVF = 0
       Id=37        DO $$CIVF = $$CIVF, MOD(int(nspec), int(4))-1
   173|               IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_183
                      $$CIVE = 0
       Id=38          DO $$CIVE = $$CIVE, int((1 + (int(je) - int(js))))-1
   174|                 IF (.NOT.(1 + (int(ie) - int(is)) > 0)) GOTO lab_182
                        $$CIVD = 0
       Id=39            DO $$CIVD = $$CIVD, int((1 + (int(ie) - int(is))))-1
   175|                   d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,&
                &           int(ks),$$CIVF + 1) = d-abun%addr%abun(int(is) + &
                &           $$CIVD,int(js) + $$CIVE,int(ks),$$CIVF + 1)
   176|                 ENDDO
                      ENDIF
   177|             ENDDO
                  ENDIF
   178|         ENDDO
              ENDIF
   172|         IF (.NOT.(int(nspec) > 0  .AND.  int(nspec) > MOD(int(nspec), 4)&
                & )) GOTO lab_130
                $$CIV18 = int(0)
       Id=13    DO $$CIV18 = $$CIV18, int((((int(nspec) - MOD(int(nspec), 4)) &
                &   - 1) / 4 + 1))-1
   173|           IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_132
                  $$CIVE = 0
       Id=14      DO $$CIVE = $$CIVE, int((1 + (int(je) - int(js))))-1
   174|             IF (.NOT.(1 + (int(ie) - int(is)) > 0)) GOTO lab_134
                    $$CIVD = 0
       Id=15        DO $$CIVD = $$CIVD, int((1 + (int(ie) - int(is))))-1
   175|               d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,1 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,1 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
                      d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,2 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,2 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
                      d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,3 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,3 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
                      d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,4 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,4 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
   176|             ENDDO
                  ENDIF
   177|         ENDDO
              ENDIF
   178|       ENDDO
                lab_130
   179|         lab_62
   180|         IF (.NOT.(lrad <> 0)) GOTO lab_75
   181|         IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_136
                $$CIV11 = 0
       Id=16    DO $$CIV11 = $$CIV11, int((1 + (int(je) - int(js))))-1
   182|           IF ((1 + (int(ie) - int(is)) > 0)) THEN
                    $$CIV10 = 0
       Id=17        DO $$CIV10 = $$CIV10, int((1 + (int(ie) - int(is))))-1
   183|               d-w3dh%addr%w3dh(int(is) + $$CIV10,int(js) + $$CIV11,int(&
                &       ks)) = d-er%addr%er(int(is) + $$CIV10,int(js) + $$CIV11,&
                &       int(ks))
   184|             ENDDO
                  ENDIF
   185|         ENDDO
                lab_136
   186|         lab_75
   188|         ix1x2x3 = 1
   189|         lab_51
   191|         lab_49
   196|         IF (.FALSE.) GOTO lab_140
                $$CIV19 = int(0)
       Id=18    DO $$CIV19 = $$CIV19, 0
   196Id=18           IF (.FALSE.) GOTO lab_151
                  $$LoopIV1 = 0
       Id=23      DO $$LoopIV1 = $$LoopIV1, 5
   197|             bvstat($$LoopIV1 + 1,int(int(($$CIV19 * 3))) + 3) = 0
                    bvstat($$LoopIV1 + 1,int(int(($$CIV19 * 3 + 1))) + 3) = 0
                    bvstat($$LoopIV1 + 1,int(int(($$CIV19 * 3 + 2))) + 3) = 0
   197|           ENDDO
                  lab_151
   200|         ENDDO
                lab_140
   196|       ELSE
   206|         lab_2
   207|         IF ((lrad <> 0)) THEN
   212|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV15 = 0
       Id=27        DO $$CIV15 = $$CIV15, MOD((1 + (int(ke) - int(ks))), int(&
                &       4))-1
   213|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV14 = 0
       Id=28            DO $$CIV14 = $$CIV14, int((1 + (int(je) - int(js))))&
                &           -1
   214|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV13 = 0
       Id=29                DO $$CIV13 = $$CIV13, int((1 + (int(ie) - int(is))&
                &               ))-1
   215|                       d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,int(ks) + $$CIV15) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,int(ks) + $$CIV15)
   216|                     ENDDO
                          ENDIF
   217|                 ENDDO
                      ENDIF
   218|             ENDDO
                  ENDIF
   212|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV1A = int(0)
       Id=19        DO $$CIV1A = $$CIV1A, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   213|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV14 = 0
       Id=20            DO $$CIV14 = $$CIV14, int((1 + (int(je) - int(js))))&
                &           -1
   214|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV13 = 0
       Id=21                DO $$CIV13 = $$CIV13, int((1 + (int(ie) - int(is))&
                &               ))-1
   215|                       d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks)&
                &               )), 4)) + int(ks)) = d-er%addr%er(int(is) + &
                &               $$CIV13,int(js) + $$CIV14,($$CIV1A * 4 + MOD((1 + &
                &               (int(ke) - int(ks))), 4)) + int(ks))
                              d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,1 + (($$CIV1A * 4 + MOD((1 + (int(ke) - &
                &               int(ks))), 4)) + int(ks))) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,1 + (($$CIV1A * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,2 + (($$CIV1A * 4 + MOD((1 + (int(ke) - &
                &               int(ks))), 4)) + int(ks))) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,2 + (($$CIV1A * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,3 + (($$CIV1A * 4 + MOD((1 + (int(ke) - &
                &               int(ks))), 4)) + int(ks))) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,3 + (($$CIV1A * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
   216|                     ENDDO
                          ENDIF
   217|                 ENDDO
                      ENDIF
   218|             ENDDO
                  ENDIF
   220|           IF (.FALSE.) GOTO lab_148
                  $$CIV16 = 0
       Id=22      DO $$CIV16 = $$CIV16, 5
   221|             bvstat($$CIV16 + 1,6) = 0
   222|           ENDDO
                  lab_148
   224|           lab_89
   227|           lab_150
                  RETURN
                END SUBROUTINE transprt_2d


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            81             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            82             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3da%addr  + d-w3da%rvo 
                                          + (d-w3da%bounds%mult[].off1224)*((long long) ks + 
                                          $$CIV3) + (d-w3da%bounds%mult[].off1248)*((long long) 
                                          js + $$CIV2) + (d-w3da%bounds%mult[].off1272)*((long 
                                          long) is + $$CIV1))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          operation in (((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns2.[(long long) ks + $$CIV3][(long 
                                          long) js + $$CIV2][(long long) is + $$CIV1] *  
                                          5.0000000000000000E-001) * (((double *)((char 
                                          *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + 
                                          $$CIV3][(long long) js + $$CIV2][($$CIV1 + (long 
                                          long) is) - 1ll] + ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long 
                                          long) js + $$CIV2][(long long) is + $$CIV1]) which is 
                                          not  suitable for SIMD vectorization.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3da%addr  + d-w3da%rvo 
                                          + (d-w3da%bounds%mult[].off1224)*((long long) ks + 
                                          $$CIV3) + (d-w3da%bounds%mult[].off1248)*((long long) 
                                          js + $$CIV2) + (d-w3da%bounds%mult[].off1272)*((long 
                                          long) is + $$CIV1)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3da%addr 
                                          + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long 
                                          long) ks + $$CIV3) + 
                                          (d-w3da%bounds%mult[].off1248)*((long long) js + 
                                          $$CIV2) + (d-w3da%bounds%mult[].off1272)*((long long) 
                                          is + $$CIV1)).
         0            85                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3db%addr  + d-w3db%rvo 
                                          + (d-w3db%bounds%mult[].off1328)*((long long) ks + 
                                          $$CIV3) + (d-w3db%bounds%mult[].off1352)*((long long) 
                                          js + $$CIV2) + (d-w3db%bounds%mult[].off1376)*((long 
                                          long) is + $$CIV1))  with non-vectorizable alignment.
         0            85                  Loop was not SIMD vectorized because it contains 
                                          operation in ((((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns5.[(long long) ks + $$CIV3][(long 
                                          long) js + $$CIV2][(long long) is + $$CIV1] *  
                                          5.0000000000000000E-001) * (((double *)((char 
                                          *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + 
                                          $$CIV3][($$CIV2 + (long long) js) - 1ll][(long long) 
                                          is + $$CIV1] + ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long 
                                          long) js + $$CIV2][(long long) is + $$CIV1])) * 
                                          ((double *)((char *)d-g2b%addr  + 
                                          d-g2b%rvo))->g2b[].rns4.[(long long) is + $$CIV1] 
                                          which is not  suitable for SIMD vectorization.
         0            85                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3db%addr  + d-w3db%rvo 
                                          + (d-w3db%bounds%mult[].off1328)*((long long) ks + 
                                          $$CIV3) + (d-w3db%bounds%mult[].off1352)*((long long) 
                                          js + $$CIV2) + (d-w3db%bounds%mult[].off1376)*((long 
                                          long) is + $$CIV1)) with  non-vectorizable strides.
         0            85                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            85                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3db%addr 
                                          + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long 
                                          long) ks + $$CIV3) + 
                                          (d-w3db%bounds%mult[].off1352)*((long long) js + 
                                          $$CIV2) + (d-w3db%bounds%mult[].off1376)*((long long) 
                                          is + $$CIV1)).
         0            87                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dc%addr  + d-w3dc%rvo 
                                          + (d-w3dc%bounds%mult[].off1432)*((long long) ks + 
                                          $$CIV3) + (d-w3dc%bounds%mult[].off1456)*((long long) 
                                          js + $$CIV2) + (d-w3dc%bounds%mult[].off1480)*((long 
                                          long) is + $$CIV1))  with non-vectorizable alignment.
         0            87                  Loop was not SIMD vectorized because it contains 
                                          operation in (((((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns9.[(long long) ks + $$CIV3][(long 
                                          long) js + $$CIV2][(long long) is + $$CIV1] *  
                                          5.0000000000000000E-001) * (((double *)((char 
                                          *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + 
                                          $$CIV3][(long long) js + $$CIV2][(long long) is + 
                                          $$CIV1] + ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns1.[(long long) ks + $$CIV3][(long 
                                          long) js + $$CIV2][(long long) is + $$CIV1])) * 
                                          ((double *)((char *)d-g31b%addr  + 
                                          d-g31b%rvo))->g31b[].rns8.[(long long) is + $$CIV1]) 
                                          * ((double *)((char *)d-g32b%addr  + 
                                          d-g32b%rvo))->g32b[].rns7.[(long long) js + $$CIV2] 
                                          which is not  suitable for SIMD vectorization.
         0            87                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dc%addr  + d-w3dc%rvo 
                                          + (d-w3dc%bounds%mult[].off1432)*((long long) ks + 
                                          $$CIV3) + (d-w3dc%bounds%mult[].off1456)*((long long) 
                                          js + $$CIV2) + (d-w3dc%bounds%mult[].off1480)*((long 
                                          long) is + $$CIV1)) with  non-vectorizable strides.
         0            87                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            87                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dc%addr 
                                          + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long 
                                          long) ks + $$CIV3) + 
                                          (d-w3dc%bounds%mult[].off1456)*((long long) js + 
                                          $$CIV2) + (d-w3dc%bounds%mult[].off1480)*((long long) 
                                          is + $$CIV1)).
         0           117             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           119                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIV5) + 
                                          (d-d%bounds%mult[].off96)*((long long) is + $$CIV4))  
                                          with non-vectorizable alignment.
         0           119                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIV5) + 
                                          (d-d%bounds%mult[].off96)*((long long) is + $$CIV4)) 
                                          with  non-vectorizable strides.
         0           119                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           119                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) 
                                          + (d-d%bounds%mult[].off72)*((long long) js + $$CIV5) 
                                          + (d-d%bounds%mult[].off96)*((long long) is + 
                                          $$CIV4)).
         0           120                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dg%addr  + d-w3dg%rvo 
                                          + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + 
                                          (d-w3dg%bounds%mult[].off1872)*((long long) js + 
                                          $$CIV5) + (d-w3dg%bounds%mult[].off1896)*((long long) 
                                          is + $$CIV4))  with non-vectorizable alignment.
         0           120                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dg%addr  + d-w3dg%rvo 
                                          + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + 
                                          (d-w3dg%bounds%mult[].off1872)*((long long) js + 
                                          $$CIV5) + (d-w3dg%bounds%mult[].off1896)*((long long) 
                                          is + $$CIV4)) with  non-vectorizable strides.
         0           120                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           120                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dg%addr 
                                          + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long 
                                          long) ks) + (d-w3dg%bounds%mult[].off1872)*((long 
                                          long) js + $$CIV5) + 
                                          (d-w3dg%bounds%mult[].off1896)*((long long) is + 
                                          $$CIV4)).
         0           124            43    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           125            44    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*($$CIV8 + 1ll) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6))  with non-vectorizable alignment.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*($$CIV8 + 1ll) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)) with  non-vectorizable strides.
         0           127                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIV8 
                                          + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) 
                                          ks) + (d-w4da%bounds%mult[].off2312)*((long long) js 
                                          + $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV6)).
         0           124             6    Outer loop has been unrolled 4 time(s).
         0           124             6    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           125             7    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6))  with non-vectorizable alignment.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)) with  non-vectorizable strides.
         0           127                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + 
                                          ($$CIV17 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)).
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6))  with non-vectorizable alignment.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)) with  non-vectorizable strides.
         0           127                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + 
                                          ($$CIV17 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)).
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6))  with non-vectorizable alignment.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)) with  non-vectorizable strides.
         0           127                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + 
                                          ($$CIV17 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)).
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6))  with non-vectorizable alignment.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV17 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)) with  non-vectorizable strides.
         0           127                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           127                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + 
                                          ($$CIV17 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIV7) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIV6)).
         0           133             9    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           135                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIVA) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV9))  with non-vectorizable alignment.
         0           135                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIVA) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV9)) with  non-vectorizable strides.
         0           135                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           135                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long 
                                          long) ks) + (d-w3dh%bounds%mult[].off2184)*((long 
                                          long) js + $$CIVA) + 
                                          (d-w3dh%bounds%mult[].off2208)*((long long) is + 
                                          $$CIV9)).
         0           165            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           167                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIVC) + 
                                          (d-d%bounds%mult[].off96)*((long long) is + $$CIVB))  
                                          with non-vectorizable alignment.
         0           167                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIVC) + 
                                          (d-d%bounds%mult[].off96)*((long long) is + $$CIVB)) 
                                          with  non-vectorizable strides.
         0           167                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           167                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks) 
                                          + (d-d%bounds%mult[].off72)*((long long) js + $$CIVC) 
                                          + (d-d%bounds%mult[].off96)*((long long) is + 
                                          $$CIVB)).
         0           168                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dg%addr  + d-w3dg%rvo 
                                          + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + 
                                          (d-w3dg%bounds%mult[].off1872)*((long long) js + 
                                          $$CIVC) + (d-w3dg%bounds%mult[].off1896)*((long long) 
                                          is + $$CIVB))  with non-vectorizable alignment.
         0           168                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dg%addr  + d-w3dg%rvo 
                                          + (d-w3dg%bounds%mult[].off1848)*((long long) ks) + 
                                          (d-w3dg%bounds%mult[].off1872)*((long long) js + 
                                          $$CIVC) + (d-w3dg%bounds%mult[].off1896)*((long long) 
                                          is + $$CIVB)) with  non-vectorizable strides.
         0           168                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           168                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dg%addr 
                                          + d-w3dg%rvo + (d-w3dg%bounds%mult[].off1848)*((long 
                                          long) ks) + (d-w3dg%bounds%mult[].off1872)*((long 
                                          long) js + $$CIVC) + 
                                          (d-w3dg%bounds%mult[].off1896)*((long long) is + 
                                          $$CIVB)).
         0           172            37    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           173            38    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*($$CIVF + 1ll) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD))  with non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*($$CIVF + 1ll) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVF 
                                          + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) 
                                          ks) + (d-w4da%bounds%mult[].off2312)*((long long) js 
                                          + $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIVD)).
         0           172            13    Outer loop has been unrolled 4 time(s).
         0           172            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           173            14    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD))  with non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + 
                                          ($$CIV18 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)).
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD))  with non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + 
                                          ($$CIV18 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)).
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD))  with non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + 
                                          ($$CIV18 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)).
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD))  with non-vectorizable alignment.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIV18 * 
                                          4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)) with  non-vectorizable strides.
         0           175                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           175                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + 
                                          ($$CIV18 * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks) + 
                                          (d-w4da%bounds%mult[].off2312)*((long long) js + 
                                          $$CIVE) + (d-w4da%bounds%mult[].off2336)*((long long) 
                                          is + $$CIVD)).
         0           181            16    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           183                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV11) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV10))  with non-vectorizable alignment.
         0           183                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV11) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV10)) with  non-vectorizable strides.
         0           183                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           183                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long 
                                          long) ks) + (d-w3dh%bounds%mult[].off2184)*((long 
                                          long) js + $$CIV11) + 
                                          (d-w3dh%bounds%mult[].off2208)*((long long) is + 
                                          $$CIV10)).
         0           196            18    Loop interchanging applied to loop nest.
         0           196            18    Outer loop has been unrolled 3 time(s).
         0           196            18    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
                                    23    Loop has been rolled.
                                    23    Loop was not SIMD vectorized because it is not 
                                          profitable to vectorize.
         0           197                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.
         0           197                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.
         0           197                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.
         0           212            27    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           213            28    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks + 
                                          $$CIV15) + (d-w3dh%bounds%mult[].off2184)*((long 
                                          long) js + $$CIV14) + 
                                          (d-w3dh%bounds%mult[].off2208)*((long long) is + 
                                          $$CIV13))  with non-vectorizable alignment.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks + 
                                          $$CIV15) + (d-w3dh%bounds%mult[].off2184)*((long 
                                          long) js + $$CIV14) + 
                                          (d-w3dh%bounds%mult[].off2208)*((long long) is + 
                                          $$CIV13)) with  non-vectorizable strides.
         0           215                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long 
                                          long) ks + $$CIV15) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)).
         0           212            19    Outer loop has been unrolled 4 time(s).
         0           212            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           213            20    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(($$CIV1A * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13))  with non-vectorizable alignment.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(($$CIV1A * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)) with  non-vectorizable strides.
         0           215                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + 
                                          (d-w3dh%bounds%mult[].off2160)*(($$CIV1A * 4ll + (1ll 
                                          + ((long long) ke - (long long) ks)) % 4ll) + (long 
                                          long) ks) + (d-w3dh%bounds%mult[].off2184)*((long 
                                          long) js + $$CIV14) + 
                                          (d-w3dh%bounds%mult[].off2208)*((long long) is + 
                                          $$CIV13)).
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIV1A * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13))  with non-vectorizable alignment.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIV1A * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)) with  non-vectorizable strides.
         0           215                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + 
                                          (($$CIV1A * 4ll + (1ll + ((long long) ke - (long 
                                          long) ks)) % 4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)).
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIV1A * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13))  with non-vectorizable alignment.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIV1A * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)) with  non-vectorizable strides.
         0           215                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + 
                                          (($$CIV1A * 4ll + (1ll + ((long long) ke - (long 
                                          long) ks)) % 4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)).
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIV1A * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13))  with non-vectorizable alignment.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIV1A * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)) with  non-vectorizable strides.
         0           215                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           215                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + 
                                          (($$CIV1A * 4ll + (1ll + ((long long) ke - (long 
                                          long) ks)) % 4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV14) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV13)).
         0           220            22    Loop was not SIMD vectorized because it is not 
                                          profitable to vectorize.
         0           221                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.


    11|         SUBROUTINE transprt_2d ()
    65|           IF (.NOT.0 <> ((xhydro  .XOR.  1)  .AND.  1)) THEN
    67|             IF ((0 <> (xmhd  .AND.  1))) THEN
    71|               CALL ct_2d()
    72|             ENDIF
    81|             IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                      $$CIV3 = 0
       Id=1           DO $$CIV3 = $$CIV3, int((1 + (int(ke) - int(ks))))-1
    82|                 IF ((1 + (int(je) - int(js)) > 0)) THEN
                          $$CIV2 = 0
       Id=2               DO $$CIV2 = $$CIV2, int((1 + (int(je) - int(js))))&
                &             -1
    83|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV1 = 0
                              $$PRC0 = d-d%addr%d(int(is) - 1,$$CIV2 + int(js),&
                &               $$CIV3 + int(ks))
                              $$ICM0 = int(js) + $$CIV2
                              $$ICM1 = int(ks) + $$CIV3
    87|                       $$ICM2 = d-g32b%addr%g32b(int(js) + $$CIV2)
    83|Id=3                   DO $$CIV1 = $$CIV1, int((1 + (int(ie) - int(is))&
                &                 ))-1
                                $$PRC1 = d-d%addr%d(int(is) + $$CIV1,$$ICM0,&
                &                 $$ICM1)
    84|                         d-w3da%addr%w3da(int(is) + $$CIV1,$$ICM0,$$ICM1)&
                &                  = (d-v1%addr%v1(int(is) + $$CIV1,$$ICM0,$$ICM1)&
                &                  *  5.0000000000000000E-001) * ($$PRC0 + $$PRC1)&
                &                 
    85|                         d-w3db%addr%w3db(int(is) + $$CIV1,$$ICM0,$$ICM1)&
                &                  = ((d-v2%addr%v2(int(is) + $$CIV1,$$ICM0,&
                &                 $$ICM1) *  5.0000000000000000E-001) * (&
                &                 d-d%addr%d(int(is) + $$CIV1,($$CIV2 + int(js)) &
                &                 - 1,$$ICM1) + $$PRC1)) * d-g2b%addr%g2b(int(is) &
                &                 + $$CIV1)
    87|                         d-w3dc%addr%w3dc(int(is) + $$CIV1,$$ICM0,$$ICM1)&
                &                  = (((d-v3%addr%v3(int(is) + $$CIV1,$$ICM0,&
                &                 $$ICM1) *  5.0000000000000000E-001) * ($$PRC1 + &
                &                 $$PRC1)) * d-g31b%addr%g31b(int(is) + $$CIV1)) &
                &                 * $$ICM2
                                $$PRC0 = $$PRC1
    89|                       ENDDO
                            ENDIF
    90|                   ENDDO
                        ENDIF
    91|               ENDDO
                    ENDIF
    95|             nseq = 0
    96|             IF ((ix1x2x3 == 1)) THEN
   103|               IF ((lrad > 0)) THEN
   104|                 CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr,&
                &         d-er%addr,d-w3dh%addr,d-abun%addr,d-w4da%addr)
   106|                 CALL advx2(d-d%addr,d-w3dd%addr,d-w3dg%addr,d-w3de%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr,&
                &         d-w3dh%addr,d-er%addr,d-w4da%addr,d-abun%addr)
   108|               ELSE
   109|                 CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr)
   110|                 CALL advx2(d-d%addr,d-w3dd%addr,d-w3dg%addr,d-w3de%addr,&
                &         d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr)
   111|               ENDIF
   117|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV5 = 0
       Id=4             DO $$CIV5 = $$CIV5, int((1 + (int(je) - int(js))))-1
   118|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV4 = 0
       Id=5                 DO $$CIV4 = $$CIV4, int((1 + (int(ie) - int(is))))&
                &               -1
   119|                       d-d%addr%d(int(is) + $$CIV4,int(js) + $$CIV5,int(&
                &               ks)) = d-w3dd%addr%w3dd(int(is) + $$CIV4,int(js) &
                &               + $$CIV5,int(ks))
   120|                       d-w3dg%addr%w3dg(int(is) + $$CIV4,int(js) + &
                &               $$CIV5,int(ks)) = d-w3de%addr%w3de(int(is) + &
                &               $$CIV4,int(js) + $$CIV5,int(ks))
   121|                     ENDDO
                          ENDIF
   122|                 ENDDO
                      ENDIF
   123|               IF ((nspec > 1)) THEN
   124|                 IF ((MOD(int(nspec), 4) > 0  .AND.  int(nspec) > 0)) &
                &         THEN
                          $$CIV8 = 0
       Id=43              DO $$CIV8 = $$CIV8, MOD(int(nspec), int(4))-1
   125|                     IF ((1 + (int(je) - int(js)) > 0)) THEN
                              $$CIV7 = 0
       Id=44                  DO $$CIV7 = $$CIV7, int((1 + (int(je) - int(js))&
                &                 ))-1
   126|                         IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                  $$CIV6 = 0
       Id=45                      DO $$CIV6 = $$CIV6, int((1 + (int(ie) - int(&
                &                     is))))-1
   127|                             d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                     $$CIV7,int(ks),$$CIV8 + 1) = &
                &                     d-abun%addr%abun(int(is) + $$CIV6,int(js) + &
                &                     $$CIV7,int(ks),$$CIV8 + 1)
   128|                           ENDDO
                                ENDIF
   129|                       ENDDO
                            ENDIF
   130|                   ENDDO
                        ENDIF
   124|                 IF (.NOT.(int(nspec) > 0  .AND.  int(nspec) > MOD(int(&
                &         nspec), 4))) GOTO lab_116
                        $$CIV17 = int(0)
       Id=6             DO $$CIV17 = $$CIV17, int((((int(nspec) - MOD(int(&
                &           nspec), 4)) - 1) / 4 + 1))-1
   125|                   IF ((1 + (int(je) - int(js)) > 0)) THEN
                            $$CIV7 = 0
       Id=7                 DO $$CIV7 = $$CIV7, int((1 + (int(je) - int(js))))&
                &               -1
   126|                       IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                $$CIV6 = 0
       Id=8                     DO $$CIV6 = $$CIV6, int((1 + (int(ie) - int(&
                &                   is))))-1
   127|                           d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),1 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),1 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
                                  d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),2 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),2 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
                                  d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),3 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),3 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
                                  d-w4da%addr%w4da(int(is) + $$CIV6,int(js) + &
                &                   $$CIV7,int(ks),4 + ($$CIV17 * 4 + MOD(int(&
                &                   nspec), 4))) = d-abun%addr%abun(int(is) + &
                &                   $$CIV6,int(js) + $$CIV7,int(ks),4 + ($$CIV17 &
                &                   * 4 + MOD(int(nspec), 4)))
   128|                         ENDDO
                              ENDIF
   129|                     ENDDO
                          ENDIF
   130|                 ENDDO
                        lab_116
   131|                 lab_27
   132|                 IF ((lrad <> 0)) THEN
   133|                   IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_122
                          $$CIVA = 0
       Id=9               DO $$CIVA = $$CIVA, int((1 + (int(je) - int(js))))&
                &             -1
   134|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV9 = 0
       Id=10                  DO $$CIV9 = $$CIV9, int((1 + (int(ie) - int(is))&
                &                 ))-1
   135|                         d-w3dh%addr%w3dh(int(is) + $$CIV9,int(js) + &
                &                 $$CIVA,int(ks)) = d-er%addr%er(int(is) + $$CIV9,&
                &                 int(js) + $$CIVA,int(ks))
   136|                       ENDDO
                            ENDIF
   137|                   ENDDO
                          lab_122
   138|                   lab_40
   140|                   ix1x2x3 = 2
   141|                 ELSE
   145|                   lab_16
                          IF (.NOT.(ix1x2x3 == 2)) GOTO lab_51
   151|                   IF (.NOT.(lrad <> 0)) GOTO lab_52
   152|                   CALL advx2(d-w3dd%addr,d-d%addr,d-w3de%addr,&
                &           d-w3dg%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr,d-er%addr,d-w3dh%addr,d-abun%addr,&
                &           d-w4da%addr)
   154|                   CALL advx1(d-d%addr,d-w3dd%addr,d-w3dg%addr,&
                &           d-w3de%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr,d-w3dh%addr,d-er%addr,d-w4da%addr,&
                &           d-abun%addr)
   156|                   GOTO lab_53
   157|                   CALL advx2(d-w3dd%addr,d-d%addr,d-w3de%addr,&
                &           d-w3dg%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr)
   158|                   CALL advx1(d-d%addr,d-w3dd%addr,d-w3dg%addr,&
                &           d-w3de%addr,d-w3df%addr,d-w3da%addr,d-w3db%addr,&
                &           d-w3dc%addr)
   159|                 ENDIF
   165|                 IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_126
                        $$CIVC = 0
       Id=11            DO $$CIVC = $$CIVC, int((1 + (int(je) - int(js))))-1
   166|                   IF (.NOT.(1 + (int(ie) - int(is)) > 0)) GOTO lab_128
                          $$CIVB = 0
       Id=12              DO $$CIVB = $$CIVB, int((1 + (int(ie) - int(is))))&
                &             -1
   167|                     d-d%addr%d(int(is) + $$CIVB,int(js) + $$CIVC,int(ks)&
                &             ) = d-w3dd%addr%w3dd(int(is) + $$CIVB,int(js) + &
                &             $$CIVC,int(ks))
   168|                     d-w3dg%addr%w3dg(int(is) + $$CIVB,int(js) + $$CIVC,&
                &             int(ks)) = d-w3de%addr%w3de(int(is) + $$CIVB,int(js)&
                &              + $$CIVC,int(ks))
   169|                   ENDDO
                        ENDIF
   170|               ENDDO
                    ENDIF
   171|             IF (.NOT.(nspec > 1)) GOTO lab_62
   172|             IF (.NOT.(MOD(int(nspec), 4) > 0  .AND.  int(nspec) > 0)) &
                &     GOTO lab_184
                    $$CIVF = 0
       Id=37        DO $$CIVF = $$CIVF, MOD(int(nspec), int(4))-1
   173|               IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_183
                      $$CIVE = 0
       Id=38          DO $$CIVE = $$CIVE, int((1 + (int(je) - int(js))))-1
   174|                 IF (.NOT.(1 + (int(ie) - int(is)) > 0)) GOTO lab_182
                        $$CIVD = 0
       Id=39            DO $$CIVD = $$CIVD, int((1 + (int(ie) - int(is))))-1
   175|                   d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,&
                &           int(ks),$$CIVF + 1) = d-abun%addr%abun(int(is) + &
                &           $$CIVD,int(js) + $$CIVE,int(ks),$$CIVF + 1)
   176|                 ENDDO
                      ENDIF
   177|             ENDDO
                  ENDIF
   178|         ENDDO
              ENDIF
   172|         IF (.NOT.(int(nspec) > 0  .AND.  int(nspec) > MOD(int(nspec), 4)&
                & )) GOTO lab_130
                $$CIV18 = int(0)
       Id=13    DO $$CIV18 = $$CIV18, int((((int(nspec) - MOD(int(nspec), 4)) &
                &   - 1) / 4 + 1))-1
   173|           IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_132
                  $$CIVE = 0
       Id=14      DO $$CIVE = $$CIVE, int((1 + (int(je) - int(js))))-1
   174|             IF (.NOT.(1 + (int(ie) - int(is)) > 0)) GOTO lab_134
                    $$CIVD = 0
       Id=15        DO $$CIVD = $$CIVD, int((1 + (int(ie) - int(is))))-1
   175|               d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,1 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,1 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
                      d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,2 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,2 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
                      d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,3 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,3 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
                      d-w4da%addr%w4da(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,4 + ($$CIV18 * 4 + MOD(int(nspec), 4))) = &
                &       d-abun%addr%abun(int(is) + $$CIVD,int(js) + $$CIVE,int(ks)&
                &       ,4 + ($$CIV18 * 4 + MOD(int(nspec), 4)))
   176|             ENDDO
                  ENDIF
   177|         ENDDO
              ENDIF
   178|       ENDDO
                lab_130
   179|         lab_62
   180|         IF (.NOT.(lrad <> 0)) GOTO lab_75
   181|         IF (.NOT.(1 + (int(je) - int(js)) > 0)) GOTO lab_136
                $$CIV11 = 0
       Id=16    DO $$CIV11 = $$CIV11, int((1 + (int(je) - int(js))))-1
   182|           IF ((1 + (int(ie) - int(is)) > 0)) THEN
                    $$CIV10 = 0
       Id=17        DO $$CIV10 = $$CIV10, int((1 + (int(ie) - int(is))))-1
   183|               d-w3dh%addr%w3dh(int(is) + $$CIV10,int(js) + $$CIV11,int(&
                &       ks)) = d-er%addr%er(int(is) + $$CIV10,int(js) + $$CIV11,&
                &       int(ks))
   184|             ENDDO
                  ENDIF
   185|         ENDDO
                lab_136
   186|         lab_75
   188|         ix1x2x3 = 1
   189|         lab_51
   191|         lab_49
   196|         IF (.FALSE.) GOTO lab_140
                $$CIV19 = int(0)
       Id=18    DO $$CIV19 = $$CIV19, 0
   196Id=18           IF (.FALSE.) GOTO lab_151
                  $$LoopIV1 = 0
       Id=23      DO $$LoopIV1 = $$LoopIV1, 5
   197|             bvstat($$LoopIV1 + 1,int(int(($$CIV19 * 3))) + 3) = 0
                    bvstat($$LoopIV1 + 1,int(int(($$CIV19 * 3 + 1))) + 3) = 0
                    bvstat($$LoopIV1 + 1,int(int(($$CIV19 * 3 + 2))) + 3) = 0
   197|           ENDDO
                  lab_151
   200|         ENDDO
                lab_140
   196|       ELSE
   206|         lab_2
   207|         IF ((lrad <> 0)) THEN
   212|           IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (int(&
                &   ke) - int(ks)) > 0)) THEN
                    $$CIV15 = 0
       Id=27        DO $$CIV15 = $$CIV15, MOD((1 + (int(ke) - int(ks))), int(&
                &       4))-1
   213|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV14 = 0
       Id=28            DO $$CIV14 = $$CIV14, int((1 + (int(je) - int(js))))&
                &           -1
   214|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV13 = 0
       Id=29                DO $$CIV13 = $$CIV13, int((1 + (int(ie) - int(is))&
                &               ))-1
   215|                       d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,int(ks) + $$CIV15) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,int(ks) + $$CIV15)
   216|                     ENDDO
                          ENDIF
   217|                 ENDDO
                      ENDIF
   218|             ENDDO
                  ENDIF
   212|           IF ((1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke) - int(&
                &   ks)) > MOD((1 + (int(ke) - int(ks))), 4))) THEN
                    $$CIV1A = int(0)
       Id=19        DO $$CIV1A = $$CIV1A, int(((int(ke) - (MOD((1 + (int(ke) &
                &       - int(ks))), 4) + int(ks))) / 4 + 1))-1
   213|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV14 = 0
       Id=20            DO $$CIV14 = $$CIV14, int((1 + (int(je) - int(js))))&
                &           -1
   214|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV13 = 0
       Id=21                DO $$CIV13 = $$CIV13, int((1 + (int(ie) - int(is))&
                &               ))-1
   215|                       d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,($$CIV1A * 4 + MOD((1 + (int(ke) - int(ks)&
                &               )), 4)) + int(ks)) = d-er%addr%er(int(is) + &
                &               $$CIV13,int(js) + $$CIV14,($$CIV1A * 4 + MOD((1 + &
                &               (int(ke) - int(ks))), 4)) + int(ks))
                              d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,1 + (($$CIV1A * 4 + MOD((1 + (int(ke) - &
                &               int(ks))), 4)) + int(ks))) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,1 + (($$CIV1A * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,2 + (($$CIV1A * 4 + MOD((1 + (int(ke) - &
                &               int(ks))), 4)) + int(ks))) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,2 + (($$CIV1A * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
                              d-w3dh%addr%w3dh(int(is) + $$CIV13,int(js) + &
                &               $$CIV14,3 + (($$CIV1A * 4 + MOD((1 + (int(ke) - &
                &               int(ks))), 4)) + int(ks))) = d-er%addr%er(int(is) &
                &               + $$CIV13,int(js) + $$CIV14,3 + (($$CIV1A * 4 + &
                &               MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))
   216|                     ENDDO
                          ENDIF
   217|                 ENDDO
                      ENDIF
   218|             ENDDO
                  ENDIF
   220|           IF (.FALSE.) GOTO lab_148
                  $$CIV16 = 0
       Id=22      DO $$CIV16 = $$CIV16, 5
   221|             bvstat($$CIV16 + 1,6) = 0
   222|           ENDDO
                  lab_148
   224|           lab_89
   227|           lab_150
                  RETURN
                END SUBROUTINE transprt_2d

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- --ss ssss
 CCR's set/used:   ss-- -sss
     | 000000                           PDEF     transprt_2d
   11|                                  PROC      
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 stfd     DB61FFD8   1     STFL      #stack(gr1,-40)=fp27
    0| 000014 stfd     DB41FFD0   1     STFL      #stack(gr1,-48)=fp26
    0| 000018 std      FBE1FFC8   1     ST8       #stack(gr1,-56)=gr31
    0| 00001C std      FBC1FFC0   1     ST8       #stack(gr1,-64)=gr30
    0| 000020 std      FBA1FFB8   1     ST8       #stack(gr1,-72)=gr29
    0| 000024 std      FB81FFB0   1     ST8       #stack(gr1,-80)=gr28
    0| 000028 std      FB61FFA8   1     ST8       #stack(gr1,-88)=gr27
    0| 00002C std      FB41FFA0   1     ST8       #stack(gr1,-96)=gr26
    0| 000030 std      FB21FF98   1     ST8       #stack(gr1,-104)=gr25
    0| 000034 std      FB01FF90   1     ST8       #stack(gr1,-112)=gr24
    0| 000038 std      FAE1FF88   1     ST8       #stack(gr1,-120)=gr23
    0| 00003C std      FAC1FF80   1     ST8       #stack(gr1,-128)=gr22
    0| 000040 std      FAA1FF78   1     ST8       #stack(gr1,-136)=gr21
    0| 000044 std      FA81FF70   1     ST8       #stack(gr1,-144)=gr20
    0| 000048 std      FA61FF68   1     ST8       #stack(gr1,-152)=gr19
    0| 00004C std      FA41FF60   1     ST8       #stack(gr1,-160)=gr18
    0| 000050 std      FA21FF58   1     ST8       #stack(gr1,-168)=gr17
    0| 000054 std      FA01FF50   1     ST8       #stack(gr1,-176)=gr16
    0| 000058 std      F9E1FF48   1     ST8       #stack(gr1,-184)=gr15
    0| 00005C std      F9C1FF40   1     ST8       #stack(gr1,-192)=gr14
    0| 000060 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 000064 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000068 stdu     F821FCC1   1     ST8U      gr1,#stack(gr1,-832)=gr1
   65| 00006C ld       E8620000   1     L8        gr3=.&&N&&config(gr2,0)
   65| 000070 lwz      80030028   1     L4Z       gr0=<s177:d40:l4>(gr3,40)
   65| 000074 andi.    70000001   1     RN4_R     gr0,cr0=gr0,0,0x1
   65| 000078 bc       41821720   1     BT        CL.2,cr0,0x4/eq,taken=50%(0,0)
   67| 00007C lwz      80030038   1     L4Z       gr0=<s177:d56:l4>(gr3,56)
   67| 000080 andi.    70000001   1     RN4_R     gr0,cr0=gr0,0,0x1
   67| 000084 bc       4182000C   1     BT        CL.3,cr0,0x4/eq,taken=60%(60,40)
   71| 000088 bl       48000001   1     CALL      ct_2d,0,#ProcAlias",ct_2d",fcr",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   71| 00008C ori      60000000   1
   72|                              CL.3:
   81| 000090 ld       E8820000   1     L8        gr4=.&&N&&grid(gr2,0)
   81| 000094 lwa      E8040016   1     L4A       gr0=<s183:d20:l4>(gr4,20)
   81| 000098 lwa      E8C40012   1     L4A       gr6=<s183:d16:l4>(gr4,16)
   81| 00009C subf     7C660050   1     S         gr3=gr0,gr6
   81| 0000A0 std      F8C100A8   1     ST8       #SPILL0(gr1,168)=gr6
   81| 0000A4 addic.   34E30001   1     AI_R      gr7,cr0=gr3,1,ca"
   81| 0000A8 std      F8E100B0   1     ST8       #SPILL1(gr1,176)=gr7
   81| 0000AC bc       40810558   1     BF        CL.106,cr0,0x2/gt,taken=50%(0,0)
   83| 0000B0 ld       EBC20000   1     L8        gr30=.&&N&field(gr2,0)
   83| 0000B4 lwa      E8040002   1     L4A       gr0=<s183:d0:l4>(gr4,0)
   84| 0000B8 ld       E9620000   1     L8        gr11=.&&N&scratch(gr2,0)
   82| 0000BC lwa      EA64000A   1     L4A       gr19=<s183:d8:l4>(gr4,8)
   82| 0000C0 lwa      EB24000E   1     L4A       gr25=<s183:d12:l4>(gr4,12)
   85| 0000C4 ld       EA420000   1     L8        gr18=.&&N&grid(gr2,0)
   83| 0000C8 ld       E87E0060   1     L8        gr3=<s39:d96:l8>(gr30,96)
   84| 0000CC ld       E8BE0200   1     L8        gr5=<s39:d512:l8>(gr30,512)
   83| 0000D0 ld       EAFE0000   1     L8        gr23=<s39:d0:l8>(gr30,0)
   82| 0000D4 std      FA6100B8   1     ST8       #SPILL2(gr1,184)=gr19
   83| 0000D8 ld       EADE0018   1     L8        gr22=<s39:d24:l8>(gr30,24)
   85| 0000DC ld       E8DE0268   1     L8        gr6=<s39:d616:l8>(gr30,616)
   83| 0000E0 mulld    7F8019D2   1     M         gr28=gr0,gr3
   84| 0000E4 ld       EB7E01A0   1     L8        gr27=<s39:d416:l8>(gr30,416)
   84| 0000E8 ld       EABE01B8   1     L8        gr21=<s39:d440:l8>(gr30,440)
   84| 0000EC ld       E90B04F8   1     L8        gr8=<s95:d1272:l8>(gr11,1272)
   83| 0000F0 add      7EF7B214   1     A         gr23=gr23,gr22
   84| 0000F4 mulld    7EC029D2   1     M         gr22=gr0,gr5
   83| 0000F8 subf     7F83E050   1     S         gr28=gr28,gr3
   85| 0000FC ld       EB1E0208   1     L8        gr24=<s39:d520:l8>(gr30,520)
   83| 000100 add      7DD7E214   1     A         gr14=gr23,gr28
   85| 000104 ld       EAFE0220   1     L8        gr23=<s39:d544:l8>(gr30,544)
   83| 000108 std      F9C100C0   1     ST8       #SPILL3(gr1,192)=gr14
   84| 00010C add      7EB5DA14   1     A         gr21=gr21,gr27
   84| 000110 ld       EB4B0498   1     L8        gr26=<s95:d1176:l8>(gr11,1176)
   84| 000114 add      7DF5B214   1     A         gr15=gr21,gr22
   85| 000118 mulld    7EC031D2   1     M         gr22=gr0,gr6
   84| 00011C ld       EA8B04B0   1     L8        gr20=<s95:d1200:l8>(gr11,1200)
   85| 000120 add      7F18BA14   1     A         gr24=gr24,gr23
   84| 000124 mulld    7EE041D2   1     M         gr23=gr0,gr8
   85| 000128 add      7E36C214   1     A         gr17=gr22,gr24
   82| 00012C subf     7F13C850   1     S         gr24=gr25,gr19
   84| 000130 add      7E94D214   1     A         gr20=gr20,gr26
   87| 000134 ld       E8FE02D0   1     L8        gr7=<s39:d720:l8>(gr30,720)
   85| 000138 ld       EBEB0500   1     L8        gr31=<s95:d1280:l8>(gr11,1280)
   82| 00013C addic.   36780001   1     AI_R      gr19,cr0=gr24,1,ca"
   85| 000140 ld       EB0B0518   1     L8        gr24=<s95:d1304:l8>(gr11,1304)
   82| 000144 std      FA6100C8   1     ST8       #SPILL4(gr1,200)=gr19
   87| 000148 ld       EAB20B98   1     L8        gr21=<s10:d2968:l8>(gr18,2968)
   84| 00014C add      7E14BA14   1     A         gr16=gr20,gr23
   85| 000150 rldicr   78171F24   1     SLL8      gr23=gr0,3
   87| 000154 ld       E99E0270   1     L8        gr12=<s39:d624:l8>(gr30,624)
   85| 000158 addi     3B37FFF8   1     AI        gr25=gr23,-8
   87| 00015C ld       EAFE0288   1     L8        gr23=<s39:d648:l8>(gr30,648)
   83| 000160 ld       E89E0048   1     L8        gr4=<s39:d72:l8>(gr30,72)
   85| 000164 ld       E92B0560   1     L8        gr9=<s95:d1376:l8>(gr11,1376)
   87| 000168 ld       E94B05C8   1     L8        gr10=<s95:d1480:l8>(gr11,1480)
   85| 00016C add      7FFFC214   1     A         gr31=gr31,gr24
   87| 000170 add      7F15CA14   1     A         gr24=gr21,gr25
   83| 000174 ld       EAA100B8   1     L8        gr21=#SPILL2(gr1,184)
   87| 000178 mulld    7F6039D2   1     M         gr27=gr0,gr7
   87| 00017C add      7D8CBA14   1     A         gr12=gr12,gr23
   87| 000180 ld       EBAB0568   1     L8        gr29=<s95:d1384:l8>(gr11,1384)
   87| 000184 ld       EACB0580   1     L8        gr22=<s95:d1408:l8>(gr11,1408)
   85| 000188 mulld    7F8049D2   1     M         gr28=gr0,gr9
   87| 00018C mulld    7F4051D2   1     M         gr26=gr0,gr10
   87| 000190 add      7EECDA14   1     A         gr23=gr12,gr27
   83| 000194 mulld    7D84A9D2   1     M         gr12=gr4,gr21
   87| 000198 add      7FBDB214   1     A         gr29=gr29,gr22
   87| 00019C ld       EA720CB0   1     L8        gr19=<s10:d3248:l8>(gr18,3248)
   85| 0001A0 ld       EA920B78   1     L8        gr20=<s10:d2936:l8>(gr18,2936)
   85| 0001A4 add      7EDCFA14   1     A         gr22=gr28,gr31
   87| 0001A8 add      7F5AEA14   1     A         gr26=gr26,gr29
   83| 0001AC add      7F6C7214   1     A         gr27=gr12,gr14
   85| 0001B0 ld       E9920B60   1     L8        gr12=<s10:d2912:l8>(gr18,2912)
   87| 0001B4 ld       EBF20BB0   1     L8        gr31=<s10:d2992:l8>(gr18,2992)
   87| 0001B8 ld       EBB20CC8   1     L8        gr29=<s10:d3272:l8>(gr18,3272)
   83| 0001BC ld       EA5E0030   1     L8        gr18=<s39:d48:l8>(gr30,48)
   85| 0001C0 add      7F34CA14   1     A         gr25=gr20,gr25
   87| 0001C4 addi     3B93FFF8   1     AI        gr28=gr19,-8
   84| 0001C8 ld       EA7E01D0   1     L8        gr19=<s39:d464:l8>(gr30,464)
   84| 0001CC ld       EA9E01E8   1     L8        gr20=<s39:d488:l8>(gr30,488)
   85| 0001D0 ld       EABE0238   1     L8        gr21=<s39:d568:l8>(gr30,568)
   83| 0001D4 std      FA4100D0   1     ST8       #SPILL5(gr1,208)=gr18
   85| 0001D8 ld       E9DE0250   1     L8        gr14=<s39:d592:l8>(gr30,592)
   87| 0001DC ld       EA5E02A0   1     L8        gr18=<s39:d672:l8>(gr30,672)
   84| 0001E0 std      FA6100D8   1     ST8       #SPILL6(gr1,216)=gr19
   84| 0001E4 std      FA8100E0   1     ST8       #SPILL7(gr1,224)=gr20
   85| 0001E8 std      FAA100E8   1     ST8       #SPILL8(gr1,232)=gr21
   87| 0001EC ld       EBDE02B8   1     L8        gr30=<s39:d696:l8>(gr30,696)
   85| 0001F0 std      F9C100F0   1     ST8       #SPILL9(gr1,240)=gr14
   87| 0001F4 std      FA4100F8   1     ST8       #SPILL10(gr1,248)=gr18
   84| 0001F8 ld       EA6B04C8   1     L8        gr19=<s95:d1224:l8>(gr11,1224)
   84| 0001FC ld       EA8B04E0   1     L8        gr20=<s95:d1248:l8>(gr11,1248)
   85| 000200 ld       EAAB0530   1     L8        gr21=<s95:d1328:l8>(gr11,1328)
   87| 000204 std      FBC10100   1     ST8       #SPILL11(gr1,256)=gr30
   85| 000208 ld       E9CB0548   1     L8        gr14=<s95:d1352:l8>(gr11,1352)
   87| 00020C ld       EA4B0598   1     L8        gr18=<s95:d1432:l8>(gr11,1432)
   84| 000210 std      FA610108   1     ST8       #SPILL12(gr1,264)=gr19
   84| 000214 std      FA810110   1     ST8       #SPILL13(gr1,272)=gr20
   85| 000218 std      FAA10118   1     ST8       #SPILL14(gr1,280)=gr21
   85| 00021C std      F9C10120   1     ST8       #SPILL15(gr1,288)=gr14
   87| 000220 std      FA410128   1     ST8       #SPILL16(gr1,296)=gr18
   87| 000224 ld       E96B05B0   1     L8        gr11=<s95:d1456:l8>(gr11,1456)
   81| 000228 addi     3BC00000   1     LI        gr30=0
   81| 00022C std      FBC10138   1     ST8       #SPILL18(gr1,312)=gr30
   87| 000230 std      F9610130   1     ST8       #SPILL17(gr1,304)=gr11
   83| 000234 ld       E9620000   1     L8        gr11=.&&N&&grid(gr2,0)
   83| 000238 lwa      E96B0006   1     L4A       gr11=<s183:d4:l4>(gr11,4)
    0| 00023C bc       408103C8   1     BF        CL.106,cr0,0x2/gt,taken=20%(20,80)
   85| 000240 add      7D8CCA14   1     A         gr12=gr12,gr25
   87| 000244 add      7FDCEA14   1     A         gr30=gr28,gr29
   85| 000248 std      F9810140   1     ST8       #SPILL19(gr1,320)=gr12
   84| 00024C subf     7F888050   1     S         gr28=gr16,gr8
   87| 000250 ld       EA0100B8   1     L8        gr16=#SPILL2(gr1,184)
   84| 000254 std      FB810158   1     ST8       #SPILL22(gr1,344)=gr28
    0| 000258 subfic   21800001   1     SFI       gr12=1,gr0,ca"
   87| 00025C add      7F38FA14   1     A         gr25=gr24,gr31
    0| 000260 add      7D8B6214   1     A         gr12=gr11,gr12
   87| 000264 std      FB210148   1     ST8       #SPILL20(gr1,328)=gr25
    0| 000268 subf     7D605850   1     S         gr11=gr11,gr0
   87| 00026C rldicr   7A1F1F24   1     SLL8      gr31=gr16,3
    0| 000270 addic.   340B0001   1     AI_R      gr0,cr0=gr11,1,ca"
   84| 000274 subf     7FA57850   1     S         gr29=gr15,gr5
   87| 000278 add      7DFEFA14   1     A         gr15=gr30,gr31
   84| 00027C std      FBA10150   1     ST8       #SPILL21(gr1,336)=gr29
   87| 000280 std      F9E10188   1     ST8       #SPILL28(gr1,392)=gr15
    0| 000284 rldicl   799EF842   1     SRL8      gr30=gr12,1
    0| 000288 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 00028C std      FBC10190   1     ST8       #SPILL29(gr1,400)=gr30
    0| 000290 andi.    71800001   1     RN4_R     gr0,cr0=gr12,0,0x1
    0| 000294 ld       E98100A8   1     L8        gr12=#SPILL0(gr1,168)
    0| 000298 ld       EBE100D0   1     L8        gr31=#SPILL5(gr1,208)
    0| 00029C ld       E9620000   1     L8        gr11=.+CONSTANT_AREA(gr2,0)
   85| 0002A0 subf     7F068850   1     S         gr24=gr17,gr6
   85| 0002A4 subf     7E29B050   1     S         gr17=gr22,gr9
   85| 0002A8 std      FB010160   1     ST8       #SPILL23(gr1,352)=gr24
   85| 0002AC std      FA210168   1     ST8       #SPILL24(gr1,360)=gr17
    0| 0002B0 mulld    7C0CF9D2   1     M         gr0=gr12,gr31
   87| 0002B4 subf     7EC7B850   1     S         gr22=gr23,gr7
    0| 0002B8 add      7C00DA14   1     A         gr0=gr0,gr27
   87| 0002BC std      FAC10170   1     ST8       #SPILL25(gr1,368)=gr22
   87| 0002C0 subf     7EEAD050   1     S         gr23=gr26,gr10
   83| 0002C4 subf     7F44D850   1     S         gr26=gr27,gr4
   87| 0002C8 std      FAE10178   1     ST8       #SPILL26(gr1,376)=gr23
    0| 0002CC subf     7F640050   1     S         gr27=gr0,gr4
   83| 0002D0 std      FB410180   1     ST8       #SPILL27(gr1,384)=gr26
    0| 0002D4 std      FB610198   1     ST8       #SPILL30(gr1,408)=gr27
    0| 0002D8 cmpdi    2FBE0000   1     C8        cr7=gr30,0
    0| 0002DC lfs      C00B0000   1     LFS       fp0=+CONSTANT_AREA(gr11,0)
   81|                              CL.107:
    0| 0002E0 ld       E80100A8   1     L8        gr0=#SPILL0(gr1,168)
    0| 0002E4 ld       E9810138   1     L8        gr12=#SPILL18(gr1,312)
   82| 0002E8 addi     39600000   1     LI        gr11=0
    0| 0002EC add      7FC06214   1     A         gr30=gr0,gr12
    0| 0002F0 bc       408502EC   1     BF        CL.108,cr1,0x2/gt,taken=20%(20,80)
    0| 0002F4 ld       E9C10108   1     L8        gr14=#SPILL12(gr1,264)
    0| 0002F8 ld       EB810128   1     L8        gr28=#SPILL16(gr1,296)
    0| 0002FC ld       EBA100D0   1     L8        gr29=#SPILL5(gr1,208)
    0| 000300 ld       EB6100F8   1     L8        gr27=#SPILL10(gr1,248)
    0| 000304 ld       EAE10118   1     L8        gr23=#SPILL14(gr1,280)
    0| 000308 ld       EAA100E8   1     L8        gr21=#SPILL8(gr1,232)
    0| 00030C mulld    7DEEF1D2   1     M         gr15=gr14,gr30
    0| 000310 ld       E9C100D8   1     L8        gr14=#SPILL6(gr1,216)
    0| 000314 mulld    7FFCF1D2   1     M         gr31=gr28,gr30
    0| 000318 ld       EB410178   1     L8        gr26=#SPILL26(gr1,376)
    0| 00031C mulld    7C1DF1D2   1     M         gr0=gr29,gr30
    0| 000320 mulld    7D9BF1D2   1     M         gr12=gr27,gr30
    0| 000324 std      F80101A8   1     ST8       #SPILL32(gr1,424)=gr0
    0| 000328 ld       EB010170   1     L8        gr24=#SPILL25(gr1,368)
    0| 00032C ld       EAC100C0   1     L8        gr22=#SPILL3(gr1,192)
    0| 000330 add      7F3AFA14   1     A         gr25=gr26,gr31
    0| 000334 ld       EA610180   1     L8        gr19=#SPILL27(gr1,384)
    0| 000338 std      FB2101A0   1     ST8       #SPILL31(gr1,416)=gr25
   83| 00033C ld       EBE10198   1     L8        gr31=#SPILL30(gr1,408)
    0| 000340 add      7E8CC214   1     A         gr20=gr12,gr24
   87| 000344 ld       E9810188   1     L8        gr12=#SPILL28(gr1,392)
    0| 000348 mulld    7E57F1D2   1     M         gr18=gr23,gr30
    0| 00034C add      7E20B214   1     A         gr17=gr0,gr22
    0| 000350 mulld    7E15F1D2   1     M         gr16=gr21,gr30
    0| 000354 mulld    7DCEF1D2   1     M         gr14=gr14,gr30
   82|                              CL.109:
   83| 000358 ld       EBA100B8   1     L8        gr29=#SPILL2(gr1,184)
   84| 00035C ld       EB0100E0   1     L8        gr24=#SPILL7(gr1,224)
   85| 000360 ld       EAC10120   1     L8        gr22=#SPILL15(gr1,288)
   87| 000364 ld       EB610100   1     L8        gr27=#SPILL11(gr1,256)
   85| 000368 ld       EB4100F0   1     L8        gr26=#SPILL9(gr1,240)
   84| 00036C ld       EAA10110   1     L8        gr21=#SPILL13(gr1,272)
   83| 000370 add      7EEBEA14   1     A         gr23=gr11,gr29
   83| 000374 ld       E80101A8   1     L8        gr0=#SPILL32(gr1,424)
   84| 000378 mulld    7F37C1D2   1     M         gr25=gr23,gr24
   85| 00037C mulld    7F16B9D2   1     M         gr24=gr22,gr23
   87| 000380 ld       EAC10130   1     L8        gr22=#SPILL17(gr1,304)
   87| 000384 mulld    7F97D9D2   1     M         gr28=gr23,gr27
   85| 000388 mulld    7F77D1D2   1     M         gr27=gr23,gr26
   84| 00038C mulld    7F55B9D2   1     M         gr26=gr21,gr23
   85| 000390 ld       EAA10160   1     L8        gr21=#SPILL23(gr1,352)
   83| 000394 mulld    7FA4B9D2   1     M         gr29=gr4,gr23
   87| 000398 mulld    7EF6B9D2   1     M         gr23=gr22,gr23
   85| 00039C add      7F7BAA14   1     A         gr27=gr27,gr21
   84| 0003A0 ld       EAC10150   1     L8        gr22=#SPILL21(gr1,336)
   85| 0003A4 ld       EAA10168   1     L8        gr21=#SPILL24(gr1,360)
    0| 0003A8 ld       EBC10190   1     L8        gr30=#SPILL29(gr1,400)
   83| 0003AC add      7FBD8A14   1     A         gr29=gr29,gr17
   87| 0003B0 add      7F9CA214   1     A         gr28=gr28,gr20
   83| 0003B4 lfdux    7CDF24EE   1     LFDU      fp6,gr31=d(gr31,gr4,0)
   84| 0003B8 add      7F39B214   1     A         gr25=gr25,gr22
   85| 0003BC add      7ED5C214   1     A         gr22=gr21,gr24
   84| 0003C0 ld       EB010158   1     L8        gr24=#SPILL22(gr1,344)
    0| 0003C4 mtspr    7FC903A6   1     LCTR      ctr=gr30
   85| 0003C8 add      7FC09A14   1     A         gr30=gr0,gr19
   87| 0003CC ld       E80101A0   1     L8        gr0=#SPILL31(gr1,416)
   87| 0003D0 lfdu     CC2C0008   1     LFDU      fp1,gr12=g32b(gr12,8)
   85| 0003D4 add      7F70DA14   1     A         gr27=gr16,gr27
   84| 0003D8 add      7EB8D214   1     A         gr21=gr24,gr26
   87| 0003DC ld       EB410148   1     L8        gr26=#SPILL20(gr1,328)
   84| 0003E0 add      7F2ECA14   1     A         gr25=gr14,gr25
   85| 0003E4 ld       EB010140   1     L8        gr24=#SPILL19(gr1,320)
   87| 0003E8 add      7EF70214   1     A         gr23=gr23,gr0
   85| 0003EC add      7ED2B214   1     A         gr22=gr18,gr22
   84| 0003F0 add      7EAFAA14   1     A         gr21=gr15,gr21
    0| 0003F4 bc       41820064   1     BT        CL.800,cr0,0x4/eq,taken=50%(0,0)
   83| 0003F8 lfdux    7C5D1CEE   1     LFDU      fp2,gr29=d(gr29,gr3,0)
   87| 0003FC lfdux    7CFC3CEE   1     LFDU      fp7,gr28=v3(gr28,gr7,0)
   85| 000400 lfdux    7D1B34EE   1     LFDU      fp8,gr27=v2(gr27,gr6,0)
   85| 000404 lfdux    7D3E1CEE   1     LFDU      fp9,gr30=d(gr30,gr3,0)
   87| 000408 lfdu     CC7A0008   1     LFDU      fp3,gr26=g31b(gr26,8)
   84| 00040C lfdux    7D592CEE   1     LFDU      fp10,gr25=v1(gr25,gr5,0)
   85| 000410 lfdu     CC980008   1     LFDU      fp4,gr24=g2b(gr24,8)
   87| 000414 fadd     FCA2102A   1     AFL       fp5=fp2,fp2,fcr
   87| 000418 fmul     FCE70032   2     MFL       fp7=fp7,fp0,fcr
   85| 00041C fmul     FD080032   2     MFL       fp8=fp8,fp0,fcr
   85| 000420 fadd     FD22482A   2     AFL       fp9=fp2,fp9,fcr
   84| 000424 fadd     FD66102A   2     AFL       fp11=fp6,fp2,fcr
   87| 000428 fmr      FCC01090   2     LRFL      fp6=fp2
   84| 00042C fmul     FD4A0032   2     MFL       fp10=fp10,fp0,fcr
   87| 000430 fmul     FC470172   2     MFL       fp2=fp7,fp5,fcr
   85| 000434 fmul     FCA80272   2     MFL       fp5=fp8,fp9,fcr
   84| 000438 fmul     FCEA02F2   2     MFL       fp7=fp10,fp11,fcr
   87| 00043C fmul     FC4200F2   2     MFL       fp2=fp2,fp3,fcr
   85| 000440 fmul     FC650132   2     MFL       fp3=fp5,fp4,fcr
   84| 000444 stfdux   7CF545EE   1     STFDU     gr21,w3da(gr21,gr8,0)=fp7
   87| 000448 fmul     FC420072   1     MFL       fp2=fp2,fp1,fcr
   85| 00044C stfdux   7C764DEE   2     STFDU     gr22,w3db(gr22,gr9,0)=fp3
   87| 000450 stfdux   7C5755EE   1     STFDU     gr23,w3dc(gr23,gr10,0)=fp2
    0| 000454 bc       419E0174   1     BT        CL.692,cr7,0x4/eq,taken=20%(20,80)
    0|                              CL.800:
   83| 000458 lfdux    7C7D1CEE   1     LFDU      fp3,gr29=d(gr29,gr3,0)
   87| 00045C lfdux    7D1C3CEE   1     LFDU      fp8,gr28=v3(gr28,gr7,0)
   85| 000460 lfdux    7FDE1CEE   1     LFDU      fp30,gr30=d(gr30,gr3,0)
   85| 000464 lfdux    7FFB34EE   1     LFDU      fp31,gr27=v2(gr27,gr6,0)
   87| 000468 lfdu     CC9A0010   1     LFDU      fp4,gr26=g31b(gr26,16)
   84| 00046C lfdux    7CF92CEE   1     LFDU      fp7,gr25=v1(gr25,gr5,0)
   85| 000470 lfdu     CCB80010   1     LFDU      fp5,gr24=g2b(gr24,16)
   87| 000474 fadd     FD83182A   1     AFL       fp12=fp3,fp3,fcr
   83| 000478 lfdux    7C5D1CEE   1     LFDU      fp2,gr29=d(gr29,gr3,0)
   87| 00047C fmul     FDA80032   1     MFL       fp13=fp8,fp0,fcr
   87| 000480 lfdux    7D5C3CEE   1     LFDU      fp10,gr28=v3(gr28,gr7,0)
   84| 000484 fadd     FCC6182A   1     AFL       fp6=fp6,fp3,fcr
   85| 000488 lfdux    7F9E1CEE   1     LFDU      fp28,gr30=d(gr30,gr3,0)
   84| 00048C fmul     FCE70032   1     MFL       fp7=fp7,fp0,fcr
   85| 000490 lfdux    7FBB34EE   1     LFDU      fp29,gr27=v2(gr27,gr6,0)
   87| 000494 fadd     FD02102A   1     AFL       fp8=fp2,fp2,fcr
   87| 000498 lfd      C93AFFF8   1     LFL       fp9=g31b(gr26,-8)
   87| 00049C fmul     FD4A0032   1     MFL       fp10=fp10,fp0,fcr
   84| 0004A0 lfdux    7D792CEE   1     LFDU      fp11,gr25=v1(gr25,gr5,0)
   87| 0004A4 fmul     FD8D0332   1     MFL       fp12=fp13,fp12,fcr
   85| 0004A8 lfd      C9B8FFF8   1     LFL       fp13=g2b(gr24,-8)
   85| 0004AC fmul     FFFF0032   1     MFL       fp31=fp31,fp0,fcr
   85| 0004B0 fadd     FFC3F02A   2     AFL       fp30=fp3,fp30,fcr
   85| 0004B4 fadd     FF82E02A   2     AFL       fp28=fp2,fp28,fcr
   85| 0004B8 fmul     FFBD0032   2     MFL       fp29=fp29,fp0,fcr
   84| 0004BC fmul     FCC701B2   2     MFL       fp6=fp7,fp6,fcr
   87| 0004C0 fmul     FCEA0232   2     MFL       fp7=fp10,fp8,fcr
   87| 0004C4 fmul     FD0C0272   2     MFL       fp8=fp12,fp9,fcr
   85| 0004C8 fmul     FD3F07B2   2     MFL       fp9=fp31,fp30,fcr
   84| 0004CC fmul     FD4B0032   2     MFL       fp10=fp11,fp0,fcr
   85| 0004D0 fmul     FD7D0732   2     MFL       fp11=fp29,fp28,fcr
   84| 0004D4 fadd     FC63102A   2     AFL       fp3=fp3,fp2,fcr
   84| 0004D8 stfdux   7CD545EE   1     STFDU     gr21,w3da(gr21,gr8,0)=fp6
   87| 0004DC fmul     FC870132   1     MFL       fp4=fp7,fp4,fcr
   87| 0004E0 fmul     FD080072   2     MFL       fp8=fp8,fp1,fcr
   85| 0004E4 fmul     FCE90372   2     MFL       fp7=fp9,fp13,fcr
   85| 0004E8 fmul     FCCB0172   2     MFL       fp6=fp11,fp5,fcr
   84| 0004EC fmul     FD4A00F2   2     MFL       fp10=fp10,fp3,fcr
    0| 0004F0 bc       424000C0   1     BCF       ctr=CL.841,taken=0%(0,100)
    0| 0004F4 ori      60210000   1     XNOP      
    0| 0004F8 ori      60210000   1     XNOP      
    0|                              CL.842:
   85| 0004FC lfdu     CC780010   1     LFDU      fp3,gr24=g2b(gr24,16)
   87| 000500 fmul     FC840072   1     MFL       fp4=fp4,fp1,fcr
   87| 000504 stfdux   7D1755EE   1     STFDU     gr23,w3dc(gr23,gr10,0)=fp8
   85| 000508 stfdux   7CF64DEE   1     STFDU     gr22,w3db(gr22,gr9,0)=fp7
   85| 00050C lfd      C8B8FFF8   1     LFL       fp5=g2b(gr24,-8)
   84| 000510 stfdux   7D5545EE   1     STFDU     gr21,w3da(gr21,gr8,0)=fp10
   84| 000514 lfdux    7FF92CEE   1     LFDU      fp31,gr25=v1(gr25,gr5,0)
   85| 000518 stfdux   7CD64DEE   1     STFDU     gr22,w3db(gr22,gr9,0)=fp6
   85| 00051C lfdux    7CDE1CEE   1     LFDU      fp6,gr30=d(gr30,gr3,0)
   83| 000520 lfdux    7CFD1CEE   1     LFDU      fp7,gr29=d(gr29,gr3,0)
   87| 000524 lfdux    7DBC3CEE   1     LFDU      fp13,gr28=v3(gr28,gr7,0)
   87| 000528 lfdu     CD1A0010   1     LFDU      fp8,gr26=g31b(gr26,16)
   85| 00052C lfdux    7D3B34EE   1     LFDU      fp9,gr27=v2(gr27,gr6,0)
   87| 000530 lfd      C95AFFF8   1     LFL       fp10=g31b(gr26,-8)
   84| 000534 lfdux    7FD92CEE   1     LFDU      fp30,gr25=v1(gr25,gr5,0)
   85| 000538 lfdux    7F9E1CEE   1     LFDU      fp28,gr30=d(gr30,gr3,0)
   84| 00053C fadd     FD62382A   1     AFL       fp11=fp2,fp7,fcr
   83| 000540 lfdux    7C5D1CEE   1     LFDU      fp2,gr29=d(gr29,gr3,0)
   87| 000544 fadd     FD87382A   1     AFL       fp12=fp7,fp7,fcr
   85| 000548 lfdux    7F5B34EE   1     LFDU      fp26,gr27=v2(gr27,gr6,0)
   87| 00054C lfdux    7F7C3CEE   1     LFDU      fp27,gr28=v3(gr28,gr7,0)
   87| 000550 fmul     FDAD0032   1     MFL       fp13=fp13,fp0,fcr
   84| 000554 fmul     FFFF0032   2     MFL       fp31=fp31,fp0,fcr
   84| 000558 fmul     FFDE0032   2     MFL       fp30=fp30,fp0,fcr
   87| 00055C fadd     FFA2102A   2     AFL       fp29=fp2,fp2,fcr
   85| 000560 fadd     FF82E02A   2     AFL       fp28=fp2,fp28,fcr
   85| 000564 fmul     FF5A0032   2     MFL       fp26=fp26,fp0,fcr
   87| 000568 fmul     FF7B0032   2     MFL       fp27=fp27,fp0,fcr
   87| 00056C fmul     FD8D0332   2     MFL       fp12=fp13,fp12,fcr
   85| 000570 fmul     FD290032   2     MFL       fp9=fp9,fp0,fcr
   85| 000574 fadd     FCC7302A   2     AFL       fp6=fp7,fp6,fcr
   84| 000578 fmul     FD7F02F2   2     MFL       fp11=fp31,fp11,fcr
   85| 00057C fmul     FDBA0732   2     MFL       fp13=fp26,fp28,fcr
   87| 000580 fmul     FFFB0772   2     MFL       fp31=fp27,fp29,fcr
   87| 000584 fmul     FD4C02B2   2     MFL       fp10=fp12,fp10,fcr
   84| 000588 fadd     FCE7102A   2     AFL       fp7=fp7,fp2,fcr
   85| 00058C fmul     FD2901B2   2     MFL       fp9=fp9,fp6,fcr
   87| 000590 stfdux   7C9755EE   1     STFDU     gr23,w3dc(gr23,gr10,0)=fp4
   85| 000594 fmul     FCCD00F2   1     MFL       fp6=fp13,fp3,fcr
   87| 000598 fmul     FC9F0232   2     MFL       fp4=fp31,fp8,fcr
   87| 00059C fmul     FD0A0072   2     MFL       fp8=fp10,fp1,fcr
   84| 0005A0 fmul     FD5E01F2   2     MFL       fp10=fp30,fp7,fcr
   85| 0005A4 fmul     FCE90172   2     MFL       fp7=fp9,fp5,fcr
   84| 0005A8 stfdux   7D7545EE   1     STFDU     gr21,w3da(gr21,gr8,0)=fp11
    0| 0005AC bc       4200FF50   1     BCT       ctr=CL.842,taken=100%(100,0)
    0|                              CL.841:
   85| 0005B0 stfdux   7CF64DEE   1     STFDU     gr22,w3db(gr22,gr9,0)=fp7
   87| 0005B4 fmul     FC240072   1     MFL       fp1=fp4,fp1,fcr
   87| 0005B8 stfdux   7D1755EE   1     STFDU     gr23,w3dc(gr23,gr10,0)=fp8
   84| 0005BC stfdux   7D5545EE   1     STFDU     gr21,w3da(gr21,gr8,0)=fp10
   85| 0005C0 stfdux   7CD64DEE   1     STFDU     gr22,w3db(gr22,gr9,0)=fp6
   87| 0005C4 stfdux   7C3755EE   1     STFDU     gr23,w3dc(gr23,gr10,0)=fp1
    0|                              CL.692:
   90| 0005C8 ld       E80100C8   1     L8        gr0=#SPILL4(gr1,200)
   90| 0005CC addi     396B0001   1     AI        gr11=gr11,1
    0| 0005D0 add      7E649A14   1     A         gr19=gr4,gr19
   90| 0005D4 cmpld    7F2B0040   1     CL8       cr6=gr11,gr0
   90| 0005D8 bc       4198FD80   1     BT        CL.109,cr6,0x8/llt,taken=80%(80,20)
   90|                              CL.108:
   91| 0005DC ld       E9610138   1     L8        gr11=#SPILL18(gr1,312)
    0| 0005E0 ld       E80100D0   1     L8        gr0=#SPILL5(gr1,208)
    0| 0005E4 ld       E9810198   1     L8        gr12=#SPILL30(gr1,408)
   91| 0005E8 ld       EBE100B0   1     L8        gr31=#SPILL1(gr1,176)
   91| 0005EC addi     396B0001   1     AI        gr11=gr11,1
   91| 0005F0 std      F9610138   1     ST8       #SPILL18(gr1,312)=gr11
    0| 0005F4 add      7D806214   1     A         gr12=gr0,gr12
   91| 0005F8 cmpld    7F2BF840   1     CL8       cr6=gr11,gr31
    0| 0005FC std      F9810198   1     ST8       #SPILL30(gr1,408)=gr12
   91| 000600 bc       4198FCE0   1     BT        CL.107,cr6,0x8/llt,taken=80%(80,20)
   91|                              CL.106:
   95| 000604 ld       E8620000   1     L8        gr3=.&&N&&root(gr2,0)
   95| 000608 addi     38800000   1     LI        gr4=0
   95| 00060C stw      908301FC   1     ST4Z      <s201:d508:l4>(gr3,508)=gr4
   96| 000610 lwz      800301E0   1     L4Z       gr0=<s201:d480:l4>(gr3,480)
   96| 000614 cmpwi    2C000001   1     C4        cr0=gr0,1
   96| 000618 bc       4082093C   1     BF        CL.16,cr0,0x4/eq,taken=50%(0,0)
  103| 00061C ld       E9620000   1     L8        gr11=.&&N&&config(gr2,0)
    0| 000620 ld       E9820000   1     L8        gr12=.&&N&field(gr2,0)
    0| 000624 ld       EBE20000   1     L8        gr31=.&&N&scratch(gr2,0)
  103| 000628 lwz      800B0018   1     L4Z       gr0=<s177:d24:l4>(gr11,24)
    0| 00062C ld       E88C0000   1     L8        gr4=<s39:d0:l8>(gr12,0)
    0| 000630 ld       E91F0498   1     L8        gr8=<s95:d1176:l8>(gr31,1176)
    0| 000634 ld       E93F0500   1     L8        gr9=<s95:d1280:l8>(gr31,1280)
    0| 000638 ld       E95F0568   1     L8        gr10=<s95:d1384:l8>(gr31,1384)
    0| 00063C ld       E87F05D0   1     L8        gr3=<s95:d1488:l8>(gr31,1488)
  103| 000640 cmpwi    2C000000   1     C4        cr0=gr0,0
    0| 000644 ld       E8BF0638   1     L8        gr5=<s95:d1592:l8>(gr31,1592)
    0| 000648 ld       E8DF0708   1     L8        gr6=<s95:d1800:l8>(gr31,1800)
    0| 00064C ld       E8FF06A0   1     L8        gr7=<s95:d1696:l8>(gr31,1696)
  103| 000650 bc       40810080   1     BF        CL.17,cr0,0x2/gt,taken=40%(40,60)
  104| 000654 ld       E80C0410   1     L8        gr0=<s39:d1040:l8>(gr12,1040)
  104| 000658 ld       E97F0840   1     L8        gr11=<s95:d2112:l8>(gr31,2112)
  104| 00065C or       7D8E6378   1     LR        gr14=gr12
  104| 000660 ld       E98C0820   1     L8        gr12=<s39:d2080:l8>(gr12,2080)
  104| 000664 or       7FEFFB78   1     LR        gr15=gr31
  104| 000668 ld       EBFF08A8   1     L8        gr31=<s95:d2216:l8>(gr31,2216)
  104| 00066C std      F8010070   1     ST8       #MX_TEMP1(gr1,112)=gr0
  104| 000670 std      F9610078   1     ST8       #MX_TEMP1(gr1,120)=gr11
  104| 000674 std      F9810080   1     ST8       #MX_TEMP1(gr1,128)=gr12
  104| 000678 std      FBE10088   1     ST8       #MX_TEMP1(gr1,136)=gr31
  104| 00067C bl       48000001   1     CALL      advx1,12,w3dd",gr3,d",gr4,w3de",gr5,w3dg",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,er",w3dh",abun",w4da",advx1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  104| 000680 ori      60000000   1
  106| 000684 ld       E8AF0840   1     L8        gr5=<s95:d2112:l8>(gr15,2112)
  106| 000688 ld       E8CE0410   1     L8        gr6=<s39:d1040:l8>(gr14,1040)
  106| 00068C ld       E8EF08A8   1     L8        gr7=<s95:d2216:l8>(gr15,2216)
  106| 000690 ld       E80E0820   1     L8        gr0=<s39:d2080:l8>(gr14,2080)
  106| 000694 ld       E86E0000   1     L8        gr3=<s39:d0:l8>(gr14,0)
  106| 000698 ld       E88F05D0   1     L8        gr4=<s95:d1488:l8>(gr15,1488)
  106| 00069C std      F8A10070   1     ST8       #MX_TEMP1(gr1,112)=gr5
  106| 0006A0 std      F8C10078   1     ST8       #MX_TEMP1(gr1,120)=gr6
  106| 0006A4 std      F8E10080   1     ST8       #MX_TEMP1(gr1,128)=gr7
  106| 0006A8 ld       E8AF0708   1     L8        gr5=<s95:d1800:l8>(gr15,1800)
  106| 0006AC ld       E8CF0638   1     L8        gr6=<s95:d1592:l8>(gr15,1592)
  106| 0006B0 ld       E8EF06A0   1     L8        gr7=<s95:d1696:l8>(gr15,1696)
  106| 0006B4 std      F8010088   1     ST8       #MX_TEMP1(gr1,136)=gr0
  106| 0006B8 ld       E90F0498   1     L8        gr8=<s95:d1176:l8>(gr15,1176)
  106| 0006BC ld       E92F0500   1     L8        gr9=<s95:d1280:l8>(gr15,1280)
  106| 0006C0 ld       E94F0568   1     L8        gr10=<s95:d1384:l8>(gr15,1384)
  106| 0006C4 bl       48000001   1     CALL      advx2,12,d",gr3,w3dd",gr4,w3dg",gr5,w3de",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,w3dh",er",w4da",abun",advx2",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  106| 0006C8 ori      60000000   1
    0| 0006CC b        48000038   1     B         CL.18,-1
  108|                              CL.17:
  109| 0006D0 bl       48000001   1     CALL      advx1,8,w3dd",gr3,d",gr4,w3de",gr5,w3dg",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,advx1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  109| 0006D4 ori      60000000   1
  110| 0006D8 ld       EBC20000   1     L8        gr30=.&&N&field(gr2,0)
  110| 0006DC ld       E89F05D0   1     L8        gr4=<s95:d1488:l8>(gr31,1488)
  110| 0006E0 ld       E8BF0708   1     L8        gr5=<s95:d1800:l8>(gr31,1800)
  110| 0006E4 ld       E8DF0638   1     L8        gr6=<s95:d1592:l8>(gr31,1592)
  110| 0006E8 ld       E8FF06A0   1     L8        gr7=<s95:d1696:l8>(gr31,1696)
  110| 0006EC ld       E91F0498   1     L8        gr8=<s95:d1176:l8>(gr31,1176)
  110| 0006F0 ld       E87E0000   1     L8        gr3=<s39:d0:l8>(gr30,0)
  110| 0006F4 ld       E93F0500   1     L8        gr9=<s95:d1280:l8>(gr31,1280)
  110| 0006F8 ld       E95F0568   1     L8        gr10=<s95:d1384:l8>(gr31,1384)
  110| 0006FC bl       48000001   1     CALL      advx2,8,d",gr3,w3dd",gr4,w3dg",gr5,w3de",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,advx2",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  110| 000700 ori      60000000   1
  111|                              CL.18:
  117| 000704 ld       E8820000   1     L8        gr4=.&&N&&grid(gr2,0)
  117| 000708 lwa      E804000E   1     L4A       gr0=<s183:d12:l4>(gr4,12)
  117| 00070C lwa      E8A4000A   1     L4A       gr5=<s183:d8:l4>(gr4,8)
  117| 000710 subf     7C650050   1     S         gr3=gr0,gr5
  117| 000714 std      F8A101B0   1     ST8       #SPILL33(gr1,432)=gr5
  117| 000718 addic.   34030001   1     AI_R      gr0,cr0=gr3,1,ca"
  117| 00071C mcrf     4C800000   1     LRCR      cr1=cr0
  117| 000720 bc       40810224   1     BF        CL.112,cr0,0x2/gt,taken=50%(0,0)
  118| 000724 lwa      E9040002   1     L4A       gr8=<s183:d0:l4>(gr4,0)
  118| 000728 lwa      E9240006   1     L4A       gr9=<s183:d4:l4>(gr4,4)
  120| 00072C ld       EA020000   1     L8        gr16=.&&N&scratch(gr2,0)
  119| 000730 ld       EA220000   1     L8        gr17=.&&N&field(gr2,0)
  119| 000734 lwa      E9440012   1     L4A       gr10=<s183:d16:l4>(gr4,16)
  118| 000738 subf     7CE84850   1     S         gr7=gr9,gr8
  120| 00073C ld       EB900668   1     L8        gr28=<s95:d1640:l8>(gr16,1640)
  118| 000740 addic.   34E70001   1     AI_R      gr7,cr0=gr7,1,ca"
  119| 000744 ld       E8710060   1     L8        gr3=<s39:d96:l8>(gr17,96)
  120| 000748 ld       E9700738   1     L8        gr11=<s95:d1848:l8>(gr16,1848)
  120| 00074C ld       E9900750   1     L8        gr12=<s95:d1872:l8>(gr16,1872)
  119| 000750 ld       EBF10048   1     L8        gr31=<s39:d72:l8>(gr17,72)
  119| 000754 ld       EBD00600   1     L8        gr30=<s95:d1536:l8>(gr16,1536)
  119| 000758 ld       EBB00618   1     L8        gr29=<s95:d1560:l8>(gr16,1560)
  119| 00075C ld       E8900630   1     L8        gr4=<s95:d1584:l8>(gr16,1584)
  120| 000760 ld       EB700680   1     L8        gr27=<s95:d1664:l8>(gr16,1664)
  120| 000764 ld       E8B00698   1     L8        gr5=<s95:d1688:l8>(gr16,1688)
  120| 000768 ld       E8D00768   1     L8        gr6=<s95:d1896:l8>(gr16,1896)
  119| 00076C ld       EB510030   1     L8        gr26=<s39:d48:l8>(gr17,48)
  117| 000770 addi     38E00000   1     LI        gr7=0
  119| 000774 ld       EB310000   1     L8        gr25=<s39:d0:l8>(gr17,0)
  119| 000778 ld       EB1005D0   1     L8        gr24=<s95:d1488:l8>(gr16,1488)
  120| 00077C ld       EAF00638   1     L8        gr23=<s95:d1592:l8>(gr16,1592)
  120| 000780 ld       EAD00708   1     L8        gr22=<s95:d1800:l8>(gr16,1800)
  119| 000784 ld       EAB10018   1     L8        gr21=<s39:d24:l8>(gr17,24)
  119| 000788 ld       EA9005E8   1     L8        gr20=<s95:d1512:l8>(gr16,1512)
  120| 00078C ld       EA700650   1     L8        gr19=<s95:d1616:l8>(gr16,1616)
  120| 000790 ld       EA500720   1     L8        gr18=<s95:d1824:l8>(gr16,1824)
    0| 000794 bc       408101B0   1     BF        CL.112,cr0,0x2/gt,taken=50%(0,0)
    0| 000798 ld       EA0101B0   1     L8        gr16=#SPILL33(gr1,432)
    0| 00079C mulld    7F8AE1D2   1     M         gr28=gr10,gr28
    0| 0007A0 mulld    7FCAF1D2   1     M         gr30=gr10,gr30
    0| 0007A4 std      FB8101C8   1     ST8       #SPILL36(gr1,456)=gr28
    0| 0007A8 mulld    7DD0F9D2   1     M         gr14=gr16,gr31
    0| 0007AC ld       EB8101B0   1     L8        gr28=#SPILL33(gr1,432)
    0| 0007B0 std      FBC101B8   1     ST8       #SPILL34(gr1,440)=gr30
    0| 0007B4 mulld    7E2A59D2   1     M         gr17=gr10,gr11
    0| 0007B8 mulld    7D6C81D2   1     M         gr11=gr12,gr16
    0| 0007BC mulld    7DE341D2   1     M         gr15=gr3,gr8
    0| 0007C0 std      F9C101C0   1     ST8       #SPILL35(gr1,448)=gr14
    0| 0007C4 mulld    7FC641D2   1     M         gr30=gr6,gr8
    0| 0007C8 add      7D6B8A14   1     A         gr11=gr11,gr17
    0| 0007CC mulld    7DD0D9D2   1     M         gr14=gr16,gr27
    0| 0007D0 mulld    7E0441D2   1     M         gr16=gr4,gr8
    0| 0007D4 mulld    7E3CE9D2   1     M         gr17=gr28,gr29
    0| 0007D8 add      7D6BF214   1     A         gr11=gr11,gr30
    0| 0007DC mulld    7FC541D2   1     M         gr30=gr5,gr8
    0| 0007E0 subfic   21080001   1     SFI       gr8=1,gr8,ca"
    0| 0007E4 subf     7F84C050   1     S         gr28=gr24,gr4
    0| 0007E8 add      7F39AA14   1     A         gr25=gr25,gr21
    0| 0007EC subf     7F037850   1     S         gr24=gr15,gr3
    0| 0007F0 add      7D084A14   1     A         gr8=gr8,gr9
    0| 0007F4 add      7D34E214   1     A         gr9=gr20,gr28
    0| 0007F8 add      7F98CA14   1     A         gr28=gr24,gr25
    0| 0007FC subf     7F26B050   1     S         gr25=gr22,gr6
    0| 000800 ld       EAC101B8   1     L8        gr22=#SPILL34(gr1,440)
    0| 000804 ld       EAA101C0   1     L8        gr21=#SPILL35(gr1,448)
    0| 000808 ld       EA8101C8   1     L8        gr20=#SPILL36(gr1,456)
    0| 00080C subf     7EE5B850   1     S         gr23=gr23,gr5
    0| 000810 mulld    7D4AD1D2   1     M         gr10=gr10,gr26
    0| 000814 add      7F53BA14   1     A         gr26=gr19,gr23
    0| 000818 add      7F10B214   1     A         gr24=gr16,gr22
    0| 00081C add      7D298A14   1     A         gr9=gr9,gr17
    0| 000820 add      7F95E214   1     A         gr28=gr21,gr28
    0| 000824 add      7EEEA214   1     A         gr23=gr14,gr20
    0| 000828 add      7FDED214   1     A         gr30=gr30,gr26
    0| 00082C add      7F52CA14   1     A         gr26=gr18,gr25
    0| 000830 rldicl   7919F082   1     SRL8      gr25=gr8,2
    0| 000834 add      7D29C214   1     A         gr9=gr9,gr24
    0| 000838 add      7D4AE214   1     A         gr10=gr10,gr28
    0| 00083C add      7FD7F214   1     A         gr30=gr23,gr30
    0| 000840 add      7D6BD214   1     A         gr11=gr11,gr26
    0| 000844 andi.    71170003   1     RN4_R     gr23,cr0=gr8,0,0x3
    0| 000848 cmpdi    2F390000   1     C8        cr6=gr25,0
  117|                              CL.113:
  119| 00084C or       7D284B78   1     LR        gr8=gr9
  120| 000850 or       7FDCF378   1     LR        gr28=gr30
  119| 000854 or       7D5A5378   1     LR        gr26=gr10
  120| 000858 or       7D785B78   1     LR        gr24=gr11
    0| 00085C bc       41820040   1     BT        CL.694,cr0,0x4/eq,taken=50%(0,0)
    0| 000860 mtspr    7EE903A6   1     LCTR      ctr=gr23
  119| 000864 lfdux    7C0824EE   1     LFDU      fp0,gr8=w3dd(gr8,gr4,0)
  120| 000868 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
    0| 00086C bc       42400024   1     BCF       ctr=CL.843,taken=0%(0,100)
    0| 000870 ori      60210000   1     XNOP      
    0|                              CL.844:
  119| 000874 lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  119| 000878 stfdux   7C1A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp0
    0| 00087C fmr      FC000890   1     LRFL      fp0=fp1
  120| 000880 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  120| 000884 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
    0| 000888 fmr      FC001090   1     LRFL      fp0=fp2
    0| 00088C bc       4200FFE8   1     BCT       ctr=CL.844,taken=100%(100,0)
    0|                              CL.843:
  119| 000890 stfdux   7C1A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp0
  120| 000894 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
    0| 000898 bc       419A0090   1     BT        CL.695,cr6,0x4/eq,taken=20%(20,80)
    0|                              CL.694:
  119| 00089C lfdux    7C2824EE   1     LFDU      fp1,gr8=w3dd(gr8,gr4,0)
  120| 0008A0 lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
    0| 0008A4 mtspr    7F2903A6   1     LCTR      ctr=gr25
  119| 0008A8 lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  119| 0008AC stfdux   7C3A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp1
  120| 0008B0 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  119| 0008B4 lfdux    7C6824EE   1     LFDU      fp3,gr8=w3dd(gr8,gr4,0)
  119| 0008B8 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
  119| 0008BC lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  119| 0008C0 stfdux   7C7A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp3
  119| 0008C4 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
    0| 0008C8 bc       42400048   1     BCF       ctr=CL.845,taken=0%(0,100)
    0|                              CL.846:
  119| 0008CC lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  120| 0008D0 lfdux    7C7C2CEE   1     LFDU      fp3,gr28=w3de(gr28,gr5,0)
  120| 0008D4 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  119| 0008D8 lfdux    7C8824EE   1     LFDU      fp4,gr8=w3dd(gr8,gr4,0)
  120| 0008DC lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
  120| 0008E0 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
  120| 0008E4 stfdux   7C7835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp3
  119| 0008E8 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
  119| 0008EC lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  120| 0008F0 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  120| 0008F4 lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
  119| 0008F8 stfdux   7C9A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp4
  119| 0008FC lfdux    7C6824EE   1     LFDU      fp3,gr8=w3dd(gr8,gr4,0)
  120| 000900 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  119| 000904 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
  119| 000908 stfdux   7C7A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp3
    0| 00090C bc       4200FFC0   1     BCT       ctr=CL.846,taken=100%(100,0)
    0|                              CL.845:
  120| 000910 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  120| 000914 lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
  120| 000918 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
  120| 00091C lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  120| 000920 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  120| 000924 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
    0|                              CL.695:
  122| 000928 addi     38E70001   1     AI        gr7=gr7,1
    0| 00092C add      7D29EA14   1     A         gr9=gr9,gr29
  122| 000930 cmpld    7FA70040   1     CL8       cr7=gr7,gr0
    0| 000934 add      7D4AFA14   1     A         gr10=gr10,gr31
    0| 000938 add      7FDBF214   1     A         gr30=gr27,gr30
    0| 00093C add      7D6B6214   1     A         gr11=gr11,gr12
  122| 000940 bc       419CFF0C   1     BT        CL.113,cr7,0x8/llt,taken=80%(80,20)
  122|                              CL.112:
  123| 000944 ld       E8620000   1     L8        gr3=.&&N&&config(gr2,0)
  123| 000948 lwa      E9030026   1     L4A       gr8=<s177:d36:l4>(gr3,36)
  123| 00094C cmpwi    2C080001   1     C4        cr0=gr8,1
  123| 000950 bc       40810390   1     BF        CL.27,cr0,0x2/gt,taken=50%(0,0)
   82| 000954 sradi    7D031674   1     SRA8CA    gr3,ca=gr8,2
   82| 000958 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  124| 00095C rldicr   78651764   1     SLL8      gr5=gr3,2
  124| 000960 subf     7CC54051   1     S_R       gr6,cr0=gr8,gr5
  124| 000964 bc       40810118   1     BF        CL.196,cr0,0x2/gt,taken=50%(0,0)
  127| 000968 ld       EAE20000   1     L8        gr23=.&&N&&grid(gr2,0)
  127| 00096C ld       EAA20000   1     L8        gr21=.&&N&field(gr2,0)
  127| 000970 ld       EA820000   1     L8        gr20=.&&N&scratch(gr2,0)
  124| 000974 addi     38E00000   1     LI        gr7=0
  127| 000978 lwa      E9370012   1     L4A       gr9=<s183:d16:l4>(gr23,16)
  126| 00097C lwa      E9570002   1     L4A       gr10=<s183:d0:l4>(gr23,0)
  127| 000980 ld       EAD50868   1     L8        gr22=<s39:d2152:l8>(gr21,2152)
  127| 000984 ld       E8750898   1     L8        gr3=<s39:d2200:l8>(gr21,2200)
  127| 000988 ld       E9750880   1     L8        gr11=<s39:d2176:l8>(gr21,2176)
  127| 00098C ld       E99408F0   1     L8        gr12=<s95:d2288:l8>(gr20,2288)
  127| 000990 ld       EBF40908   1     L8        gr31=<s95:d2312:l8>(gr20,2312)
  127| 000994 ld       E8940920   1     L8        gr4=<s95:d2336:l8>(gr20,2336)
  127| 000998 ld       EBD408A8   1     L8        gr30=<s95:d2216:l8>(gr20,2216)
  127| 00099C ld       EBB50820   1     L8        gr29=<s39:d2080:l8>(gr21,2080)
  126| 0009A0 lwa      EB970006   1     L4A       gr28=<s183:d4:l4>(gr23,4)
  127| 0009A4 ld       EB750838   1     L8        gr27=<s39:d2104:l8>(gr21,2104)
  127| 0009A8 ld       EB5408C0   1     L8        gr26=<s95:d2240:l8>(gr20,2240)
  127| 0009AC ld       EB350850   1     L8        gr25=<s39:d2128:l8>(gr21,2128)
  127| 0009B0 ld       EB1408D8   1     L8        gr24=<s95:d2264:l8>(gr20,2264)
    0| 0009B4 bc       4085058C   1     BF        CL.499,cr1,0x2/gt,taken=50%(0,0)
    0| 0009B8 ld       EA6101B0   1     L8        gr19=#SPILL33(gr1,432)
    0| 0009BC mulld    7EE351D2   1     M         gr23=gr3,gr10
    0| 0009C0 mulld    7ED649D2   1     M         gr22=gr22,gr9
    0| 0009C4 mulld    7EA451D2   1     M         gr21=gr4,gr10
    0| 0009C8 mulld    7E93F9D2   1     M         gr20=gr19,gr31
    0| 0009CC mulld    7D2961D2   1     M         gr9=gr9,gr12
    0| 0009D0 mulld    7D8B99D2   1     M         gr12=gr11,gr19
    0| 0009D4 subf     7FC4F050   1     S         gr30=gr30,gr4
    0| 0009D8 subf     7FA3E850   1     S         gr29=gr29,gr3
    0| 0009DC add      7FDAF214   1     A         gr30=gr26,gr30
    0| 0009E0 add      7F56BA14   1     A         gr26=gr22,gr23
    0| 0009E4 add      7EF4AA14   1     A         gr23=gr20,gr21
    0| 0009E8 add      7D29F214   1     A         gr9=gr9,gr30
    0| 0009EC add      7FDBEA14   1     A         gr30=gr27,gr29
    0| 0009F0 add      7D8CD214   1     A         gr12=gr12,gr26
    0| 0009F4 subfic   23AA0001   1     SFI       gr29=1,gr10,ca"
    0| 0009F8 add      7D29BA14   1     A         gr9=gr9,gr23
    0| 0009FC add      7D8CF214   1     A         gr12=gr12,gr30
    0| 000A00 subf     7D4AE050   1     S         gr10=gr28,gr10
    0| 000A04 add      7FDCEA14   1     A         gr30=gr28,gr29
    0| 000A08 add      7D29C214   1     A         gr9=gr9,gr24
    0| 000A0C add      7D8CCA14   1     A         gr12=gr12,gr25
    0| 000A10 addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
  124|                              CL.191:
  125| 000A14 addi     39400000   1     LI        gr10=0
    0| 000A18 bc       40810050   1     BF        CL.195,cr0,0x2/gt,taken=20%(20,80)
    0| 000A1C or       7D9B6378   1     LR        gr27=gr12
    0| 000A20 or       7D3A4B78   1     LR        gr26=gr9
  125|                              CL.192:
  127| 000A24 or       7F7DDB78   1     LR        gr29=gr27
    0| 000A28 mtspr    7FC903A6   1     LCTR      ctr=gr30
  127| 000A2C lfdux    7C1D1CEE   1     LFDU      fp0,gr29=abun(gr29,gr3,0)
  127| 000A30 or       7F5CD378   1     LR        gr28=gr26
    0| 000A34 bc       4240001C   1     BCF       ctr=CL.847,taken=0%(0,100)
    0| 000A38 ori      60210000   1     XNOP      
    0| 000A3C ori      60210000   1     XNOP      
    0|                              CL.848:
  127| 000A40 lfdux    7C3D1CEE   1     LFDU      fp1,gr29=abun(gr29,gr3,0)
  127| 000A44 stfdux   7C1C25EE   1     STFDU     gr28,w4da(gr28,gr4,0)=fp0
    0| 000A48 fmr      FC000890   1     LRFL      fp0=fp1
    0| 000A4C bc       4200FFF4   1     BCT       ctr=CL.848,taken=100%(100,0)
    0|                              CL.847:
  129| 000A50 addi     394A0001   1     AI        gr10=gr10,1
  127| 000A54 stfdux   7C1C25EE   1     STFDU     gr28,w4da(gr28,gr4,0)=fp0
  129| 000A58 cmpld    7F2A0040   1     CL8       cr6=gr10,gr0
    0| 000A5C add      7F6BDA14   1     A         gr27=gr11,gr27
    0| 000A60 add      7F5AFA14   1     A         gr26=gr26,gr31
  129| 000A64 bc       4198FFC0   1     BT        CL.192,cr6,0x8/llt,taken=80%(80,20)
  129|                              CL.195:
  130| 000A68 addi     38E70001   1     AI        gr7=gr7,1
    0| 000A6C add      7D8CCA14   1     A         gr12=gr12,gr25
  130| 000A70 cmpd     7F263800   1     C8        cr6=gr6,gr7
    0| 000A74 add      7D29C214   1     A         gr9=gr9,gr24
  130| 000A78 bc       4199FF9C   1     BT        CL.191,cr6,0x2/gt,taken=80%(80,20)
  130|                              CL.196:
  124| 000A7C cmpd     7C283000   1     C8        cr0=gr8,gr6
  124| 000A80 bc       40810260   1     BF        CL.27,cr0,0x2/gt,taken=50%(0,0)
  127| 000A84 ld       EB220000   1     L8        gr25=.&&N&&grid(gr2,0)
  127| 000A88 ld       EAE20000   1     L8        gr23=.&&N&field(gr2,0)
  127| 000A8C ld       EAC20000   1     L8        gr22=.&&N&scratch(gr2,0)
  130| 000A90 addi     38A5FFFF   1     AI        gr5=gr5,-1
  124| 000A94 addi     3AA00000   1     LI        gr21=0
  130| 000A98 sradi    7CA51674   1     SRA8CA    gr5,ca=gr5,2
  124| 000A9C std      FAA101D0   1     ST8       #SPILL37(gr1,464)=gr21
  127| 000AA0 lwa      E9790012   1     L4A       gr11=<s183:d16:l4>(gr25,16)
  126| 000AA4 lwa      E9390002   1     L4A       gr9=<s183:d0:l4>(gr25,0)
  127| 000AA8 ld       EB170868   1     L8        gr24=<s39:d2152:l8>(gr23,2152)
  127| 000AAC ld       E8770898   1     L8        gr3=<s39:d2200:l8>(gr23,2200)
  127| 000AB0 ld       EBD70880   1     L8        gr30=<s39:d2176:l8>(gr23,2176)
  127| 000AB4 ld       E99608F0   1     L8        gr12=<s95:d2288:l8>(gr22,2288)
  127| 000AB8 ld       EBB60908   1     L8        gr29=<s95:d2312:l8>(gr22,2312)
  127| 000ABC ld       E8960920   1     L8        gr4=<s95:d2336:l8>(gr22,2336)
  127| 000AC0 ld       EBF608A8   1     L8        gr31=<s95:d2216:l8>(gr22,2216)
  127| 000AC4 ld       E8F70850   1     L8        gr7=<s39:d2128:l8>(gr23,2128)
  127| 000AC8 ld       E91608D8   1     L8        gr8=<s95:d2264:l8>(gr22,2264)
  127| 000ACC ld       EB970820   1     L8        gr28=<s39:d2080:l8>(gr23,2080)
  126| 000AD0 lwa      E9590006   1     L4A       gr10=<s183:d4:l4>(gr25,4)
  127| 000AD4 ld       EB770838   1     L8        gr27=<s39:d2104:l8>(gr23,2104)
  127| 000AD8 ld       EB5608C0   1     L8        gr26=<s95:d2240:l8>(gr22,2240)
  130| 000ADC addze    7CA50194   1     ADDE      gr5,ca=gr5,0,ca
    0| 000AE0 bc       40850200   1     BF        CL.27,cr1,0x2/gt,taken=50%(0,0)
    0| 000AE4 ld       EA8101B0   1     L8        gr20=#SPILL33(gr1,432)
    0| 000AE8 mulld    7F2349D2   1     M         gr25=gr3,gr9
    0| 000AEC mulld    7F0BC1D2   1     M         gr24=gr11,gr24
    0| 000AF0 mulld    7EE449D2   1     M         gr23=gr4,gr9
    0| 000AF4 mulld    7ED4E9D2   1     M         gr22=gr20,gr29
    0| 000AF8 mulld    7D6B61D2   1     M         gr11=gr11,gr12
    0| 000AFC mulld    7D94F1D2   1     M         gr12=gr20,gr30
    0| 000B00 subf     7FE4F850   1     S         gr31=gr31,gr4
    0| 000B04 subf     7F83E050   1     S         gr28=gr28,gr3
    0| 000B08 add      7F5AFA14   1     A         gr26=gr26,gr31
    0| 000B0C add      7F18CA14   1     A         gr24=gr24,gr25
    0| 000B10 mulld    7FE641D2   1     M         gr31=gr6,gr8
    0| 000B14 mulld    7F2639D2   1     M         gr25=gr6,gr7
    0| 000B18 add      7EF6BA14   1     A         gr23=gr22,gr23
    0| 000B1C add      7D6BD214   1     A         gr11=gr11,gr26
    0| 000B20 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 000B24 add      7D8CC214   1     A         gr12=gr12,gr24
    0| 000B28 subfic   20C90001   1     SFI       gr6=1,gr9,ca"
    0| 000B2C rldicr   79131764   1     SLL8      gr19=gr8,2
    0| 000B30 add      7F4BBA14   1     A         gr26=gr11,gr23
    0| 000B34 std      FA6101D8   1     ST8       #SPILL38(gr1,472)=gr19
    0| 000B38 add      7D6CE214   1     A         gr11=gr12,gr28
    0| 000B3C subf     7F895050   1     S         gr28=gr10,gr9
    0| 000B40 add      7CC65214   1     A         gr6=gr6,gr10
    0| 000B44 rldicr   78F21764   1     SLL8      gr18=gr7,2
    0| 000B48 rldicr   791B0FA4   1     SLL8      gr27=gr8,1
    0| 000B4C std      FA4101E0   1     ST8       #SPILL39(gr1,480)=gr18
    0| 000B50 add      7D9AFA14   1     A         gr12=gr26,gr31
    0| 000B54 subf     7FE89850   1     S         gr31=gr19,gr8
    0| 000B58 add      7D2BCA14   1     A         gr9=gr11,gr25
    0| 000B5C addic.   379C0001   1     AI_R      gr28,cr0=gr28,1,ca"
    0| 000B60 rldicr   78EA0FA4   1     SLL8      gr10=gr7,1
    0| 000B64 subf     7D679050   1     S         gr11=gr18,gr7
    0| 000B68 rldicl   78DCF842   1     SRL8      gr28=gr6,1
    0| 000B6C add      7E2CDA14   1     A         gr17=gr12,gr27
    0| 000B70 add      7E0CFA14   1     A         gr16=gr12,gr31
    0| 000B74 std      FA2101E8   1     ST8       #SPILL40(gr1,488)=gr17
    0| 000B78 std      FA0101F0   1     ST8       #SPILL41(gr1,496)=gr16
    0| 000B7C add      7EC74A14   1     A         gr22=gr7,gr9
    0| 000B80 addi     38E50001   1     AI        gr7=gr5,1
    0| 000B84 add      7F6C9A14   1     A         gr27=gr12,gr19
    0| 000B88 std      F8E101F8   1     ST8       #SPILL42(gr1,504)=gr7
    0| 000B8C add      7F486214   1     A         gr26=gr8,gr12
    0| 000B90 add      7F295214   1     A         gr25=gr9,gr10
    0| 000B94 add      7F095A14   1     A         gr24=gr9,gr11
    0| 000B98 add      7EE99214   1     A         gr23=gr9,gr18
    0| 000B9C mcrf     4F000000   1     LRCR      cr6=cr0
    0| 000BA0 andi.    70C50001   1     RN4_R     gr5,cr0=gr6,0,0x1
    0| 000BA4 cmpdi    2EBC0000   1     C8        cr5=gr28,0
  124|                              CL.117:
  125| 000BA8 addi     38A00000   1     LI        gr5=0
    0| 000BAC bc       409900E4   1     BF        CL.118,cr6,0x2/gt,taken=20%(20,80)
    0| 000BB0 or       7F15C378   1     LR        gr21=gr24
    0| 000BB4 or       7EF4BB78   1     LR        gr20=gr23
    0| 000BB8 or       7ED3B378   1     LR        gr19=gr22
    0| 000BBC or       7F32CB78   1     LR        gr18=gr25
    0| 000BC0 ld       EA2101F0   1     L8        gr17=#SPILL41(gr1,496)
    0| 000BC4 or       7F70DB78   1     LR        gr16=gr27
    0| 000BC8 or       7F4FD378   1     LR        gr15=gr26
    0| 000BCC ld       E9C101E8   1     L8        gr14=#SPILL40(gr1,488)
    0| 000BD0 ori      60210000   1     XNOP      
  125|                              CL.119:
  127| 000BD4 or       7E669B78   1     LR        gr6=gr19
  127| 000BD8 or       7E479378   1     LR        gr7=gr18
  127| 000BDC or       7EA8AB78   1     LR        gr8=gr21
  127| 000BE0 or       7E89A378   1     LR        gr9=gr20
  127| 000BE4 or       7DEA7B78   1     LR        gr10=gr15
  127| 000BE8 or       7DCB7378   1     LR        gr11=gr14
  127| 000BEC or       7E2C8B78   1     LR        gr12=gr17
  127| 000BF0 or       7E1F8378   1     LR        gr31=gr16
    0| 000BF4 mtspr    7F8903A6   1     LCTR      ctr=gr28
    0| 000BF8 bc       41820028   1     BT        CL.811,cr0,0x4/eq,taken=50%(0,0)
  127| 000BFC lfdux    7C061CEE   1     LFDU      fp0,gr6=abun(gr6,gr3,0)
  127| 000C00 lfdux    7C271CEE   1     LFDU      fp1,gr7=abun(gr7,gr3,0)
  127| 000C04 lfdux    7C481CEE   1     LFDU      fp2,gr8=abun(gr8,gr3,0)
  127| 000C08 lfdux    7C691CEE   1     LFDU      fp3,gr9=abun(gr9,gr3,0)
  127| 000C0C stfdux   7C0A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp0
  127| 000C10 stfdux   7C2B25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp1
  127| 000C14 stfdux   7C4C25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp2
  127| 000C18 stfdux   7C7F25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp3
    0| 000C1C bc       41960048   1     BT        CL.701,cr5,0x4/eq,taken=20%(20,80)
    0|                              CL.811:
  127| 000C20 lfdux    7C061CEE   1     LFDU      fp0,gr6=abun(gr6,gr3,0)
  127| 000C24 lfdux    7C271CEE   1     LFDU      fp1,gr7=abun(gr7,gr3,0)
  127| 000C28 lfdux    7C481CEE   1     LFDU      fp2,gr8=abun(gr8,gr3,0)
  127| 000C2C lfdux    7C691CEE   1     LFDU      fp3,gr9=abun(gr9,gr3,0)
  127| 000C30 lfdux    7C861CEE   1     LFDU      fp4,gr6=abun(gr6,gr3,0)
  127| 000C34 lfdux    7CA71CEE   1     LFDU      fp5,gr7=abun(gr7,gr3,0)
  127| 000C38 lfdux    7CC81CEE   1     LFDU      fp6,gr8=abun(gr8,gr3,0)
  127| 000C3C lfdux    7CE91CEE   1     LFDU      fp7,gr9=abun(gr9,gr3,0)
  127| 000C40 stfdux   7C0A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp0
  127| 000C44 stfdux   7C2B25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp1
  127| 000C48 stfdux   7C4C25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp2
  127| 000C4C stfdux   7C7F25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp3
  127| 000C50 stfdux   7C8A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp4
  127| 000C54 stfdux   7CAB25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp5
  127| 000C58 stfdux   7CCC25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp6
  127| 000C5C stfdux   7CFF25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp7
    0| 000C60 bc       4200FFC0   1     BCT       ctr=CL.811,taken=100%(100,0)
    0|                              CL.701:
  129| 000C64 addi     38A50001   1     AI        gr5=gr5,1
    0| 000C68 add      7EB5F214   1     A         gr21=gr21,gr30
  129| 000C6C cmpld    7FA50040   1     CL8       cr7=gr5,gr0
    0| 000C70 add      7E94F214   1     A         gr20=gr20,gr30
    0| 000C74 add      7E73F214   1     A         gr19=gr19,gr30
    0| 000C78 add      7E52F214   1     A         gr18=gr18,gr30
    0| 000C7C add      7E31EA14   1     A         gr17=gr17,gr29
    0| 000C80 add      7E10EA14   1     A         gr16=gr16,gr29
    0| 000C84 add      7DEFEA14   1     A         gr15=gr15,gr29
    0| 000C88 add      7DCEEA14   1     A         gr14=gr14,gr29
  129| 000C8C bc       419CFF48   1     BT        CL.119,cr7,0x8/llt,taken=80%(80,20)
  129|                              CL.118:
  130| 000C90 ld       E8A101D0   1     L8        gr5=#SPILL37(gr1,464)
    0| 000C94 ld       E8C101D8   1     L8        gr6=#SPILL38(gr1,472)
    0| 000C98 ld       E8E101E8   1     L8        gr7=#SPILL40(gr1,488)
  130| 000C9C ld       E90101F8   1     L8        gr8=#SPILL42(gr1,504)
    0| 000CA0 ld       E92101F0   1     L8        gr9=#SPILL41(gr1,496)
    0| 000CA4 ld       E94101E0   1     L8        gr10=#SPILL39(gr1,480)
  130| 000CA8 addi     38A50001   1     AI        gr5=gr5,1
    0| 000CAC add      7F66DA14   1     A         gr27=gr6,gr27
  130| 000CB0 std      F8A101D0   1     ST8       #SPILL37(gr1,464)=gr5
    0| 000CB4 add      7CE63A14   1     A         gr7=gr6,gr7
  130| 000CB8 cmpld    7FA54040   1     CL8       cr7=gr5,gr8
    0| 000CBC add      7D264A14   1     A         gr9=gr6,gr9
    0| 000CC0 std      F8E101E8   1     ST8       #SPILL40(gr1,488)=gr7
    0| 000CC4 std      F92101F0   1     ST8       #SPILL41(gr1,496)=gr9
    0| 000CC8 add      7F46D214   1     A         gr26=gr6,gr26
    0| 000CCC add      7F2ACA14   1     A         gr25=gr10,gr25
    0| 000CD0 add      7F0AC214   1     A         gr24=gr10,gr24
    0| 000CD4 add      7EEABA14   1     A         gr23=gr10,gr23
    0| 000CD8 add      7ECAB214   1     A         gr22=gr10,gr22
  130| 000CDC bc       419CFECC   1     BT        CL.117,cr7,0x8/llt,taken=80%(80,20)
  131|                              CL.27:
  132| 000CE0 ld       E8820000   1     L8        gr4=.&&N&&config(gr2,0)
  132| 000CE4 lwz      80640018   1     L4Z       gr3=<s177:d24:l4>(gr4,24)
  132| 000CE8 cmpdi    2C230000   1     C8        cr0=gr3,0
  132| 000CEC bc       41820148   1     BT        CL.40,cr0,0x4/eq,taken=50%(0,0)
  133| 000CF0 bc       40850144   1     BF        CL.40,cr1,0x2/gt,taken=50%(0,0)
  134| 000CF4 ld       EB620000   1     L8        gr27=.&&N&&grid(gr2,0)
  135| 000CF8 ld       EB420000   1     L8        gr26=.&&N&field(gr2,0)
  135| 000CFC ld       EB220000   1     L8        gr25=.&&N&scratch(gr2,0)
  134| 000D00 lwa      E87B0002   1     L4A       gr3=<s183:d0:l4>(gr27,0)
  134| 000D04 lwa      E89B0006   1     L4A       gr4=<s183:d4:l4>(gr27,4)
  135| 000D08 lwa      E91B0012   1     L4A       gr8=<s183:d16:l4>(gr27,16)
  135| 000D0C ld       E93A0440   1     L8        gr9=<s39:d1088:l8>(gr26,1088)
  135| 000D10 ld       E95A0458   1     L8        gr10=<s39:d1112:l8>(gr26,1112)
  135| 000D14 ld       E8BA0470   1     L8        gr5=<s39:d1136:l8>(gr26,1136)
  135| 000D18 ld       E9790870   1     L8        gr11=<s95:d2160:l8>(gr25,2160)
  134| 000D1C subf     7CE32050   1     S         gr7=gr4,gr3
  135| 000D20 ld       E9990888   1     L8        gr12=<s95:d2184:l8>(gr25,2184)
  134| 000D24 addic.   34E70001   1     AI_R      gr7,cr0=gr7,1,ca"
  135| 000D28 ld       E8D908A0   1     L8        gr6=<s95:d2208:l8>(gr25,2208)
  135| 000D2C ld       EBFA0410   1     L8        gr31=<s39:d1040:l8>(gr26,1040)
  135| 000D30 ld       EBD90840   1     L8        gr30=<s95:d2112:l8>(gr25,2112)
  135| 000D34 ld       EBBA0428   1     L8        gr29=<s39:d1064:l8>(gr26,1064)
  135| 000D38 ld       EB990858   1     L8        gr28=<s95:d2136:l8>(gr25,2136)
  133| 000D3C addi     38E00000   1     LI        gr7=0
    0| 000D40 bc       408100F4   1     BF        CL.40,cr0,0x2/gt,taken=50%(0,0)
    0| 000D44 ld       EB0101B0   1     L8        gr24=#SPILL33(gr1,432)
    0| 000D48 mulld    7F6329D2   1     M         gr27=gr3,gr5
    0| 000D4C mulld    7D2941D2   1     M         gr9=gr9,gr8
    0| 000D50 mulld    7F4AC1D2   1     M         gr26=gr10,gr24
    0| 000D54 mulld    7D0859D2   1     M         gr8=gr8,gr11
    0| 000D58 mulld    7D6CC1D2   1     M         gr11=gr12,gr24
    0| 000D5C mulld    7F2331D2   1     M         gr25=gr3,gr6
    0| 000D60 subfic   20630001   1     SFI       gr3=1,gr3,ca"
    0| 000D64 subf     7FE5F850   1     S         gr31=gr31,gr5
    0| 000D68 subf     7FC6F050   1     S         gr30=gr30,gr6
    0| 000D6C add      7C632214   1     A         gr3=gr3,gr4
    0| 000D70 add      7C9DFA14   1     A         gr4=gr29,gr31
    0| 000D74 add      7FFCF214   1     A         gr31=gr28,gr30
    0| 000D78 add      7D29DA14   1     A         gr9=gr9,gr27
    0| 000D7C add      7C84D214   1     A         gr4=gr4,gr26
    0| 000D80 add      7D085A14   1     A         gr8=gr8,gr11
    0| 000D84 add      7D79FA14   1     A         gr11=gr25,gr31
    0| 000D88 rldicl   787FE8C2   1     SRL8      gr31=gr3,3
    0| 000D8C add      7C844A14   1     A         gr4=gr4,gr9
    0| 000D90 add      7D085A14   1     A         gr8=gr8,gr11
    0| 000D94 andi.    706B0007   1     RN4_R     gr11,cr0=gr3,0,0x7
    0| 000D98 cmpdi    2CBF0000   1     C8        cr1=gr31,0
  133|                              CL.123:
  135| 000D9C or       7C832378   1     LR        gr3=gr4
  135| 000DA0 or       7D094378   1     LR        gr9=gr8
    0| 000DA4 bc       41820034   1     BT        CL.703,cr0,0x4/eq,taken=50%(0,0)
    0| 000DA8 mtspr    7D6903A6   1     LCTR      ctr=gr11
  135| 000DAC lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
    0| 000DB0 bc       42400020   1     BCF       ctr=CL.850,taken=0%(0,100)
    0| 000DB4 ori      60210000   1     XNOP      
    0| 000DB8 ori      60210000   1     XNOP      
    0| 000DBC ori      60210000   1     XNOP      
    0|                              CL.851:
  135| 000DC0 lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  135| 000DC4 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
    0| 000DC8 fmr      FC000890   1     LRFL      fp0=fp1
    0| 000DCC bc       4200FFF4   1     BCT       ctr=CL.851,taken=100%(100,0)
    0|                              CL.850:
  135| 000DD0 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
    0| 000DD4 bc       4186004C   1     BT        CL.704,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.703:
    0| 000DD8 mtspr    7FE903A6   1     LCTR      ctr=gr31
    0|                              CL.852:
  135| 000DDC lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
  135| 000DE0 lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  135| 000DE4 lfdux    7C432CEE   1     LFDU      fp2,gr3=er(gr3,gr5,0)
  135| 000DE8 lfdux    7C632CEE   1     LFDU      fp3,gr3=er(gr3,gr5,0)
  135| 000DEC stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
  135| 000DF0 stfdux   7C2935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp1
  135| 000DF4 lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
  135| 000DF8 stfdux   7C4935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp2
  135| 000DFC lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  135| 000E00 stfdux   7C6935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp3
  135| 000E04 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
  135| 000E08 lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
  135| 000E0C stfdux   7C2935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp1
  135| 000E10 lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  135| 000E14 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
  135| 000E18 stfdux   7C2935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp1
    0| 000E1C bc       4200FFC0   1     BCT       ctr=CL.852,taken=100%(100,0)
    0|                              CL.704:
  137| 000E20 addi     38E70001   1     AI        gr7=gr7,1
    0| 000E24 add      7C845214   1     A         gr4=gr4,gr10
  137| 000E28 cmpld    7F270040   1     CL8       cr6=gr7,gr0
    0| 000E2C add      7D086214   1     A         gr8=gr8,gr12
  137| 000E30 bc       4198FF6C   1     BT        CL.123,cr6,0x8/llt,taken=80%(80,20)
  138|                              CL.40:
  140| 000E34 ld       E8620000   1     L8        gr3=.&&N&&root(gr2,0)
  140| 000E38 addi     38000002   1     LI        gr0=2
  140| 000E3C stw      900301E0   1     ST4Z      <s201:d480:l4>(gr3,480)=gr0
  191|                              CL.49:
    0| 000E40 ld       E8820000   1     L8        gr4=.&&N&&bndry(gr2,0)
  196| 000E44 addi     38600000   1     LI        gr3=0
  197| 000E48 addi     38E00000   1     LI        gr7=0
  197| 000E4C addi     38040324   1     AI        gr0=gr4,804
    0| 000E50 ori      60210000   1     XNOP      
  196|                              CL.141:
  197| 000E54 rldicr   78641764   1     SLL8      gr4=gr3,2
  197| 000E58 subf     7C832050   1     S         gr4=gr4,gr3
  200| 000E5C addi     38630001   1     AI        gr3=gr3,1
  197| 000E60 addi     38A40001   1     AI        gr5=gr4,1
  197| 000E64 addi     38C40002   1     AI        gr6=gr4,2
  197| 000E68 rldic    78842EC8   1     RN8       gr4=gr4,5,0x1FFFFFFFE0
  197| 000E6C rldic    78A52EC8   1     RN8       gr5=gr5,5,0x1FFFFFFFE0
  197| 000E70 rldic    78C62EC8   1     RN8       gr6=gr6,5,0x1FFFFFFFE0
  197| 000E74 add      7C802214   1     A         gr4=gr0,gr4
  197| 000E78 add      7CA02A14   1     A         gr5=gr0,gr5
  197| 000E7C stw      90E40004   1     ST4Z      bvstat[](gr4,4)=gr7
  197| 000E80 stw      90E50004   1     ST4Z      bvstat[](gr5,4)=gr7
  197| 000E84 add      7CC03214   1     A         gr6=gr0,gr6
  200| 000E88 cmpldi   28230001   1     CL8       cr0=gr3,1
  197| 000E8C stw      90E60004   1     ST4Z      bvstat[](gr6,4)=gr7
  197| 000E90 stw      90E40008   1     ST4Z      bvstat[](gr4,8)=gr7
  197| 000E94 stw      90E50008   1     ST4Z      bvstat[](gr5,8)=gr7
  197| 000E98 stw      90E60008   1     ST4Z      bvstat[](gr6,8)=gr7
  197| 000E9C stw      90E4000C   1     ST4Z      bvstat[](gr4,12)=gr7
  197| 000EA0 stw      90E5000C   1     ST4Z      bvstat[](gr5,12)=gr7
  197| 000EA4 stw      90E6000C   1     ST4Z      bvstat[](gr6,12)=gr7
  197| 000EA8 stw      90E40010   1     ST4Z      bvstat[](gr4,16)=gr7
  197| 000EAC stw      90E50010   1     ST4Z      bvstat[](gr5,16)=gr7
  197| 000EB0 stw      90E60010   1     ST4Z      bvstat[](gr6,16)=gr7
  197| 000EB4 stw      90E40014   1     ST4Z      bvstat[](gr4,20)=gr7
  197| 000EB8 stw      90E50014   1     ST4Z      bvstat[](gr5,20)=gr7
  197| 000EBC stw      90E60014   1     ST4Z      bvstat[](gr6,20)=gr7
  197| 000EC0 stwu     94E40018   1     ST4U      gr4,bvstat[](gr4,24)=gr7
  197| 000EC4 stwu     94E50018   1     ST4U      gr5,bvstat[](gr5,24)=gr7
  197| 000EC8 stwu     94E60018   1     ST4U      gr6,bvstat[](gr6,24)=gr7
  200| 000ECC bc       4180FF88   1     BT        CL.141,cr0,0x8/llt,taken=80%(80,20)
  227| 000ED0 ld       E9810350   1     L8        gr12=#stack(gr1,848)
  227| 000ED4 lfd      CBE10338   1     LFL       fp31=#stack(gr1,824)
  227| 000ED8 lfd      CBC10330   1     LFL       fp30=#stack(gr1,816)
  227| 000EDC lfd      CBA10328   1     LFL       fp29=#stack(gr1,808)
  227| 000EE0 lfd      CB810320   1     LFL       fp28=#stack(gr1,800)
  227| 000EE4 lfd      CB610318   1     LFL       fp27=#stack(gr1,792)
  227| 000EE8 mtspr    7D8803A6   1     LLR       lr=gr12
  227| 000EEC lfd      CB410310   1     LFL       fp26=#stack(gr1,784)
  227| 000EF0 addi     38210340   1     AI        gr1=gr1,832
  227| 000EF4 ld       E9C1FF40   1     L8        gr14=#stack(gr1,-192)
  227| 000EF8 ld       E9E1FF48   1     L8        gr15=#stack(gr1,-184)
  227| 000EFC ld       EA01FF50   1     L8        gr16=#stack(gr1,-176)
  227| 000F00 ld       EA21FF58   1     L8        gr17=#stack(gr1,-168)
  227| 000F04 ld       EA41FF60   1     L8        gr18=#stack(gr1,-160)
  227| 000F08 ld       EA61FF68   1     L8        gr19=#stack(gr1,-152)
  227| 000F0C ld       EA81FF70   1     L8        gr20=#stack(gr1,-144)
  227| 000F10 ld       EAA1FF78   1     L8        gr21=#stack(gr1,-136)
  227| 000F14 ld       EAC1FF80   1     L8        gr22=#stack(gr1,-128)
  227| 000F18 ld       EAE1FF88   1     L8        gr23=#stack(gr1,-120)
  227| 000F1C ld       EB01FF90   1     L8        gr24=#stack(gr1,-112)
  227| 000F20 ld       EB21FF98   1     L8        gr25=#stack(gr1,-104)
  227| 000F24 ld       EB41FFA0   1     L8        gr26=#stack(gr1,-96)
  227| 000F28 ld       EB61FFA8   1     L8        gr27=#stack(gr1,-88)
  227| 000F2C ld       EB81FFB0   1     L8        gr28=#stack(gr1,-80)
  227| 000F30 ld       EBA1FFB8   1     L8        gr29=#stack(gr1,-72)
  227| 000F34 ld       EBC1FFC0   1     L8        gr30=#stack(gr1,-64)
  227| 000F38 ld       EBE1FFC8   1     L8        gr31=#stack(gr1,-56)
  227| 000F3C bclr     4E800020   1     BA        lr
  129|                              CL.499:
    0| 000F40 mtspr    7CC903A6   1     LCTR      ctr=gr6
  129|                              CL.459:
  130| 000F44 addi     38E70001   1     AI        gr7=gr7,1
  130| 000F48 cmpd     7C273000   1     C8        cr0=gr7,gr6
  130| 000F4C bc       4100FFF8   1     BCTT      ctr=CL.459,cr0,0x1/lt,taken=80%(80,20)
    0| 000F50 b        4BFFFB2C   1     B         CL.196,-1
  145|                              CL.16:
  145| 000F54 cmpwi    2C000002   1     C4        cr0=gr0,2
  145| 000F58 bc       4082FEE8   1     BF        CL.49,cr0,0x4/eq,taken=50%(0,0)
  151| 000F5C ld       E9620000   1     L8        gr11=.&&N&&config(gr2,0)
    0| 000F60 ld       E9820000   1     L8        gr12=.&&N&field(gr2,0)
    0| 000F64 ld       EBE20000   1     L8        gr31=.&&N&scratch(gr2,0)
  151| 000F68 lwz      800B0018   1     L4Z       gr0=<s177:d24:l4>(gr11,24)
    0| 000F6C ld       E88C0000   1     L8        gr4=<s39:d0:l8>(gr12,0)
    0| 000F70 ld       E91F0498   1     L8        gr8=<s95:d1176:l8>(gr31,1176)
    0| 000F74 ld       E93F0500   1     L8        gr9=<s95:d1280:l8>(gr31,1280)
    0| 000F78 ld       E95F0568   1     L8        gr10=<s95:d1384:l8>(gr31,1384)
    0| 000F7C ld       E87F05D0   1     L8        gr3=<s95:d1488:l8>(gr31,1488)
  151| 000F80 cmpdi    2C200000   1     C8        cr0=gr0,0
    0| 000F84 ld       E8BF0638   1     L8        gr5=<s95:d1592:l8>(gr31,1592)
    0| 000F88 ld       E8DF0708   1     L8        gr6=<s95:d1800:l8>(gr31,1800)
    0| 000F8C ld       E8FF06A0   1     L8        gr7=<s95:d1696:l8>(gr31,1696)
  151| 000F90 bc       41820080   1     BT        CL.52,cr0,0x4/eq,taken=50%(0,0)
  152| 000F94 ld       E80C0410   1     L8        gr0=<s39:d1040:l8>(gr12,1040)
  152| 000F98 ld       E97F0840   1     L8        gr11=<s95:d2112:l8>(gr31,2112)
  152| 000F9C or       7D8E6378   1     LR        gr14=gr12
  152| 000FA0 ld       E98C0820   1     L8        gr12=<s39:d2080:l8>(gr12,2080)
  152| 000FA4 or       7FEFFB78   1     LR        gr15=gr31
  152| 000FA8 ld       EBFF08A8   1     L8        gr31=<s95:d2216:l8>(gr31,2216)
  152| 000FAC std      F8010070   1     ST8       #MX_TEMP1(gr1,112)=gr0
  152| 000FB0 std      F9610078   1     ST8       #MX_TEMP1(gr1,120)=gr11
  152| 000FB4 std      F9810080   1     ST8       #MX_TEMP1(gr1,128)=gr12
  152| 000FB8 std      FBE10088   1     ST8       #MX_TEMP1(gr1,136)=gr31
  152| 000FBC bl       48000001   1     CALL      advx2,12,w3dd",gr3,d",gr4,w3de",gr5,w3dg",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,er",w3dh",abun",w4da",advx2",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  152| 000FC0 ori      60000000   1
  154| 000FC4 ld       E8AF0840   1     L8        gr5=<s95:d2112:l8>(gr15,2112)
  154| 000FC8 ld       E8CE0410   1     L8        gr6=<s39:d1040:l8>(gr14,1040)
  154| 000FCC ld       E8EF08A8   1     L8        gr7=<s95:d2216:l8>(gr15,2216)
  154| 000FD0 ld       E80E0820   1     L8        gr0=<s39:d2080:l8>(gr14,2080)
  154| 000FD4 ld       E86E0000   1     L8        gr3=<s39:d0:l8>(gr14,0)
  154| 000FD8 ld       E88F05D0   1     L8        gr4=<s95:d1488:l8>(gr15,1488)
  154| 000FDC std      F8A10070   1     ST8       #MX_TEMP1(gr1,112)=gr5
  154| 000FE0 std      F8C10078   1     ST8       #MX_TEMP1(gr1,120)=gr6
  154| 000FE4 std      F8E10080   1     ST8       #MX_TEMP1(gr1,128)=gr7
  154| 000FE8 ld       E8AF0708   1     L8        gr5=<s95:d1800:l8>(gr15,1800)
  154| 000FEC ld       E8CF0638   1     L8        gr6=<s95:d1592:l8>(gr15,1592)
  154| 000FF0 ld       E8EF06A0   1     L8        gr7=<s95:d1696:l8>(gr15,1696)
  154| 000FF4 std      F8010088   1     ST8       #MX_TEMP1(gr1,136)=gr0
  154| 000FF8 ld       E90F0498   1     L8        gr8=<s95:d1176:l8>(gr15,1176)
  154| 000FFC ld       E92F0500   1     L8        gr9=<s95:d1280:l8>(gr15,1280)
  154| 001000 ld       E94F0568   1     L8        gr10=<s95:d1384:l8>(gr15,1384)
  154| 001004 bl       48000001   1     CALL      advx1,12,d",gr3,w3dd",gr4,w3dg",gr5,w3de",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,w3dh",er",w4da",abun",advx1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  154| 001008 ori      60000000   1
    0| 00100C b        48000038   1     B         CL.53,-1
  156|                              CL.52:
  157| 001010 bl       48000001   1     CALL      advx2,8,w3dd",gr3,d",gr4,w3de",gr5,w3dg",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,advx2",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  157| 001014 ori      60000000   1
  158| 001018 ld       EBC20000   1     L8        gr30=.&&N&field(gr2,0)
  158| 00101C ld       E89F05D0   1     L8        gr4=<s95:d1488:l8>(gr31,1488)
  158| 001020 ld       E8BF0708   1     L8        gr5=<s95:d1800:l8>(gr31,1800)
  158| 001024 ld       E8DF0638   1     L8        gr6=<s95:d1592:l8>(gr31,1592)
  158| 001028 ld       E8FF06A0   1     L8        gr7=<s95:d1696:l8>(gr31,1696)
  158| 00102C ld       E91F0498   1     L8        gr8=<s95:d1176:l8>(gr31,1176)
  158| 001030 ld       E87E0000   1     L8        gr3=<s39:d0:l8>(gr30,0)
  158| 001034 ld       E93F0500   1     L8        gr9=<s95:d1280:l8>(gr31,1280)
  158| 001038 ld       E95F0568   1     L8        gr10=<s95:d1384:l8>(gr31,1384)
  158| 00103C bl       48000001   1     CALL      advx1,8,d",gr3,w3dd",gr4,w3dg",gr5,w3de",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,advx1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  158| 001040 ori      60000000   1
  159|                              CL.53:
  165| 001044 ld       E8820000   1     L8        gr4=.&&N&&grid(gr2,0)
  165| 001048 lwa      E804000E   1     L4A       gr0=<s183:d12:l4>(gr4,12)
  165| 00104C lwa      E8A4000A   1     L4A       gr5=<s183:d8:l4>(gr4,8)
  165| 001050 subf     7C650050   1     S         gr3=gr0,gr5
  165| 001054 std      F8A10200   1     ST8       #SPILL43(gr1,512)=gr5
  165| 001058 addic.   34030001   1     AI_R      gr0,cr0=gr3,1,ca"
  165| 00105C mcrf     4C800000   1     LRCR      cr1=cr0
  165| 001060 bc       40810224   1     BF        CL.126,cr0,0x2/gt,taken=50%(0,0)
  166| 001064 lwa      E9040002   1     L4A       gr8=<s183:d0:l4>(gr4,0)
  166| 001068 lwa      E9240006   1     L4A       gr9=<s183:d4:l4>(gr4,4)
  168| 00106C ld       EA020000   1     L8        gr16=.&&N&scratch(gr2,0)
  167| 001070 ld       EA220000   1     L8        gr17=.&&N&field(gr2,0)
  167| 001074 lwa      E9440012   1     L4A       gr10=<s183:d16:l4>(gr4,16)
  166| 001078 subf     7CE84850   1     S         gr7=gr9,gr8
  168| 00107C ld       EB900668   1     L8        gr28=<s95:d1640:l8>(gr16,1640)
  166| 001080 addic.   34E70001   1     AI_R      gr7,cr0=gr7,1,ca"
  167| 001084 ld       E8710060   1     L8        gr3=<s39:d96:l8>(gr17,96)
  168| 001088 ld       E9700738   1     L8        gr11=<s95:d1848:l8>(gr16,1848)
  168| 00108C ld       E9900750   1     L8        gr12=<s95:d1872:l8>(gr16,1872)
  167| 001090 ld       EBF10048   1     L8        gr31=<s39:d72:l8>(gr17,72)
  167| 001094 ld       EBD00600   1     L8        gr30=<s95:d1536:l8>(gr16,1536)
  167| 001098 ld       EBB00618   1     L8        gr29=<s95:d1560:l8>(gr16,1560)
  167| 00109C ld       E8900630   1     L8        gr4=<s95:d1584:l8>(gr16,1584)
  168| 0010A0 ld       EB700680   1     L8        gr27=<s95:d1664:l8>(gr16,1664)
  168| 0010A4 ld       E8B00698   1     L8        gr5=<s95:d1688:l8>(gr16,1688)
  168| 0010A8 ld       E8D00768   1     L8        gr6=<s95:d1896:l8>(gr16,1896)
  167| 0010AC ld       EB510030   1     L8        gr26=<s39:d48:l8>(gr17,48)
  165| 0010B0 addi     38E00000   1     LI        gr7=0
  167| 0010B4 ld       EB310000   1     L8        gr25=<s39:d0:l8>(gr17,0)
  167| 0010B8 ld       EB1005D0   1     L8        gr24=<s95:d1488:l8>(gr16,1488)
  168| 0010BC ld       EAF00638   1     L8        gr23=<s95:d1592:l8>(gr16,1592)
  168| 0010C0 ld       EAD00708   1     L8        gr22=<s95:d1800:l8>(gr16,1800)
  167| 0010C4 ld       EAB10018   1     L8        gr21=<s39:d24:l8>(gr17,24)
  167| 0010C8 ld       EA9005E8   1     L8        gr20=<s95:d1512:l8>(gr16,1512)
  168| 0010CC ld       EA700650   1     L8        gr19=<s95:d1616:l8>(gr16,1616)
  168| 0010D0 ld       EA500720   1     L8        gr18=<s95:d1824:l8>(gr16,1824)
    0| 0010D4 bc       408101B0   1     BF        CL.126,cr0,0x2/gt,taken=50%(0,0)
    0| 0010D8 ld       EA010200   1     L8        gr16=#SPILL43(gr1,512)
    0| 0010DC mulld    7F8AE1D2   1     M         gr28=gr10,gr28
    0| 0010E0 mulld    7FCAF1D2   1     M         gr30=gr10,gr30
    0| 0010E4 std      FB810218   1     ST8       #SPILL46(gr1,536)=gr28
    0| 0010E8 mulld    7DD0F9D2   1     M         gr14=gr16,gr31
    0| 0010EC ld       EB810200   1     L8        gr28=#SPILL43(gr1,512)
    0| 0010F0 std      FBC10208   1     ST8       #SPILL44(gr1,520)=gr30
    0| 0010F4 mulld    7E2A59D2   1     M         gr17=gr10,gr11
    0| 0010F8 mulld    7D6C81D2   1     M         gr11=gr12,gr16
    0| 0010FC mulld    7DE341D2   1     M         gr15=gr3,gr8
    0| 001100 std      F9C10210   1     ST8       #SPILL45(gr1,528)=gr14
    0| 001104 mulld    7FC641D2   1     M         gr30=gr6,gr8
    0| 001108 add      7D6B8A14   1     A         gr11=gr11,gr17
    0| 00110C mulld    7DD0D9D2   1     M         gr14=gr16,gr27
    0| 001110 mulld    7E0441D2   1     M         gr16=gr4,gr8
    0| 001114 mulld    7E3CE9D2   1     M         gr17=gr28,gr29
    0| 001118 add      7D6BF214   1     A         gr11=gr11,gr30
    0| 00111C mulld    7FC541D2   1     M         gr30=gr5,gr8
    0| 001120 subfic   21080001   1     SFI       gr8=1,gr8,ca"
    0| 001124 subf     7F84C050   1     S         gr28=gr24,gr4
    0| 001128 add      7F39AA14   1     A         gr25=gr25,gr21
    0| 00112C subf     7F037850   1     S         gr24=gr15,gr3
    0| 001130 add      7D084A14   1     A         gr8=gr8,gr9
    0| 001134 add      7D34E214   1     A         gr9=gr20,gr28
    0| 001138 add      7F98CA14   1     A         gr28=gr24,gr25
    0| 00113C subf     7F26B050   1     S         gr25=gr22,gr6
    0| 001140 ld       EAC10208   1     L8        gr22=#SPILL44(gr1,520)
    0| 001144 ld       EAA10210   1     L8        gr21=#SPILL45(gr1,528)
    0| 001148 ld       EA810218   1     L8        gr20=#SPILL46(gr1,536)
    0| 00114C subf     7EE5B850   1     S         gr23=gr23,gr5
    0| 001150 mulld    7D4AD1D2   1     M         gr10=gr10,gr26
    0| 001154 add      7F53BA14   1     A         gr26=gr19,gr23
    0| 001158 add      7F10B214   1     A         gr24=gr16,gr22
    0| 00115C add      7D298A14   1     A         gr9=gr9,gr17
    0| 001160 add      7F95E214   1     A         gr28=gr21,gr28
    0| 001164 add      7EEEA214   1     A         gr23=gr14,gr20
    0| 001168 add      7FDED214   1     A         gr30=gr30,gr26
    0| 00116C add      7F52CA14   1     A         gr26=gr18,gr25
    0| 001170 rldicl   7919F082   1     SRL8      gr25=gr8,2
    0| 001174 add      7D29C214   1     A         gr9=gr9,gr24
    0| 001178 add      7D4AE214   1     A         gr10=gr10,gr28
    0| 00117C add      7FD7F214   1     A         gr30=gr23,gr30
    0| 001180 add      7D6BD214   1     A         gr11=gr11,gr26
    0| 001184 andi.    71170003   1     RN4_R     gr23,cr0=gr8,0,0x3
    0| 001188 cmpdi    2F390000   1     C8        cr6=gr25,0
  165|                              CL.127:
  167| 00118C or       7D284B78   1     LR        gr8=gr9
  168| 001190 or       7FDCF378   1     LR        gr28=gr30
  167| 001194 or       7D5A5378   1     LR        gr26=gr10
  168| 001198 or       7D785B78   1     LR        gr24=gr11
    0| 00119C bc       41820040   1     BT        CL.706,cr0,0x4/eq,taken=50%(0,0)
    0| 0011A0 mtspr    7EE903A6   1     LCTR      ctr=gr23
  167| 0011A4 lfdux    7C0824EE   1     LFDU      fp0,gr8=w3dd(gr8,gr4,0)
  168| 0011A8 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
    0| 0011AC bc       42400024   1     BCF       ctr=CL.853,taken=0%(0,100)
    0| 0011B0 ori      60210000   1     XNOP      
    0|                              CL.854:
  167| 0011B4 lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  167| 0011B8 stfdux   7C1A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp0
    0| 0011BC fmr      FC000890   1     LRFL      fp0=fp1
  168| 0011C0 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  168| 0011C4 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
    0| 0011C8 fmr      FC001090   1     LRFL      fp0=fp2
    0| 0011CC bc       4200FFE8   1     BCT       ctr=CL.854,taken=100%(100,0)
    0|                              CL.853:
  167| 0011D0 stfdux   7C1A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp0
  168| 0011D4 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
    0| 0011D8 bc       419A0090   1     BT        CL.707,cr6,0x4/eq,taken=20%(20,80)
    0|                              CL.706:
  167| 0011DC lfdux    7C2824EE   1     LFDU      fp1,gr8=w3dd(gr8,gr4,0)
  168| 0011E0 lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
    0| 0011E4 mtspr    7F2903A6   1     LCTR      ctr=gr25
  167| 0011E8 lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  167| 0011EC stfdux   7C3A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp1
  168| 0011F0 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  167| 0011F4 lfdux    7C6824EE   1     LFDU      fp3,gr8=w3dd(gr8,gr4,0)
  167| 0011F8 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
  167| 0011FC lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  167| 001200 stfdux   7C7A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp3
  167| 001204 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
    0| 001208 bc       42400048   1     BCF       ctr=CL.855,taken=0%(0,100)
    0|                              CL.856:
  167| 00120C lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  168| 001210 lfdux    7C7C2CEE   1     LFDU      fp3,gr28=w3de(gr28,gr5,0)
  168| 001214 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  167| 001218 lfdux    7C8824EE   1     LFDU      fp4,gr8=w3dd(gr8,gr4,0)
  168| 00121C lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
  168| 001220 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
  168| 001224 stfdux   7C7835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp3
  167| 001228 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
  167| 00122C lfdux    7C4824EE   1     LFDU      fp2,gr8=w3dd(gr8,gr4,0)
  168| 001230 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  168| 001234 lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
  167| 001238 stfdux   7C9A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp4
  167| 00123C lfdux    7C6824EE   1     LFDU      fp3,gr8=w3dd(gr8,gr4,0)
  168| 001240 lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  167| 001244 stfdux   7C5A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp2
  167| 001248 stfdux   7C7A1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp3
    0| 00124C bc       4200FFC0   1     BCT       ctr=CL.856,taken=100%(100,0)
    0|                              CL.855:
  168| 001250 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  168| 001254 lfdux    7C1C2CEE   1     LFDU      fp0,gr28=w3de(gr28,gr5,0)
  168| 001258 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
  168| 00125C lfdux    7C3C2CEE   1     LFDU      fp1,gr28=w3de(gr28,gr5,0)
  168| 001260 stfdux   7C1835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp0
  168| 001264 stfdux   7C3835EE   1     STFDU     gr24,w3dg(gr24,gr6,0)=fp1
    0|                              CL.707:
  170| 001268 addi     38E70001   1     AI        gr7=gr7,1
    0| 00126C add      7D29EA14   1     A         gr9=gr9,gr29
  170| 001270 cmpld    7FA70040   1     CL8       cr7=gr7,gr0
    0| 001274 add      7D4AFA14   1     A         gr10=gr10,gr31
    0| 001278 add      7FDBF214   1     A         gr30=gr27,gr30
    0| 00127C add      7D6B6214   1     A         gr11=gr11,gr12
  170| 001280 bc       419CFF0C   1     BT        CL.127,cr7,0x8/llt,taken=80%(80,20)
  170|                              CL.126:
  171| 001284 ld       E8620000   1     L8        gr3=.&&N&&config(gr2,0)
  171| 001288 lwa      E9030026   1     L4A       gr8=<s177:d36:l4>(gr3,36)
  171| 00128C cmpwi    2C080001   1     C4        cr0=gr8,1
  171| 001290 bc       40810390   1     BF        CL.62,cr0,0x2/gt,taken=50%(0,0)
   87| 001294 sradi    7D031674   1     SRA8CA    gr3,ca=gr8,2
   87| 001298 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  172| 00129C rldicr   78651764   1     SLL8      gr5=gr3,2
  172| 0012A0 subf     7CC54051   1     S_R       gr6,cr0=gr8,gr5
  172| 0012A4 bc       40810118   1     BF        CL.184,cr0,0x2/gt,taken=50%(0,0)
  175| 0012A8 ld       EAE20000   1     L8        gr23=.&&N&&grid(gr2,0)
  175| 0012AC ld       EAA20000   1     L8        gr21=.&&N&field(gr2,0)
  175| 0012B0 ld       EA820000   1     L8        gr20=.&&N&scratch(gr2,0)
  172| 0012B4 addi     38E00000   1     LI        gr7=0
  175| 0012B8 lwa      E9370012   1     L4A       gr9=<s183:d16:l4>(gr23,16)
  174| 0012BC lwa      E9570002   1     L4A       gr10=<s183:d0:l4>(gr23,0)
  175| 0012C0 ld       EAD50868   1     L8        gr22=<s39:d2152:l8>(gr21,2152)
  175| 0012C4 ld       E8750898   1     L8        gr3=<s39:d2200:l8>(gr21,2200)
  175| 0012C8 ld       E9750880   1     L8        gr11=<s39:d2176:l8>(gr21,2176)
  175| 0012CC ld       E99408F0   1     L8        gr12=<s95:d2288:l8>(gr20,2288)
  175| 0012D0 ld       EBF40908   1     L8        gr31=<s95:d2312:l8>(gr20,2312)
  175| 0012D4 ld       E8940920   1     L8        gr4=<s95:d2336:l8>(gr20,2336)
  175| 0012D8 ld       EBD408A8   1     L8        gr30=<s95:d2216:l8>(gr20,2216)
  175| 0012DC ld       EBB50820   1     L8        gr29=<s39:d2080:l8>(gr21,2080)
  174| 0012E0 lwa      EB970006   1     L4A       gr28=<s183:d4:l4>(gr23,4)
  175| 0012E4 ld       EB750838   1     L8        gr27=<s39:d2104:l8>(gr21,2104)
  175| 0012E8 ld       EB5408C0   1     L8        gr26=<s95:d2240:l8>(gr20,2240)
  175| 0012EC ld       EB350850   1     L8        gr25=<s39:d2128:l8>(gr21,2128)
  175| 0012F0 ld       EB1408D8   1     L8        gr24=<s95:d2264:l8>(gr20,2264)
    0| 0012F4 bc       40850490   1     BF        CL.515,cr1,0x2/gt,taken=50%(0,0)
    0| 0012F8 ld       EA610200   1     L8        gr19=#SPILL43(gr1,512)
    0| 0012FC mulld    7EE351D2   1     M         gr23=gr3,gr10
    0| 001300 mulld    7EC9B1D2   1     M         gr22=gr9,gr22
    0| 001304 mulld    7EA451D2   1     M         gr21=gr4,gr10
    0| 001308 mulld    7E93F9D2   1     M         gr20=gr19,gr31
    0| 00130C mulld    7D2961D2   1     M         gr9=gr9,gr12
    0| 001310 mulld    7D8B99D2   1     M         gr12=gr11,gr19
    0| 001314 subf     7FC4F050   1     S         gr30=gr30,gr4
    0| 001318 subf     7FA3E850   1     S         gr29=gr29,gr3
    0| 00131C add      7FDAF214   1     A         gr30=gr26,gr30
    0| 001320 add      7F56BA14   1     A         gr26=gr22,gr23
    0| 001324 add      7EF4AA14   1     A         gr23=gr20,gr21
    0| 001328 add      7D29F214   1     A         gr9=gr9,gr30
    0| 00132C add      7FDBEA14   1     A         gr30=gr27,gr29
    0| 001330 add      7D8CD214   1     A         gr12=gr12,gr26
    0| 001334 subfic   23AA0001   1     SFI       gr29=1,gr10,ca"
    0| 001338 add      7D29BA14   1     A         gr9=gr9,gr23
    0| 00133C add      7D8CF214   1     A         gr12=gr12,gr30
    0| 001340 subf     7D4AE050   1     S         gr10=gr28,gr10
    0| 001344 add      7FDCEA14   1     A         gr30=gr28,gr29
    0| 001348 add      7D29C214   1     A         gr9=gr9,gr24
    0| 00134C add      7D8CCA14   1     A         gr12=gr12,gr25
    0| 001350 addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
  172|                              CL.179:
  173| 001354 addi     39400000   1     LI        gr10=0
    0| 001358 bc       40810050   1     BF        CL.183,cr0,0x2/gt,taken=20%(20,80)
    0| 00135C or       7D9B6378   1     LR        gr27=gr12
    0| 001360 or       7D3A4B78   1     LR        gr26=gr9
  173|                              CL.180:
  175| 001364 or       7F7DDB78   1     LR        gr29=gr27
    0| 001368 mtspr    7FC903A6   1     LCTR      ctr=gr30
  175| 00136C lfdux    7C1D1CEE   1     LFDU      fp0,gr29=abun(gr29,gr3,0)
  175| 001370 or       7F5CD378   1     LR        gr28=gr26
    0| 001374 bc       4240001C   1     BCF       ctr=CL.857,taken=0%(0,100)
    0| 001378 ori      60210000   1     XNOP      
    0| 00137C ori      60210000   1     XNOP      
    0|                              CL.858:
  175| 001380 lfdux    7C3D1CEE   1     LFDU      fp1,gr29=abun(gr29,gr3,0)
  175| 001384 stfdux   7C1C25EE   1     STFDU     gr28,w4da(gr28,gr4,0)=fp0
    0| 001388 fmr      FC000890   1     LRFL      fp0=fp1
    0| 00138C bc       4200FFF4   1     BCT       ctr=CL.858,taken=100%(100,0)
    0|                              CL.857:
  177| 001390 addi     394A0001   1     AI        gr10=gr10,1
  175| 001394 stfdux   7C1C25EE   1     STFDU     gr28,w4da(gr28,gr4,0)=fp0
  177| 001398 cmpld    7F2A0040   1     CL8       cr6=gr10,gr0
    0| 00139C add      7F6BDA14   1     A         gr27=gr11,gr27
    0| 0013A0 add      7F5AFA14   1     A         gr26=gr26,gr31
  177| 0013A4 bc       4198FFC0   1     BT        CL.180,cr6,0x8/llt,taken=80%(80,20)
  177|                              CL.183:
  178| 0013A8 addi     38E70001   1     AI        gr7=gr7,1
    0| 0013AC add      7D8CCA14   1     A         gr12=gr12,gr25
  178| 0013B0 cmpd     7F263800   1     C8        cr6=gr6,gr7
    0| 0013B4 add      7D29C214   1     A         gr9=gr9,gr24
  178| 0013B8 bc       4199FF9C   1     BT        CL.179,cr6,0x2/gt,taken=80%(80,20)
  178|                              CL.184:
  172| 0013BC cmpd     7C283000   1     C8        cr0=gr8,gr6
  172| 0013C0 bc       40810260   1     BF        CL.62,cr0,0x2/gt,taken=50%(0,0)
  175| 0013C4 ld       EB220000   1     L8        gr25=.&&N&&grid(gr2,0)
  175| 0013C8 ld       EAE20000   1     L8        gr23=.&&N&field(gr2,0)
  175| 0013CC ld       EAC20000   1     L8        gr22=.&&N&scratch(gr2,0)
  178| 0013D0 addi     38A5FFFF   1     AI        gr5=gr5,-1
  172| 0013D4 addi     3AA00000   1     LI        gr21=0
  178| 0013D8 sradi    7CA51674   1     SRA8CA    gr5,ca=gr5,2
  172| 0013DC std      FAA10220   1     ST8       #SPILL47(gr1,544)=gr21
  175| 0013E0 lwa      E9790012   1     L4A       gr11=<s183:d16:l4>(gr25,16)
  174| 0013E4 lwa      E9390002   1     L4A       gr9=<s183:d0:l4>(gr25,0)
  175| 0013E8 ld       EB170868   1     L8        gr24=<s39:d2152:l8>(gr23,2152)
  175| 0013EC ld       E8770898   1     L8        gr3=<s39:d2200:l8>(gr23,2200)
  175| 0013F0 ld       EBD70880   1     L8        gr30=<s39:d2176:l8>(gr23,2176)
  175| 0013F4 ld       E99608F0   1     L8        gr12=<s95:d2288:l8>(gr22,2288)
  175| 0013F8 ld       EBB60908   1     L8        gr29=<s95:d2312:l8>(gr22,2312)
  175| 0013FC ld       E8960920   1     L8        gr4=<s95:d2336:l8>(gr22,2336)
  175| 001400 ld       EBF608A8   1     L8        gr31=<s95:d2216:l8>(gr22,2216)
  175| 001404 ld       E8F70850   1     L8        gr7=<s39:d2128:l8>(gr23,2128)
  175| 001408 ld       E91608D8   1     L8        gr8=<s95:d2264:l8>(gr22,2264)
  175| 00140C ld       EB970820   1     L8        gr28=<s39:d2080:l8>(gr23,2080)
  174| 001410 lwa      E9590006   1     L4A       gr10=<s183:d4:l4>(gr25,4)
  175| 001414 ld       EB770838   1     L8        gr27=<s39:d2104:l8>(gr23,2104)
  175| 001418 ld       EB5608C0   1     L8        gr26=<s95:d2240:l8>(gr22,2240)
  178| 00141C addze    7CA50194   1     ADDE      gr5,ca=gr5,0,ca
    0| 001420 bc       40850200   1     BF        CL.62,cr1,0x2/gt,taken=50%(0,0)
    0| 001424 ld       EA810200   1     L8        gr20=#SPILL43(gr1,512)
    0| 001428 mulld    7F2349D2   1     M         gr25=gr3,gr9
    0| 00142C mulld    7F0BC1D2   1     M         gr24=gr11,gr24
    0| 001430 mulld    7EE449D2   1     M         gr23=gr4,gr9
    0| 001434 mulld    7ED4E9D2   1     M         gr22=gr20,gr29
    0| 001438 mulld    7D6B61D2   1     M         gr11=gr11,gr12
    0| 00143C mulld    7D94F1D2   1     M         gr12=gr20,gr30
    0| 001440 subf     7FE4F850   1     S         gr31=gr31,gr4
    0| 001444 subf     7F83E050   1     S         gr28=gr28,gr3
    0| 001448 add      7F5AFA14   1     A         gr26=gr26,gr31
    0| 00144C add      7F18CA14   1     A         gr24=gr24,gr25
    0| 001450 mulld    7FE641D2   1     M         gr31=gr6,gr8
    0| 001454 mulld    7F2639D2   1     M         gr25=gr6,gr7
    0| 001458 add      7EF6BA14   1     A         gr23=gr22,gr23
    0| 00145C add      7D6BD214   1     A         gr11=gr11,gr26
    0| 001460 add      7F9BE214   1     A         gr28=gr27,gr28
    0| 001464 add      7D8CC214   1     A         gr12=gr12,gr24
    0| 001468 subfic   20C90001   1     SFI       gr6=1,gr9,ca"
    0| 00146C rldicr   79131764   1     SLL8      gr19=gr8,2
    0| 001470 add      7F4BBA14   1     A         gr26=gr11,gr23
    0| 001474 std      FA610228   1     ST8       #SPILL48(gr1,552)=gr19
    0| 001478 add      7D6CE214   1     A         gr11=gr12,gr28
    0| 00147C subf     7F895050   1     S         gr28=gr10,gr9
    0| 001480 add      7CC65214   1     A         gr6=gr6,gr10
    0| 001484 rldicr   78F21764   1     SLL8      gr18=gr7,2
    0| 001488 rldicr   791B0FA4   1     SLL8      gr27=gr8,1
    0| 00148C std      FA410230   1     ST8       #SPILL49(gr1,560)=gr18
    0| 001490 add      7D9AFA14   1     A         gr12=gr26,gr31
    0| 001494 subf     7FE89850   1     S         gr31=gr19,gr8
    0| 001498 add      7D2BCA14   1     A         gr9=gr11,gr25
    0| 00149C addic.   379C0001   1     AI_R      gr28,cr0=gr28,1,ca"
    0| 0014A0 rldicr   78EA0FA4   1     SLL8      gr10=gr7,1
    0| 0014A4 subf     7D679050   1     S         gr11=gr18,gr7
    0| 0014A8 rldicl   78DCF842   1     SRL8      gr28=gr6,1
    0| 0014AC add      7E2CDA14   1     A         gr17=gr12,gr27
    0| 0014B0 add      7E0CFA14   1     A         gr16=gr12,gr31
    0| 0014B4 std      FA210238   1     ST8       #SPILL50(gr1,568)=gr17
    0| 0014B8 std      FA010240   1     ST8       #SPILL51(gr1,576)=gr16
    0| 0014BC add      7EC74A14   1     A         gr22=gr7,gr9
    0| 0014C0 addi     38E50001   1     AI        gr7=gr5,1
    0| 0014C4 add      7F6C9A14   1     A         gr27=gr12,gr19
    0| 0014C8 std      F8E10248   1     ST8       #SPILL52(gr1,584)=gr7
    0| 0014CC add      7F486214   1     A         gr26=gr8,gr12
    0| 0014D0 add      7F295214   1     A         gr25=gr9,gr10
    0| 0014D4 add      7F095A14   1     A         gr24=gr9,gr11
    0| 0014D8 add      7EE99214   1     A         gr23=gr9,gr18
    0| 0014DC mcrf     4F000000   1     LRCR      cr6=cr0
    0| 0014E0 andi.    70C50001   1     RN4_R     gr5,cr0=gr6,0,0x1
    0| 0014E4 cmpdi    2EBC0000   1     C8        cr5=gr28,0
  172|                              CL.131:
  173| 0014E8 addi     38A00000   1     LI        gr5=0
    0| 0014EC bc       409900E4   1     BF        CL.132,cr6,0x2/gt,taken=20%(20,80)
    0| 0014F0 or       7F15C378   1     LR        gr21=gr24
    0| 0014F4 or       7EF4BB78   1     LR        gr20=gr23
    0| 0014F8 or       7ED3B378   1     LR        gr19=gr22
    0| 0014FC or       7F32CB78   1     LR        gr18=gr25
    0| 001500 ld       EA210240   1     L8        gr17=#SPILL51(gr1,576)
    0| 001504 or       7F70DB78   1     LR        gr16=gr27
    0| 001508 or       7F4FD378   1     LR        gr15=gr26
    0| 00150C ld       E9C10238   1     L8        gr14=#SPILL50(gr1,568)
    0| 001510 ori      60210000   1     XNOP      
  173|                              CL.133:
  175| 001514 or       7E669B78   1     LR        gr6=gr19
  175| 001518 or       7E479378   1     LR        gr7=gr18
  175| 00151C or       7EA8AB78   1     LR        gr8=gr21
  175| 001520 or       7E89A378   1     LR        gr9=gr20
  175| 001524 or       7DEA7B78   1     LR        gr10=gr15
  175| 001528 or       7DCB7378   1     LR        gr11=gr14
  175| 00152C or       7E2C8B78   1     LR        gr12=gr17
  175| 001530 or       7E1F8378   1     LR        gr31=gr16
    0| 001534 mtspr    7F8903A6   1     LCTR      ctr=gr28
    0| 001538 bc       41820028   1     BT        CL.825,cr0,0x4/eq,taken=50%(0,0)
  175| 00153C lfdux    7C061CEE   1     LFDU      fp0,gr6=abun(gr6,gr3,0)
  175| 001540 lfdux    7C271CEE   1     LFDU      fp1,gr7=abun(gr7,gr3,0)
  175| 001544 lfdux    7C481CEE   1     LFDU      fp2,gr8=abun(gr8,gr3,0)
  175| 001548 lfdux    7C691CEE   1     LFDU      fp3,gr9=abun(gr9,gr3,0)
  175| 00154C stfdux   7C0A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp0
  175| 001550 stfdux   7C2B25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp1
  175| 001554 stfdux   7C4C25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp2
  175| 001558 stfdux   7C7F25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp3
    0| 00155C bc       41960048   1     BT        CL.713,cr5,0x4/eq,taken=20%(20,80)
    0|                              CL.825:
  175| 001560 lfdux    7C061CEE   1     LFDU      fp0,gr6=abun(gr6,gr3,0)
  175| 001564 lfdux    7C271CEE   1     LFDU      fp1,gr7=abun(gr7,gr3,0)
  175| 001568 lfdux    7C481CEE   1     LFDU      fp2,gr8=abun(gr8,gr3,0)
  175| 00156C lfdux    7C691CEE   1     LFDU      fp3,gr9=abun(gr9,gr3,0)
  175| 001570 lfdux    7C861CEE   1     LFDU      fp4,gr6=abun(gr6,gr3,0)
  175| 001574 lfdux    7CA71CEE   1     LFDU      fp5,gr7=abun(gr7,gr3,0)
  175| 001578 lfdux    7CC81CEE   1     LFDU      fp6,gr8=abun(gr8,gr3,0)
  175| 00157C lfdux    7CE91CEE   1     LFDU      fp7,gr9=abun(gr9,gr3,0)
  175| 001580 stfdux   7C0A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp0
  175| 001584 stfdux   7C2B25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp1
  175| 001588 stfdux   7C4C25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp2
  175| 00158C stfdux   7C7F25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp3
  175| 001590 stfdux   7C8A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp4
  175| 001594 stfdux   7CAB25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp5
  175| 001598 stfdux   7CCC25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp6
  175| 00159C stfdux   7CFF25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp7
    0| 0015A0 bc       4200FFC0   1     BCT       ctr=CL.825,taken=100%(100,0)
    0|                              CL.713:
  177| 0015A4 addi     38A50001   1     AI        gr5=gr5,1
    0| 0015A8 add      7EB5F214   1     A         gr21=gr21,gr30
  177| 0015AC cmpld    7FA50040   1     CL8       cr7=gr5,gr0
    0| 0015B0 add      7E94F214   1     A         gr20=gr20,gr30
    0| 0015B4 add      7E73F214   1     A         gr19=gr19,gr30
    0| 0015B8 add      7E52F214   1     A         gr18=gr18,gr30
    0| 0015BC add      7E31EA14   1     A         gr17=gr17,gr29
    0| 0015C0 add      7E10EA14   1     A         gr16=gr16,gr29
    0| 0015C4 add      7DEFEA14   1     A         gr15=gr15,gr29
    0| 0015C8 add      7DCEEA14   1     A         gr14=gr14,gr29
  177| 0015CC bc       419CFF48   1     BT        CL.133,cr7,0x8/llt,taken=80%(80,20)
  177|                              CL.132:
  178| 0015D0 ld       E8A10220   1     L8        gr5=#SPILL47(gr1,544)
    0| 0015D4 ld       E8C10228   1     L8        gr6=#SPILL48(gr1,552)
    0| 0015D8 ld       E8E10238   1     L8        gr7=#SPILL50(gr1,568)
  178| 0015DC ld       E9010248   1     L8        gr8=#SPILL52(gr1,584)
    0| 0015E0 ld       E9210240   1     L8        gr9=#SPILL51(gr1,576)
    0| 0015E4 ld       E9410230   1     L8        gr10=#SPILL49(gr1,560)
  178| 0015E8 addi     38A50001   1     AI        gr5=gr5,1
    0| 0015EC add      7F66DA14   1     A         gr27=gr6,gr27
  178| 0015F0 std      F8A10220   1     ST8       #SPILL47(gr1,544)=gr5
    0| 0015F4 add      7CE63A14   1     A         gr7=gr6,gr7
  178| 0015F8 cmpld    7FA54040   1     CL8       cr7=gr5,gr8
    0| 0015FC add      7D264A14   1     A         gr9=gr6,gr9
    0| 001600 std      F8E10238   1     ST8       #SPILL50(gr1,568)=gr7
    0| 001604 std      F9210240   1     ST8       #SPILL51(gr1,576)=gr9
    0| 001608 add      7F46D214   1     A         gr26=gr6,gr26
    0| 00160C add      7F2ACA14   1     A         gr25=gr10,gr25
    0| 001610 add      7F0AC214   1     A         gr24=gr10,gr24
    0| 001614 add      7EEABA14   1     A         gr23=gr10,gr23
    0| 001618 add      7ECAB214   1     A         gr22=gr10,gr22
  178| 00161C bc       419CFECC   1     BT        CL.131,cr7,0x8/llt,taken=80%(80,20)
  179|                              CL.62:
  180| 001620 ld       E8820000   1     L8        gr4=.&&N&&config(gr2,0)
  180| 001624 lwz      80640018   1     L4Z       gr3=<s177:d24:l4>(gr4,24)
  180| 001628 cmpdi    2C230000   1     C8        cr0=gr3,0
  180| 00162C bc       41820148   1     BT        CL.75,cr0,0x4/eq,taken=50%(0,0)
  181| 001630 bc       40850144   1     BF        CL.75,cr1,0x2/gt,taken=50%(0,0)
  182| 001634 ld       EB620000   1     L8        gr27=.&&N&&grid(gr2,0)
  183| 001638 ld       EB420000   1     L8        gr26=.&&N&field(gr2,0)
  183| 00163C ld       EB220000   1     L8        gr25=.&&N&scratch(gr2,0)
  182| 001640 lwa      E87B0002   1     L4A       gr3=<s183:d0:l4>(gr27,0)
  182| 001644 lwa      E89B0006   1     L4A       gr4=<s183:d4:l4>(gr27,4)
  183| 001648 lwa      E91B0012   1     L4A       gr8=<s183:d16:l4>(gr27,16)
  183| 00164C ld       E93A0440   1     L8        gr9=<s39:d1088:l8>(gr26,1088)
  183| 001650 ld       E95A0458   1     L8        gr10=<s39:d1112:l8>(gr26,1112)
  183| 001654 ld       E8BA0470   1     L8        gr5=<s39:d1136:l8>(gr26,1136)
  183| 001658 ld       E9790870   1     L8        gr11=<s95:d2160:l8>(gr25,2160)
  182| 00165C subf     7CE32050   1     S         gr7=gr4,gr3
  183| 001660 ld       E9990888   1     L8        gr12=<s95:d2184:l8>(gr25,2184)
  182| 001664 addic.   34E70001   1     AI_R      gr7,cr0=gr7,1,ca"
  183| 001668 ld       E8D908A0   1     L8        gr6=<s95:d2208:l8>(gr25,2208)
  183| 00166C ld       EBFA0410   1     L8        gr31=<s39:d1040:l8>(gr26,1040)
  183| 001670 ld       EBD90840   1     L8        gr30=<s95:d2112:l8>(gr25,2112)
  183| 001674 ld       EBBA0428   1     L8        gr29=<s39:d1064:l8>(gr26,1064)
  183| 001678 ld       EB990858   1     L8        gr28=<s95:d2136:l8>(gr25,2136)
  181| 00167C addi     38E00000   1     LI        gr7=0
    0| 001680 bc       408100F4   1     BF        CL.75,cr0,0x2/gt,taken=50%(0,0)
    0| 001684 ld       EB010200   1     L8        gr24=#SPILL43(gr1,512)
    0| 001688 mulld    7F6329D2   1     M         gr27=gr3,gr5
    0| 00168C mulld    7D2849D2   1     M         gr9=gr8,gr9
    0| 001690 mulld    7F4AC1D2   1     M         gr26=gr10,gr24
    0| 001694 mulld    7D0859D2   1     M         gr8=gr8,gr11
    0| 001698 mulld    7D6CC1D2   1     M         gr11=gr12,gr24
    0| 00169C mulld    7F2331D2   1     M         gr25=gr3,gr6
    0| 0016A0 subfic   20630001   1     SFI       gr3=1,gr3,ca"
    0| 0016A4 subf     7FE5F850   1     S         gr31=gr31,gr5
    0| 0016A8 subf     7FC6F050   1     S         gr30=gr30,gr6
    0| 0016AC add      7C632214   1     A         gr3=gr3,gr4
    0| 0016B0 add      7C9DFA14   1     A         gr4=gr29,gr31
    0| 0016B4 add      7FFCF214   1     A         gr31=gr28,gr30
    0| 0016B8 add      7D29DA14   1     A         gr9=gr9,gr27
    0| 0016BC add      7C84D214   1     A         gr4=gr4,gr26
    0| 0016C0 add      7D085A14   1     A         gr8=gr8,gr11
    0| 0016C4 add      7D79FA14   1     A         gr11=gr25,gr31
    0| 0016C8 rldicl   787FE8C2   1     SRL8      gr31=gr3,3
    0| 0016CC add      7C844A14   1     A         gr4=gr4,gr9
    0| 0016D0 add      7D085A14   1     A         gr8=gr8,gr11
    0| 0016D4 andi.    706B0007   1     RN4_R     gr11,cr0=gr3,0,0x7
    0| 0016D8 cmpdi    2CBF0000   1     C8        cr1=gr31,0
  181|                              CL.137:
  183| 0016DC or       7C832378   1     LR        gr3=gr4
  183| 0016E0 or       7D094378   1     LR        gr9=gr8
    0| 0016E4 bc       41820034   1     BT        CL.715,cr0,0x4/eq,taken=50%(0,0)
    0| 0016E8 mtspr    7D6903A6   1     LCTR      ctr=gr11
  183| 0016EC lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
    0| 0016F0 bc       42400020   1     BCF       ctr=CL.860,taken=0%(0,100)
    0| 0016F4 ori      60210000   1     XNOP      
    0| 0016F8 ori      60210000   1     XNOP      
    0| 0016FC ori      60210000   1     XNOP      
    0|                              CL.861:
  183| 001700 lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  183| 001704 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
    0| 001708 fmr      FC000890   1     LRFL      fp0=fp1
    0| 00170C bc       4200FFF4   1     BCT       ctr=CL.861,taken=100%(100,0)
    0|                              CL.860:
  183| 001710 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
    0| 001714 bc       4186004C   1     BT        CL.716,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.715:
    0| 001718 mtspr    7FE903A6   1     LCTR      ctr=gr31
    0|                              CL.862:
  183| 00171C lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
  183| 001720 lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  183| 001724 lfdux    7C432CEE   1     LFDU      fp2,gr3=er(gr3,gr5,0)
  183| 001728 lfdux    7C632CEE   1     LFDU      fp3,gr3=er(gr3,gr5,0)
  183| 00172C stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
  183| 001730 stfdux   7C2935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp1
  183| 001734 lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
  183| 001738 stfdux   7C4935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp2
  183| 00173C lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  183| 001740 stfdux   7C6935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp3
  183| 001744 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
  183| 001748 lfdux    7C032CEE   1     LFDU      fp0,gr3=er(gr3,gr5,0)
  183| 00174C stfdux   7C2935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp1
  183| 001750 lfdux    7C232CEE   1     LFDU      fp1,gr3=er(gr3,gr5,0)
  183| 001754 stfdux   7C0935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp0
  183| 001758 stfdux   7C2935EE   1     STFDU     gr9,w3dh(gr9,gr6,0)=fp1
    0| 00175C bc       4200FFC0   1     BCT       ctr=CL.862,taken=100%(100,0)
    0|                              CL.716:
  185| 001760 addi     38E70001   1     AI        gr7=gr7,1
    0| 001764 add      7C845214   1     A         gr4=gr4,gr10
  185| 001768 cmpld    7F270040   1     CL8       cr6=gr7,gr0
    0| 00176C add      7D086214   1     A         gr8=gr8,gr12
  185| 001770 bc       4198FF6C   1     BT        CL.137,cr6,0x8/llt,taken=80%(80,20)
  186|                              CL.75:
  188| 001774 ld       E8620000   1     L8        gr3=.&&N&&root(gr2,0)
  188| 001778 addi     38000001   1     LI        gr0=1
  188| 00177C stw      900301E0   1     ST4Z      <s201:d480:l4>(gr3,480)=gr0
    0| 001780 b        4BFFF6C0   1     B         CL.49,-1
  177|                              CL.515:
    0| 001784 mtspr    7CC903A6   1     LCTR      ctr=gr6
  177|                              CL.417:
  178| 001788 addi     38E70001   1     AI        gr7=gr7,1
  178| 00178C cmpd     7C273000   1     C8        cr0=gr7,gr6
  178| 001790 bc       4100FFF8   1     BCTT      ctr=CL.417,cr0,0x1/lt,taken=80%(80,20)
    0| 001794 b        4BFFFC28   1     B         CL.184,-1
  206|                              CL.2:
  207| 001798 lwz      80030018   1     L4Z       gr0=<s177:d24:l4>(gr3,24)
  207| 00179C cmpdi    2C200000   1     C8        cr0=gr0,0
  207| 0017A0 bc       41820424   1     BT        CL.1229,cr0,0x4/eq,taken=30%(30,70)
  212| 0017A4 ld       E8A20000   1     L8        gr5=.&&N&&grid(gr2,0)
    0| 0017A8 addi     38800000   1     LI        gr4=0
  212| 0017AC lwa      E8050016   1     L4A       gr0=<s183:d20:l4>(gr5,20)
  212| 0017B0 lwa      E8C50012   1     L4A       gr6=<s183:d16:l4>(gr5,16)
  212| 0017B4 subf     7C660050   1     S         gr3=gr0,gr6
  212| 0017B8 addi     38030001   1     AI        gr0=gr3,1
  212| 0017BC sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
  212| 0017C0 cmpdi    2F200000   1     C8        cr6=gr0,0
  212| 0017C4 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  212| 0017C8 rldicr   78631764   1     SLL8      gr3=gr3,2
  212| 0017CC subf     7CE30051   1     S_R       gr7,cr0=gr0,gr3
  212| 0017D0 crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  212| 0017D4 bc       40810108   1     BF        CL.164,cr0,0x2/gt,taken=50%(0,0)
  215| 0017D8 ld       EB420000   1     L8        gr26=.&&N&field(gr2,0)
  215| 0017DC ld       EB220000   1     L8        gr25=.&&N&scratch(gr2,0)
  213| 0017E0 lwa      E945000A   1     L4A       gr10=<s183:d8:l4>(gr5,8)
  213| 0017E4 lwa      EB05000E   1     L4A       gr24=<s183:d12:l4>(gr5,12)
  214| 0017E8 lwa      E9650002   1     L4A       gr11=<s183:d0:l4>(gr5,0)
  214| 0017EC lwa      EAE50006   1     L4A       gr23=<s183:d4:l4>(gr5,4)
  215| 0017F0 ld       E99A0440   1     L8        gr12=<s39:d1088:l8>(gr26,1088)
  215| 0017F4 ld       EBFA0458   1     L8        gr31=<s39:d1112:l8>(gr26,1112)
  215| 0017F8 ld       E91A0470   1     L8        gr8=<s39:d1136:l8>(gr26,1136)
  215| 0017FC ld       EBD90870   1     L8        gr30=<s95:d2160:l8>(gr25,2160)
  215| 001800 ld       EBB90888   1     L8        gr29=<s95:d2184:l8>(gr25,2184)
  215| 001804 ld       E93908A0   1     L8        gr9=<s95:d2208:l8>(gr25,2208)
  215| 001808 ld       EB9A0410   1     L8        gr28=<s39:d1040:l8>(gr26,1040)
  215| 00180C ld       EB790840   1     L8        gr27=<s95:d2112:l8>(gr25,2112)
  213| 001810 subf     7F0AC050   1     S         gr24=gr24,gr10
  215| 001814 ld       EB5A0428   1     L8        gr26=<s39:d1064:l8>(gr26,1064)
  215| 001818 ld       EB390858   1     L8        gr25=<s95:d2136:l8>(gr25,2136)
  213| 00181C addic.   37180001   1     AI_R      gr24,cr0=gr24,1,ca"
    0| 001820 bc       40810390   1     BF        CL.530,cr0,0x2/gt,taken=50%(0,0)
    0| 001824 mulld    7EC6F1D2   1     M         gr22=gr6,gr30
    0| 001828 mulld    7EAAE9D2   1     M         gr21=gr10,gr29
    0| 00182C mulld    7E8959D2   1     M         gr20=gr9,gr11
    0| 001830 mulld    7E6859D2   1     M         gr19=gr8,gr11
    0| 001834 mulld    7E4661D2   1     M         gr18=gr6,gr12
    0| 001838 mulld    7D4AF9D2   1     M         gr10=gr10,gr31
    0| 00183C subf     7F69D850   1     S         gr27=gr27,gr9
    0| 001840 subf     7F88E050   1     S         gr28=gr28,gr8
    0| 001844 add      7F79DA14   1     A         gr27=gr25,gr27
    0| 001848 add      7F9AE214   1     A         gr28=gr26,gr28
    0| 00184C subfic   234B0001   1     SFI       gr26=1,gr11,ca"
    0| 001850 add      7F35B214   1     A         gr25=gr21,gr22
    0| 001854 add      7F74DA14   1     A         gr27=gr20,gr27
    0| 001858 add      7ED29A14   1     A         gr22=gr18,gr19
    0| 00185C add      7D4AE214   1     A         gr10=gr10,gr28
    0| 001860 subf     7D6BB850   1     S         gr11=gr23,gr11
    0| 001864 add      7F97D214   1     A         gr28=gr23,gr26
    0| 001868 add      7F79DA14   1     A         gr27=gr25,gr27
    0| 00186C add      7D4AB214   1     A         gr10=gr10,gr22
    0| 001870 addic.   356B0001   1     AI_R      gr11,cr0=gr11,1,ca"
  212|                              CL.159:
  213| 001874 addi     39600000   1     LI        gr11=0
    0| 001878 bc       40810050   1     BF        CL.163,cr0,0x2/gt,taken=20%(20,80)
    0| 00187C or       7D575378   1     LR        gr23=gr10
    0| 001880 or       7F76DB78   1     LR        gr22=gr27
  213|                              CL.160:
  215| 001884 or       7EFABB78   1     LR        gr26=gr23
    0| 001888 mtspr    7F8903A6   1     LCTR      ctr=gr28
  215| 00188C lfdux    7C1A44EE   1     LFDU      fp0,gr26=er(gr26,gr8,0)
  215| 001890 or       7ED9B378   1     LR        gr25=gr22
    0| 001894 bc       4240001C   1     BCF       ctr=CL.863,taken=0%(0,100)
    0| 001898 ori      60210000   1     XNOP      
    0| 00189C ori      60210000   1     XNOP      
    0|                              CL.864:
  215| 0018A0 lfdux    7C3A44EE   1     LFDU      fp1,gr26=er(gr26,gr8,0)
  215| 0018A4 stfdux   7C194DEE   1     STFDU     gr25,w3dh(gr25,gr9,0)=fp0
    0| 0018A8 fmr      FC000890   1     LRFL      fp0=fp1
    0| 0018AC bc       4200FFF4   1     BCT       ctr=CL.864,taken=100%(100,0)
    0|                              CL.863:
  217| 0018B0 addi     396B0001   1     AI        gr11=gr11,1
  215| 0018B4 stfdux   7C194DEE   1     STFDU     gr25,w3dh(gr25,gr9,0)=fp0
  217| 0018B8 cmpld    7CABC040   1     CL8       cr1=gr11,gr24
    0| 0018BC add      7EF7FA14   1     A         gr23=gr23,gr31
    0| 0018C0 add      7ED6EA14   1     A         gr22=gr22,gr29
  217| 0018C4 bc       4184FFC0   1     BT        CL.160,cr1,0x8/llt,taken=80%(80,20)
  217|                              CL.163:
  218| 0018C8 addi     38840001   1     AI        gr4=gr4,1
    0| 0018CC add      7D4A6214   1     A         gr10=gr10,gr12
  218| 0018D0 cmpd     7CA72000   1     C8        cr1=gr7,gr4
    0| 0018D4 add      7F7BF214   1     A         gr27=gr27,gr30
  218| 0018D8 bc       4185FF9C   1     BT        CL.159,cr1,0x2/gt,taken=80%(80,20)
  218|                              CL.164:
  212| 0018DC cmpd     7CA03800   1     C8        cr1=gr0,gr7
  212| 0018E0 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  212| 0018E4 bc       40810258   1     BF        CL.142,cr0,0x2/gt,taken=50%(0,0)
  215| 0018E8 ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  215| 0018EC ld       E9820000   1     L8        gr12=.&&N&scratch(gr2,0)
  213| 0018F0 lwa      E965000A   1     L4A       gr11=<s183:d8:l4>(gr5,8)
  213| 0018F4 lwa      EBE5000E   1     L4A       gr31=<s183:d12:l4>(gr5,12)
  214| 0018F8 lwa      E9450002   1     L4A       gr10=<s183:d0:l4>(gr5,0)
  218| 0018FC addi     3B43FFFF   1     AI        gr26=gr3,-1
  215| 001900 ld       E91D0440   1     L8        gr8=<s39:d1088:l8>(gr29,1088)
  215| 001904 ld       E81D0458   1     L8        gr0=<s39:d1112:l8>(gr29,1112)
  215| 001908 ld       E87D0470   1     L8        gr3=<s39:d1136:l8>(gr29,1136)
  215| 00190C ld       E92C0870   1     L8        gr9=<s95:d2160:l8>(gr12,2160)
  215| 001910 ld       EBCC0888   1     L8        gr30=<s95:d2184:l8>(gr12,2184)
  215| 001914 ld       E88C08A0   1     L8        gr4=<s95:d2208:l8>(gr12,2208)
  215| 001918 ld       EB3D0410   1     L8        gr25=<s39:d1040:l8>(gr29,1040)
  215| 00191C ld       EB0C0840   1     L8        gr24=<s95:d2112:l8>(gr12,2112)
  213| 001920 subf     7FEBF850   1     S         gr31=gr31,gr11
  215| 001924 ld       EAFD0428   1     L8        gr23=<s39:d1064:l8>(gr29,1064)
  215| 001928 ld       EACC0858   1     L8        gr22=<s95:d2136:l8>(gr12,2136)
    0| 00192C mulld    7D8649D2   1     M         gr12=gr6,gr9
  213| 001930 addic.   37BF0001   1     AI_R      gr29,cr0=gr31,1,ca"
    0| 001934 mulld    7FEBF1D2   1     M         gr31=gr11,gr30
    0| 001938 mulld    7F8451D2   1     M         gr28=gr4,gr10
    0| 00193C mulld    7F6351D2   1     M         gr27=gr3,gr10
    0| 001940 mulld    7CC641D2   1     M         gr6=gr6,gr8
    0| 001944 mulld    7D6B01D2   1     M         gr11=gr11,gr0
    0| 001948 subf     7F23C850   1     S         gr25=gr25,gr3
    0| 00194C subf     7F04C050   1     S         gr24=gr24,gr4
  218| 001950 sradi    7F551674   1     SRA8CA    gr21,ca=gr26,2
  214| 001954 lwa      EB450006   1     L4A       gr26=<s183:d4:l4>(gr5,4)
    0| 001958 add      7F16C214   1     A         gr24=gr22,gr24
    0| 00195C add      7EF7CA14   1     A         gr23=gr23,gr25
    0| 001960 mulld    7F2749D2   1     M         gr25=gr7,gr9
    0| 001964 mulld    7CE741D2   1     M         gr7=gr7,gr8
    0| 001968 add      7D8CFA14   1     A         gr12=gr12,gr31
    0| 00196C add      7FF8E214   1     A         gr31=gr24,gr28
    0| 001970 add      7CC6DA14   1     A         gr6=gr6,gr27
    0| 001974 add      7D6BBA14   1     A         gr11=gr11,gr23
    0| 001978 rldicr   79341764   1     SLL8      gr20=gr9,2
    0| 00197C rldicr   79131764   1     SLL8      gr19=gr8,2
    0| 001980 std      FA810250   1     ST8       #SPILL53(gr1,592)=gr20
    0| 001984 std      FA610258   1     ST8       #SPILL54(gr1,600)=gr19
  212| 001988 addi     3A400000   1     LI        gr18=0
  218| 00198C addze    7CB50194   1     ADDE      gr5,ca=gr21,0,ca
  212| 001990 std      FA410260   1     ST8       #SPILL55(gr1,608)=gr18
    0| 001994 add      7D8CFA14   1     A         gr12=gr12,gr31
    0| 001998 add      7F665A14   1     A         gr27=gr6,gr11
    0| 00199C subf     7D6AD050   1     S         gr11=gr26,gr10
    0| 0019A0 bc       4081019C   1     BF        CL.142,cr0,0x2/gt,taken=20%(20,80)
    0| 0019A4 subfic   20CA0001   1     SFI       gr6=1,gr10,ca"
    0| 0019A8 add      7F8CCA14   1     A         gr28=gr12,gr25
    0| 0019AC add      7CC6D214   1     A         gr6=gr6,gr26
    0| 0019B0 subf     7FE9A050   1     S         gr31=gr20,gr9
    0| 0019B4 add      7F67DA14   1     A         gr27=gr7,gr27
    0| 0019B8 rldicr   792C0FA4   1     SLL8      gr12=gr9,1
    0| 0019BC rldicr   790A0FA4   1     SLL8      gr10=gr8,1
    0| 0019C0 subf     7CE89850   1     S         gr7=gr19,gr8
    0| 0019C4 addic.   356B0001   1     AI_R      gr11,cr0=gr11,1,ca"
    0| 0019C8 rldicl   78DAF842   1     SRL8      gr26=gr6,1
    0| 0019CC add      7EA9E214   1     A         gr21=gr9,gr28
    0| 0019D0 add      7D3CFA14   1     A         gr9=gr28,gr31
    0| 0019D4 std      FAA10268   1     ST8       #SPILL56(gr1,616)=gr21
    0| 0019D8 std      F9210270   1     ST8       #SPILL57(gr1,624)=gr9
    0| 0019DC add      7EC8DA14   1     A         gr22=gr8,gr27
    0| 0019E0 addi     39050001   1     AI        gr8=gr5,1
    0| 0019E4 add      7F2CE214   1     A         gr25=gr12,gr28
    0| 0019E8 std      F9010278   1     ST8       #SPILL58(gr1,632)=gr8
    0| 0019EC add      7F0ADA14   1     A         gr24=gr10,gr27
    0| 0019F0 add      7EE7DA14   1     A         gr23=gr7,gr27
    0| 0019F4 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 0019F8 andi.    70C50001   1     RN4_R     gr5,cr0=gr6,0,0x1
    0| 0019FC cmpdi    2FBA0000   1     C8        cr7=gr26,0
  212|                              CL.143:
  213| 001A00 addi     38A00000   1     LI        gr5=0
    0| 001A04 bc       408500E8   1     BF        CL.144,cr1,0x2/gt,taken=20%(20,80)
    0| 001A08 or       7F75DB78   1     LR        gr21=gr27
    0| 001A0C or       7EF4BB78   1     LR        gr20=gr23
    0| 001A10 or       7ED3B378   1     LR        gr19=gr22
    0| 001A14 or       7F12C378   1     LR        gr18=gr24
    0| 001A18 or       7F91E378   1     LR        gr17=gr28
    0| 001A1C ld       EA010270   1     L8        gr16=#SPILL57(gr1,624)
    0| 001A20 or       7F2FCB78   1     LR        gr15=gr25
    0| 001A24 ld       E9C10268   1     L8        gr14=#SPILL56(gr1,616)
  213|                              CL.145:
  215| 001A28 or       7EA6AB78   1     LR        gr6=gr21
  215| 001A2C or       7E679B78   1     LR        gr7=gr19
  215| 001A30 or       7E489378   1     LR        gr8=gr18
  215| 001A34 or       7E89A378   1     LR        gr9=gr20
  215| 001A38 or       7E2A8B78   1     LR        gr10=gr17
  215| 001A3C or       7DCB7378   1     LR        gr11=gr14
  215| 001A40 or       7DEC7B78   1     LR        gr12=gr15
  215| 001A44 or       7E1F8378   1     LR        gr31=gr16
    0| 001A48 mtspr    7F4903A6   1     LCTR      ctr=gr26
    0| 001A4C bc       41820030   1     BT        CL.837,cr0,0x4/eq,taken=50%(0,0)
  215| 001A50 lfdux    7C061CEE   1     LFDU      fp0,gr6=er(gr6,gr3,0)
  215| 001A54 lfdux    7C271CEE   1     LFDU      fp1,gr7=er(gr7,gr3,0)
  215| 001A58 lfdux    7C481CEE   1     LFDU      fp2,gr8=er(gr8,gr3,0)
  215| 001A5C lfdux    7C691CEE   1     LFDU      fp3,gr9=er(gr9,gr3,0)
  215| 001A60 stfdux   7C0A25EE   1     STFDU     gr10,w3dh(gr10,gr4,0)=fp0
  215| 001A64 stfdux   7C2B25EE   1     STFDU     gr11,w3dh(gr11,gr4,0)=fp1
  215| 001A68 stfdux   7C4C25EE   1     STFDU     gr12,w3dh(gr12,gr4,0)=fp2
  215| 001A6C stfdux   7C7F25EE   1     STFDU     gr31,w3dh(gr31,gr4,0)=fp3
    0| 001A70 bc       419E0050   1     BT        CL.722,cr7,0x4/eq,taken=20%(20,80)
    0| 001A74 ori      60210000   1     XNOP      
    0| 001A78 ori      60210000   1     XNOP      
    0|                              CL.837:
  215| 001A7C lfdux    7C061CEE   1     LFDU      fp0,gr6=er(gr6,gr3,0)
  215| 001A80 lfdux    7C271CEE   1     LFDU      fp1,gr7=er(gr7,gr3,0)
  215| 001A84 lfdux    7C481CEE   1     LFDU      fp2,gr8=er(gr8,gr3,0)
  215| 001A88 lfdux    7C691CEE   1     LFDU      fp3,gr9=er(gr9,gr3,0)
  215| 001A8C lfdux    7C861CEE   1     LFDU      fp4,gr6=er(gr6,gr3,0)
  215| 001A90 lfdux    7CA71CEE   1     LFDU      fp5,gr7=er(gr7,gr3,0)
  215| 001A94 lfdux    7CC81CEE   1     LFDU      fp6,gr8=er(gr8,gr3,0)
  215| 001A98 lfdux    7CE91CEE   1     LFDU      fp7,gr9=er(gr9,gr3,0)
  215| 001A9C stfdux   7C0A25EE   1     STFDU     gr10,w3dh(gr10,gr4,0)=fp0
  215| 001AA0 stfdux   7C2B25EE   1     STFDU     gr11,w3dh(gr11,gr4,0)=fp1
  215| 001AA4 stfdux   7C4C25EE   1     STFDU     gr12,w3dh(gr12,gr4,0)=fp2
  215| 001AA8 stfdux   7C7F25EE   1     STFDU     gr31,w3dh(gr31,gr4,0)=fp3
  215| 001AAC stfdux   7C8A25EE   1     STFDU     gr10,w3dh(gr10,gr4,0)=fp4
  215| 001AB0 stfdux   7CAB25EE   1     STFDU     gr11,w3dh(gr11,gr4,0)=fp5
  215| 001AB4 stfdux   7CCC25EE   1     STFDU     gr12,w3dh(gr12,gr4,0)=fp6
  215| 001AB8 stfdux   7CFF25EE   1     STFDU     gr31,w3dh(gr31,gr4,0)=fp7
    0| 001ABC bc       4200FFC0   1     BCT       ctr=CL.837,taken=100%(100,0)
    0|                              CL.722:
  217| 001AC0 addi     38A50001   1     AI        gr5=gr5,1
    0| 001AC4 add      7EA0AA14   1     A         gr21=gr0,gr21
  217| 001AC8 cmpld    7F25E840   1     CL8       cr6=gr5,gr29
    0| 001ACC add      7E80A214   1     A         gr20=gr0,gr20
    0| 001AD0 add      7E609A14   1     A         gr19=gr0,gr19
    0| 001AD4 add      7E409214   1     A         gr18=gr0,gr18
    0| 001AD8 add      7E31F214   1     A         gr17=gr17,gr30
    0| 001ADC add      7E10F214   1     A         gr16=gr16,gr30
    0| 001AE0 add      7DEFF214   1     A         gr15=gr15,gr30
    0| 001AE4 add      7DCEF214   1     A         gr14=gr14,gr30
  217| 001AE8 bc       4198FF40   1     BT        CL.145,cr6,0x8/llt,taken=80%(80,20)
  217|                              CL.144:
  218| 001AEC ld       E8A10260   1     L8        gr5=#SPILL55(gr1,608)
    0| 001AF0 ld       E8C10250   1     L8        gr6=#SPILL53(gr1,592)
    0| 001AF4 ld       E8E10268   1     L8        gr7=#SPILL56(gr1,616)
  218| 001AF8 ld       E9010278   1     L8        gr8=#SPILL58(gr1,632)
    0| 001AFC ld       E9210270   1     L8        gr9=#SPILL57(gr1,624)
    0| 001B00 ld       E9410258   1     L8        gr10=#SPILL54(gr1,600)
  218| 001B04 addi     38A50001   1     AI        gr5=gr5,1
    0| 001B08 add      7F26CA14   1     A         gr25=gr6,gr25
  218| 001B0C std      F8A10260   1     ST8       #SPILL55(gr1,608)=gr5
    0| 001B10 add      7CE63A14   1     A         gr7=gr6,gr7
  218| 001B14 cmpld    7F254040   1     CL8       cr6=gr5,gr8
    0| 001B18 add      7D264A14   1     A         gr9=gr6,gr9
    0| 001B1C std      F8E10268   1     ST8       #SPILL56(gr1,616)=gr7
    0| 001B20 std      F9210270   1     ST8       #SPILL57(gr1,624)=gr9
    0| 001B24 add      7F86E214   1     A         gr28=gr6,gr28
    0| 001B28 add      7F0AC214   1     A         gr24=gr10,gr24
    0| 001B2C add      7EEABA14   1     A         gr23=gr10,gr23
    0| 001B30 add      7ECAB214   1     A         gr22=gr10,gr22
    0| 001B34 add      7F6ADA14   1     A         gr27=gr10,gr27
  218| 001B38 bc       4198FEC8   1     BT        CL.143,cr6,0x8/llt,taken=80%(80,20)
  218|                              CL.142:
    0| 001B3C ld       E8620000   1     L8        gr3=.&&N&&bndry(gr2,0)
  221| 001B40 addi     38000000   1     LI        gr0=0
  227| 001B44 ld       E9C10280   1     L8        gr14=#stack(gr1,640)
  227| 001B48 ld       E9E10288   1     L8        gr15=#stack(gr1,648)
  227| 001B4C ld       EA010290   1     L8        gr16=#stack(gr1,656)
  227| 001B50 ld       EA210298   1     L8        gr17=#stack(gr1,664)
  221| 001B54 stw      90030388   1     ST4Z      bvstat[](gr3,904)=gr0
  221| 001B58 stw      9003038C   1     ST4Z      bvstat[](gr3,908)=gr0
  221| 001B5C stw      90030390   1     ST4Z      bvstat[](gr3,912)=gr0
  221| 001B60 stw      90030394   1     ST4Z      bvstat[](gr3,916)=gr0
  221| 001B64 stw      90030398   1     ST4Z      bvstat[](gr3,920)=gr0
  221| 001B68 addi     38630384   1     AI        gr3=gr3,900
  227| 001B6C ld       EA4102A0   1     L8        gr18=#stack(gr1,672)
  227| 001B70 ld       EA6102A8   1     L8        gr19=#stack(gr1,680)
  227| 001B74 ld       EA8102B0   1     L8        gr20=#stack(gr1,688)
  227| 001B78 ld       EAA102B8   1     L8        gr21=#stack(gr1,696)
  227| 001B7C ld       EAC102C0   1     L8        gr22=#stack(gr1,704)
  227| 001B80 ld       EAE102C8   1     L8        gr23=#stack(gr1,712)
  227| 001B84 ld       EB0102D0   1     L8        gr24=#stack(gr1,720)
  227| 001B88 ld       EB2102D8   1     L8        gr25=#stack(gr1,728)
  227| 001B8C ld       EB4102E0   1     L8        gr26=#stack(gr1,736)
  227| 001B90 ld       EB6102E8   1     L8        gr27=#stack(gr1,744)
  227| 001B94 ld       EB8102F0   1     L8        gr28=#stack(gr1,752)
  227| 001B98 ld       EBA102F8   1     L8        gr29=#stack(gr1,760)
  227| 001B9C ld       EBC10300   1     L8        gr30=#stack(gr1,768)
  227| 001BA0 ld       EBE10308   1     L8        gr31=#stack(gr1,776)
  227| 001BA4 addi     38210340   1     AI        gr1=gr1,832
  221| 001BA8 stwu     94030018   1     ST4U      gr3,bvstat[](gr3,24)=gr0
  227| 001BAC bclr     4E800020   1     BA        lr
  217|                              CL.530:
    0| 001BB0 mtspr    7CE903A6   1     LCTR      ctr=gr7
  217|                              CL.381:
  218| 001BB4 addi     38840001   1     AI        gr4=gr4,1
  218| 001BB8 cmpd     7CA43800   1     C8        cr1=gr4,gr7
  218| 001BBC bc       4104FFF8   1     BCTT      ctr=CL.381,cr1,0x1/lt,taken=80%(80,20)
    0| 001BC0 b        4BFFFD1C   1     B         CL.164,-1
  207|                              CL.1229:
  227| 001BC4 addi     38210340   1     AI        gr1=gr1,832
  227| 001BC8 bclr     4E800020   1     BA        lr
     |               Tag Table
     | 001BCC        00000000 00012201 86120000 00001BCC
     |               Instruction count         1779
     |               Straight-line exec time   1824
     |               Constant Area
     | 000000        3F000000

 
 
>>>>> COMPILATION UNIT EPILOGUE SECTION <<<<<
 
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    4           0         0           0          4            0
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    transprt_2D.f90             07/08/15   15:48:39
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    4           0         0           0          4            0
 
 
    Source records read.......................................     234
1501-510  Compilation successful for file transprt_2D.f90.
1501-543  Object file created.
