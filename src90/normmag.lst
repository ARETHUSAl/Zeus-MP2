IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- normmag.f90 07/08/15 15:48:55
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** normmag   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at normmag.f90 <line 23> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at normmag.f90 <line 25> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 3) at normmag.f90 <line 27> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 3) at normmag.f90 <line 27> was not SIMD vectorized because it contains memory references norm = ((norm + (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) * (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) + (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) * (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) + (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) * (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at normmag.f90 <line 29> was not SIMD vectorized because it contains memory references norm = ((norm + (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) * (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) + (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) * (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) + (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) * (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at normmag.f90 <line 29> was not SIMD vectorized because it contains operation in ((norm + (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) * (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) + (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) * (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) + (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) * (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at normmag.f90 <line 29> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*((long long) ks + $$CIV2) + (d-b1%bounds%mult[].off800)*((long long) js + $$CIV1) + (d-b1%bounds%mult[].off824)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at normmag.f90 <line 29> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at normmag.f90 <line 29> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 4) at normmag.f90 <line 37> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at normmag.f90 <line 37> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] = ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[3ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] = ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[3ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[2ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] = ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[2ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[1ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] = ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[1ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[3ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[2ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[1ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normmag.f90 <line 37> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + d-b1%bounds%lbound[].off760)) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)).
1586-534 (I) Loop (loop index 7) at normmag.f90 <line 38> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 8) at normmag.f90 <line 38> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[3ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] = ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[3ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[1ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] = ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[1ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[2ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] = ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[2ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] = ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[3ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)).
1586-536 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[1ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)).
1586-536 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[2ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864)) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)).
1586-536 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normmag.f90 <line 38> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) + d-b2%bounds%lbound[].off864) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)).
1586-534 (I) Loop (loop index 10) at normmag.f90 <line 39> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 11) at normmag.f90 <line 39> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] = ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[2ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] = ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[2ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] = ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[3ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] = ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[3ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[2ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[3ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normmag.f90 <line 39> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) + d-b3%bounds%lbound[].off968)) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)).
1586-534 (I) Loop (loop index 13) at normmag.f90 <line 42> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 14) at normmag.f90 <line 44> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 15) at normmag.f90 <line 46> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 15) at normmag.f90 <line 46> was not SIMD vectorized because it contains memory references $$SCREP0 = $$SCREP0 + (((((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) * (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) * (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC])) + (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) * (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC])) *  2.5000000000000000E-001; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at normmag.f90 <line 46> was not SIMD vectorized because it contains memory references av = av + (((((((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) *  5.0000000000000000E-001; with non-vectorizable strides.
1586-536 (I) Loop (loop index 15) at normmag.f90 <line 51> was not SIMD vectorized because it contains memory references $$SCREP0 = $$SCREP0 + (((((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) * (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) * (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC])) + (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) * (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC])) *  2.5000000000000000E-001; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 15) at normmag.f90 <line 51> was not SIMD vectorized because it contains operation in $$SCREP0 + (((((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) * (((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) * (((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC])) + (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) * (((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC])) *  2.5000000000000000E-001 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 15) at normmag.f90 <line 51> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*((long long) ks + $$CIVE) + (d-b1%bounds%mult[].off800)*((long long) js + $$CIVD) + (d-b1%bounds%mult[].off824)*((long long) is + $$CIVC)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 15) at normmag.f90 <line 51> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 15) at normmag.f90 <line 51> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 15) at normmag.f90 <line 48> was not SIMD vectorized because it contains memory references av = av + (((((((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) *  5.0000000000000000E-001; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 15) at normmag.f90 <line 48> was not SIMD vectorized because it contains operation in av + (((((((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) *  5.0000000000000000E-001 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 15) at normmag.f90 <line 48> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*((long long) ks + $$CIVE) + (d-b1%bounds%mult[].off800)*((long long) js + $$CIVD) + (d-b1%bounds%mult[].off824)*((long long) is + $$CIVC)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 15) at normmag.f90 <line 48> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 15) at normmag.f90 <line 48> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 19) at normmag.f90 <line 39> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 20) at normmag.f90 <line 39> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[d-b3%bounds%lbound[].off968 + $$CIVB][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] = ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[d-b3%bounds%lbound[].off968 + $$CIVB][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(d-b3%bounds%lbound[].off968 + $$CIVB) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns2.[d-b3%bounds%lbound[].off968 + $$CIVB][d-b3%bounds%lbound[].off992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(d-b3%bounds%lbound[].off968 + $$CIVB) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 21) at normmag.f90 <line 39> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(d-b3%bounds%lbound[].off968 + $$CIVB) + (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off992 + $$CIVA) + (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off1016 + $$CIV9)).
1586-534 (I) Loop (loop index 25) at normmag.f90 <line 38> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 26) at normmag.f90 <line 38> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[d-b2%bounds%lbound[].off864 + $$CIV8][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] = ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[d-b2%bounds%lbound[].off864 + $$CIV8][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(d-b2%bounds%lbound[].off864 + $$CIV8) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns3.[d-b2%bounds%lbound[].off864 + $$CIV8][d-b2%bounds%lbound[].off888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(d-b2%bounds%lbound[].off864 + $$CIV8) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at normmag.f90 <line 38> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(d-b2%bounds%lbound[].off864 + $$CIV8) + (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off888 + $$CIV7) + (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off912 + $$CIV6)).
1586-534 (I) Loop (loop index 31) at normmag.f90 <line 37> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 32) at normmag.f90 <line 37> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[d-b1%bounds%lbound[].off760 + $$CIV5][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] = ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[d-b1%bounds%lbound[].off760 + $$CIV5][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(d-b1%bounds%lbound[].off760 + $$CIV5) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because it contains operation in ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[d-b1%bounds%lbound[].off760 + $$CIV5][d-b1%bounds%lbound[].off784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(d-b1%bounds%lbound[].off760 + $$CIV5) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 33) at normmag.f90 <line 37> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(d-b1%bounds%lbound[].off760 + $$CIV5) + (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off784 + $$CIV4) + (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off808 + $$CIV3)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"8">. Total number of the innermost loops SIMD vectorized <"0">.


     8|         REAL*8 FUNCTION normmag (rms)
    22|           norm =  0.0000000000000000E+000
    23|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIV2 = 0
       Id=1         DO $$CIV2 = $$CIV2, int((1 + (int(ke) - int(ks))))-1
    25|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int((1 + (int(je) - int(js))))-1
    27|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV0 = 0
                            $$PRC2 = d-b1%addr%b1(int(is),$$CIV1 + int(js),&
                &             $$CIV2 + int(ks))
       Id=3                 DO $$CIV0 = $$CIV0, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC3 = d-b1%addr%b1(1 + (int(is) + $$CIV0),&
                &               $$CIV1 + int(js),$$CIV2 + int(ks))
    29|                       norm = ((norm + ($$PRC2 + $$PRC3) * ($$PRC2 + &
                &               $$PRC3)) + (d-b2%addr%b2(int(is) + $$CIV0,int(js) &
                &               + $$CIV1,int(ks) + $$CIV2) + d-b2%addr%b2(int(is) &
                &               + $$CIV0,1 + ($$CIV1 + int(js)),int(ks) + $$CIV2))&
                &                * (d-b2%addr%b2(int(is) + $$CIV0,int(js) + &
                &               $$CIV1,int(ks) + $$CIV2) + d-b2%addr%b2(int(is) + &
                &               $$CIV0,1 + ($$CIV1 + int(js)),int(ks) + $$CIV2))) &
                &               + (d-b3%addr%b3(int(is) + $$CIV0,int(js) + $$CIV1,&
                &               int(ks) + $$CIV2) + d-b3%addr%b3(int(is) + $$CIV0,&
                &               int(js) + $$CIV1,1 + ($$CIV2 + int(ks)))) * (&
                &               d-b3%addr%b3(int(is) + $$CIV0,int(js) + $$CIV1,&
                &               int(ks) + $$CIV2) + d-b3%addr%b3(int(is) + $$CIV0,&
                &               int(js) + $$CIV1,1 + ($$CIV2 + int(ks))))
                              $$PRC2 = $$PRC3
    33|                     ENDDO
                          ENDIF
    34|                 ENDDO
                      ENDIF
    35|             ENDDO
                  ENDIF
    36|           norm.rnn13 = _sqrt(((%VAL(norm) *  2.5000000000000000E-001) / &
                &   real(real((real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
    37|           IF ((MOD(d-b1%bounds%extent[].off768, 4) > 0  .AND.  &
                &   d-b1%bounds%extent[].off768 > 0)) THEN
                    $$CIV5 = 0
       Id=31        DO $$CIV5 = $$CIV5, MOD(d-b1%bounds%extent[].off768, int(&
                &       4))-1
                      IF ((d-b1%bounds%extent[].off792 > 0)) THEN
                        $$CIV4 = 0
       Id=32            DO $$CIV4 = $$CIV4, int(d-b1%bounds%extent[].off792)&
                &           -1
                          IF ((d-b1%bounds%extent[].off816 > 0)) THEN
                            $$CIV3 = 0
       Id=33                DO $$CIV3 = $$CIV3, int(&
                &               d-b1%bounds%extent[].off816)-1
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,&
                &               d-b1%bounds%lbound[].off760 + $$CIV5) = &
                &               d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,&
                &               d-b1%bounds%lbound[].off760 + $$CIV5) / _sqrt(((&
                &               %VAL(norm) *  2.5000000000000000E-001) / real(&
                &               real((real(real(%VAL(ijkn))) ** 3)))) / (rms * &
                &               rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-b1%bounds%extent[].off768 > 0  .AND.  &
                &   d-b1%bounds%extent[].off768 > MOD(d-b1%bounds%extent[].off768,&
                &    4))) THEN
                    $$CIVF = int(0)
       Id=4         DO $$CIVF = $$CIVF, int((((d-b1%bounds%extent[].off768 - &
                &       MOD(d-b1%bounds%extent[].off768, 4)) - 1) / 4 + 1))-1
                      IF ((d-b1%bounds%extent[].off792 > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int(d-b1%bounds%extent[].off792)&
                &           -1
                          IF ((d-b1%bounds%extent[].off816 > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(&
                &               d-b1%bounds%extent[].off816)-1
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760) / _sqrt(((%VAL(norm) &
                &               *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) / _sqrt(((%VAL(norm)&
                &                *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) / _sqrt(((%VAL(norm)&
                &                *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) / _sqrt(((%VAL(norm)&
                &                *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    38|           IF ((MOD(d-b2%bounds%extent[].off872, 4) > 0  .AND.  &
                &   d-b2%bounds%extent[].off872 > 0)) THEN
                    $$CIV8 = 0
       Id=25        DO $$CIV8 = $$CIV8, MOD(d-b2%bounds%extent[].off872, int(&
                &       4))-1
                      IF ((d-b2%bounds%extent[].off896 > 0)) THEN
                        $$CIV7 = 0
       Id=26            DO $$CIV7 = $$CIV7, int(d-b2%bounds%extent[].off896)&
                &           -1
                          IF ((d-b2%bounds%extent[].off920 > 0)) THEN
                            $$CIV6 = 0
       Id=27                DO $$CIV6 = $$CIV6, int(&
                &               d-b2%bounds%extent[].off920)-1
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,&
                &               d-b2%bounds%lbound[].off864 + $$CIV8) = &
                &               d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,&
                &               d-b2%bounds%lbound[].off864 + $$CIV8) / _sqrt(((&
                &               %VAL(norm) *  2.5000000000000000E-001) / real(&
                &               real((real(real(%VAL(ijkn))) ** 3)))) / (rms * &
                &               rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-b2%bounds%extent[].off872 > 0  .AND.  &
                &   d-b2%bounds%extent[].off872 > MOD(d-b2%bounds%extent[].off872,&
                &    4))) THEN
                    $$CIV10 = int(0)
       Id=7         DO $$CIV10 = $$CIV10, int((((d-b2%bounds%extent[].off872 &
                &       - MOD(d-b2%bounds%extent[].off872, 4)) - 1) / 4 + 1))-1
                      IF ((d-b2%bounds%extent[].off896 > 0)) THEN
                        $$CIV7 = 0
       Id=8             DO $$CIV7 = $$CIV7, int(d-b2%bounds%extent[].off896)&
                &           -1
                          IF ((d-b2%bounds%extent[].off920 > 0)) THEN
                            $$CIV6 = 0
       Id=9                 DO $$CIV6 = $$CIV6, int(&
                &               d-b2%bounds%extent[].off920)-1
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-b2%bounds%extent[].off872, 4)) + &
                &               d-b2%bounds%lbound[].off864) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-b2%bounds%extent[].off872, 4)) + &
                &               d-b2%bounds%lbound[].off864) / _sqrt(((%VAL(norm) &
                &               *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    39|           IF ((MOD(d-b3%bounds%extent[].off976, 4) > 0  .AND.  &
                &   d-b3%bounds%extent[].off976 > 0)) THEN
                    $$CIVB = 0
       Id=19        DO $$CIVB = $$CIVB, MOD(d-b3%bounds%extent[].off976, int(&
                &       4))-1
                      IF ((d-b3%bounds%extent[].off1000 > 0)) THEN
                        $$CIVA = 0
       Id=20            DO $$CIVA = $$CIVA, int(d-b3%bounds%extent[].off1000)&
                &           -1
                          IF ((d-b3%bounds%extent[].off1024 > 0)) THEN
                            $$CIV9 = 0
       Id=21                DO $$CIV9 = $$CIV9, int(&
                &               d-b3%bounds%extent[].off1024)-1
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,&
                &               d-b3%bounds%lbound[].off968 + $$CIVB) = &
                &               d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,&
                &               d-b3%bounds%lbound[].off968 + $$CIVB) / _sqrt(((&
                &               %VAL(norm) *  2.5000000000000000E-001) / real(&
                &               real((real(real(%VAL(ijkn))) ** 3)))) / (rms * &
                &               rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-b3%bounds%extent[].off976 > 0  .AND.  &
                &   d-b3%bounds%extent[].off976 > MOD(d-b3%bounds%extent[].off976,&
                &    4))) THEN
                    $$CIV11 = int(0)
       Id=10        DO $$CIV11 = $$CIV11, int((((d-b3%bounds%extent[].off976 &
                &       - MOD(d-b3%bounds%extent[].off976, 4)) - 1) / 4 + 1))-1
                      IF ((d-b3%bounds%extent[].off1000 > 0)) THEN
                        $$CIVA = 0
       Id=11            DO $$CIVA = $$CIVA, int(d-b3%bounds%extent[].off1000)&
                &           -1
                          IF ((d-b3%bounds%extent[].off1024 > 0)) THEN
                            $$CIV9 = 0
       Id=12                DO $$CIV9 = $$CIV9, int(&
                &               d-b3%bounds%extent[].off1024)-1
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,(&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,($$CIV11 * 4 &
                &               + MOD(d-b3%bounds%extent[].off976, 4)) + &
                &               d-b3%bounds%lbound[].off968) / _sqrt(((%VAL(norm) &
                &               *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    40|           av =  0.0000000000000000E+000
    41|           rms =  0.0000000000000000E+000
    42|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIVE = 0
       Id=13        DO $$CIVE = $$CIVE, int((1 + (int(ke) - int(ks))))-1
    44|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVD = 0
       Id=14            DO $$CIVD = $$CIVD, int((1 + (int(je) - int(js))))-1
    46|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIVC = 0
                            $$SCREP0 = rms
                            $$PRC0 = d-b1%addr%b1(int(is),$$CIVD + int(js),&
                &             $$CIVE + int(ks))
       Id=15                DO $$CIVC = $$CIVC, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC1 = d-b1%addr%b1(1 + (int(is) + $$CIVC),&
                &               $$CIVD + int(js),$$CIVE + int(ks))
    48|                       av = av + ((((($$PRC0 + $$PRC1) + d-b2%addr%b2(&
                &               int(is) + $$CIVC,int(js) + $$CIVD,int(ks) + &
                &               $$CIVE)) + d-b2%addr%b2(int(is) + $$CIVC,1 + (&
                &               $$CIVD + int(js)),int(ks) + $$CIVE)) + &
                &               d-b3%addr%b3(int(is) + $$CIVC,int(js) + $$CIVD,&
                &               int(ks) + $$CIVE)) + d-b3%addr%b3(int(is) + &
                &               $$CIVC,int(js) + $$CIVD,1 + ($$CIVE + int(ks)))) &
                &               *  5.0000000000000000E-001
    51|                       $$SCREP0 = $$SCREP0 + ((($$PRC0 + $$PRC1) * (&
                &               $$PRC0 + $$PRC1) + (d-b2%addr%b2(int(is) + $$CIVC,&
                &               int(js) + $$CIVD,int(ks) + $$CIVE) + d-b2%addr%b2(&
                &               int(is) + $$CIVC,1 + ($$CIVD + int(js)),int(ks) + &
                &               $$CIVE)) * (d-b2%addr%b2(int(is) + $$CIVC,int(js) &
                &               + $$CIVD,int(ks) + $$CIVE) + d-b2%addr%b2(int(is) &
                &               + $$CIVC,1 + ($$CIVD + int(js)),int(ks) + $$CIVE))&
                &               ) + (d-b3%addr%b3(int(is) + $$CIVC,int(js) + &
                &               $$CIVD,int(ks) + $$CIVE) + d-b3%addr%b3(int(is) + &
                &               $$CIVC,int(js) + $$CIVD,1 + ($$CIVE + int(ks)))) &
                &               * (d-b3%addr%b3(int(is) + $$CIVC,int(js) + $$CIVD,&
                &               int(ks) + $$CIVE) + d-b3%addr%b3(int(is) + $$CIVC,&
                &               int(js) + $$CIVD,1 + ($$CIVE + int(ks))))) *  &
                &               2.5000000000000000E-001
                              $$PRC0 = $$PRC1
    54|                     ENDDO
                            rms = $$SCREP0
                          ENDIF
    55|                 ENDDO
                      ENDIF
    56|             ENDDO
                  ENDIF
    58|           rms = _sqrt(rms / real((real(%VAL(ijkn)) **  3.00000000E+00)))&
                &   
    59|           IF ((myid_w == 0)) THEN
    60|             #2 = _xlfBeginIO(6,257,#1,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#2),"NORMMAG  :",10,1)
                    _xlfEndIO(%VAL(#2))
    61|             #4 = _xlfBeginIO(6,257,#3,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#4),"NORMMAG  : Norm     = ",22,1)&
                &     
                    #5 = norm.rnn13
                    CALL _xlfWriteLDReal(%VAL(#4),#5,8,8)
                    _xlfEndIO(%VAL(#4))
    62|             #7 = _xlfBeginIO(6,257,#6,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#7),"NORMMAG  : Average  = ",22,1)&
                &     
                    #8 = av / real(( 3.00000000E+00 * (real(ijkn) **  &
                &     3.00000000E+00)))
                    CALL _xlfWriteLDReal(%VAL(#7),#8,8,8)
                    _xlfEndIO(%VAL(#7))
    63|             #10 = _xlfBeginIO(6,257,#9,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#10),"NORMMAG  : B_rms    = ",22,&
                &     1)
                    CALL _xlfWriteLDReal(%VAL(#10),rms,8,8)
                    _xlfEndIO(%VAL(#10))
    64|             #12 = _xlfBeginIO(6,257,#11,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#12),"NORMMAG  :",10,1)
                    _xlfEndIO(%VAL(#12))
    65|           ENDIF
    67|           RETURN
    68|         END FUNCTION normmag


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            23             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            25             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            27             3    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            29                  Loop was not SIMD vectorized because it contains 
                                          memory references norm = ((norm + (((double *)((char 
                                          *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks 
                                          + $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) 
                                          * (((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) 
                                          + (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) 
                                          * (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) 
                                          + (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) * (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]);  with non-vectorizable alignment.
         0            29                  Loop was not SIMD vectorized because it contains 
                                          operation in ((norm + (((double *)((char *)d-b1%addr  
                                          + d-b1%rvo))->b1[].rns4.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) 
                                          * (((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) 
                                          + (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) 
                                          * (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) 
                                          + (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) * (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) which is not  suitable for SIMD 
                                          vectorization.
         0            29                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*((long long) ks + $$CIV2) 
                                          + (d-b1%bounds%mult[].off800)*((long long) js + 
                                          $$CIV1) + (d-b1%bounds%mult[].off824)*((long long) is 
                                          + $$CIV0)) with  non-vectorizable strides.
         0            29                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0            29                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            37            31    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            37            32    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            37            33    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(d-b1%bounds%lbound[].off7
                                          60 + $$CIV5) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3))  with non-vectorizable alignment.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[d-b1%bounds%lbound[].off760 + 
                                          $$CIV5][d-b1%bounds%lbound[].off784 + 
                                          $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(d-b1%bounds%lbound[].off7
                                          60 + $$CIV5) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)) with  non-vectorizable strides.
         0            37                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(d-b1%bounds%lbound[].off7
                                          60 + $$CIV5) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)).
         0            37             4    Outer loop has been unrolled 4 time(s).
         0            37             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            37             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            37             6    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3))  with non-vectorizable alignment.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760][d-b1%bounds%lbound[].off7
                                          84 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)) with  non-vectorizable strides.
         0            37                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(($$CIVF * 
                                          4ll + d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)).
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(1ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3))  with non-vectorizable alignment.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[1ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off
                                          784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(1ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)) with  non-vectorizable strides.
         0            37                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + 
                                          (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)).
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(2ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3))  with non-vectorizable alignment.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[2ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off
                                          784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(2ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)) with  non-vectorizable strides.
         0            37                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + 
                                          (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)).
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(3ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3))  with non-vectorizable alignment.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[3ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)][d-b1%bounds%lbound[].off
                                          784 + $$CIV4][d-b1%bounds%lbound[].off808 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(3ll + (($$CIVF * 4ll + 
                                          d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)) with  non-vectorizable strides.
         0            37                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            37                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + 
                                          (($$CIVF * 4ll + d-b1%bounds%extent[].off768 % 4ll) + 
                                          d-b1%bounds%lbound[].off760)) + 
                                          (d-b1%bounds%mult[].off800)*(d-b1%bounds%lbound[].off7
                                          84 + $$CIV4) + 
                                          (d-b1%bounds%mult[].off824)*(d-b1%bounds%lbound[].off8
                                          08 + $$CIV3)).
         0            38            25    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            38            26    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            38            27    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(d-b2%bounds%lbound[].off8
                                          64 + $$CIV8) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6))  with non-vectorizable alignment.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[d-b2%bounds%lbound[].off864 + 
                                          $$CIV8][d-b2%bounds%lbound[].off888 + 
                                          $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(d-b2%bounds%lbound[].off8
                                          64 + $$CIV8) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)) with  non-vectorizable strides.
         0            38                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(d-b2%bounds%lbound[].off8
                                          64 + $$CIV8) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)).
         0            38             7    Outer loop has been unrolled 4 time(s).
         0            38             7    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            38             8    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            38             9    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6))  with non-vectorizable alignment.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864][d-b2%bounds%lbound[].off8
                                          88 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)) with  non-vectorizable strides.
         0            38                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(($$CIV10 * 
                                          4ll + d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)).
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(1ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6))  with non-vectorizable alignment.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[1ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off
                                          888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(1ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)) with  non-vectorizable strides.
         0            38                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + 
                                          (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) 
                                          + d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)).
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(2ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6))  with non-vectorizable alignment.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[2ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off
                                          888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(2ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)) with  non-vectorizable strides.
         0            38                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + 
                                          (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) 
                                          + d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)).
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(3ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6))  with non-vectorizable alignment.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[3ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)][d-b2%bounds%lbound[].off
                                          888 + $$CIV7][d-b2%bounds%lbound[].off912 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(3ll + (($$CIV10 * 4ll + 
                                          d-b2%bounds%extent[].off872 % 4ll) + 
                                          d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)) with  non-vectorizable strides.
         0            38                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            38                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + 
                                          (($$CIV10 * 4ll + d-b2%bounds%extent[].off872 % 4ll) 
                                          + d-b2%bounds%lbound[].off864)) + 
                                          (d-b2%bounds%mult[].off904)*(d-b2%bounds%lbound[].off8
                                          88 + $$CIV7) + 
                                          (d-b2%bounds%mult[].off928)*(d-b2%bounds%lbound[].off9
                                          12 + $$CIV6)).
         0            39            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            39            20    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            39            21    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(d-b3%bounds%lbound[].off9
                                          68 + $$CIVB) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9))  with non-vectorizable alignment.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[d-b3%bounds%lbound[].off968 + 
                                          $$CIVB][d-b3%bounds%lbound[].off992 + 
                                          $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(d-b3%bounds%lbound[].off9
                                          68 + $$CIVB) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)) with  non-vectorizable strides.
         0            39                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(d-b3%bounds%lbound[].off9
                                          68 + $$CIVB) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)).
         0            39            10    Outer loop has been unrolled 4 time(s).
         0            39            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            39            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            39            12    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9))  with non-vectorizable alignment.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968][d-b3%bounds%lbound[].off9
                                          92 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)) with  non-vectorizable strides.
         0            39                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(($$CIV11 * 
                                          4ll + d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)).
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(1ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9))  with non-vectorizable alignment.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off
                                          992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] 
                                          / _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(1ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)) with  non-vectorizable strides.
         0            39                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + 
                                          (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) 
                                          + d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)).
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(2ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9))  with non-vectorizable alignment.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[2ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off
                                          992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] 
                                          / _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(2ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)) with  non-vectorizable strides.
         0            39                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + 
                                          (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) 
                                          + d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)).
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(3ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9))  with non-vectorizable alignment.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[3ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)][d-b3%bounds%lbound[].off
                                          992 + $$CIVA][d-b3%bounds%lbound[].off1016 + $$CIV9] 
                                          / _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(3ll + (($$CIV11 * 4ll + 
                                          d-b3%bounds%extent[].off976 % 4ll) + 
                                          d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)) with  non-vectorizable strides.
         0            39                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            39                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + 
                                          (($$CIV11 * 4ll + d-b3%bounds%extent[].off976 % 4ll) 
                                          + d-b3%bounds%lbound[].off968)) + 
                                          (d-b3%bounds%mult[].off1008)*(d-b3%bounds%lbound[].off
                                          992 + $$CIVA) + 
                                          (d-b3%bounds%mult[].off1032)*(d-b3%bounds%lbound[].off
                                          1016 + $$CIV9)).
         0            42            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            44            14    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            46            15    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            48                  Loop was not SIMD vectorized because it contains 
                                          memory references av = av + (((((((double *)((char 
                                          *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks 
                                          + $$CIVE][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC] + ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          + ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) *  5.0000000000000000E-001;  with 
                                          non-vectorizable alignment.
         0            48                  Loop was not SIMD vectorized because it contains 
                                          operation in av + (((((((double *)((char *)d-b1%addr  
                                          + d-b1%rvo))->b1[].rns4.[(long long) ks + 
                                          $$CIVE][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC] + ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          + ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) *  5.0000000000000000E-001 which is not  
                                          suitable for SIMD vectorization.
         0            48                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*((long long) ks + $$CIVE) 
                                          + (d-b1%bounds%mult[].off800)*((long long) js + 
                                          $$CIVD) + (d-b1%bounds%mult[].off824)*((long long) is 
                                          + $$CIVC)) with  non-vectorizable strides.
         0            48                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0            48                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            51                  Loop was not SIMD vectorized because it contains 
                                          memory references $$SCREP0 = $$SCREP0 + (((((double 
                                          *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long 
                                          long) ks + $$CIVE][(long long) js + $$CIVD][(long 
                                          long) is + $$CIVC] + ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          * (((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          * (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC])) 
                                          + (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) * (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC])) *  2.5000000000000000E-001;  with 
                                          non-vectorizable alignment.
         0            51                  Loop was not SIMD vectorized because it contains 
                                          operation in $$SCREP0 + (((((double *)((char 
                                          *)d-b1%addr  + d-b1%rvo))->b1[].rns4.[(long long) ks 
                                          + $$CIVE][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC] + ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          * (((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b1%addr  + 
                                          d-b1%rvo))->b1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          * (((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b2%addr  + 
                                          d-b2%rvo))->b2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC])) 
                                          + (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) * (((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-b3%addr  + 
                                          d-b3%rvo))->b3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC])) *  2.5000000000000000E-001 which is not  
                                          suitable for SIMD vectorization.
         0            51                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*((long long) ks + $$CIVE) 
                                          + (d-b1%bounds%mult[].off800)*((long long) js + 
                                          $$CIVD) + (d-b1%bounds%mult[].off824)*((long long) is 
                                          + $$CIVC)) with  non-vectorizable strides.
         0            51                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0            51                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.


     8|         REAL*8 FUNCTION normmag (rms)
    22|           norm =  0.0000000000000000E+000
    23|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIV2 = 0
       Id=1         DO $$CIV2 = $$CIV2, int((1 + (int(ke) - int(ks))))-1
    25|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int((1 + (int(je) - int(js))))-1
    27|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV0 = 0
                            $$PRC2 = d-b1%addr%b1(int(is),$$CIV1 + int(js),&
                &             $$CIV2 + int(ks))
                            $$ICM0 = $$CIV1 + int(js)
                            $$ICM1 = $$CIV2 + int(ks)
    29|                     $$ICM2 = 1 + ($$CIV2 + int(ks))
                            $$ICM3 = 1 + ($$CIV1 + int(js))
    27|Id=3                 DO $$CIV0 = $$CIV0, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC3 = d-b1%addr%b1(1 + (int(is) + $$CIV0),&
                &               $$ICM0,$$ICM1)
    29|                       norm = ((norm + ($$PRC2 + $$PRC3) * ($$PRC2 + &
                &               $$PRC3)) + (d-b2%addr%b2(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-b2%addr%b2(int(is) + $$CIV0,$$ICM3,&
                &               $$ICM1)) * (d-b2%addr%b2(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-b2%addr%b2(int(is) + $$CIV0,$$ICM3,&
                &               $$ICM1))) + (d-b3%addr%b3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-b3%addr%b3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM2)) * (d-b3%addr%b3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-b3%addr%b3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM2))
                              $$PRC2 = $$PRC3
    33|                     ENDDO
                          ENDIF
    34|                 ENDDO
                      ENDIF
    35|             ENDDO
                  ENDIF
    36|           $$csx0 = _sqrt(((%VAL(norm) *  2.5000000000000000E-001) / &
                &   real(real((real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                  norm.rnn13 = $$csx0
    37|           IF ((MOD(d-b1%bounds%extent[].off768, 4) > 0  .AND.  &
                &   d-b1%bounds%extent[].off768 > 0)) THEN
                    $$CIV5 = 0
       Id=31        DO $$CIV5 = $$CIV5, MOD(d-b1%bounds%extent[].off768, int(&
                &       4))-1
                      IF ((d-b1%bounds%extent[].off792 > 0)) THEN
                        $$CIV4 = 0
       Id=32            DO $$CIV4 = $$CIV4, int(d-b1%bounds%extent[].off792)&
                &           -1
                          IF ((d-b1%bounds%extent[].off816 > 0)) THEN
                            $$CIV3 = 0
       Id=33                DO $$CIV3 = $$CIV3, int(&
                &               d-b1%bounds%extent[].off816)-1
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,&
                &               d-b1%bounds%lbound[].off760 + $$CIV5) = &
                &               d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,&
                &               d-b1%bounds%lbound[].off760 + $$CIV5) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-b1%bounds%extent[].off768 > 0  .AND.  &
                &   d-b1%bounds%extent[].off768 > MOD(d-b1%bounds%extent[].off768,&
                &    4))) THEN
                    $$CIVF = int(0)
       Id=4         DO $$CIVF = $$CIVF, int((((d-b1%bounds%extent[].off768 - &
                &       MOD(d-b1%bounds%extent[].off768, 4)) - 1) / 4 + 1))-1
                      IF ((d-b1%bounds%extent[].off792 > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int(d-b1%bounds%extent[].off792)&
                &           -1
                          IF ((d-b1%bounds%extent[].off816 > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(&
                &               d-b1%bounds%extent[].off816)-1
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760) / $$csx0
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) / $$csx0
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) / $$csx0
                              d-b1%addr%b1(d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) = d-b1%addr%b1(&
                &               d-b1%bounds%lbound[].off808 + $$CIV3,&
                &               d-b1%bounds%lbound[].off784 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-b1%bounds%extent[].off768, 4)) + &
                &               d-b1%bounds%lbound[].off760)) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    38|           IF ((MOD(d-b2%bounds%extent[].off872, 4) > 0  .AND.  &
                &   d-b2%bounds%extent[].off872 > 0)) THEN
                    $$CIV8 = 0
       Id=25        DO $$CIV8 = $$CIV8, MOD(d-b2%bounds%extent[].off872, int(&
                &       4))-1
                      IF ((d-b2%bounds%extent[].off896 > 0)) THEN
                        $$CIV7 = 0
       Id=26            DO $$CIV7 = $$CIV7, int(d-b2%bounds%extent[].off896)&
                &           -1
                          IF ((d-b2%bounds%extent[].off920 > 0)) THEN
                            $$CIV6 = 0
       Id=27                DO $$CIV6 = $$CIV6, int(&
                &               d-b2%bounds%extent[].off920)-1
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,&
                &               d-b2%bounds%lbound[].off864 + $$CIV8) = &
                &               d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,&
                &               d-b2%bounds%lbound[].off864 + $$CIV8) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-b2%bounds%extent[].off872 > 0  .AND.  &
                &   d-b2%bounds%extent[].off872 > MOD(d-b2%bounds%extent[].off872,&
                &    4))) THEN
                    $$CIV10 = int(0)
       Id=7         DO $$CIV10 = $$CIV10, int((((d-b2%bounds%extent[].off872 &
                &       - MOD(d-b2%bounds%extent[].off872, 4)) - 1) / 4 + 1))-1
                      IF ((d-b2%bounds%extent[].off896 > 0)) THEN
                        $$CIV7 = 0
       Id=8             DO $$CIV7 = $$CIV7, int(d-b2%bounds%extent[].off896)&
                &           -1
                          IF ((d-b2%bounds%extent[].off920 > 0)) THEN
                            $$CIV6 = 0
       Id=9                 DO $$CIV6 = $$CIV6, int(&
                &               d-b2%bounds%extent[].off920)-1
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-b2%bounds%extent[].off872, 4)) + &
                &               d-b2%bounds%lbound[].off864) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-b2%bounds%extent[].off872, 4)) + &
                &               d-b2%bounds%lbound[].off864) / $$csx0
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) / $$csx0
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) / $$csx0
                              d-b2%addr%b2(d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) = d-b2%addr%b2(&
                &               d-b2%bounds%lbound[].off912 + $$CIV6,&
                &               d-b2%bounds%lbound[].off888 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-b2%bounds%extent[].off872, 4))&
                &                + d-b2%bounds%lbound[].off864)) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    39|           IF ((MOD(d-b3%bounds%extent[].off976, 4) > 0  .AND.  &
                &   d-b3%bounds%extent[].off976 > 0)) THEN
                    $$CIVB = 0
       Id=19        DO $$CIVB = $$CIVB, MOD(d-b3%bounds%extent[].off976, int(&
                &       4))-1
                      IF ((d-b3%bounds%extent[].off1000 > 0)) THEN
                        $$CIVA = 0
       Id=20            DO $$CIVA = $$CIVA, int(d-b3%bounds%extent[].off1000)&
                &           -1
                          IF ((d-b3%bounds%extent[].off1024 > 0)) THEN
                            $$CIV9 = 0
       Id=21                DO $$CIV9 = $$CIV9, int(&
                &               d-b3%bounds%extent[].off1024)-1
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,&
                &               d-b3%bounds%lbound[].off968 + $$CIVB) = &
                &               d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,&
                &               d-b3%bounds%lbound[].off968 + $$CIVB) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-b3%bounds%extent[].off976 > 0  .AND.  &
                &   d-b3%bounds%extent[].off976 > MOD(d-b3%bounds%extent[].off976,&
                &    4))) THEN
                    $$CIV11 = int(0)
       Id=10        DO $$CIV11 = $$CIV11, int((((d-b3%bounds%extent[].off976 &
                &       - MOD(d-b3%bounds%extent[].off976, 4)) - 1) / 4 + 1))-1
                      IF ((d-b3%bounds%extent[].off1000 > 0)) THEN
                        $$CIVA = 0
       Id=11            DO $$CIVA = $$CIVA, int(d-b3%bounds%extent[].off1000)&
                &           -1
                          IF ((d-b3%bounds%extent[].off1024 > 0)) THEN
                            $$CIV9 = 0
       Id=12                DO $$CIV9 = $$CIV9, int(&
                &               d-b3%bounds%extent[].off1024)-1
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,(&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,($$CIV11 * 4 &
                &               + MOD(d-b3%bounds%extent[].off976, 4)) + &
                &               d-b3%bounds%lbound[].off968) / $$csx0
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) / $$csx0
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) / $$csx0
                              d-b3%addr%b3(d-b3%bounds%lbound[].off1016 + &
                &               $$CIV9,d-b3%bounds%lbound[].off992 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) = d-b3%addr%b3(&
                &               d-b3%bounds%lbound[].off1016 + $$CIV9,&
                &               d-b3%bounds%lbound[].off992 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-b3%bounds%extent[].off976, 4))&
                &                + d-b3%bounds%lbound[].off968)) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    40|           av =  0.0000000000000000E+000
    41|           rms =  0.0000000000000000E+000
    42|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIVE = 0
       Id=13        DO $$CIVE = $$CIVE, int((1 + (int(ke) - int(ks))))-1
    44|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVD = 0
       Id=14            DO $$CIVD = $$CIVD, int((1 + (int(je) - int(js))))-1
    46|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIVC = 0
                            $$SCREP0 = rms
                            $$PRC0 = d-b1%addr%b1(int(is),$$CIVD + int(js),&
                &             $$CIVE + int(ks))
                            $$ICM4 = $$CIVD + int(js)
                            $$ICM5 = $$CIVE + int(ks)
    48|                     $$ICM6 = 1 + ($$CIVE + int(ks))
                            $$ICM7 = 1 + ($$CIVD + int(js))
    46|Id=15                DO $$CIVC = $$CIVC, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC1 = d-b1%addr%b1(1 + (int(is) + $$CIVC),&
                &               $$ICM4,$$ICM5)
    48|                       av = av + ((((($$PRC0 + $$PRC1) + d-b2%addr%b2(&
                &               int(is) + $$CIVC,$$ICM4,$$ICM5)) + d-b2%addr%b2(&
                &               int(is) + $$CIVC,$$ICM7,$$ICM5)) + d-b3%addr%b3(&
                &               int(is) + $$CIVC,$$ICM4,$$ICM5)) + d-b3%addr%b3(&
                &               int(is) + $$CIVC,$$ICM4,$$ICM6)) *  &
                &               5.0000000000000000E-001
    51|                       $$SCREP0 = $$SCREP0 + ((($$PRC0 + $$PRC1) * (&
                &               $$PRC0 + $$PRC1) + (d-b2%addr%b2(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-b2%addr%b2(int(is) + $$CIVC,&
                &               $$ICM7,$$ICM5)) * (d-b2%addr%b2(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-b2%addr%b2(int(is) + $$CIVC,&
                &               $$ICM7,$$ICM5))) + (d-b3%addr%b3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-b3%addr%b3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM6)) * (d-b3%addr%b3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-b3%addr%b3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM6))) *  2.5000000000000000E-001
                              $$PRC0 = $$PRC1
    54|                     ENDDO
                            rms = $$SCREP0
                          ENDIF
    55|                 ENDDO
                      ENDIF
    56|             ENDDO
                  ENDIF
    58|           rms = _sqrt(rms / real((real(%VAL(ijkn)) **  3.00000000E+00)))&
                &   
    59|           IF ((myid_w == 0)) THEN
    60|             #2 = _xlfBeginIO(6,257,#1,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#2),"NORMMAG  :",10,1)
                    _xlfEndIO(%VAL(#2))
    61|             #4 = _xlfBeginIO(6,257,#3,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#4),"NORMMAG  : Norm     = ",22,1)&
                &     
                    #5 = norm.rnn13
                    CALL _xlfWriteLDReal(%VAL(#4),#5,8,8)
                    _xlfEndIO(%VAL(#4))
    62|             #7 = _xlfBeginIO(6,257,#6,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#7),"NORMMAG  : Average  = ",22,1)&
                &     
                    #8 = av / real(( 3.00000000E+00 * (real(ijkn) **  &
                &     3.00000000E+00)))
                    CALL _xlfWriteLDReal(%VAL(#7),#8,8,8)
                    _xlfEndIO(%VAL(#7))
    63|             #10 = _xlfBeginIO(6,257,#9,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#10),"NORMMAG  : B_rms    = ",22,&
                &     1)
                    CALL _xlfWriteLDReal(%VAL(#10),rms,8,8)
                    _xlfEndIO(%VAL(#10))
    64|             #12 = _xlfBeginIO(6,257,#11,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#12),"NORMMAG  :",10,1)
                    _xlfEndIO(%VAL(#12))
    65|           ENDIF
    67|           RETURN
    68|         END FUNCTION normmag

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- -sss ssss
 CCR's set/used:   sss- ssss
     | 000000                           PDEF     normmag
    8|                                  PROC      .rms,gr3
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 stfd     DB61FFD8   1     STFL      #stack(gr1,-40)=fp27
    0| 000014 stfd     DB41FFD0   1     STFL      #stack(gr1,-48)=fp26
    0| 000018 stfd     DB21FFC8   1     STFL      #stack(gr1,-56)=fp25
    0| 00001C std      FBE1FFC0   1     ST8       #stack(gr1,-64)=gr31
    0| 000020 std      FBC1FFB8   1     ST8       #stack(gr1,-72)=gr30
    0| 000024 std      FBA1FFB0   1     ST8       #stack(gr1,-80)=gr29
    0| 000028 std      FB81FFA8   1     ST8       #stack(gr1,-88)=gr28
    0| 00002C std      FB61FFA0   1     ST8       #stack(gr1,-96)=gr27
    0| 000030 std      FB41FF98   1     ST8       #stack(gr1,-104)=gr26
    0| 000034 std      FB21FF90   1     ST8       #stack(gr1,-112)=gr25
    0| 000038 std      FB01FF88   1     ST8       #stack(gr1,-120)=gr24
    0| 00003C std      FAE1FF80   1     ST8       #stack(gr1,-128)=gr23
    0| 000040 std      FAC1FF78   1     ST8       #stack(gr1,-136)=gr22
    0| 000044 std      FAA1FF70   1     ST8       #stack(gr1,-144)=gr21
    0| 000048 std      FA81FF68   1     ST8       #stack(gr1,-152)=gr20
    0| 00004C std      FA61FF60   1     ST8       #stack(gr1,-160)=gr19
    0| 000050 std      FA41FF58   1     ST8       #stack(gr1,-168)=gr18
    0| 000054 std      FA21FF50   1     ST8       #stack(gr1,-176)=gr17
    0| 000058 std      FA01FF48   1     ST8       #stack(gr1,-184)=gr16
    0| 00005C std      F9E1FF40   1     ST8       #stack(gr1,-192)=gr15
    0| 000060 std      F9C1FF38   1     ST8       #stack(gr1,-200)=gr14
    0| 000064 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 000068 mfcr     7D800026   1     LFCR      gr12=cr[24],2
    0| 00006C stw      91810008   1     ST4A      #stack(gr1,8)=gr12
    0| 000070 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000074 stdu     F821FD41   1     ST8U      gr1,#stack(gr1,-704)=gr1
    0| 000078 std      F86100A8   1     ST8       #SPILL0(gr1,168)=gr3
   27| 00007C ld       E8E20000   1     L8        gr7=.&&N&field(gr2,0)
   23| 000080 ld       E9220000   1     L8        gr9=.&&N&&grid(gr2,0)
   36| 000084 ld       E9020000   1     L8        gr8=.&&N&&param(gr2,0)
   22| 000088 ld       EBA20000   1     L8        gr29=.+CONSTANT_AREA(gr2,0)
   39| 00008C ld       EB8703D0   1     L8        gr28=<s17:d976:l8>(gr7,976)
   38| 000090 ld       EB670368   1     L8        gr27=<s17:d872:l8>(gr7,872)
   37| 000094 ld       EB470300   1     L8        gr26=<s17:d768:l8>(gr7,768)
   23| 000098 lwa      EB290012   1     L4A       gr25=<s46:d16:l4>(gr9,16)
   23| 00009C lwa      E8A90016   1     L4A       gr5=<s46:d20:l4>(gr9,20)
   25| 0000A0 lwa      EA89000E   1     L4A       gr20=<s46:d12:l4>(gr9,12)
   39| 0000A4 std      FB8100B0   1     ST8       #SPILL1(gr1,176)=gr28
   38| 0000A8 std      FB6100B8   1     ST8       #SPILL2(gr1,184)=gr27
   37| 0000AC std      FB4100C0   1     ST8       #SPILL3(gr1,192)=gr26
   39| 0000B0 sradi    7F801674   1     SRA8CA    gr0,ca=gr28,2
   23| 0000B4 std      FB2100C8   1     ST8       #SPILL4(gr1,200)=gr25
   27| 0000B8 ld       EA6702F0   1     L8        gr19=<s17:d752:l8>(gr7,752)
   29| 0000BC ld       E9C70340   1     L8        gr14=<s17:d832:l8>(gr7,832)
   25| 0000C0 std      FA8100E0   1     ST8       #SPILL7(gr1,224)=gr20
   39| 0000C4 addze    7FE00194   1     ADDE      gr31,ca=gr0,0,ca
   39| 0000C8 sradi    7F641674   1     SRA8CA    gr4,ca=gr27,2
   23| 0000CC subf     7D592850   1     S         gr10=gr5,gr25
   27| 0000D0 std      FA6100E8   1     ST8       #SPILL8(gr1,232)=gr19
   29| 0000D4 ld       EAE703D8   1     L8        gr23=<s17:d984:l8>(gr7,984)
   29| 0000D8 std      F9C10110   1     ST8       #SPILL13(gr1,272)=gr14
   39| 0000DC addze    7D840194   1     ADDE      gr12,ca=gr4,0,ca
   38| 0000E0 sradi    7F461674   1     SRA8CA    gr6,ca=gr26,2
   39| 0000E4 rldicr   7BF21764   1     SLL8      gr18=gr31,2
   38| 0000E8 addze    7D660194   1     ADDE      gr11,ca=gr6,0,ca
   39| 0000EC std      FA4100F0   1     ST8       #SPILL9(gr1,240)=gr18
   29| 0000F0 std      FAE10148   1     ST8       #SPILL20(gr1,328)=gr23
   37| 0000F4 rldicr   79701764   1     SLL8      gr16=gr11,2
   29| 0000F8 ld       E96703C0   1     L8        gr11=<s17:d960:l8>(gr7,960)
   37| 0000FC std      FA010100   1     ST8       #SPILL11(gr1,256)=gr16
   23| 000100 addic.   35EA0001   1     AI_R      gr15,cr0=gr10,1,ca"
   36| 000104 lwa      E948000E   1     L4A       gr10=<s57:d12:l4>(gr8,12)
   23| 000108 std      F9E10108   1     ST8       #SPILL12(gr1,264)=gr15
   27| 00010C lwa      EB090002   1     L4A       gr24=<s46:d0:l4>(gr9,0)
   29| 000110 std      F9610130   1     ST8       #SPILL17(gr1,304)=gr11
   39| 000114 ld       E96100B0   1     L8        gr11=#SPILL1(gr1,176)
   25| 000118 lwa      E809000A   1     L4A       gr0=<s46:d8:l4>(gr9,8)
   36| 00011C std      F9410138   1     ST8       #SPILL18(gr1,312)=gr10
   27| 000120 ld       EAA702D8   1     L8        gr21=<s17:d728:l8>(gr7,728)
   38| 000124 rldicr   79911764   1     SLL8      gr17=gr12,2
   29| 000128 ld       EBE703A8   1     L8        gr31=<s17:d936:l8>(gr7,936)
   27| 00012C std      FB0100D0   1     ST8       #SPILL5(gr1,208)=gr24
   39| 000130 subf     7D525850   1     S         gr10=gr11,gr18
   38| 000134 ld       E96100B8   1     L8        gr11=#SPILL2(gr1,184)
   27| 000138 std      FAA100D8   1     ST8       #SPILL6(gr1,216)=gr21
   39| 00013C std      F9410198   1     ST8       #SPILL30(gr1,408)=gr10
   27| 000140 lwa      E9890006   1     L4A       gr12=<s46:d4:l4>(gr9,4)
   29| 000144 ld       E9270358   1     L8        gr9=<s17:d856:l8>(gr7,856)
   29| 000148 std      FBE10118   1     ST8       #SPILL14(gr1,280)=gr31
   38| 00014C subf     7E515850   1     S         gr18=gr11,gr17
   37| 000150 ld       E96100C0   1     L8        gr11=#SPILL3(gr1,192)
   38| 000154 std      FA4101A0   1     ST8       #SPILL31(gr1,416)=gr18
   22| 000158 lfs      C3FD000C   1     LFS       fp31=+CONSTANT_AREA(gr29,12)
   27| 00015C std      F9810120   1     ST8       #SPILL15(gr1,288)=gr12
   27| 000160 ld       E8670338   1     L8        gr3=<s17:d824:l8>(gr7,824)
   27| 000164 ld       EBC70308   1     L8        gr30=<s17:d776:l8>(gr7,776)
   29| 000168 std      F9210128   1     ST8       #SPILL16(gr1,296)=gr9
   27| 00016C ld       E8870320   1     L8        gr4=<s17:d800:l8>(gr7,800)
   29| 000170 ld       E8A703A0   1     L8        gr5=<s17:d928:l8>(gr7,928)
   22| 000174 fmr      FC40F890   1     LRFL      fp2=fp31
   37| 000178 ld       EBA70330   1     L8        gr29=<s17:d816:l8>(gr7,816)
   37| 00017C ld       EB870318   1     L8        gr28=<s17:d792:l8>(gr7,792)
   38| 000180 std      FA2100F8   1     ST8       #SPILL10(gr1,248)=gr17
   29| 000184 ld       E8C70408   1     L8        gr6=<s17:d1032:l8>(gr7,1032)
   29| 000188 ld       E9070370   1     L8        gr8=<s17:d880:l8>(gr7,880)
   29| 00018C ld       EB670388   1     L8        gr27=<s17:d904:l8>(gr7,904)
   38| 000190 ld       EB470398   1     L8        gr26=<s17:d920:l8>(gr7,920)
   38| 000194 ld       EB270380   1     L8        gr25=<s17:d896:l8>(gr7,896)
   29| 000198 ld       EB0703F0   1     L8        gr24=<s17:d1008:l8>(gr7,1008)
   39| 00019C ld       EAE70400   1     L8        gr23=<s17:d1024:l8>(gr7,1024)
   29| 0001A0 std      F9010140   1     ST8       #SPILL19(gr1,320)=gr8
   39| 0001A4 ld       EAC703E8   1     L8        gr22=<s17:d1000:l8>(gr7,1000)
   37| 0001A8 ld       EAA70328   1     L8        gr21=<s17:d808:l8>(gr7,808)
   37| 0001AC ld       EA870310   1     L8        gr20=<s17:d784:l8>(gr7,784)
   37| 0001B0 ld       EA6702F8   1     L8        gr19=<s17:d760:l8>(gr7,760)
   38| 0001B4 ld       E9E70390   1     L8        gr15=<s17:d912:l8>(gr7,912)
   38| 0001B8 ld       E9C70378   1     L8        gr14=<s17:d888:l8>(gr7,888)
   38| 0001BC ld       EBE70360   1     L8        gr31=<s17:d864:l8>(gr7,864)
   37| 0001C0 std      FAA10150   1     ST8       #SPILL21(gr1,336)=gr21
   37| 0001C4 std      FA810158   1     ST8       #SPILL22(gr1,344)=gr20
   37| 0001C8 std      FA610160   1     ST8       #SPILL23(gr1,352)=gr19
   38| 0001CC std      F9E10168   1     ST8       #SPILL24(gr1,360)=gr15
   38| 0001D0 std      F9C10170   1     ST8       #SPILL25(gr1,368)=gr14
   38| 0001D4 std      FBE10178   1     ST8       #SPILL26(gr1,376)=gr31
   39| 0001D8 ld       E98703F8   1     L8        gr12=<s17:d1016:l8>(gr7,1016)
   39| 0001DC ld       E92703E0   1     L8        gr9=<s17:d992:l8>(gr7,992)
   39| 0001E0 ld       E8E703C8   1     L8        gr7=<s17:d968:l8>(gr7,968)
   37| 0001E4 subf     7E305850   1     S         gr17=gr11,gr16
   23| 0001E8 mcrf     4C800000   1     LRCR      cr1=cr0
   37| 0001EC std      FA2101A8   1     ST8       #SPILL32(gr1,424)=gr17
   39| 0001F0 std      F9810180   1     ST8       #SPILL27(gr1,384)=gr12
   39| 0001F4 std      F9210188   1     ST8       #SPILL28(gr1,392)=gr9
   39| 0001F8 std      F8E10190   1     ST8       #SPILL29(gr1,400)=gr7
   23| 0001FC bc       40810258   1     BF        CL.62,cr0,0x2/gt,taken=50%(0,0)
   25| 000200 ld       E90100E0   1     L8        gr8=#SPILL7(gr1,224)
   23| 000204 addi     3AA00000   1     LI        gr21=0
   25| 000208 subf     7CE04050   1     S         gr7=gr8,gr0
   23| 00020C std      FAA101B0   1     ST8       #SPILL33(gr1,432)=gr21
   25| 000210 addic.   36A70001   1     AI_R      gr21,cr0=gr7,1,ca"
    0| 000214 bc       40810240   1     BF        CL.62,cr0,0x2/gt,taken=20%(20,80)
    0| 000218 ld       E96100C8   1     L8        gr11=#SPILL4(gr1,200)
    0| 00021C ld       E8E100D0   1     L8        gr7=#SPILL5(gr1,208)
    0| 000220 mulld    7D2021D2   1     M         gr9=gr0,gr4
    0| 000224 mulld    7D4BF1D2   1     M         gr10=gr11,gr30
    0| 000228 ld       EA6100D8   1     L8        gr19=#SPILL6(gr1,216)
    0| 00022C ld       EA4100E8   1     L8        gr18=#SPILL8(gr1,232)
    0| 000230 add      7D895214   1     A         gr12=gr9,gr10
    0| 000234 mulld    7D0339D2   1     M         gr8=gr3,gr7
    0| 000238 add      7D329A14   1     A         gr9=gr18,gr19
   29| 00023C ld       EA4100D0   1     L8        gr18=#SPILL5(gr1,208)
    0| 000240 add      7E684A14   1     A         gr19=gr8,gr9
   29| 000244 ld       E9010110   1     L8        gr8=#SPILL13(gr1,272)
    0| 000248 ld       EA810120   1     L8        gr20=#SPILL15(gr1,288)
    0| 00024C subfic   20E70001   1     SFI       gr7=1,gr7,ca"
   29| 000250 mulld    7D2591D2   1     M         gr9=gr5,gr18
   29| 000254 mulld    7D6691D2   1     M         gr11=gr6,gr18
   29| 000258 ld       EA410118   1     L8        gr18=#SPILL14(gr1,280)
   29| 00025C subf     7D454050   1     S         gr10=gr8,gr5
    0| 000260 add      7CE7A214   1     A         gr7=gr7,gr20
    0| 000264 rldicl   78F4F842   1     SRL8      gr20=gr7,1
   29| 000268 subf     7D069050   1     S         gr8=gr18,gr6
   29| 00026C ld       EA410128   1     L8        gr18=#SPILL16(gr1,296)
    0| 000270 cmpdi    2F340000   1     C8        cr6=gr20,0
   29| 000274 add      7D4A9214   1     A         gr10=gr10,gr18
   29| 000278 ld       EA410130   1     L8        gr18=#SPILL17(gr1,304)
   29| 00027C add      7D295214   1     A         gr9=gr9,gr10
    0| 000280 ld       E94100D0   1     L8        gr10=#SPILL5(gr1,208)
   29| 000284 std      F92101B8   1     ST8       #SPILL34(gr1,440)=gr9
   29| 000288 add      7D089214   1     A         gr8=gr8,gr18
   29| 00028C add      7E485A14   1     A         gr18=gr8,gr11
    0| 000290 ld       E9610120   1     L8        gr11=#SPILL15(gr1,288)
    0| 000294 subf     7D049850   1     S         gr8=gr19,gr4
    0| 000298 subf     7D2A5850   1     S         gr9=gr11,gr10
    0| 00029C addic.   35290001   1     AI_R      gr9,cr0=gr9,1,ca"
    0| 0002A0 add      7D286214   1     A         gr9=gr8,gr12
    0| 0002A4 mcrf     4F800000   1     LRCR      cr7=cr0
    0| 0002A8 std      F92101C0   1     ST8       #SPILL35(gr1,448)=gr9
    0| 0002AC andi.    70E70001   1     RN4_R     gr7,cr0=gr7,0,0x1
   23|                              CL.63:
    0| 0002B0 ld       E90100C8   1     L8        gr8=#SPILL4(gr1,200)
    0| 0002B4 ld       E94101B0   1     L8        gr10=#SPILL33(gr1,432)
   25| 0002B8 addi     38E00000   1     LI        gr7=0
    0| 0002BC add      7D285214   1     A         gr9=gr8,gr10
    0| 0002C0 bc       409D0170   1     BF        CL.64,cr7,0x2/gt,taken=20%(20,80)
    0| 0002C4 ld       E9810140   1     L8        gr12=#SPILL19(gr1,320)
    0| 0002C8 ld       EBE10148   1     L8        gr31=#SPILL20(gr1,328)
    0| 0002CC addi     39690001   1     AI        gr11=gr9,1
    0| 0002D0 mulld    7E29F1D2   1     M         gr17=gr9,gr30
    0| 0002D4 mulld    7D4961D2   1     M         gr10=gr9,gr12
    0| 0002D8 mulld    7DE9F9D2   1     M         gr15=gr9,gr31
    0| 0002DC ld       E92101B8   1     L8        gr9=#SPILL34(gr1,440)
   27| 0002E0 ld       E90101C0   1     L8        gr8=#SPILL35(gr1,448)
    0| 0002E4 mulld    7E0BF9D2   1     M         gr16=gr11,gr31
    0| 0002E8 add      7DC95214   1     A         gr14=gr9,gr10
    0| 0002EC ori      60210000   1     XNOP      
    0| 0002F0 ori      60210000   1     XNOP      
    0| 0002F4 ori      60210000   1     XNOP      
   25|                              CL.65:
   27| 0002F8 add      7D203A14   1     A         gr9=gr0,gr7
   27| 0002FC lfdux    7C0824EE   1     LFDU      fp0,gr8=b1(gr8,gr4,0)
   29| 000300 addi     39490001   1     AI        gr10=gr9,1
   27| 000304 mulld    7D8449D2   1     M         gr12=gr4,gr9
   29| 000308 mulld    7FE9C1D2   1     M         gr31=gr9,gr24
   29| 00030C mulld    7D4AD9D2   1     M         gr10=gr10,gr27
   29| 000310 mulld    7D69D9D2   1     M         gr11=gr9,gr27
   27| 000314 add      7D2C9A14   1     A         gr9=gr12,gr19
   29| 000318 add      7FFF9214   1     A         gr31=gr31,gr18
   27| 00031C add      7D298A14   1     A         gr9=gr9,gr17
   29| 000320 add      7D4A7214   1     A         gr10=gr10,gr14
   29| 000324 add      7D6B7214   1     A         gr11=gr11,gr14
   29| 000328 add      7D90FA14   1     A         gr12=gr16,gr31
   29| 00032C add      7FFF7A14   1     A         gr31=gr31,gr15
    0| 000330 mtspr    7E8903A6   1     LCTR      ctr=gr20
    0| 000334 bc       41820038   1     BT        CL.621,cr0,0x4/eq,taken=50%(0,0)
   27| 000338 lfdux    7C291CEE   1     LFDU      fp1,gr9=b1(gr9,gr3,0)
   29| 00033C lfdux    7C6B2CEE   1     LFDU      fp3,gr11=b2(gr11,gr5,0)
   29| 000340 lfdux    7C8A2CEE   1     LFDU      fp4,gr10=b2(gr10,gr5,0)
   29| 000344 lfdux    7CBF34EE   1     LFDU      fp5,gr31=b3(gr31,gr6,0)
   29| 000348 lfdux    7CCC34EE   1     LFDU      fp6,gr12=b3(gr12,gr6,0)
   29| 00034C fadd     FCE0082A   1     AFL       fp7=fp0,fp1,fcr
   29| 000350 fmr      FC000890   2     LRFL      fp0=fp1
   29| 000354 fadd     FC23202A   2     AFL       fp1=fp3,fp4,fcr
   29| 000358 fadd     FC65302A   2     AFL       fp3=fp5,fp6,fcr
   29| 00035C fmadd    FC4711FA   2     FMA       fp2=fp2,fp7,fp7,fcr
   29| 000360 fmadd    FC21107A   2     FMA       fp1=fp2,fp1,fp1,fcr
   29| 000364 fmadd    FC4308FA   2     FMA       fp2=fp1,fp3,fp3,fcr
    0| 000368 bc       419A00BC   1     BT        CL.522,cr6,0x4/eq,taken=20%(20,80)
    0|                              CL.621:
   27| 00036C lfdux    7D091CEE   1     LFDU      fp8,gr9=b1(gr9,gr3,0)
   29| 000370 lfdux    7C6B2CEE   1     LFDU      fp3,gr11=b2(gr11,gr5,0)
   29| 000374 lfdux    7C8A2CEE   1     LFDU      fp4,gr10=b2(gr10,gr5,0)
   29| 000378 lfdux    7C3F34EE   1     LFDU      fp1,gr31=b3(gr31,gr6,0)
   29| 00037C lfdux    7CAC34EE   1     LFDU      fp5,gr12=b3(gr12,gr6,0)
   29| 000380 fadd     FCC0402A   1     AFL       fp6=fp0,fp8,fcr
   27| 000384 lfdux    7C091CEE   1     LFDU      fp0,gr9=b1(gr9,gr3,0)
   29| 000388 fadd     FC63202A   1     AFL       fp3=fp3,fp4,fcr
   29| 00038C lfdux    7C8B2CEE   1     LFDU      fp4,gr11=b2(gr11,gr5,0)
   29| 000390 fadd     FC21282A   1     AFL       fp1=fp1,fp5,fcr
   29| 000394 lfdux    7CAA2CEE   1     LFDU      fp5,gr10=b2(gr10,gr5,0)
   29| 000398 fmadd    FCE611BA   1     FMA       fp7=fp2,fp6,fp6,fcr
   29| 00039C lfdux    7CDF34EE   1     LFDU      fp6,gr31=b3(gr31,gr6,0)
   29| 0003A0 fadd     FC48002A   1     AFL       fp2=fp8,fp0,fcr
   29| 0003A4 lfdux    7D0C34EE   1     LFDU      fp8,gr12=b3(gr12,gr6,0)
   29| 0003A8 fmadd    FC6338FA   1     FMA       fp3=fp7,fp3,fp3,fcr
   29| 0003AC fadd     FCE4282A   2     AFL       fp7=fp4,fp5,fcr
   29| 0003B0 fadd     FCC6402A   2     AFL       fp6=fp6,fp8,fcr
   29| 0003B4 fmadd    FC21187A   2     FMA       fp1=fp3,fp1,fp1,fcr
   29| 0003B8 fmadd    FC4208BA   2     FMA       fp2=fp1,fp2,fp2,fcr
    0| 0003BC bc       42400060   1     BCF       ctr=CL.653,taken=0%(0,100)
    0|                              CL.654:
   27| 0003C0 lfdux    7C291CEE   1     LFDU      fp1,gr9=b1(gr9,gr3,0)
   29| 0003C4 fmadd    FCE711FA   1     FMA       fp7=fp2,fp7,fp7,fcr
   29| 0003C8 lfdux    7C4B2CEE   1     LFDU      fp2,gr11=b2(gr11,gr5,0)
   29| 0003CC lfdux    7C6A2CEE   1     LFDU      fp3,gr10=b2(gr10,gr5,0)
   29| 0003D0 lfdux    7C9F34EE   1     LFDU      fp4,gr31=b3(gr31,gr6,0)
   29| 0003D4 fadd     FCA0082A   1     AFL       fp5=fp0,fp1,fcr
   29| 0003D8 fmadd    FCC639BA   2     FMA       fp6=fp7,fp6,fp6,fcr
   29| 0003DC lfdux    7CEC34EE   1     LFDU      fp7,gr12=b3(gr12,gr6,0)
   29| 0003E0 lfdux    7D0B2CEE   1     LFDU      fp8,gr11=b2(gr11,gr5,0)
   29| 0003E4 lfdux    7D2A2CEE   1     LFDU      fp9,gr10=b2(gr10,gr5,0)
   29| 0003E8 fadd     FC42182A   1     AFL       fp2=fp2,fp3,fcr
   27| 0003EC lfdux    7C091CEE   1     LFDU      fp0,gr9=b1(gr9,gr3,0)
   29| 0003F0 fmadd    FC65317A   1     FMA       fp3=fp6,fp5,fp5,fcr
   29| 0003F4 lfdux    7CBF34EE   1     LFDU      fp5,gr31=b3(gr31,gr6,0)
   29| 0003F8 lfdux    7CCC34EE   1     LFDU      fp6,gr12=b3(gr12,gr6,0)
   29| 0003FC fadd     FC84382A   1     AFL       fp4=fp4,fp7,fcr
   29| 000400 fadd     FCE8482A   2     AFL       fp7=fp8,fp9,fcr
   29| 000404 fmadd    FC4218BA   2     FMA       fp2=fp3,fp2,fp2,fcr
   29| 000408 fadd     FC21002A   2     AFL       fp1=fp1,fp0,fcr
   29| 00040C fmadd    FC44113A   2     FMA       fp2=fp2,fp4,fp4,fcr
   29| 000410 fadd     FCC5302A   2     AFL       fp6=fp5,fp6,fcr
   29| 000414 fmadd    FC41107A   2     FMA       fp2=fp2,fp1,fp1,fcr
    0| 000418 bc       4200FFA8   1     BCT       ctr=CL.654,taken=100%(100,0)
    0|                              CL.653:
   29| 00041C fmadd    FC0711FA   1     FMA       fp0=fp2,fp7,fp7,fcr
   29| 000420 fmadd    FC4601BA   2     FMA       fp2=fp0,fp6,fp6,fcr
    0|                              CL.522:
   34| 000424 addi     38E70001   1     AI        gr7=gr7,1
   34| 000428 cmpld    7EA7A840   1     CL8       cr5=gr7,gr21
   34| 00042C bc       4194FECC   1     BT        CL.65,cr5,0x8/llt,taken=80%(80,20)
   34|                              CL.64:
   35| 000430 ld       E8E101B0   1     L8        gr7=#SPILL33(gr1,432)
    0| 000434 ld       E90101C0   1     L8        gr8=#SPILL35(gr1,448)
   35| 000438 ld       E9210108   1     L8        gr9=#SPILL12(gr1,264)
   35| 00043C addi     38E70001   1     AI        gr7=gr7,1
    0| 000440 add      7D08F214   1     A         gr8=gr8,gr30
   35| 000444 std      F8E101B0   1     ST8       #SPILL33(gr1,432)=gr7
   35| 000448 cmpld    7EA74840   1     CL8       cr5=gr7,gr9
    0| 00044C std      F90101C0   1     ST8       #SPILL35(gr1,448)=gr8
   35| 000450 bc       4194FE60   1     BT        CL.63,cr5,0x8/llt,taken=80%(80,20)
   35|                              CL.62:
   36| 000454 ld       E8E10138   1     L8        gr7=#SPILL18(gr1,312)
   36| 000458 ld       E92100A8   1     L8        gr9=#SPILL0(gr1,168)
   36| 00045C ld       E9020000   1     L8        gr8=.+CONSTANT_AREA(gr2,0)
   37| 000460 ld       E94101A8   1     L8        gr10=#SPILL32(gr1,424)
   37| 000464 ld       E96100C0   1     L8        gr11=#SPILL3(gr1,192)
   36| 000468 std      F8E10090   1     ST8       #MX_CONVF1_0(gr1,144)=gr7
   36| 00046C lfd      C9090000   1     LFL       fp8=rms(gr9,0)
   36| 000470 lfs      C0080010   1     LFS       fp0=+CONSTANT_AREA(gr8,16)
   36| 000474 lfs      C0680014   1     LFS       fp3=+CONSTANT_AREA(gr8,20)
   36| 000478 lfs      C0880018   1     LFS       fp4=+CONSTANT_AREA(gr8,24)
   36| 00047C lfs      C0A8001C   1     LFS       fp5=+CONSTANT_AREA(gr8,28)
   36| 000480 lfs      C0C80020   1     LFS       fp6=+CONSTANT_AREA(gr8,32)
   36| 000484 lfs      C0E80024   1     LFS       fp7=+CONSTANT_AREA(gr8,36)
   36| 000488 fmul     FD080232   1     MFL       fp8=fp8,fp8,fcr
   36| 00048C lfd      CBC10090   2     LFL       fp30=#MX_CONVF1_0(gr1,144)
   36| 000490 fmul     FD220032   1     MFL       fp9=fp2,fp0,fcr
   36| 000494 lfs      C0280028   1     LFS       fp1=+CONSTANT_AREA(gr8,40)
   37| 000498 cmpdi    2C2A0000   1     C8        cr0=gr10,0
   36| 00049C qvfre    11404030   1     QVFRE     fp10=fp8
   37| 0004A0 cror     4E210B82   1     CR_O      cr4=cr[00],0x2/gt,0x2/gt,0x2/gt,cr4
   36| 0004A4 fcfid    FC40F69C   1     FCFID     fp2=fp30,fcr
   37| 0004A8 cmpdi    2C2B0000   1     C8        cr0=gr11,0
   37| 0004AC cror     4D210B82   1     CR_O      cr2=cr[00],0x2/gt,0x2/gt,0x2/gt,cr2
   36| 0004B0 fmsub    FD681AB8   1     FMS       fp11=fp3,fp8,fp10,fcr
   37| 0004B4 crand    4E298A02   1     CR_N      cr4=cr[24],0x2/gt,0x2/gt,0x2/gt,cr4
   36| 0004B8 frsp     FC401018   1     CVLS      fp2=fp2,fcr
   36| 0004BC fnmsub   FD4A52FC   2     FNMS      fp10=fp10,fp10,fp11,fcr
   36| 0004C0 fmul     FD6200B2   2     MFL       fp11=fp2,fp2,fcr
   36| 0004C4 fmsub    FD881AB8   2     FMS       fp12=fp3,fp8,fp10,fcr
   36| 0004C8 fmul     FD6202F2   2     MFL       fp11=fp2,fp11,fcr
   36| 0004CC fnmsub   FD4A533C   2     FNMS      fp10=fp10,fp10,fp12,fcr
   36| 0004D0 frsp     FDA05818   2     CVLS      fp13=fp11,fcr
   36| 0004D4 qvfre    11606830   1     QVFRE     fp11=fp13
   36| 0004D8 fmsub    FD8D1AF8   1     FMS       fp12=fp3,fp13,fp11,fcr
   36| 0004DC fnmsub   FD6B5B3C   2     FNMS      fp11=fp11,fp11,fp12,fcr
   36| 0004E0 fmsub    FD8D1AF8   2     FMS       fp12=fp3,fp13,fp11,fcr
   36| 0004E4 fnmsub   FD6B5B3C   2     FNMS      fp11=fp11,fp11,fp12,fcr
   36| 0004E8 fmul     FD8902F2   2     MFL       fp12=fp9,fp11,fcr
   36| 0004EC fmsub    FD2D4B38   2     FMS       fp9=fp9,fp13,fp12,fcr
   36| 0004F0 fnmsub   FD2B627C   2     FNMS      fp9=fp12,fp11,fp9,fcr
   36| 0004F4 fmul     FD6902B2   2     MFL       fp11=fp9,fp10,fcr
   36| 0004F8 fmsub    FD084AF8   2     FMS       fp8=fp9,fp8,fp11,fcr
   36| 0004FC fnmsub   FD0A5A3C   2     FNMS      fp8=fp11,fp10,fp8,fcr
   36| 000500 frsqrte  FD204034   2     FRSQRE    fp9=fp8
   36| 000504 fnabs    FD404110   2     NABSFL    fp10=fp8
   36| 000508 fmul     FD680272   2     MFL       fp11=fp8,fp9,fcr
   36| 00050C fmadd    FC8922FA   2     FMA       fp4=fp4,fp9,fp11,fcr
   36| 000510 fmadd    FCA4317A   2     FMA       fp5=fp6,fp4,fp5,fcr
   36| 000514 fmadd    FCA4397A   2     FMA       fp5=fp7,fp4,fp5,fcr
   36| 000518 fmul     FC840172   2     MFL       fp4=fp4,fp5,fcr
   36| 00051C fmul     FCAB0132   2     MFL       fp5=fp11,fp4,fcr
   36| 000520 fmadd    FC89493A   2     FMA       fp4=fp9,fp9,fp4,fcr
   36| 000524 fmadd    FCA82A7A   2     FMA       fp5=fp5,fp8,fp9,fcr
   36| 000528 fmul     FC840072   2     MFL       fp4=fp4,fp1,fcr
   36| 00052C fmsub    FCC54178   2     FMS       fp6=fp8,fp5,fp5,fcr
   36| 000530 fnmsub   FC86293C   2     FNMS      fp4=fp5,fp6,fp4,fcr
   36| 000534 fsel     FFAA222E   2     FSEL      fp29=fp10,fp4,fp8
   37| 000538 bc       4091009C   1     BF        CL.127,cr4,0x2/gt,taken=50%(0,0)
    0| 00053C cmpdi    2C3C0000   1     C8        cr0=gr28,0
   37| 000540 addi     38E00000   1     LI        gr7=0
    0| 000544 bc       40810DD4   1     BF        CL.378,cr0,0x2/gt,taken=40%(40,60)
    0| 000548 ld       E9810158   1     L8        gr12=#SPILL22(gr1,344)
    0| 00054C ld       EBE10160   1     L8        gr31=#SPILL23(gr1,352)
    0| 000550 ld       EAA10150   1     L8        gr21=#SPILL21(gr1,336)
    0| 000554 ld       EA8100D8   1     L8        gr20=#SPILL6(gr1,216)
    0| 000558 ld       EA6100E8   1     L8        gr19=#SPILL8(gr1,232)
    0| 00055C cmpdi    2C3D0000   1     C8        cr0=gr29,0
    0| 000560 mulld    7D0461D2   1     M         gr8=gr4,gr12
    0| 000564 mulld    7D3EF9D2   1     M         gr9=gr30,gr31
    0| 000568 mulld    7D43A9D2   1     M         gr10=gr3,gr21
    0| 00056C add      7D73A214   1     A         gr11=gr19,gr20
    0| 000570 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000574 subf     7D235850   1     S         gr9=gr11,gr3
    0| 000578 add      7D295214   1     A         gr9=gr9,gr10
    0| 00057C add      7D484A14   1     A         gr10=gr8,gr9
   37|                              CL.122:
   37| 000580 addi     39000000   1     LI        gr8=0
    0| 000584 bc       4081003C   1     BF        CL.126,cr0,0x2/gt,taken=20%(20,80)
    0| 000588 or       7D4B5378   1     LR        gr11=gr10
   37|                              CL.123:
   37| 00058C or       7D695B78   1     LR        gr9=gr11
    0| 000590 mtspr    7FA903A6   1     LCTR      ctr=gr29
    0| 000594 ori      60210000   1     XNOP      
    0| 000598 ori      60210000   1     XNOP      
    0| 00059C ori      60210000   1     XNOP      
    0|                              CL.655:
   37| 0005A0 lfdux    7C891CEE   1     LFDU      fp4,gr9=b1(gr9,gr3,0)
   37| 0005A4 fdiv     FC84E824   1     DFL       fp4=fp4,fp29,fcr
   37| 0005A8 stfd     D8890000   1     STFL      b1(gr9,0)=fp4
    0| 0005AC bc       4200FFF4   1     BCT       ctr=CL.655,taken=100%(100,0)
   37| 0005B0 addi     39080001   1     AI        gr8=gr8,1
    0| 0005B4 add      7D645A14   1     A         gr11=gr4,gr11
   37| 0005B8 cmpld    7F28E040   1     CL8       cr6=gr8,gr28
   37| 0005BC bc       4198FFD0   1     BT        CL.123,cr6,0x8/llt,taken=80%(80,20)
   37|                              CL.126:
   37| 0005C0 ld       E90101A8   1     L8        gr8=#SPILL32(gr1,424)
   37| 0005C4 addi     38E70001   1     AI        gr7=gr7,1
    0| 0005C8 add      7D4AF214   1     A         gr10=gr10,gr30
   37| 0005CC cmpd     7F283800   1     C8        cr6=gr8,gr7
   37| 0005D0 bc       4199FFB0   1     BT        CL.122,cr6,0x2/gt,taken=80%(80,20)
   37|                              CL.127:
   37| 0005D4 ld       E8E100C0   1     L8        gr7=#SPILL3(gr1,192)
   37| 0005D8 ld       E90101A8   1     L8        gr8=#SPILL32(gr1,424)
   37| 0005DC cmpd     7C274000   1     C8        cr0=gr7,gr8
   37| 0005E0 crand    4D214A02   1     CR_N      cr2=cr[02],0x2/gt,0x2/gt,0x2/gt,cr2
   37| 0005E4 bc       4089020C   1     BF        CL.68,cr2,0x2/gt,taken=50%(0,0)
    0| 0005E8 ld       EA810158   1     L8        gr20=#SPILL22(gr1,344)
    0| 0005EC ld       EA610160   1     L8        gr19=#SPILL23(gr1,352)
    0| 0005F0 ld       EA410150   1     L8        gr18=#SPILL21(gr1,336)
    0| 0005F4 ld       EA2100D8   1     L8        gr17=#SPILL6(gr1,216)
    0| 0005F8 ld       EA0100E8   1     L8        gr16=#SPILL8(gr1,232)
    0| 0005FC ld       E9E101A8   1     L8        gr15=#SPILL32(gr1,424)
   37| 000600 ld       E9C10100   1     L8        gr14=#SPILL11(gr1,256)
    0| 000604 mulld    7CE4A1D2   1     M         gr7=gr4,gr20
    0| 000608 mulld    7D13F1D2   1     M         gr8=gr19,gr30
    0| 00060C mulld    7D6391D2   1     M         gr11=gr3,gr18
    0| 000610 add      7D508A14   1     A         gr10=gr16,gr17
   37| 000614 addi     398EFFFF   1     AI        gr12=gr14,-1
    0| 000618 subf     7D435050   1     S         gr10=gr10,gr3
    0| 00061C mulld    7D2FF1D2   1     M         gr9=gr15,gr30
    0| 000620 add      7CE74214   1     A         gr7=gr7,gr8
    0| 000624 add      7D4A5A14   1     A         gr10=gr10,gr11
   37| 000628 sradi    7D8B1674   1     SRA8CA    gr11,ca=gr12,2
    0| 00062C rldicr   7BC81764   1     SLL8      gr8=gr30,2
    0| 000630 add      7CE75214   1     A         gr7=gr7,gr10
   37| 000634 addze    7EAB0194   1     ADDE      gr21,ca=gr11,0,ca
    0| 000638 cmpdi    2C3C0000   1     C8        cr0=gr28,0
    0| 00063C add      7D293A14   1     A         gr9=gr9,gr7
    0| 000640 subf     7D9E4050   1     S         gr12=gr8,gr30
    0| 000644 rldicr   7BDF0FA4   1     SLL8      gr31=gr30,1
    0| 000648 rldicl   7BAAF842   1     SRL8      gr10=gr29,1
   37| 00064C addi     38E00000   1     LI        gr7=0
    0| 000650 bc       408101A0   1     BF        CL.68,cr0,0x2/gt,taken=20%(20,80)
    0| 000654 qvfre    1080E830   1     QVFRE     fp4=fp29
    0| 000658 addi     3A950001   1     AI        gr20=gr21,1
    0| 00065C add      7D69F214   1     A         gr11=gr9,gr30
    0| 000660 std      FA8101C8   1     ST8       #SPILL36(gr1,456)=gr20
    0| 000664 add      7D896214   1     A         gr12=gr9,gr12
    0| 000668 add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 00066C cmpdi    2F3D0000   1     C8        cr6=gr29,0
    0| 000670 andi.    73BD0001   1     RN4_R     gr29,cr0=gr29,0,0x1
    0| 000674 cmpdi    2EAA0000   1     C8        cr5=gr10,0
   37|                              CL.69:
   37| 000678 addi     3BA00000   1     LI        gr29=0
    0| 00067C bc       40990154   1     BF        CL.70,cr6,0x2/gt,taken=20%(20,80)
    0| 000680 fmsub    FCBD1938   1     FMS       fp5=fp3,fp29,fp4,fcr
    0| 000684 or       7D314B78   2     LR        gr17=gr9
    0| 000688 or       7D906378   1     LR        gr16=gr12
    0| 00068C or       7FEFFB78   1     LR        gr15=gr31
    0| 000690 or       7D6E5B78   1     LR        gr14=gr11
    0| 000694 fnmsub   FCA4217C   1     FNMS      fp5=fp4,fp4,fp5,fcr
    0| 000698 fmsub    FCDD1978   2     FMS       fp6=fp3,fp29,fp5,fcr
    0| 00069C fnmsub   FCA529BC   2     FNMS      fp5=fp5,fp5,fp6,fcr
    0| 0006A0 ori      60210000   2     XNOP      
   37|                              CL.71:
   37| 0006A4 or       7E358B78   1     LR        gr21=gr17
   37| 0006A8 or       7DD47378   1     LR        gr20=gr14
   37| 0006AC or       7DF37B78   1     LR        gr19=gr15
   37| 0006B0 or       7E128378   1     LR        gr18=gr16
    0| 0006B4 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 0006B8 bc       41820058   1     BT        CL.629,cr0,0x4/eq,taken=50%(0,0)
   37| 0006BC lfdux    7CD51CEE   1     LFDU      fp6,gr21=b1(gr21,gr3,0)
   37| 0006C0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 0006C4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 0006C8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 0006CC stfd     D8D50000   1     STFL      b1(gr21,0)=fp6
   37| 0006D0 lfdux    7CD41CEE   1     LFDU      fp6,gr20=b1(gr20,gr3,0)
   37| 0006D4 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 0006D8 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 0006DC fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 0006E0 stfd     D8D40000   1     STFL      b1(gr20,0)=fp6
   37| 0006E4 lfdux    7CD31CEE   1     LFDU      fp6,gr19=b1(gr19,gr3,0)
   37| 0006E8 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 0006EC fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 0006F0 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 0006F4 stfd     D8D30000   1     STFL      b1(gr19,0)=fp6
   37| 0006F8 lfdux    7CD21CEE   1     LFDU      fp6,gr18=b1(gr18,gr3,0)
   37| 0006FC fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 000700 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 000704 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000708 stfd     D8D20000   1     STFL      b1(gr18,0)=fp6
    0| 00070C bc       419600A8   1     BT        CL.528,cr5,0x4/eq,taken=20%(20,80)
    0|                              CL.629:
   37| 000710 lfdux    7CD51CEE   1     LFDU      fp6,gr21=b1(gr21,gr3,0)
   37| 000714 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 000718 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 00071C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000720 stfd     D8D50000   1     STFL      b1(gr21,0)=fp6
   37| 000724 lfdux    7CD41CEE   1     LFDU      fp6,gr20=b1(gr20,gr3,0)
   37| 000728 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 00072C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 000730 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000734 stfd     D8D40000   1     STFL      b1(gr20,0)=fp6
   37| 000738 lfdux    7CD31CEE   1     LFDU      fp6,gr19=b1(gr19,gr3,0)
   37| 00073C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 000740 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 000744 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000748 stfd     D8D30000   1     STFL      b1(gr19,0)=fp6
   37| 00074C lfdux    7CD21CEE   1     LFDU      fp6,gr18=b1(gr18,gr3,0)
   37| 000750 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 000754 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 000758 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 00075C stfd     D8D20000   1     STFL      b1(gr18,0)=fp6
   37| 000760 lfdux    7CD51CEE   1     LFDU      fp6,gr21=b1(gr21,gr3,0)
   37| 000764 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 000768 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 00076C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000770 stfd     D8D50000   1     STFL      b1(gr21,0)=fp6
   37| 000774 lfdux    7CD41CEE   1     LFDU      fp6,gr20=b1(gr20,gr3,0)
   37| 000778 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 00077C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 000780 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000784 stfd     D8D40000   1     STFL      b1(gr20,0)=fp6
   37| 000788 lfdux    7CD31CEE   1     LFDU      fp6,gr19=b1(gr19,gr3,0)
   37| 00078C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 000790 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 000794 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 000798 stfd     D8D30000   1     STFL      b1(gr19,0)=fp6
   37| 00079C lfdux    7CD21CEE   1     LFDU      fp6,gr18=b1(gr18,gr3,0)
   37| 0007A0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   37| 0007A4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   37| 0007A8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   37| 0007AC stfd     D8D20000   1     STFL      b1(gr18,0)=fp6
    0| 0007B0 bc       4200FF60   1     BCT       ctr=CL.629,taken=100%(100,0)
    0|                              CL.528:
   37| 0007B4 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 0007B8 add      7E248A14   1     A         gr17=gr4,gr17
   37| 0007BC cmpld    7FBDE040   1     CL8       cr7=gr29,gr28
    0| 0007C0 add      7E048214   1     A         gr16=gr4,gr16
    0| 0007C4 add      7DE47A14   1     A         gr15=gr4,gr15
    0| 0007C8 add      7DC47214   1     A         gr14=gr4,gr14
   37| 0007CC bc       419CFED8   1     BT        CL.71,cr7,0x8/llt,taken=80%(80,20)
   37|                              CL.70:
   37| 0007D0 ld       EBA101C8   1     L8        gr29=#SPILL36(gr1,456)
   37| 0007D4 addi     38E70001   1     AI        gr7=gr7,1
    0| 0007D8 add      7D685A14   1     A         gr11=gr8,gr11
    0| 0007DC add      7D886214   1     A         gr12=gr8,gr12
    0| 0007E0 add      7FE8FA14   1     A         gr31=gr8,gr31
    0| 0007E4 add      7D284A14   1     A         gr9=gr8,gr9
   37| 0007E8 cmpld    7FA7E840   1     CL8       cr7=gr7,gr29
   37| 0007EC bc       419CFE8C   1     BT        CL.69,cr7,0x8/llt,taken=80%(80,20)
   37|                              CL.68:
   38| 0007F0 ld       E8E101A0   1     L8        gr7=#SPILL31(gr1,416)
   38| 0007F4 ld       E90100B8   1     L8        gr8=#SPILL2(gr1,184)
   38| 0007F8 cmpdi    2C270000   1     C8        cr0=gr7,0
   38| 0007FC cror     4D210B82   1     CR_O      cr2=cr[00],0x2/gt,0x2/gt,0x2/gt,cr2
   38| 000800 cmpdi    2C280000   1     C8        cr0=gr8,0
   38| 000804 cror     4E210B82   1     CR_O      cr4=cr[00],0x2/gt,0x2/gt,0x2/gt,cr4
   38| 000808 crand    4D314A02   1     CR_N      cr2=cr[42],0x2/gt,0x2/gt,0x2/gt,cr2
   38| 00080C bc       4089009C   1     BF        CL.115,cr2,0x2/gt,taken=50%(0,0)
    0| 000810 cmpdi    2C390000   1     C8        cr0=gr25,0
   38| 000814 addi     38E00000   1     LI        gr7=0
    0| 000818 bc       40810AE4   1     BF        CL.388,cr0,0x2/gt,taken=40%(40,60)
    0| 00081C ld       E9810170   1     L8        gr12=#SPILL25(gr1,368)
    0| 000820 ld       EBA10168   1     L8        gr29=#SPILL24(gr1,360)
    0| 000824 ld       EBE10110   1     L8        gr31=#SPILL13(gr1,272)
    0| 000828 ld       EAA10140   1     L8        gr21=#SPILL19(gr1,320)
    0| 00082C ld       EA810178   1     L8        gr20=#SPILL26(gr1,376)
    0| 000830 ld       EB810128   1     L8        gr28=#SPILL16(gr1,296)
    0| 000834 mulld    7D0CD9D2   1     M         gr8=gr12,gr27
    0| 000838 subf     7D45F850   1     S         gr10=gr31,gr5
    0| 00083C mulld    7D25E9D2   1     M         gr9=gr5,gr29
    0| 000840 add      7D4AE214   1     A         gr10=gr10,gr28
    0| 000844 mulld    7D74A9D2   1     M         gr11=gr20,gr21
    0| 000848 add      7D085214   1     A         gr8=gr8,gr10
    0| 00084C cmpdi    2C3A0000   1     C8        cr0=gr26,0
    0| 000850 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000854 add      7D485A14   1     A         gr10=gr8,gr11
   38|                              CL.110:
   38| 000858 addi     39000000   1     LI        gr8=0
    0| 00085C bc       40810034   1     BF        CL.114,cr0,0x2/gt,taken=20%(20,80)
    0| 000860 or       7D4B5378   1     LR        gr11=gr10
   38|                              CL.111:
   38| 000864 or       7D695B78   1     LR        gr9=gr11
    0| 000868 mtspr    7F4903A6   1     LCTR      ctr=gr26
    0| 00086C ori      60210000   1     XNOP      
    0|                              CL.657:
   38| 000870 lfdux    7C892CEE   1     LFDU      fp4,gr9=b2(gr9,gr5,0)
   38| 000874 fdiv     FC84E824   1     DFL       fp4=fp4,fp29,fcr
   38| 000878 stfd     D8890000   1     STFL      b2(gr9,0)=fp4
    0| 00087C bc       4200FFF4   1     BCT       ctr=CL.657,taken=100%(100,0)
   38| 000880 addi     39080001   1     AI        gr8=gr8,1
    0| 000884 add      7D6BDA14   1     A         gr11=gr11,gr27
   38| 000888 cmpld    7F28C840   1     CL8       cr6=gr8,gr25
   38| 00088C bc       4198FFD8   1     BT        CL.111,cr6,0x8/llt,taken=80%(80,20)
   38|                              CL.114:
   38| 000890 ld       E92101A0   1     L8        gr9=#SPILL31(gr1,416)
    0| 000894 ld       E9010140   1     L8        gr8=#SPILL19(gr1,320)
   38| 000898 addi     38E70001   1     AI        gr7=gr7,1
   38| 00089C cmpd     7F293800   1     C8        cr6=gr9,gr7
    0| 0008A0 add      7D485214   1     A         gr10=gr8,gr10
   38| 0008A4 bc       4199FFB4   1     BT        CL.110,cr6,0x2/gt,taken=80%(80,20)
   38|                              CL.115:
   38| 0008A8 ld       E8E100B8   1     L8        gr7=#SPILL2(gr1,184)
   38| 0008AC ld       E90101A0   1     L8        gr8=#SPILL31(gr1,416)
   38| 0008B0 cmpd     7D274000   1     C8        cr2=gr7,gr8
   38| 0008B4 crand    4D314A02   1     CR_N      cr2=cr[42],0x2/gt,0x2/gt,0x2/gt,cr2
   38| 0008B8 bc       40890210   1     BF        CL.74,cr2,0x2/gt,taken=50%(0,0)
    0| 0008BC ld       EB810170   1     L8        gr28=#SPILL25(gr1,368)
    0| 0008C0 ld       EA810168   1     L8        gr20=#SPILL24(gr1,360)
    0| 0008C4 ld       EAA10110   1     L8        gr21=#SPILL13(gr1,272)
    0| 0008C8 ld       EA410140   1     L8        gr18=#SPILL19(gr1,320)
    0| 0008CC ld       EA210178   1     L8        gr17=#SPILL26(gr1,376)
    0| 0008D0 ld       EA610128   1     L8        gr19=#SPILL16(gr1,296)
    0| 0008D4 or       7D0B4378   1     LR        gr11=gr8
    0| 0008D8 mulld    7D1BE1D2   1     M         gr8=gr27,gr28
    0| 0008DC subf     7CE5A850   1     S         gr7=gr21,gr5
    0| 0008E0 mulld    7D45A1D2   1     M         gr10=gr5,gr20
    0| 0008E4 add      7D279A14   1     A         gr9=gr7,gr19
   38| 0008E8 ld       EA0100F8   1     L8        gr16=#SPILL10(gr1,248)
    0| 0008EC mulld    7CF191D2   1     M         gr7=gr17,gr18
    0| 0008F0 add      7D084A14   1     A         gr8=gr8,gr9
    0| 0008F4 mulld    7D2B91D2   1     M         gr9=gr11,gr18
   38| 0008F8 addi     3970FFFF   1     AI        gr11=gr16,-1
    0| 0008FC add      7D4A4214   1     A         gr10=gr10,gr8
   38| 000900 sradi    7D6B1674   1     SRA8CA    gr11,ca=gr11,2
    0| 000904 rldicr   7A481764   1     SLL8      gr8=gr18,2
    0| 000908 add      7CE75214   1     A         gr7=gr7,gr10
   38| 00090C addze    7FAB0194   1     ADDE      gr29,ca=gr11,0,ca
    0| 000910 cmpdi    2C390000   1     C8        cr0=gr25,0
    0| 000914 add      7D293A14   1     A         gr9=gr9,gr7
    0| 000918 subf     7D924050   1     S         gr12=gr8,gr18
    0| 00091C rldicr   7A5F0FA4   1     SLL8      gr31=gr18,1
    0| 000920 rldicl   7B4AF842   1     SRL8      gr10=gr26,1
   38| 000924 addi     38E00000   1     LI        gr7=0
    0| 000928 bc       408101A0   1     BF        CL.74,cr0,0x2/gt,taken=20%(20,80)
    0| 00092C qvfre    1080E830   1     QVFRE     fp4=fp29
    0| 000930 add      7D699214   1     A         gr11=gr9,gr18
    0| 000934 add      7D896214   1     A         gr12=gr9,gr12
    0| 000938 add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 00093C addi     39FD0001   1     AI        gr15=gr29,1
    0| 000940 cmpdi    2F3A0000   1     C8        cr6=gr26,0
    0| 000944 andi.    735D0001   1     RN4_R     gr29,cr0=gr26,0,0x1
    0| 000948 cmpdi    2EAA0000   1     C8        cr5=gr10,0
   38|                              CL.75:
   38| 00094C addi     3BA00000   1     LI        gr29=0
    0| 000950 bc       4099015C   1     BF        CL.76,cr6,0x2/gt,taken=20%(20,80)
    0| 000954 fmsub    FCBD1938   1     FMS       fp5=fp3,fp29,fp4,fcr
    0| 000958 or       7D334B78   2     LR        gr19=gr9
    0| 00095C or       7D926378   1     LR        gr18=gr12
    0| 000960 or       7FF1FB78   1     LR        gr17=gr31
    0| 000964 or       7D705B78   1     LR        gr16=gr11
    0| 000968 fnmsub   FCA4217C   1     FNMS      fp5=fp4,fp4,fp5,fcr
    0| 00096C fmsub    FCDD1978   2     FMS       fp6=fp3,fp29,fp5,fcr
    0| 000970 fnmsub   FCA529BC   2     FNMS      fp5=fp5,fp5,fp6,fcr
   38|                              CL.77:
   38| 000974 or       7E7C9B78   1     LR        gr28=gr19
   38| 000978 or       7E1A8378   1     LR        gr26=gr16
   38| 00097C or       7E358B78   1     LR        gr21=gr17
   38| 000980 or       7E549378   1     LR        gr20=gr18
    0| 000984 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 000988 bc       41820064   1     BT        CL.637,cr0,0x4/eq,taken=50%(0,0)
   38| 00098C lfdux    7CDC2CEE   1     LFDU      fp6,gr28=b2(gr28,gr5,0)
   38| 000990 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000994 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000998 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 00099C stfd     D8DC0000   1     STFL      b2(gr28,0)=fp6
   38| 0009A0 lfdux    7CDA2CEE   1     LFDU      fp6,gr26=b2(gr26,gr5,0)
   38| 0009A4 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 0009A8 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 0009AC fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 0009B0 stfd     D8DA0000   1     STFL      b2(gr26,0)=fp6
   38| 0009B4 lfdux    7CD52CEE   1     LFDU      fp6,gr21=b2(gr21,gr5,0)
   38| 0009B8 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 0009BC fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 0009C0 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 0009C4 stfd     D8D50000   1     STFL      b2(gr21,0)=fp6
   38| 0009C8 lfdux    7CD42CEE   1     LFDU      fp6,gr20=b2(gr20,gr5,0)
   38| 0009CC fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 0009D0 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 0009D4 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 0009D8 stfd     D8D40000   1     STFL      b2(gr20,0)=fp6
    0| 0009DC bc       419600B4   1     BT        CL.534,cr5,0x4/eq,taken=20%(20,80)
    0| 0009E0 ori      60210000   1     XNOP      
    0| 0009E4 ori      60210000   1     XNOP      
    0| 0009E8 ori      60210000   1     XNOP      
    0|                              CL.637:
   38| 0009EC lfdux    7CDC2CEE   1     LFDU      fp6,gr28=b2(gr28,gr5,0)
   38| 0009F0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 0009F4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 0009F8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 0009FC stfd     D8DC0000   1     STFL      b2(gr28,0)=fp6
   38| 000A00 lfdux    7CDA2CEE   1     LFDU      fp6,gr26=b2(gr26,gr5,0)
   38| 000A04 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A08 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A0C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A10 stfd     D8DA0000   1     STFL      b2(gr26,0)=fp6
   38| 000A14 lfdux    7CD52CEE   1     LFDU      fp6,gr21=b2(gr21,gr5,0)
   38| 000A18 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A1C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A20 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A24 stfd     D8D50000   1     STFL      b2(gr21,0)=fp6
   38| 000A28 lfdux    7CD42CEE   1     LFDU      fp6,gr20=b2(gr20,gr5,0)
   38| 000A2C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A30 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A34 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A38 stfd     D8D40000   1     STFL      b2(gr20,0)=fp6
   38| 000A3C lfdux    7CDC2CEE   1     LFDU      fp6,gr28=b2(gr28,gr5,0)
   38| 000A40 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A44 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A48 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A4C stfd     D8DC0000   1     STFL      b2(gr28,0)=fp6
   38| 000A50 lfdux    7CDA2CEE   1     LFDU      fp6,gr26=b2(gr26,gr5,0)
   38| 000A54 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A58 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A5C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A60 stfd     D8DA0000   1     STFL      b2(gr26,0)=fp6
   38| 000A64 lfdux    7CD52CEE   1     LFDU      fp6,gr21=b2(gr21,gr5,0)
   38| 000A68 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A6C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A70 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A74 stfd     D8D50000   1     STFL      b2(gr21,0)=fp6
   38| 000A78 lfdux    7CD42CEE   1     LFDU      fp6,gr20=b2(gr20,gr5,0)
   38| 000A7C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   38| 000A80 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   38| 000A84 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   38| 000A88 stfd     D8D40000   1     STFL      b2(gr20,0)=fp6
    0| 000A8C bc       4200FF60   1     BCT       ctr=CL.637,taken=100%(100,0)
    0|                              CL.534:
   38| 000A90 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 000A94 add      7E73DA14   1     A         gr19=gr19,gr27
   38| 000A98 cmpld    7FBDC840   1     CL8       cr7=gr29,gr25
    0| 000A9C add      7E52DA14   1     A         gr18=gr18,gr27
    0| 000AA0 add      7E31DA14   1     A         gr17=gr17,gr27
    0| 000AA4 add      7E10DA14   1     A         gr16=gr16,gr27
   38| 000AA8 bc       419CFECC   1     BT        CL.77,cr7,0x8/llt,taken=80%(80,20)
   38|                              CL.76:
   38| 000AAC addi     38E70001   1     AI        gr7=gr7,1
    0| 000AB0 add      7D685A14   1     A         gr11=gr8,gr11
   38| 000AB4 cmpld    7FA77840   1     CL8       cr7=gr7,gr15
    0| 000AB8 add      7D886214   1     A         gr12=gr8,gr12
    0| 000ABC add      7FE8FA14   1     A         gr31=gr8,gr31
    0| 000AC0 add      7D284A14   1     A         gr9=gr8,gr9
   38| 000AC4 bc       419CFE88   1     BT        CL.75,cr7,0x8/llt,taken=80%(80,20)
   38|                              CL.74:
   39| 000AC8 ld       E8E10198   1     L8        gr7=#SPILL30(gr1,408)
   39| 000ACC ld       E90100B0   1     L8        gr8=#SPILL1(gr1,176)
   39| 000AD0 cmpdi    2C270000   1     C8        cr0=gr7,0
   39| 000AD4 cror     4E210B82   1     CR_O      cr4=cr[00],0x2/gt,0x2/gt,0x2/gt,cr4
   39| 000AD8 cmpdi    2C280000   1     C8        cr0=gr8,0
   39| 000ADC cror     4D210B82   1     CR_O      cr2=cr[00],0x2/gt,0x2/gt,0x2/gt,cr2
   39| 000AE0 crand    4E298A02   1     CR_N      cr4=cr[24],0x2/gt,0x2/gt,0x2/gt,cr4
   39| 000AE4 bc       409100A4   1     BF        CL.103,cr4,0x2/gt,taken=50%(0,0)
    0| 000AE8 cmpdi    2C360000   1     C8        cr0=gr22,0
   39| 000AEC addi     38E00000   1     LI        gr7=0
    0| 000AF0 bc       408107F0   1     BF        CL.398,cr0,0x2/gt,taken=40%(40,60)
    0| 000AF4 ld       E9810188   1     L8        gr12=#SPILL28(gr1,392)
    0| 000AF8 ld       EBA10180   1     L8        gr29=#SPILL27(gr1,384)
    0| 000AFC ld       EBE10118   1     L8        gr31=#SPILL14(gr1,280)
    0| 000B00 ld       EB410148   1     L8        gr26=#SPILL20(gr1,328)
    0| 000B04 ld       EB210190   1     L8        gr25=#SPILL29(gr1,400)
    0| 000B08 ld       EB810130   1     L8        gr28=#SPILL17(gr1,304)
    0| 000B0C mulld    7D0CC1D2   1     M         gr8=gr12,gr24
    0| 000B10 subf     7D46F850   1     S         gr10=gr31,gr6
    0| 000B14 mulld    7D26E9D2   1     M         gr9=gr6,gr29
    0| 000B18 add      7D4AE214   1     A         gr10=gr10,gr28
    0| 000B1C mulld    7D79D1D2   1     M         gr11=gr25,gr26
    0| 000B20 add      7D085214   1     A         gr8=gr8,gr10
    0| 000B24 cmpdi    2C370000   1     C8        cr0=gr23,0
    0| 000B28 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000B2C add      7D485A14   1     A         gr10=gr8,gr11
   39|                              CL.98:
   39| 000B30 addi     39000000   1     LI        gr8=0
    0| 000B34 bc       4081003C   1     BF        CL.102,cr0,0x2/gt,taken=20%(20,80)
    0| 000B38 or       7D4B5378   1     LR        gr11=gr10
   39|                              CL.99:
   39| 000B3C or       7D695B78   1     LR        gr9=gr11
    0| 000B40 mtspr    7EE903A6   1     LCTR      ctr=gr23
    0| 000B44 ori      60210000   1     XNOP      
    0| 000B48 ori      60210000   1     XNOP      
    0| 000B4C ori      60210000   1     XNOP      
    0|                              CL.659:
   39| 000B50 lfdux    7C8934EE   1     LFDU      fp4,gr9=b3(gr9,gr6,0)
   39| 000B54 fdiv     FC84E824   1     DFL       fp4=fp4,fp29,fcr
   39| 000B58 stfd     D8890000   1     STFL      b3(gr9,0)=fp4
    0| 000B5C bc       4200FFF4   1     BCT       ctr=CL.659,taken=100%(100,0)
   39| 000B60 addi     39080001   1     AI        gr8=gr8,1
    0| 000B64 add      7D6BC214   1     A         gr11=gr11,gr24
   39| 000B68 cmpld    7F28B040   1     CL8       cr6=gr8,gr22
   39| 000B6C bc       4198FFD0   1     BT        CL.99,cr6,0x8/llt,taken=80%(80,20)
   39|                              CL.102:
   39| 000B70 ld       E9210198   1     L8        gr9=#SPILL30(gr1,408)
    0| 000B74 ld       E9010148   1     L8        gr8=#SPILL20(gr1,328)
   39| 000B78 addi     38E70001   1     AI        gr7=gr7,1
   39| 000B7C cmpd     7F293800   1     C8        cr6=gr9,gr7
    0| 000B80 add      7D485214   1     A         gr10=gr8,gr10
   39| 000B84 bc       4199FFAC   1     BT        CL.98,cr6,0x2/gt,taken=80%(80,20)
   39|                              CL.103:
   39| 000B88 ld       E8E100B0   1     L8        gr7=#SPILL1(gr1,176)
   39| 000B8C ld       E9010198   1     L8        gr8=#SPILL30(gr1,408)
   39| 000B90 cmpd     7E274000   1     C8        cr4=gr7,gr8
   39| 000B94 crand    4D314A02   1     CR_N      cr2=cr[42],0x2/gt,0x2/gt,0x2/gt,cr2
   39| 000B98 bc       40890210   1     BF        CL.80,cr2,0x2/gt,taken=50%(0,0)
    0| 000B9C ld       EB810188   1     L8        gr28=#SPILL28(gr1,392)
    0| 000BA0 ld       EB210180   1     L8        gr25=#SPILL27(gr1,384)
    0| 000BA4 ld       EB410118   1     L8        gr26=#SPILL14(gr1,280)
    0| 000BA8 ld       EA810148   1     L8        gr20=#SPILL20(gr1,328)
    0| 000BAC ld       EA610190   1     L8        gr19=#SPILL29(gr1,400)
    0| 000BB0 ld       EAA10130   1     L8        gr21=#SPILL17(gr1,304)
    0| 000BB4 ld       EA410198   1     L8        gr18=#SPILL30(gr1,408)
    0| 000BB8 mulld    7CF8E1D2   1     M         gr7=gr24,gr28
    0| 000BBC subf     7D06D050   1     S         gr8=gr26,gr6
    0| 000BC0 mulld    7D26C9D2   1     M         gr9=gr6,gr25
   39| 000BC4 ld       EA2100F0   1     L8        gr17=#SPILL9(gr1,240)
    0| 000BC8 add      7D08AA14   1     A         gr8=gr8,gr21
    0| 000BCC mulld    7D53A1D2   1     M         gr10=gr19,gr20
    0| 000BD0 add      7D074214   1     A         gr8=gr7,gr8
    0| 000BD4 mulld    7CF2A1D2   1     M         gr7=gr18,gr20
   39| 000BD8 addi     3971FFFF   1     AI        gr11=gr17,-1
    0| 000BDC add      7D294214   1     A         gr9=gr9,gr8
   39| 000BE0 sradi    7D6B1674   1     SRA8CA    gr11,ca=gr11,2
    0| 000BE4 rldicr   7A881764   1     SLL8      gr8=gr20,2
    0| 000BE8 add      7D295214   1     A         gr9=gr9,gr10
   39| 000BEC addze    7FAB0194   1     ADDE      gr29,ca=gr11,0,ca
    0| 000BF0 cmpdi    2C360000   1     C8        cr0=gr22,0
    0| 000BF4 add      7D274A14   1     A         gr9=gr7,gr9
    0| 000BF8 subf     7D944050   1     S         gr12=gr8,gr20
    0| 000BFC rldicr   7A9F0FA4   1     SLL8      gr31=gr20,1
    0| 000C00 rldicl   7AEAF842   1     SRL8      gr10=gr23,1
   39| 000C04 addi     38E00000   1     LI        gr7=0
    0| 000C08 bc       408101A0   1     BF        CL.80,cr0,0x2/gt,taken=20%(20,80)
    0| 000C0C qvfre    1080E830   1     QVFRE     fp4=fp29
    0| 000C10 add      7D69A214   1     A         gr11=gr9,gr20
    0| 000C14 add      7D896214   1     A         gr12=gr9,gr12
    0| 000C18 add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 000C1C addi     3BBD0001   1     AI        gr29=gr29,1
    0| 000C20 cmpdi    2F370000   1     C8        cr6=gr23,0
    0| 000C24 andi.    72FC0001   1     RN4_R     gr28,cr0=gr23,0,0x1
    0| 000C28 cmpdi    2EAA0000   1     C8        cr5=gr10,0
   39|                              CL.81:
   39| 000C2C addi     3B800000   1     LI        gr28=0
    0| 000C30 bc       4099015C   1     BF        CL.82,cr6,0x2/gt,taken=20%(20,80)
    0| 000C34 fmsub    FCBD1938   1     FMS       fp5=fp3,fp29,fp4,fcr
    0| 000C38 or       7D344B78   1     LR        gr20=gr9
    0| 000C3C or       7D936378   1     LR        gr19=gr12
    0| 000C40 or       7FF2FB78   1     LR        gr18=gr31
    0| 000C44 or       7D715B78   1     LR        gr17=gr11
    0| 000C48 fnmsub   FCA4217C   1     FNMS      fp5=fp4,fp4,fp5,fcr
    0| 000C4C fmsub    FCDD1978   2     FMS       fp6=fp3,fp29,fp5,fcr
    0| 000C50 fnmsub   FCA529BC   2     FNMS      fp5=fp5,fp5,fp6,fcr
   39|                              CL.83:
   39| 000C54 or       7E9AA378   1     LR        gr26=gr20
   39| 000C58 or       7E398B78   1     LR        gr25=gr17
   39| 000C5C or       7E579378   1     LR        gr23=gr18
   39| 000C60 or       7E759B78   1     LR        gr21=gr19
    0| 000C64 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 000C68 bc       41820064   1     BT        CL.645,cr0,0x4/eq,taken=50%(0,0)
   39| 000C6C lfdux    7CDA34EE   1     LFDU      fp6,gr26=b3(gr26,gr6,0)
   39| 000C70 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000C74 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000C78 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000C7C stfd     D8DA0000   1     STFL      b3(gr26,0)=fp6
   39| 000C80 lfdux    7CD934EE   1     LFDU      fp6,gr25=b3(gr25,gr6,0)
   39| 000C84 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000C88 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000C8C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000C90 stfd     D8D90000   1     STFL      b3(gr25,0)=fp6
   39| 000C94 lfdux    7CD734EE   1     LFDU      fp6,gr23=b3(gr23,gr6,0)
   39| 000C98 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000C9C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000CA0 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000CA4 stfd     D8D70000   1     STFL      b3(gr23,0)=fp6
   39| 000CA8 lfdux    7CD534EE   1     LFDU      fp6,gr21=b3(gr21,gr6,0)
   39| 000CAC fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000CB0 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000CB4 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000CB8 stfd     D8D50000   1     STFL      b3(gr21,0)=fp6
    0| 000CBC bc       419600B4   1     BT        CL.540,cr5,0x4/eq,taken=20%(20,80)
    0| 000CC0 ori      60210000   1     XNOP      
    0| 000CC4 ori      60210000   1     XNOP      
    0| 000CC8 ori      60210000   1     XNOP      
    0|                              CL.645:
   39| 000CCC lfdux    7CDA34EE   1     LFDU      fp6,gr26=b3(gr26,gr6,0)
   39| 000CD0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000CD4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000CD8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000CDC stfd     D8DA0000   1     STFL      b3(gr26,0)=fp6
   39| 000CE0 lfdux    7CD934EE   1     LFDU      fp6,gr25=b3(gr25,gr6,0)
   39| 000CE4 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000CE8 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000CEC fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000CF0 stfd     D8D90000   1     STFL      b3(gr25,0)=fp6
   39| 000CF4 lfdux    7CD734EE   1     LFDU      fp6,gr23=b3(gr23,gr6,0)
   39| 000CF8 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000CFC fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000D00 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000D04 stfd     D8D70000   1     STFL      b3(gr23,0)=fp6
   39| 000D08 lfdux    7CD534EE   1     LFDU      fp6,gr21=b3(gr21,gr6,0)
   39| 000D0C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000D10 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000D14 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000D18 stfd     D8D50000   1     STFL      b3(gr21,0)=fp6
   39| 000D1C lfdux    7CDA34EE   1     LFDU      fp6,gr26=b3(gr26,gr6,0)
   39| 000D20 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000D24 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000D28 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000D2C stfd     D8DA0000   1     STFL      b3(gr26,0)=fp6
   39| 000D30 lfdux    7CD934EE   1     LFDU      fp6,gr25=b3(gr25,gr6,0)
   39| 000D34 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000D38 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000D3C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000D40 stfd     D8D90000   1     STFL      b3(gr25,0)=fp6
   39| 000D44 lfdux    7CD734EE   1     LFDU      fp6,gr23=b3(gr23,gr6,0)
   39| 000D48 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000D4C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000D50 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000D54 stfd     D8D70000   1     STFL      b3(gr23,0)=fp6
   39| 000D58 lfdux    7CD534EE   1     LFDU      fp6,gr21=b3(gr21,gr6,0)
   39| 000D5C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   39| 000D60 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   39| 000D64 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   39| 000D68 stfd     D8D50000   1     STFL      b3(gr21,0)=fp6
    0| 000D6C bc       4200FF60   1     BCT       ctr=CL.645,taken=100%(100,0)
    0|                              CL.540:
   39| 000D70 addi     3B9C0001   1     AI        gr28=gr28,1
    0| 000D74 add      7E94C214   1     A         gr20=gr20,gr24
   39| 000D78 cmpld    7FBCB040   1     CL8       cr7=gr28,gr22
    0| 000D7C add      7E73C214   1     A         gr19=gr19,gr24
    0| 000D80 add      7E52C214   1     A         gr18=gr18,gr24
    0| 000D84 add      7E31C214   1     A         gr17=gr17,gr24
   39| 000D88 bc       419CFECC   1     BT        CL.83,cr7,0x8/llt,taken=80%(80,20)
   39|                              CL.82:
   39| 000D8C addi     38E70001   1     AI        gr7=gr7,1
    0| 000D90 add      7D685A14   1     A         gr11=gr8,gr11
   39| 000D94 cmpld    7FA7E840   1     CL8       cr7=gr7,gr29
    0| 000D98 add      7D886214   1     A         gr12=gr8,gr12
    0| 000D9C add      7FE8FA14   1     A         gr31=gr8,gr31
    0| 000DA0 add      7D284A14   1     A         gr9=gr8,gr9
   39| 000DA4 bc       419CFE88   1     BT        CL.81,cr7,0x8/llt,taken=80%(80,20)
   39|                              CL.80:
   41| 000DA8 fmr      FC60F890   1     LRFL      fp3=fp31
   42| 000DAC bc       408502A0   1     BF        CL.86,cr1,0x2/gt,taken=50%(0,0)
    0| 000DB0 ld       EB2100D0   1     L8        gr25=#SPILL5(gr1,208)
   44| 000DB4 ld       EAC100E0   1     L8        gr22=#SPILL7(gr1,224)
    0| 000DB8 ld       EAE100C8   1     L8        gr23=#SPILL4(gr1,200)
    0| 000DBC ld       EAA100D8   1     L8        gr21=#SPILL6(gr1,216)
    0| 000DC0 ld       EA8100E8   1     L8        gr20=#SPILL8(gr1,232)
   48| 000DC4 ld       EA610110   1     L8        gr19=#SPILL13(gr1,272)
    0| 000DC8 mulld    7CE3C9D2   1     M         gr7=gr3,gr25
   48| 000DCC ld       EA410118   1     L8        gr18=#SPILL14(gr1,280)
    0| 000DD0 ld       EA210120   1     L8        gr17=#SPILL15(gr1,288)
   44| 000DD4 subf     7D80B050   1     S         gr12=gr22,gr0
    0| 000DD8 add      7FF4AA14   1     A         gr31=gr20,gr21
   44| 000DDC addic.   378C0001   1     AI_R      gr28,cr0=gr12,1,ca"
    0| 000DE0 mulld    7D0021D2   1     M         gr8=gr0,gr4
    0| 000DE4 mulld    7D37F1D2   1     M         gr9=gr23,gr30
   48| 000DE8 mulld    7D65C9D2   1     M         gr11=gr5,gr25
   48| 000DEC mulld    7D46C9D2   1     M         gr10=gr6,gr25
   48| 000DF0 subf     7D859850   1     S         gr12=gr19,gr5
   48| 000DF4 subf     7FA69050   1     S         gr29=gr18,gr6
    0| 000DF8 add      7F47FA14   1     A         gr26=gr7,gr31
    0| 000DFC subf     7FF98850   1     S         gr31=gr17,gr25
   42| 000E00 addi     38E00000   1     LI        gr7=0
    0| 000E04 bc       40810248   1     BF        CL.86,cr0,0x2/gt,taken=20%(20,80)
   48| 000E08 ld       EA810128   1     L8        gr20=#SPILL16(gr1,296)
   48| 000E0C ld       EA610130   1     L8        gr19=#SPILL17(gr1,304)
    0| 000E10 subfic   23390001   1     SFI       gr25=1,gr25,ca"
    0| 000E14 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000E18 subf     7D24D050   1     S         gr9=gr26,gr4
    0| 000E1C addic.   37FF0001   1     AI_R      gr31,cr0=gr31,1,ca"
   48| 000E20 add      7EECA214   1     A         gr23=gr12,gr20
    0| 000E24 add      7D91CA14   1     A         gr12=gr17,gr25
   48| 000E28 add      7FB3EA14   1     A         gr29=gr19,gr29
    0| 000E2C rldicl   7999F842   1     SRL8      gr25=gr12,1
   48| 000E30 add      7EEBBA14   1     A         gr23=gr11,gr23
   48| 000E34 add      7ECAEA14   1     A         gr22=gr10,gr29
    0| 000E38 add      7EA84A14   1     A         gr21=gr8,gr9
    0| 000E3C mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000E40 andi.    71880001   1     RN4_R     gr8,cr0=gr12,0,0x1
    0| 000E44 cmpdi    2FB90000   1     C8        cr7=gr25,0
   42|                              CL.87:
    0| 000E48 ld       E92100C8   1     L8        gr9=#SPILL4(gr1,200)
   44| 000E4C addi     39000000   1     LI        gr8=0
    0| 000E50 add      7D474A14   1     A         gr10=gr7,gr9
    0| 000E54 bc       408501E4   1     BF        CL.88,cr1,0x2/gt,taken=20%(20,80)
    0| 000E58 ld       EBE10140   1     L8        gr31=#SPILL19(gr1,320)
    0| 000E5C ld       EBA10148   1     L8        gr29=#SPILL20(gr1,328)
    0| 000E60 addi     398A0001   1     AI        gr12=gr10,1
   46| 000E64 or       7EA9AB78   1     LR        gr9=gr21
    0| 000E68 mulld    7E8AF1D2   1     M         gr20=gr10,gr30
    0| 000E6C mulld    7D6AF9D2   1     M         gr11=gr10,gr31
    0| 000E70 mulld    7E6CE9D2   1     M         gr19=gr12,gr29
    0| 000E74 mulld    7E4AE9D2   1     M         gr18=gr10,gr29
    0| 000E78 add      7E2BBA14   1     A         gr17=gr11,gr23
   44|                              CL.89:
   46| 000E7C add      7D404214   1     A         gr10=gr0,gr8
   46| 000E80 lfdux    7C8924EE   1     LFDU      fp4,gr9=b1(gr9,gr4,0)
   48| 000E84 addi     396A0001   1     AI        gr11=gr10,1
   46| 000E88 mulld    7FE451D2   1     M         gr31=gr4,gr10
   48| 000E8C mulld    7FAAC1D2   1     M         gr29=gr10,gr24
   48| 000E90 mulld    7D6BD9D2   1     M         gr11=gr11,gr27
   48| 000E94 mulld    7D8AD9D2   1     M         gr12=gr10,gr27
   46| 000E98 add      7D5AFA14   1     A         gr10=gr26,gr31
   48| 000E9C add      7FBDB214   1     A         gr29=gr29,gr22
   46| 000EA0 add      7D4AA214   1     A         gr10=gr10,gr20
   48| 000EA4 add      7D6B8A14   1     A         gr11=gr11,gr17
   48| 000EA8 add      7D8C8A14   1     A         gr12=gr12,gr17
    0| 000EAC mtspr    7F2903A6   1     LCTR      ctr=gr25
   48| 000EB0 add      7FF3EA14   1     A         gr31=gr19,gr29
   48| 000EB4 add      7FBD9214   1     A         gr29=gr29,gr18
    0| 000EB8 bc       41820050   1     BT        CL.649,cr0,0x4/eq,taken=50%(0,0)
   46| 000EBC lfdux    7D2A1CEE   1     LFDU      fp9,gr10=b1(gr10,gr3,0)
   48| 000EC0 lfdux    7D6C2CEE   1     LFDU      fp11,gr12=b2(gr12,gr5,0)
   48| 000EC4 lfdux    7CAB2CEE   1     LFDU      fp5,gr11=b2(gr11,gr5,0)
   48| 000EC8 lfdux    7CDD34EE   1     LFDU      fp6,gr29=b3(gr29,gr6,0)
   48| 000ECC lfdux    7CFF34EE   1     LFDU      fp7,gr31=b3(gr31,gr6,0)
   48| 000ED0 fadd     FD04482A   1     AFL       fp8=fp4,fp9,fcr
   51| 000ED4 fmr      FC804890   2     LRFL      fp4=fp9
   51| 000ED8 fadd     FD4B282A   2     AFL       fp10=fp11,fp5,fcr
   51| 000EDC fadd     FD26382A   2     AFL       fp9=fp6,fp7,fcr
   48| 000EE0 fadd     FD68582A   2     AFL       fp11=fp8,fp11,fcr
   51| 000EE4 fmul     FD4A02B2   2     MFL       fp10=fp10,fp10,fcr
   48| 000EE8 fadd     FCAB282A   2     AFL       fp5=fp11,fp5,fcr
   51| 000EEC fmadd    FD08523A   2     FMA       fp8=fp10,fp8,fp8,fcr
   48| 000EF0 fadd     FCA5302A   2     AFL       fp5=fp5,fp6,fcr
   51| 000EF4 fmadd    FCC9427A   2     FMA       fp6=fp8,fp9,fp9,fcr
   48| 000EF8 fadd     FCA5382A   2     AFL       fp5=fp5,fp7,fcr
   51| 000EFC fmadd    FC66183A   2     FMA       fp3=fp3,fp6,fp0,fcr
   48| 000F00 fmadd    FFE5F87A   2     FMA       fp31=fp31,fp5,fp1,fcr
    0| 000F04 bc       419E0128   1     BT        CL.543,cr7,0x4/eq,taken=20%(20,80)
    0|                              CL.649:
   46| 000F08 lfdux    7CCA1CEE   1     LFDU      fp6,gr10=b1(gr10,gr3,0)
   48| 000F0C lfdux    7CEC2CEE   1     LFDU      fp7,gr12=b2(gr12,gr5,0)
   48| 000F10 lfdux    7F6B2CEE   1     LFDU      fp27,gr11=b2(gr11,gr5,0)
   48| 000F14 lfdux    7D7D34EE   1     LFDU      fp11,gr29=b3(gr29,gr6,0)
   48| 000F18 lfdux    7D3F34EE   1     LFDU      fp9,gr31=b3(gr31,gr6,0)
   48| 000F1C fadd     FD44302A   1     AFL       fp10=fp4,fp6,fcr
   46| 000F20 lfdux    7C8A1CEE   1     LFDU      fp4,gr10=b1(gr10,gr3,0)
   51| 000F24 fadd     FF87D82A   1     AFL       fp28=fp7,fp27,fcr
   48| 000F28 lfdux    7D0C2CEE   1     LFDU      fp8,gr12=b2(gr12,gr5,0)
   51| 000F2C fadd     FCAB482A   1     AFL       fp5=fp11,fp9,fcr
   48| 000F30 lfdux    7D8B2CEE   1     LFDU      fp12,gr11=b2(gr11,gr5,0)
   48| 000F34 fadd     FF4A382A   1     AFL       fp26=fp10,fp7,fcr
   48| 000F38 lfdux    7CFD34EE   1     LFDU      fp7,gr29=b3(gr29,gr6,0)
   48| 000F3C fadd     FDA6202A   1     AFL       fp13=fp6,fp4,fcr
   48| 000F40 lfdux    7CDF34EE   1     LFDU      fp6,gr31=b3(gr31,gr6,0)
   51| 000F44 fmul     FF3C0732   1     MFL       fp25=fp28,fp28,fcr
   51| 000F48 fadd     FF88602A   2     AFL       fp28=fp8,fp12,fcr
   48| 000F4C fadd     FF7AD82A   2     AFL       fp27=fp26,fp27,fcr
   48| 000F50 fadd     FF4D402A   2     AFL       fp26=fp13,fp8,fcr
   51| 000F54 fadd     FD07302A   2     AFL       fp8=fp7,fp6,fcr
   51| 000F58 fmadd    FD4ACABA   2     FMA       fp10=fp25,fp10,fp10,fcr
   51| 000F5C fmul     FF9C0732   2     MFL       fp28=fp28,fp28,fcr
   48| 000F60 fadd     FD7B582A   2     AFL       fp11=fp27,fp11,fcr
   48| 000F64 fadd     FD9A602A   2     AFL       fp12=fp26,fp12,fcr
   51| 000F68 fmadd    FDADE37A   2     FMA       fp13=fp28,fp13,fp13,fcr
   48| 000F6C fadd     FD2B482A   2     AFL       fp9=fp11,fp9,fcr
   48| 000F70 fadd     FD8C382A   2     AFL       fp12=fp12,fp7,fcr
    0| 000F74 bc       4240009C   1     BCF       ctr=CL.661,taken=0%(0,100)
    0| 000F78 ori      60210000   1     XNOP      
    0| 000F7C ori      60210000   1     XNOP      
    0| 000F80 ori      60210000   1     XNOP      
    0|                              CL.662:
   46| 000F84 lfdux    7D6A1CEE   1     LFDU      fp11,gr10=b1(gr10,gr3,0)
   51| 000F88 fmadd    FCA5517A   1     FMA       fp5=fp10,fp5,fp5,fcr
   48| 000F8C lfdux    7CFD34EE   1     LFDU      fp7,gr29=b3(gr29,gr6,0)
   48| 000F90 fadd     FF8C302A   1     AFL       fp28=fp12,fp6,fcr
   48| 000F94 fmadd    FFE9F87A   2     FMA       fp31=fp31,fp9,fp1,fcr
   51| 000F98 fmadd    FD086A3A   2     FMA       fp8=fp13,fp8,fp8,fcr
   48| 000F9C lfdux    7D3F34EE   1     LFDU      fp9,gr31=b3(gr31,gr6,0)
   48| 000FA0 fadd     FD44582A   1     AFL       fp10=fp4,fp11,fcr
   46| 000FA4 lfdux    7C8A1CEE   1     LFDU      fp4,gr10=b1(gr10,gr3,0)
   48| 000FA8 lfdux    7CCC2CEE   1     LFDU      fp6,gr12=b2(gr12,gr5,0)
   48| 000FAC lfdux    7F6B2CEE   1     LFDU      fp27,gr11=b2(gr11,gr5,0)
   51| 000FB0 fmadd    FC65183A   1     FMA       fp3=fp3,fp5,fp0,fcr
   51| 000FB4 fadd     FCA7482A   2     AFL       fp5=fp7,fp9,fcr
   48| 000FB8 fadd     FD6B202A   2     AFL       fp11=fp11,fp4,fcr
   48| 000FBC lfdux    7F4C2CEE   1     LFDU      fp26,gr12=b2(gr12,gr5,0)
   48| 000FC0 lfdux    7D8B2CEE   1     LFDU      fp12,gr11=b2(gr11,gr5,0)
   48| 000FC4 fadd     FF2A302A   1     AFL       fp25=fp10,fp6,fcr
   51| 000FC8 fadd     FCC6D82A   2     AFL       fp6=fp6,fp27,fcr
   48| 000FCC lfdux    7DBD34EE   1     LFDU      fp13,gr29=b3(gr29,gr6,0)
   48| 000FD0 fmadd    FFFCF87A   1     FMA       fp31=fp31,fp28,fp1,fcr
   48| 000FD4 fadd     FF8BD02A   2     AFL       fp28=fp11,fp26,fcr
   48| 000FD8 fadd     FF79D82A   2     AFL       fp27=fp25,fp27,fcr
   51| 000FDC fadd     FF5A602A   2     AFL       fp26=fp26,fp12,fcr
   51| 000FE0 fmul     FF2601B2   2     MFL       fp25=fp6,fp6,fcr
   48| 000FE4 lfdux    7CDF34EE   1     LFDU      fp6,gr31=b3(gr31,gr6,0)
   51| 000FE8 fmadd    FC68183A   1     FMA       fp3=fp3,fp8,fp0,fcr
   48| 000FEC fadd     FD9C602A   2     AFL       fp12=fp28,fp12,fcr
   48| 000FF0 fadd     FCFB382A   2     AFL       fp7=fp27,fp7,fcr
   51| 000FF4 fmul     FF9A06B2   2     MFL       fp28=fp26,fp26,fcr
   51| 000FF8 fmadd    FD4ACABA   2     FMA       fp10=fp25,fp10,fp10,fcr
   51| 000FFC fadd     FD0D302A   2     AFL       fp8=fp13,fp6,fcr
   48| 001000 fadd     FD8C682A   2     AFL       fp12=fp12,fp13,fcr
   48| 001004 fadd     FD27482A   2     AFL       fp9=fp7,fp9,fcr
   51| 001008 fmadd    FDABE2FA   2     FMA       fp13=fp28,fp11,fp11,fcr
    0| 00100C bc       4200FF78   1     BCT       ctr=CL.662,taken=100%(100,0)
    0|                              CL.661:
   51| 001010 fmadd    FC85517A   1     FMA       fp4=fp10,fp5,fp5,fcr
   51| 001014 fmadd    FCA86A3A   2     FMA       fp5=fp13,fp8,fp8,fcr
   48| 001018 fmadd    FCE9F87A   2     FMA       fp7=fp31,fp9,fp1,fcr
   48| 00101C fadd     FCCC302A   2     AFL       fp6=fp12,fp6,fcr
   51| 001020 fmadd    FC64183A   2     FMA       fp3=fp3,fp4,fp0,fcr
   48| 001024 fmadd    FFE6387A   2     FMA       fp31=fp7,fp6,fp1,fcr
   51| 001028 fmadd    FC65183A   2     FMA       fp3=fp3,fp5,fp0,fcr
    0|                              CL.543:
   55| 00102C addi     39080001   1     AI        gr8=gr8,1
   55| 001030 cmpld    7F28E040   1     CL8       cr6=gr8,gr28
   55| 001034 bc       4198FE48   1     BT        CL.89,cr6,0x8/llt,taken=80%(80,20)
   55|                              CL.88:
   56| 001038 ld       E9010108   1     L8        gr8=#SPILL12(gr1,264)
   56| 00103C addi     38E70001   1     AI        gr7=gr7,1
    0| 001040 add      7EB5F214   1     A         gr21=gr21,gr30
   56| 001044 cmpld    7F274040   1     CL8       cr6=gr7,gr8
   56| 001048 bc       4198FE00   1     BT        CL.87,cr6,0x8/llt,taken=80%(80,20)
   56|                              CL.86:
   58| 00104C fmuls    EC0200B2   1     MFS       fp0=fp2,fp2,fcr
   59| 001050 ld       E8620000   1     L8        gr3=.&&N&&mpipar(gr2,0)
   58| 001054 ld       E88100A8   1     L8        gr4=#SPILL0(gr1,168)
   58| 001058 fmuls    EC020032   1     MFS       fp0=fp2,fp0,fcr
   59| 00105C lwz      80030008   1     L4Z       gr0=<s61:d8:l4>(gr3,8)
   58| 001060 fdiv     FC030024   1     DFL       fp0=fp3,fp0,fcr
   59| 001064 cmpdi    2C200000   1     C8        cr0=gr0,0
   58| 001068 fsqrt    FC00002C   1     SQRT      fp0=fp0,fcr
   58| 00106C stfd     D8040000   1     STFL      rms(gr4,0)=fp0
   59| 001070 bc       41820088   1     BT        CL.893,cr0,0x4/eq,taken=40%(40,60)
   65|                              CL.61:
   68| 001074 ld       E80102D0   1     L8        gr0=#stack(gr1,720)
   67| 001078 fmr      FC20E890   1     LRFL      fp1=fp29
   68| 00107C lwa      E98102CA   1     L4A       gr12=#stack(gr1,712)
   68| 001080 lfd      CBE102B8   1     LFL       fp31=#stack(gr1,696)
   68| 001084 lfd      CBC102B0   1     LFL       fp30=#stack(gr1,688)
   68| 001088 lfd      CBA102A8   1     LFL       fp29=#stack(gr1,680)
   68| 00108C lfd      CB8102A0   1     LFL       fp28=#stack(gr1,672)
   68| 001090 lfd      CB610298   1     LFL       fp27=#stack(gr1,664)
   68| 001094 lfd      CB410290   1     LFL       fp26=#stack(gr1,656)
   68| 001098 lfd      CB210288   1     LFL       fp25=#stack(gr1,648)
   68| 00109C addi     382102C0   1     AI        gr1=gr1,704
   68| 0010A0 mtspr    7C0803A6   1     LLR       lr=gr0
   68| 0010A4 mtcrf    7D820120   1     MTCRF     cr2=gr12
   68| 0010A8 mtcrf    7D808120   1     MTCRF     cr4=gr12
   68| 0010AC ld       E9C1FF38   1     L8        gr14=#stack(gr1,-200)
   68| 0010B0 ld       E9E1FF40   1     L8        gr15=#stack(gr1,-192)
   68| 0010B4 ld       EA01FF48   1     L8        gr16=#stack(gr1,-184)
   68| 0010B8 ld       EA21FF50   1     L8        gr17=#stack(gr1,-176)
   68| 0010BC ld       EA41FF58   1     L8        gr18=#stack(gr1,-168)
   68| 0010C0 ld       EA61FF60   1     L8        gr19=#stack(gr1,-160)
   68| 0010C4 ld       EA81FF68   1     L8        gr20=#stack(gr1,-152)
   68| 0010C8 ld       EAA1FF70   1     L8        gr21=#stack(gr1,-144)
   68| 0010CC ld       EAC1FF78   1     L8        gr22=#stack(gr1,-136)
   68| 0010D0 ld       EAE1FF80   1     L8        gr23=#stack(gr1,-128)
   68| 0010D4 ld       EB01FF88   1     L8        gr24=#stack(gr1,-120)
   68| 0010D8 ld       EB21FF90   1     L8        gr25=#stack(gr1,-112)
   68| 0010DC ld       EB41FF98   1     L8        gr26=#stack(gr1,-104)
   68| 0010E0 ld       EB61FFA0   1     L8        gr27=#stack(gr1,-96)
   68| 0010E4 ld       EB81FFA8   1     L8        gr28=#stack(gr1,-88)
   68| 0010E8 ld       EBA1FFB0   1     L8        gr29=#stack(gr1,-80)
   68| 0010EC ld       EBC1FFB8   1     L8        gr30=#stack(gr1,-72)
   68| 0010F0 ld       EBE1FFC0   1     L8        gr31=#stack(gr1,-64)
   68| 0010F4 bclr     4E800020   1     BA        lr
    0|                              CL.893:
   60| 0010F8 ld       EBE20000   1     L8        gr31=.$STATIC(gr2,0)
   60| 0010FC addi     38000000   1     LI        gr0=0
   60| 001100 addi     38600006   1     LI        gr3=6
   60| 001104 ori      601D8000   1     OIL       gr29=gr0,0x8000
   60| 001108 addi     38800101   1     LI        gr4=257
   60| 00110C or       7FA6EB78   1     LR        gr6=gr29
   60| 001110 or       7FE5FB78   1     LR        gr5=gr31
   60| 001114 addi     38E00000   1     LI        gr7=0
   60| 001118 addi     39000000   1     LI        gr8=0
   60| 00111C addi     39200000   1     LI        gr9=0
   60| 001120 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#1",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   60| 001124 ori      60000000   1
   60| 001128 ld       EB820000   1     L8        gr28=.+CONSTANT_AREA(gr2,0)
   60| 00112C or       7C7E1B78   1     LR        gr30=gr3
   60| 001130 addi     38A0000A   1     LI        gr5=10
   60| 001134 addi     38C00001   1     LI        gr6=1
   60| 001138 addi     389C002C   1     AI        gr4=gr28,44
   60| 00113C bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   60| 001140 ori      60000000   1
   60| 001144 or       7FC3F378   1     LR        gr3=gr30
   60| 001148 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   60| 00114C ori      60000000   1
   61| 001150 addi     38BF0040   1     AI        gr5=gr31,64
   61| 001154 addi     38600006   1     LI        gr3=6
   61| 001158 addi     38800101   1     LI        gr4=257
   61| 00115C or       7FA6EB78   1     LR        gr6=gr29
   61| 001160 addi     38E00000   1     LI        gr7=0
   61| 001164 addi     39000000   1     LI        gr8=0
   61| 001168 addi     39200000   1     LI        gr9=0
   61| 00116C bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#3",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   61| 001170 ori      60000000   1
   61| 001174 or       7C7E1B78   1     LR        gr30=gr3
   61| 001178 addi     389C0038   1     AI        gr4=gr28,56
   61| 00117C addi     38A00016   1     LI        gr5=22
   61| 001180 addi     38C00001   1     LI        gr6=1
   61| 001184 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   61| 001188 ori      60000000   1
   61| 00118C stfd     DBA10080   1     STFL      #5(gr1,128)=fp29
   61| 001190 or       7FC3F378   1     LR        gr3=gr30
   61| 001194 addi     38810080   1     AI        gr4=gr1,128
   61| 001198 addi     38A00008   1     LI        gr5=8
   61| 00119C addi     38C00008   1     LI        gr6=8
   61| 0011A0 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,#5,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   61| 0011A4 ori      60000000   1
   61| 0011A8 or       7FC3F378   1     LR        gr3=gr30
   61| 0011AC bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   61| 0011B0 ori      60000000   1
   62| 0011B4 addi     38BF0080   1     AI        gr5=gr31,128
   62| 0011B8 or       7FA6EB78   1     LR        gr6=gr29
   62| 0011BC addi     38600006   1     LI        gr3=6
   62| 0011C0 addi     38800101   1     LI        gr4=257
   62| 0011C4 addi     38E00000   1     LI        gr7=0
   62| 0011C8 addi     39000000   1     LI        gr8=0
   62| 0011CC addi     39200000   1     LI        gr9=0
   62| 0011D0 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#6",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   62| 0011D4 ori      60000000   1
   62| 0011D8 or       7C7E1B78   1     LR        gr30=gr3
   62| 0011DC addi     389C0050   1     AI        gr4=gr28,80
   62| 0011E0 addi     38A00016   1     LI        gr5=22
   62| 0011E4 addi     38C00001   1     LI        gr6=1
   62| 0011E8 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   62| 0011EC ori      60000000   1
   62| 0011F0 fcfid    FC00F69C   1     FCFID     fp0=fp30,fcr
   62| 0011F4 lfs      C05C0068   1     LFS       fp2=+CONSTANT_AREA(gr28,104)
   62| 0011F8 or       7FC3F378   1     LR        gr3=gr30
   62| 0011FC addi     38810088   1     AI        gr4=gr1,136
   62| 001200 addi     38A00008   1     LI        gr5=8
   62| 001204 addi     38C00008   1     LI        gr6=8
   62| 001208 frsp     FC000018   1     CVLS      fp0=fp0,fcr
   62| 00120C fmuls    EC200032   1     MFS       fp1=fp0,fp0,fcr
   62| 001210 fmuls    EC000072   1     MFS       fp0=fp0,fp1,fcr
   62| 001214 fmuls    EC0000B2   1     MFS       fp0=fp0,fp2,fcr
   62| 001218 fdiv     FC1F0024   1     DFL       fp0=fp31,fp0,fcr
   62| 00121C stfd     D8010088   1     STFL      #8(gr1,136)=fp0
   62| 001220 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,#8,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   62| 001224 ori      60000000   1
   62| 001228 or       7FC3F378   1     LR        gr3=gr30
   62| 00122C bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   62| 001230 ori      60000000   1
   63| 001234 addi     38BF00C0   1     AI        gr5=gr31,192
   63| 001238 or       7FA6EB78   1     LR        gr6=gr29
   63| 00123C addi     38600006   1     LI        gr3=6
   63| 001240 addi     38800101   1     LI        gr4=257
   63| 001244 addi     38E00000   1     LI        gr7=0
   63| 001248 addi     39000000   1     LI        gr8=0
   63| 00124C addi     39200000   1     LI        gr9=0
   63| 001250 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#9",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   63| 001254 ori      60000000   1
   63| 001258 or       7C7E1B78   1     LR        gr30=gr3
   63| 00125C addi     389C006C   1     AI        gr4=gr28,108
   63| 001260 addi     38A00016   1     LI        gr5=22
   63| 001264 addi     38C00001   1     LI        gr6=1
   63| 001268 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   63| 00126C ori      60000000   1
   63| 001270 or       7FC3F378   1     LR        gr3=gr30
   63| 001274 ld       E88100A8   1     L8        gr4=#SPILL0(gr1,168)
   63| 001278 addi     38A00008   1     LI        gr5=8
   63| 00127C addi     38C00008   1     LI        gr6=8
   63| 001280 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,rms,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   63| 001284 ori      60000000   1
   63| 001288 or       7FC3F378   1     LR        gr3=gr30
   63| 00128C bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   63| 001290 ori      60000000   1
   64| 001294 addi     38BF0100   1     AI        gr5=gr31,256
   64| 001298 or       7FA6EB78   1     LR        gr6=gr29
   64| 00129C addi     38600006   1     LI        gr3=6
   64| 0012A0 addi     38800101   1     LI        gr4=257
   64| 0012A4 addi     38E00000   1     LI        gr7=0
   64| 0012A8 addi     39000000   1     LI        gr8=0
   64| 0012AC addi     39200000   1     LI        gr9=0
   64| 0012B0 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#11",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   64| 0012B4 ori      60000000   1
   64| 0012B8 addi     389C002C   1     AI        gr4=gr28,44
   64| 0012BC or       7C7F1B78   1     LR        gr31=gr3
   64| 0012C0 addi     38A0000A   1     LI        gr5=10
   64| 0012C4 addi     38C00001   1     LI        gr6=1
   64| 0012C8 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   64| 0012CC ori      60000000   1
   64| 0012D0 or       7FE3FB78   1     LR        gr3=gr31
   64| 0012D4 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   64| 0012D8 ori      60000000   1
    0| 0012DC b        4BFFFD98   1     B         CL.61,-1
   39|                              CL.398:
    0| 0012E0 ld       E9210198   1     L8        gr9=#SPILL30(gr1,408)
    0| 0012E4 mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 0012E8 or       7D284B78   1     LR        gr8=gr9
   39|                              CL.287:
   39| 0012EC addi     38E70001   1     AI        gr7=gr7,1
   39| 0012F0 cmpd     7C274000   1     C8        cr0=gr7,gr8
   39| 0012F4 bc       4100FFF8   1     BCTT      ctr=CL.287,cr0,0x1/lt,taken=80%(80,20)
    0| 0012F8 b        4BFFF890   1     B         CL.103,-1
   38|                              CL.388:
    0| 0012FC ld       E92101A0   1     L8        gr9=#SPILL31(gr1,416)
    0| 001300 mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 001304 or       7D284B78   1     LR        gr8=gr9
   38|                              CL.317:
   38| 001308 addi     38E70001   1     AI        gr7=gr7,1
   38| 00130C cmpd     7C274000   1     C8        cr0=gr7,gr8
   38| 001310 bc       4100FFF8   1     BCTT      ctr=CL.317,cr0,0x1/lt,taken=80%(80,20)
    0| 001314 b        4BFFF594   1     B         CL.115,-1
   37|                              CL.378:
    0| 001318 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 00131C or       7D485378   1     LR        gr8=gr10
   37|                              CL.347:
   37| 001320 addi     38E70001   1     AI        gr7=gr7,1
   37| 001324 cmpd     7C274000   1     C8        cr0=gr7,gr8
   37| 001328 bc       4100FFF8   1     BCTT      ctr=CL.347,cr0,0x1/lt,taken=80%(80,20)
    0| 00132C b        4BFFF2A8   1     B         CL.127,-1
     |               Tag Table
     | 001330        00000000 00012203 87120000 00001330
     |               Instruction count         1228
     |               Straight-line exec time   1403
     |               Constant Area
     | 000000        6E6F726D 6D61672E 66393049 00000000 3E800000 3F800000
     | 000018        BF800000 BEA00000 3EC00000 BF000000 3F000000 4E4F524D
     | 000030        4D414720 203A4942 4E4F524D 4D414720 203A204E 6F726D20
     | 000048        20202020 3D204942 4E4F524D 4D414720 203A2041 76657261
     | 000060        67652020 3D204942 40400000 4E4F524D 4D414720 203A2042
     | 000078        5F726D73 20202020 3D20

 
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    normmag.f90                 07/08/15   15:48:55
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................      68
1501-510  Compilation successful for file normmag.f90.
1501-543  Object file created.
