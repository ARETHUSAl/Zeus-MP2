IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- transprt_1D.f90 07/08/15 15:48:39
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 "transprt_1D.f90", line 106.17: 1513-029 (W) The number of arguments to "advx1" differs from the number of arguments in a previous reference. You should use the OPTIONAL attribute and an explicit interface to define a procedure with optional arguments.
** transprt_1d   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at transprt_1D.f90 <line 81> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at transprt_1D.f90 <line 82> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 3) at transprt_1D.f90 <line 83> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dc%addr  + d-w3dc%rvo))->w3dc[].rns6.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] = (((((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns9.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) * ((double *)((char *)d-g31b%addr  + d-g31b%rvo))->g31b[].rns8.[(long long) is + $$CIV0]) * ((double *)((char *)d-g32b%addr  + d-g32b%rvo))->g32b[].rns7.[(long long) js + $$CIV1]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at transprt_1D.f90 <line 83> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3da%addr  + d-w3da%rvo))->w3da[].rns0.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] = (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][($$CIV0 + (long long) is) - 1ll] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at transprt_1D.f90 <line 83> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3db%addr  + d-w3db%rvo))->w3db[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] = ((((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns5.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) * ((double *)((char *)d-g2b%addr  + d-g2b%rvo))->g2b[].rns4.[(long long) is + $$CIV0]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at transprt_1D.f90 <line 87> was not SIMD vectorized because it contains memory references ((char *)d-w3dc%addr  + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long long) ks + $$CIV2) + (d-w3dc%bounds%mult[].off1456)*((long long) js + $$CIV1) + (d-w3dc%bounds%mult[].off1480)*((long long) is + $$CIV0)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at transprt_1D.f90 <line 87> was not SIMD vectorized because it contains operation in (((((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns9.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) * ((double *)((char *)d-g31b%addr  + d-g31b%rvo))->g31b[].rns8.[(long long) is + $$CIV0]) * ((double *)((char *)d-g32b%addr  + d-g32b%rvo))->g32b[].rns7.[(long long) js + $$CIV1] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at transprt_1D.f90 <line 87> was not SIMD vectorized because it contains memory references ((char *)d-w3dc%addr  + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long long) ks + $$CIV2) + (d-w3dc%bounds%mult[].off1456)*((long long) js + $$CIV1) + (d-w3dc%bounds%mult[].off1480)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at transprt_1D.f90 <line 87> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at transprt_1D.f90 <line 87> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dc%addr  + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long long) ks + $$CIV2) + (d-w3dc%bounds%mult[].off1456)*((long long) js + $$CIV1) + (d-w3dc%bounds%mult[].off1480)*((long long) is + $$CIV0)).
1586-536 (I) Loop (loop index 3) at transprt_1D.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-w3da%addr  + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long long) ks + $$CIV2) + (d-w3da%bounds%mult[].off1248)*((long long) js + $$CIV1) + (d-w3da%bounds%mult[].off1272)*((long long) is + $$CIV0)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at transprt_1D.f90 <line 84> was not SIMD vectorized because it contains operation in (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][($$CIV0 + (long long) is) - 1ll] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at transprt_1D.f90 <line 84> was not SIMD vectorized because it contains memory references ((char *)d-w3da%addr  + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long long) ks + $$CIV2) + (d-w3da%bounds%mult[].off1248)*((long long) js + $$CIV1) + (d-w3da%bounds%mult[].off1272)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at transprt_1D.f90 <line 84> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at transprt_1D.f90 <line 84> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3da%addr  + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long long) ks + $$CIV2) + (d-w3da%bounds%mult[].off1248)*((long long) js + $$CIV1) + (d-w3da%bounds%mult[].off1272)*((long long) is + $$CIV0)).
1586-536 (I) Loop (loop index 3) at transprt_1D.f90 <line 85> was not SIMD vectorized because it contains memory references ((char *)d-w3db%addr  + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long long) ks + $$CIV2) + (d-w3db%bounds%mult[].off1352)*((long long) js + $$CIV1) + (d-w3db%bounds%mult[].off1376)*((long long) is + $$CIV0)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at transprt_1D.f90 <line 85> was not SIMD vectorized because it contains operation in ((((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns5.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * (((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) * ((double *)((char *)d-g2b%addr  + d-g2b%rvo))->g2b[].rns4.[(long long) is + $$CIV0] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at transprt_1D.f90 <line 85> was not SIMD vectorized because it contains memory references ((char *)d-w3db%addr  + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long long) ks + $$CIV2) + (d-w3db%bounds%mult[].off1352)*((long long) js + $$CIV1) + (d-w3db%bounds%mult[].off1376)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at transprt_1D.f90 <line 85> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at transprt_1D.f90 <line 85> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3db%addr  + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long long) ks + $$CIV2) + (d-w3db%bounds%mult[].off1352)*((long long) js + $$CIV1) + (d-w3db%bounds%mult[].off1376)*((long long) is + $$CIV0)).
1586-550 (I) Loop (loop index 4) at transprt_1D.f90 <line 114> was not SIMD vectorized because it is not profitable to vectorize.
1586-551 (I) Loop (loop index 4) at transprt_1D.f90 <line 115> was not SIMD vectorized because it contains unsupported vector data types.
1586-534 (I) Loop (loop index 5) at transprt_1D.f90 <line 128> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 6) at transprt_1D.f90 <line 129> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 130> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns18.[2ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns19.[2ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 130> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns18.[($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns19.[($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 130> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns18.[3ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns19.[3ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 130> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns18.[1ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns19.[1ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)).
1586-536 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)).
1586-536 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)).
1586-536 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 7) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) ks)) % 4ll) + (long long) ks)) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)).
1586-534 (I) Loop (loop index 8) at transprt_1D.f90 <line 140> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 9) at transprt_1D.f90 <line 141> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 10) at transprt_1D.f90 <line 142> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns20.[4ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns21.[4ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns20.[3ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns21.[3ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns20.[2ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns21.[2ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns20.[1ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns21.[1ll + ($$CIVD * 4ll + (long long) nspec % 4ll)][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)).
1586-536 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)).
1586-536 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)).
1586-536 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 11) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIVD * 4ll + (long long) nspec % 4ll)) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)).
1586-550 (I) Loop (loop index 12) at transprt_1D.f90 <line 151> was not SIMD vectorized because it is not profitable to vectorize.
1586-551 (I) Loop (loop index 12) at transprt_1D.f90 <line 152> was not SIMD vectorized because it contains unsupported vector data types.
1586-551 (I) Loop (loop index 12) at transprt_1D.f90 <line 153> was not SIMD vectorized because it contains unsupported vector data types.
1586-534 (I) Loop (loop index 17) at transprt_1D.f90 <line 140> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 18) at transprt_1D.f90 <line 141> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 19) at transprt_1D.f90 <line 142> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 20) at transprt_1D.f90 <line 143> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w4da%addr  + d-w4da%rvo))->w4da[].rns20.[$$CIVA + 1ll][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7] = ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns21.[$$CIVA + 1ll][(long long) ks + $$CIV9][(long long) js + $$CIV8][(long long) is + $$CIV7]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 20) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVA + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains memory references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVA + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at transprt_1D.f90 <line 144> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 20) at transprt_1D.f90 <line 144> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w4da%addr  + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVA + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long long) is + $$CIV7)).
1586-534 (I) Loop (loop index 24) at transprt_1D.f90 <line 128> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 25) at transprt_1D.f90 <line 129> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 26) at transprt_1D.f90 <line 130> was not SIMD vectorized because it contains memory references ((double *)((char *)d-w3dh%addr  + d-w3dh%rvo))->w3dh[].rns18.[(long long) ks + $$CIV6][(long long) js + $$CIV5][(long long) is + $$CIV4] = ((double *)((char *)d-er%addr  + d-er%rvo))->er[].rns19.[(long long) ks + $$CIV6][(long long) js + $$CIV5][(long long) is + $$CIV4]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 26) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks + $$CIV6) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 26) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains memory references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks + $$CIV6) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 26) at transprt_1D.f90 <line 131> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 26) at transprt_1D.f90 <line 131> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-w3dh%addr  + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long long) ks + $$CIV6) + (d-w3dh%bounds%mult[].off2184)*((long long) js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) is + $$CIV4)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"7">. Total number of the innermost loops SIMD vectorized <"0">.


    11|         SUBROUTINE transprt_1d ()
    65|           IF (.NOT.0 <> ((xhydro  .XOR.  1)  .AND.  1)) THEN
    67|             IF ((0 <> (xmhd  .AND.  1))) THEN
    71|               CALL ct_1d()
    72|             ENDIF
    81|             IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                      $$CIV2 = 0
       Id=1           DO $$CIV2 = $$CIV2, int((1 + (int(ke) - int(ks))))-1
    82|                 IF ((1 + (int(je) - int(js)) > 0)) THEN
                          $$CIV1 = 0
       Id=2               DO $$CIV1 = $$CIV1, int((1 + (int(je) - int(js))))&
                &             -1
    83|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV0 = 0
                              $$PRC0 = d-d%addr%d(int(is) - 1,$$CIV1 + int(js),&
                &               $$CIV2 + int(ks))
       Id=3                   DO $$CIV0 = $$CIV0, int((1 + (int(ie) - int(is))&
                &                 ))-1
                                $$PRC1 = d-d%addr%d(int(is) + $$CIV0,$$CIV1 + &
                &                 int(js),$$CIV2 + int(ks))
    84|                         d-w3da%addr%w3da(int(is) + $$CIV0,int(js) + &
                &                 $$CIV1,int(ks) + $$CIV2) = (d-v1%addr%v1(int(is)&
                &                  + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2) *  &
                &                 5.0000000000000000E-001) * ($$PRC0 + $$PRC1)
    85|                         d-w3db%addr%w3db(int(is) + $$CIV0,int(js) + &
                &                 $$CIV1,int(ks) + $$CIV2) = ((d-v2%addr%v2(int(&
                &                 is) + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2) &
                &                 *  5.0000000000000000E-001) * ($$PRC1 + $$PRC1))&
                &                  * d-g2b%addr%g2b(int(is) + $$CIV0)
    87|                         d-w3dc%addr%w3dc(int(is) + $$CIV0,int(js) + &
                &                 $$CIV1,int(ks) + $$CIV2) = (((d-v3%addr%v3(int(&
                &                 is) + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2) &
                &                 *  5.0000000000000000E-001) * ($$PRC1 + $$PRC1))&
                &                  * d-g31b%addr%g31b(int(is) + $$CIV0)) * &
                &                 d-g32b%addr%g32b(int(js) + $$CIV1)
                                $$PRC0 = $$PRC1
    89|                       ENDDO
                            ENDIF
    90|                   ENDDO
                        ENDIF
    91|               ENDDO
                    ENDIF
    95|             nseq = 0
   102|             IF ((lrad <> 0)) THEN
   103|               CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &       d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr,d-er%addr,&
                &       d-w3dh%addr,d-abun%addr,d-w4da%addr)
   105|             ELSE
   106|               CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &       d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr)
   107|             ENDIF
   114|             IF (.FALSE.) GOTO lab_63
                    $$CIV3 = 0
       Id=4         DO $$CIV3 = $$CIV3, 5
   115|               bvstat($$CIV3 + 1,3) = 0
   116|             ENDDO
                    lab_63
   121|           ELSE
   122|             lab_2
   123|             IF ((lrad <> 0)) THEN
   128|               IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (&
                &       int(ke) - int(ks)) > 0)) THEN
                        $$CIV6 = 0
       Id=24            DO $$CIV6 = $$CIV6, MOD((1 + (int(ke) - int(ks))), &
                &           int(4))-1
   129|                   IF ((1 + (int(je) - int(js)) > 0)) THEN
                            $$CIV5 = 0
       Id=25                DO $$CIV5 = $$CIV5, int((1 + (int(je) - int(js))))&
                &               -1
   130|                       IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                $$CIV4 = 0
       Id=26                    DO $$CIV4 = $$CIV4, int((1 + (int(ie) - int(&
                &                   is))))-1
   131|                           d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                   $$CIV5,int(ks) + $$CIV6) = d-er%addr%er(int(&
                &                   is) + $$CIV4,int(js) + $$CIV5,int(ks) + &
                &                   $$CIV6)
   132|                         ENDDO
                              ENDIF
   133|                     ENDDO
                          ENDIF
   134|                 ENDDO
                      ENDIF
   128|               IF (.NOT.(1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke)&
                &        - int(ks)) > MOD((1 + (int(ke) - int(ks))), 4))) GOTO &
                &       lab_65
                      $$CIVC = int(0)
       Id=5           DO $$CIVC = $$CIVC, int(((int(ke) - (MOD((1 + (int(ke) &
                &         - int(ks))), 4) + int(ks))) / 4 + 1))-1
   129|                 IF ((1 + (int(je) - int(js)) > 0)) THEN
                          $$CIV5 = 0
       Id=6               DO $$CIV5 = $$CIV5, int((1 + (int(je) - int(js))))&
                &             -1
   130|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV4 = 0
       Id=7                   DO $$CIV4 = $$CIV4, int((1 + (int(ie) - int(is))&
                &                 ))-1
   131|                         d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,($$CIVC * 4 + MOD((1 + (int(ke) - int(ks)&
                &                 )), 4)) + int(ks)) = d-er%addr%er(int(is) + &
                &                 $$CIV4,int(js) + $$CIV5,($$CIVC * 4 + MOD((1 + (&
                &                 int(ke) - int(ks))), 4)) + int(ks))
                                d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,1 + (($$CIVC * 4 + MOD((1 + (int(ke) - &
                &                 int(ks))), 4)) + int(ks))) = d-er%addr%er(int(&
                &                 is) + $$CIV4,int(js) + $$CIV5,1 + (($$CIVC * 4 &
                &                 + MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))&
                &                 
                                d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,2 + (($$CIVC * 4 + MOD((1 + (int(ke) - &
                &                 int(ks))), 4)) + int(ks))) = d-er%addr%er(int(&
                &                 is) + $$CIV4,int(js) + $$CIV5,2 + (($$CIVC * 4 &
                &                 + MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))&
                &                 
                                d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,3 + (($$CIVC * 4 + MOD((1 + (int(ke) - &
                &                 int(ks))), 4)) + int(ks))) = d-er%addr%er(int(&
                &                 is) + $$CIV4,int(js) + $$CIV5,3 + (($$CIVC * 4 &
                &                 + MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))&
                &                 
   132|                       ENDDO
                            ENDIF
   133|                   ENDDO
                        ENDIF
   134|               ENDDO
                      lab_65
   135|               lab_23
   139|               IF ((nspec > 1)) THEN
   140|                 IF ((MOD(int(nspec), 4) > 0  .AND.  int(nspec) > 0)) &
                &         THEN
                          $$CIVA = 0
       Id=17              DO $$CIVA = $$CIVA, MOD(int(nspec), int(4))-1
   141|                     IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                              $$CIV9 = 0
       Id=18                  DO $$CIV9 = $$CIV9, int((1 + (int(ke) - int(ks))&
                &                 ))-1
   142|                         IF ((1 + (int(je) - int(js)) > 0)) THEN
                                  $$CIV8 = 0
       Id=19                      DO $$CIV8 = $$CIV8, int((1 + (int(je) - int(&
                &                     js))))-1
   143|                             IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                      $$CIV7 = 0
       Id=20                          DO $$CIV7 = $$CIV7, int((1 + (int(ie) - &
                &                         int(is))))-1
   144|                                 d-w4da%addr%w4da(int(is) + $$CIV7,int(&
                &                         js) + $$CIV8,int(ks) + $$CIV9,$$CIVA + &
                &                         1) = d-abun%addr%abun(int(is) + $$CIV7,&
                &                         int(js) + $$CIV8,int(ks) + $$CIV9,&
                &                         $$CIVA + 1)
   145|                               ENDDO
                                    ENDIF
   146|                           ENDDO
                                ENDIF
   147|                       ENDDO
                            ENDIF
   148|                   ENDDO
                        ENDIF
   140|                 IF (.NOT.(int(nspec) > 0  .AND.  int(nspec) > MOD(int(&
                &         nspec), 4))) GOTO lab_71
                        $$CIVD = int(0)
       Id=8             DO $$CIVD = $$CIVD, int((((int(nspec) - MOD(int(nspec)&
                &           , 4)) - 1) / 4 + 1))-1
   141|                   IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                            $$CIV9 = 0
       Id=9                 DO $$CIV9 = $$CIV9, int((1 + (int(ke) - int(ks))))&
                &               -1
   142|                       IF ((1 + (int(je) - int(js)) > 0)) THEN
                                $$CIV8 = 0
       Id=10                    DO $$CIV8 = $$CIV8, int((1 + (int(je) - int(&
                &                   js))))-1
   143|                           IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                    $$CIV7 = 0
       Id=11                        DO $$CIV7 = $$CIV7, int((1 + (int(ie) - &
                &                       int(is))))-1
   144|                               d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,1 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,1 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
                                      d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,2 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,2 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
                                      d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,3 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,3 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
                                      d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,4 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,4 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
   145|                             ENDDO
                                  ENDIF
   146|                         ENDDO
                              ENDIF
   147|                     ENDDO
                          ENDIF
   148|                 ENDDO
                        lab_71
   149|                 lab_36
   151|                 IF (.FALSE.) GOTO lab_79
                        $$CIVB = 0
       Id=12            DO $$CIVB = $$CIVB, 5
   152|                   bvstat($$CIVB + 1,6) = 0
   153|                   bvstat($$CIVB + 1,7) = 0
   154|                 ENDDO
                        lab_79
   157|                 lab_81
                        RETURN
                      END SUBROUTINE transprt_1d


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            81             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            82             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3da%addr  + d-w3da%rvo 
                                          + (d-w3da%bounds%mult[].off1224)*((long long) ks + 
                                          $$CIV2) + (d-w3da%bounds%mult[].off1248)*((long long) 
                                          js + $$CIV1) + (d-w3da%bounds%mult[].off1272)*((long 
                                          long) is + $$CIV0))  with non-vectorizable alignment.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          operation in (((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] *  
                                          5.0000000000000000E-001) * (((double *)((char 
                                          *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][($$CIV0 + (long 
                                          long) is) - 1ll] + ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0]) which is 
                                          not  suitable for SIMD vectorization.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3da%addr  + d-w3da%rvo 
                                          + (d-w3da%bounds%mult[].off1224)*((long long) ks + 
                                          $$CIV2) + (d-w3da%bounds%mult[].off1248)*((long long) 
                                          js + $$CIV1) + (d-w3da%bounds%mult[].off1272)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0            84                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            84                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3da%addr 
                                          + d-w3da%rvo + (d-w3da%bounds%mult[].off1224)*((long 
                                          long) ks + $$CIV2) + 
                                          (d-w3da%bounds%mult[].off1248)*((long long) js + 
                                          $$CIV1) + (d-w3da%bounds%mult[].off1272)*((long long) 
                                          is + $$CIV0)).
         0            85                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3db%addr  + d-w3db%rvo 
                                          + (d-w3db%bounds%mult[].off1328)*((long long) ks + 
                                          $$CIV2) + (d-w3db%bounds%mult[].off1352)*((long long) 
                                          js + $$CIV1) + (d-w3db%bounds%mult[].off1376)*((long 
                                          long) is + $$CIV0))  with non-vectorizable alignment.
         0            85                  Loop was not SIMD vectorized because it contains 
                                          operation in ((((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns5.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] *  
                                          5.0000000000000000E-001) * (((double *)((char 
                                          *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0])) * 
                                          ((double *)((char *)d-g2b%addr  + 
                                          d-g2b%rvo))->g2b[].rns4.[(long long) is + $$CIV0] 
                                          which is not  suitable for SIMD vectorization.
         0            85                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3db%addr  + d-w3db%rvo 
                                          + (d-w3db%bounds%mult[].off1328)*((long long) ks + 
                                          $$CIV2) + (d-w3db%bounds%mult[].off1352)*((long long) 
                                          js + $$CIV1) + (d-w3db%bounds%mult[].off1376)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0            85                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            85                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3db%addr 
                                          + d-w3db%rvo + (d-w3db%bounds%mult[].off1328)*((long 
                                          long) ks + $$CIV2) + 
                                          (d-w3db%bounds%mult[].off1352)*((long long) js + 
                                          $$CIV1) + (d-w3db%bounds%mult[].off1376)*((long long) 
                                          is + $$CIV0)).
         0            87                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dc%addr  + d-w3dc%rvo 
                                          + (d-w3dc%bounds%mult[].off1432)*((long long) ks + 
                                          $$CIV2) + (d-w3dc%bounds%mult[].off1456)*((long long) 
                                          js + $$CIV1) + (d-w3dc%bounds%mult[].off1480)*((long 
                                          long) is + $$CIV0))  with non-vectorizable alignment.
         0            87                  Loop was not SIMD vectorized because it contains 
                                          operation in (((((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns9.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] *  
                                          5.0000000000000000E-001) * (((double *)((char 
                                          *)d-d%addr  + d-d%rvo))->d[].rns1.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns1.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0])) * 
                                          ((double *)((char *)d-g31b%addr  + 
                                          d-g31b%rvo))->g31b[].rns8.[(long long) is + $$CIV0]) 
                                          * ((double *)((char *)d-g32b%addr  + 
                                          d-g32b%rvo))->g32b[].rns7.[(long long) js + $$CIV1] 
                                          which is not  suitable for SIMD vectorization.
         0            87                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dc%addr  + d-w3dc%rvo 
                                          + (d-w3dc%bounds%mult[].off1432)*((long long) ks + 
                                          $$CIV2) + (d-w3dc%bounds%mult[].off1456)*((long long) 
                                          js + $$CIV1) + (d-w3dc%bounds%mult[].off1480)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0            87                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            87                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dc%addr 
                                          + d-w3dc%rvo + (d-w3dc%bounds%mult[].off1432)*((long 
                                          long) ks + $$CIV2) + 
                                          (d-w3dc%bounds%mult[].off1456)*((long long) js + 
                                          $$CIV1) + (d-w3dc%bounds%mult[].off1480)*((long long) 
                                          is + $$CIV0)).
         0           114             4    Loop was not SIMD vectorized because it is not 
                                          profitable to vectorize.
         0           115                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.
         0           128            24    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           129            25    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks + 
                                          $$CIV6) + (d-w3dh%bounds%mult[].off2184)*((long long) 
                                          js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV4))  with non-vectorizable alignment.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*((long long) ks + 
                                          $$CIV6) + (d-w3dh%bounds%mult[].off2184)*((long long) 
                                          js + $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long 
                                          long) is + $$CIV4)) with  non-vectorizable strides.
         0           131                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*((long 
                                          long) ks + $$CIV6) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)).
         0           128             5    Outer loop has been unrolled 4 time(s).
         0           128             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           129             6    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(($$CIVC * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4))  with non-vectorizable alignment.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(($$CIVC * 4ll + 
                                          (1ll + ((long long) ke - (long long) ks)) % 4ll) + 
                                          (long long) ks) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)) with  non-vectorizable strides.
         0           131                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + 
                                          (d-w3dh%bounds%mult[].off2160)*(($$CIVC * 4ll + (1ll 
                                          + ((long long) ke - (long long) ks)) % 4ll) + (long 
                                          long) ks) + (d-w3dh%bounds%mult[].off2184)*((long 
                                          long) js + $$CIV5) + 
                                          (d-w3dh%bounds%mult[].off2208)*((long long) is + 
                                          $$CIV4)).
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIVC * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4))  with non-vectorizable alignment.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(1ll + (($$CIVC * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)) with  non-vectorizable strides.
         0           131                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(1ll + 
                                          (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) 
                                          ks)) % 4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)).
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIVC * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4))  with non-vectorizable alignment.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(2ll + (($$CIVC * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)) with  non-vectorizable strides.
         0           131                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(2ll + 
                                          (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) 
                                          ks)) % 4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)).
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIVC * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4))  with non-vectorizable alignment.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w3dh%addr  + d-w3dh%rvo 
                                          + (d-w3dh%bounds%mult[].off2160)*(3ll + (($$CIVC * 
                                          4ll + (1ll + ((long long) ke - (long long) ks)) % 
                                          4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)) with  non-vectorizable strides.
         0           131                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           131                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w3dh%addr 
                                          + d-w3dh%rvo + (d-w3dh%bounds%mult[].off2160)*(3ll + 
                                          (($$CIVC * 4ll + (1ll + ((long long) ke - (long long) 
                                          ks)) % 4ll) + (long long) ks)) + 
                                          (d-w3dh%bounds%mult[].off2184)*((long long) js + 
                                          $$CIV5) + (d-w3dh%bounds%mult[].off2208)*((long long) 
                                          is + $$CIV4)).
         0           140            17    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           141            18    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           142            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*($$CIVA + 1ll) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7))  with non-vectorizable alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*($$CIVA + 1ll) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*($$CIVA 
                                          + 1ll) + (d-w4da%bounds%mult[].off2288)*((long long) 
                                          ks + $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long 
                                          long) js + $$CIV8) + 
                                          (d-w4da%bounds%mult[].off2336)*((long long) is + 
                                          $$CIV7)).
         0           140             8    Outer loop has been unrolled 4 time(s).
         0           140             8    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           141             9    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           142            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7))  with non-vectorizable alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(1ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(1ll + 
                                          ($$CIVD * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)).
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7))  with non-vectorizable alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(2ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(2ll + 
                                          ($$CIVD * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)).
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7))  with non-vectorizable alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(3ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(3ll + 
                                          ($$CIVD * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)).
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7))  with non-vectorizable alignment.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-w4da%addr  + d-w4da%rvo 
                                          + (d-w4da%bounds%mult[].off2264)*(4ll + ($$CIVD * 4ll 
                                          + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)) with  non-vectorizable strides.
         0           144                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           144                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-w4da%addr 
                                          + d-w4da%rvo + (d-w4da%bounds%mult[].off2264)*(4ll + 
                                          ($$CIVD * 4ll + (long long) nspec % 4ll)) + 
                                          (d-w4da%bounds%mult[].off2288)*((long long) ks + 
                                          $$CIV9) + (d-w4da%bounds%mult[].off2312)*((long long) 
                                          js + $$CIV8) + (d-w4da%bounds%mult[].off2336)*((long 
                                          long) is + $$CIV7)).
         0           151            12    Loop was not SIMD vectorized because it is not 
                                          profitable to vectorize.
         0           152                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.
         0           153                  Loop was not SIMD vectorized because it contains 
                                          unsupported vector data types.


    11|         SUBROUTINE transprt_1d ()
    65|           IF (.NOT.0 <> ((xhydro  .XOR.  1)  .AND.  1)) THEN
    67|             IF ((0 <> (xmhd  .AND.  1))) THEN
    71|               CALL ct_1d()
    72|             ENDIF
    81|             IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                      $$CIV2 = 0
       Id=1           DO $$CIV2 = $$CIV2, int((1 + (int(ke) - int(ks))))-1
    82|                 IF ((1 + (int(je) - int(js)) > 0)) THEN
                          $$CIV1 = 0
       Id=2               DO $$CIV1 = $$CIV1, int((1 + (int(je) - int(js))))&
                &             -1
    83|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV0 = 0
                              $$PRC0 = d-d%addr%d(int(is) - 1,$$CIV1 + int(js),&
                &               $$CIV2 + int(ks))
                              $$ICM0 = int(js) + $$CIV1
                              $$ICM1 = int(ks) + $$CIV2
    87|                       $$ICM2 = d-g32b%addr%g32b(int(js) + $$CIV1)
    83|Id=3                   DO $$CIV0 = $$CIV0, int((1 + (int(ie) - int(is))&
                &                 ))-1
                                $$PRC1 = d-d%addr%d(int(is) + $$CIV0,$$ICM0,&
                &                 $$ICM1)
    84|                         d-w3da%addr%w3da(int(is) + $$CIV0,$$ICM0,$$ICM1)&
                &                  = (d-v1%addr%v1(int(is) + $$CIV0,$$ICM0,$$ICM1)&
                &                  *  5.0000000000000000E-001) * ($$PRC0 + $$PRC1)&
                &                 
    85|                         d-w3db%addr%w3db(int(is) + $$CIV0,$$ICM0,$$ICM1)&
                &                  = ((d-v2%addr%v2(int(is) + $$CIV0,$$ICM0,&
                &                 $$ICM1) *  5.0000000000000000E-001) * ($$PRC1 + &
                &                 $$PRC1)) * d-g2b%addr%g2b(int(is) + $$CIV0)
    87|                         d-w3dc%addr%w3dc(int(is) + $$CIV0,$$ICM0,$$ICM1)&
                &                  = (((d-v3%addr%v3(int(is) + $$CIV0,$$ICM0,&
                &                 $$ICM1) *  5.0000000000000000E-001) * ($$PRC1 + &
                &                 $$PRC1)) * d-g31b%addr%g31b(int(is) + $$CIV0)) &
                &                 * $$ICM2
                                $$PRC0 = $$PRC1
    89|                       ENDDO
                            ENDIF
    90|                   ENDDO
                        ENDIF
    91|               ENDDO
                    ENDIF
    95|             nseq = 0
   102|             IF ((lrad <> 0)) THEN
   103|               CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &       d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr,d-er%addr,&
                &       d-w3dh%addr,d-abun%addr,d-w4da%addr)
   105|             ELSE
   106|               CALL advx1(d-w3dd%addr,d-d%addr,d-w3de%addr,d-w3dg%addr,&
                &       d-w3df%addr,d-w3da%addr,d-w3db%addr,d-w3dc%addr)
   107|             ENDIF
   114|             IF (.FALSE.) GOTO lab_63
                    $$CIV3 = 0
       Id=4         DO $$CIV3 = $$CIV3, 5
   115|               bvstat($$CIV3 + 1,3) = 0
   116|             ENDDO
                    lab_63
   121|           ELSE
   122|             lab_2
   123|             IF ((lrad <> 0)) THEN
   128|               IF ((MOD((1 + (int(ke) - int(ks))), 4) > 0  .AND.  1 + (&
                &       int(ke) - int(ks)) > 0)) THEN
                        $$CIV6 = 0
       Id=24            DO $$CIV6 = $$CIV6, MOD((1 + (int(ke) - int(ks))), &
                &           int(4))-1
   129|                   IF ((1 + (int(je) - int(js)) > 0)) THEN
                            $$CIV5 = 0
       Id=25                DO $$CIV5 = $$CIV5, int((1 + (int(je) - int(js))))&
                &               -1
   130|                       IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                $$CIV4 = 0
       Id=26                    DO $$CIV4 = $$CIV4, int((1 + (int(ie) - int(&
                &                   is))))-1
   131|                           d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                   $$CIV5,int(ks) + $$CIV6) = d-er%addr%er(int(&
                &                   is) + $$CIV4,int(js) + $$CIV5,int(ks) + &
                &                   $$CIV6)
   132|                         ENDDO
                              ENDIF
   133|                     ENDDO
                          ENDIF
   134|                 ENDDO
                      ENDIF
   128|               IF (.NOT.(1 + (int(ke) - int(ks)) > 0  .AND.  1 + (int(ke)&
                &        - int(ks)) > MOD((1 + (int(ke) - int(ks))), 4))) GOTO &
                &       lab_65
                      $$CIVC = int(0)
       Id=5           DO $$CIVC = $$CIVC, int(((int(ke) - (MOD((1 + (int(ke) &
                &         - int(ks))), 4) + int(ks))) / 4 + 1))-1
   129|                 IF ((1 + (int(je) - int(js)) > 0)) THEN
                          $$CIV5 = 0
       Id=6               DO $$CIV5 = $$CIV5, int((1 + (int(je) - int(js))))&
                &             -1
   130|                     IF ((1 + (int(ie) - int(is)) > 0)) THEN
                              $$CIV4 = 0
       Id=7                   DO $$CIV4 = $$CIV4, int((1 + (int(ie) - int(is))&
                &                 ))-1
   131|                         d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,($$CIVC * 4 + MOD((1 + (int(ke) - int(ks)&
                &                 )), 4)) + int(ks)) = d-er%addr%er(int(is) + &
                &                 $$CIV4,int(js) + $$CIV5,($$CIVC * 4 + MOD((1 + (&
                &                 int(ke) - int(ks))), 4)) + int(ks))
                                d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,1 + (($$CIVC * 4 + MOD((1 + (int(ke) - &
                &                 int(ks))), 4)) + int(ks))) = d-er%addr%er(int(&
                &                 is) + $$CIV4,int(js) + $$CIV5,1 + (($$CIVC * 4 &
                &                 + MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))&
                &                 
                                d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,2 + (($$CIVC * 4 + MOD((1 + (int(ke) - &
                &                 int(ks))), 4)) + int(ks))) = d-er%addr%er(int(&
                &                 is) + $$CIV4,int(js) + $$CIV5,2 + (($$CIVC * 4 &
                &                 + MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))&
                &                 
                                d-w3dh%addr%w3dh(int(is) + $$CIV4,int(js) + &
                &                 $$CIV5,3 + (($$CIVC * 4 + MOD((1 + (int(ke) - &
                &                 int(ks))), 4)) + int(ks))) = d-er%addr%er(int(&
                &                 is) + $$CIV4,int(js) + $$CIV5,3 + (($$CIVC * 4 &
                &                 + MOD((1 + (int(ke) - int(ks))), 4)) + int(ks)))&
                &                 
   132|                       ENDDO
                            ENDIF
   133|                   ENDDO
                        ENDIF
   134|               ENDDO
                      lab_65
   135|               lab_23
   139|               IF ((nspec > 1)) THEN
   140|                 IF ((MOD(int(nspec), 4) > 0  .AND.  int(nspec) > 0)) &
                &         THEN
                          $$CIVA = 0
       Id=17              DO $$CIVA = $$CIVA, MOD(int(nspec), int(4))-1
   141|                     IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                              $$CIV9 = 0
       Id=18                  DO $$CIV9 = $$CIV9, int((1 + (int(ke) - int(ks))&
                &                 ))-1
   142|                         IF ((1 + (int(je) - int(js)) > 0)) THEN
                                  $$CIV8 = 0
       Id=19                      DO $$CIV8 = $$CIV8, int((1 + (int(je) - int(&
                &                     js))))-1
   143|                             IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                      $$CIV7 = 0
       Id=20                          DO $$CIV7 = $$CIV7, int((1 + (int(ie) - &
                &                         int(is))))-1
   144|                                 d-w4da%addr%w4da(int(is) + $$CIV7,int(&
                &                         js) + $$CIV8,int(ks) + $$CIV9,$$CIVA + &
                &                         1) = d-abun%addr%abun(int(is) + $$CIV7,&
                &                         int(js) + $$CIV8,int(ks) + $$CIV9,&
                &                         $$CIVA + 1)
   145|                               ENDDO
                                    ENDIF
   146|                           ENDDO
                                ENDIF
   147|                       ENDDO
                            ENDIF
   148|                   ENDDO
                        ENDIF
   140|                 IF (.NOT.(int(nspec) > 0  .AND.  int(nspec) > MOD(int(&
                &         nspec), 4))) GOTO lab_71
                        $$CIVD = int(0)
       Id=8             DO $$CIVD = $$CIVD, int((((int(nspec) - MOD(int(nspec)&
                &           , 4)) - 1) / 4 + 1))-1
   141|                   IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                            $$CIV9 = 0
       Id=9                 DO $$CIV9 = $$CIV9, int((1 + (int(ke) - int(ks))))&
                &               -1
   142|                       IF ((1 + (int(je) - int(js)) > 0)) THEN
                                $$CIV8 = 0
       Id=10                    DO $$CIV8 = $$CIV8, int((1 + (int(je) - int(&
                &                   js))))-1
   143|                           IF ((1 + (int(ie) - int(is)) > 0)) THEN
                                    $$CIV7 = 0
       Id=11                        DO $$CIV7 = $$CIV7, int((1 + (int(ie) - &
                &                       int(is))))-1
   144|                               d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,1 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,1 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
                                      d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,2 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,2 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
                                      d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,3 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,3 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
                                      d-w4da%addr%w4da(int(is) + $$CIV7,int(js) &
                &                       + $$CIV8,int(ks) + $$CIV9,4 + ($$CIVD * 4 &
                &                       + MOD(int(nspec), 4))) = d-abun%addr%abun(&
                &                       int(is) + $$CIV7,int(js) + $$CIV8,int(ks) &
                &                       + $$CIV9,4 + ($$CIVD * 4 + MOD(int(nspec),&
                &                        4)))
   145|                             ENDDO
                                  ENDIF
   146|                         ENDDO
                              ENDIF
   147|                     ENDDO
                          ENDIF
   148|                 ENDDO
                        lab_71
   149|                 lab_36
   151|                 IF (.FALSE.) GOTO lab_79
                        $$CIVB = 0
       Id=12            DO $$CIVB = $$CIVB, 5
   152|                   bvstat($$CIVB + 1,6) = 0
   153|                   bvstat($$CIVB + 1,7) = 0
   154|                 ENDDO
                        lab_79
   157|                 lab_81
                        RETURN
                      END SUBROUTINE transprt_1d

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- ---- ssss
 CCR's set/used:   ss-- -sss
     | 000000                           PDEF     transprt_1d
   11|                                  PROC      
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 std      FBE1FFD8   1     ST8       #stack(gr1,-40)=gr31
    0| 000014 std      FBC1FFD0   1     ST8       #stack(gr1,-48)=gr30
    0| 000018 std      FBA1FFC8   1     ST8       #stack(gr1,-56)=gr29
    0| 00001C std      FB81FFC0   1     ST8       #stack(gr1,-64)=gr28
    0| 000020 std      FB61FFB8   1     ST8       #stack(gr1,-72)=gr27
    0| 000024 std      FB41FFB0   1     ST8       #stack(gr1,-80)=gr26
    0| 000028 std      FB21FFA8   1     ST8       #stack(gr1,-88)=gr25
    0| 00002C std      FB01FFA0   1     ST8       #stack(gr1,-96)=gr24
    0| 000030 std      FAE1FF98   1     ST8       #stack(gr1,-104)=gr23
    0| 000034 std      FAC1FF90   1     ST8       #stack(gr1,-112)=gr22
    0| 000038 std      FAA1FF88   1     ST8       #stack(gr1,-120)=gr21
    0| 00003C std      FA81FF80   1     ST8       #stack(gr1,-128)=gr20
    0| 000040 std      FA61FF78   1     ST8       #stack(gr1,-136)=gr19
    0| 000044 std      FA41FF70   1     ST8       #stack(gr1,-144)=gr18
    0| 000048 std      FA21FF68   1     ST8       #stack(gr1,-152)=gr17
    0| 00004C std      FA01FF60   1     ST8       #stack(gr1,-160)=gr16
    0| 000050 std      F9E1FF58   1     ST8       #stack(gr1,-168)=gr15
    0| 000054 std      F9C1FF50   1     ST8       #stack(gr1,-176)=gr14
    0| 000058 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 00005C std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000060 stdu     F821FCC1   1     ST8U      gr1,#stack(gr1,-832)=gr1
   65| 000064 ld       E8620000   1     L8        gr3=.&&N&&config(gr2,0)
   65| 000068 lwz      80030028   1     L4Z       gr0=<s177:d40:l4>(gr3,40)
   65| 00006C andi.    70000001   1     RN4_R     gr0,cr0=gr0,0,0x1
   65| 000070 bc       4182065C   1     BT        CL.2,cr0,0x4/eq,taken=50%(0,0)
   67| 000074 lwz      80030038   1     L4Z       gr0=<s177:d56:l4>(gr3,56)
   67| 000078 andi.    70000001   1     RN4_R     gr0,cr0=gr0,0,0x1
   67| 00007C bc       4182000C   1     BT        CL.3,cr0,0x4/eq,taken=60%(60,40)
   71| 000080 bl       48000001   1     CALL      ct_1d,0,#ProcAlias",ct_1d",fcr",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   71| 000084 ori      60000000   1
   72|                              CL.3:
   81| 000088 ld       EB820000   1     L8        gr28=.&&N&&grid(gr2,0)
    0| 00008C ld       E8A20000   1     L8        gr5=.&&N&field(gr2,0)
    0| 000090 ld       E8C20000   1     L8        gr6=.&&N&scratch(gr2,0)
   81| 000094 lwa      E81C0016   1     L4A       gr0=<s183:d20:l4>(gr28,20)
   81| 000098 lwa      E97C0012   1     L4A       gr11=<s183:d16:l4>(gr28,16)
    0| 00009C ld       E8850000   1     L8        gr4=<s39:d0:l8>(gr5,0)
    0| 0000A0 ld       E9060498   1     L8        gr8=<s95:d1176:l8>(gr6,1176)
    0| 0000A4 ld       E9260500   1     L8        gr9=<s95:d1280:l8>(gr6,1280)
    0| 0000A8 ld       E9460568   1     L8        gr10=<s95:d1384:l8>(gr6,1384)
   81| 0000AC std      F96100A8   1     ST8       #SPILL0(gr1,168)=gr11
   81| 0000B0 subf     7C6B0050   1     S         gr3=gr0,gr11
   81| 0000B4 addic.   35830001   1     AI_R      gr12,cr0=gr3,1,ca"
   81| 0000B8 std      F98100B0   1     ST8       #SPILL1(gr1,176)=gr12
   81| 0000BC bc       40810520   1     BF        CL.57,cr0,0x2/gt,taken=50%(0,0)
   83| 0000C0 std      F90100C8   1     ST8       #SPILL4(gr1,200)=gr8
   84| 0000C4 ld       EAA20000   1     L8        gr21=.&&N&scratch(gr2,0)
   83| 0000C8 lwa      E81C0002   1     L4A       gr0=<s183:d0:l4>(gr28,0)
   82| 0000CC lwa      EA1C000A   1     L4A       gr16=<s183:d8:l4>(gr28,8)
   82| 0000D0 lwa      EB5C000E   1     L4A       gr26=<s183:d12:l4>(gr28,12)
   83| 0000D4 lwa      EA5C0006   1     L4A       gr18=<s183:d4:l4>(gr28,4)
   85| 0000D8 ld       EB220000   1     L8        gr25=.&&N&grid(gr2,0)
   84| 0000DC ld       E97504F8   1     L8        gr11=<s95:d1272:l8>(gr21,1272)
   84| 0000E0 ld       EAD504B0   1     L8        gr22=<s95:d1200:l8>(gr21,1200)
   82| 0000E4 std      FA0100B8   1     ST8       #SPILL2(gr1,184)=gr16
   82| 0000E8 subf     7F50D050   1     S         gr26=gr26,gr16
   84| 0000EC or       7CAE2B78   1     LR        gr14=gr5
   82| 0000F0 addic.   369A0001   1     AI_R      gr20,cr0=gr26,1,ca"
   84| 0000F4 mulld    7F8059D2   1     M         gr28=gr0,gr11
   82| 0000F8 std      FA8100C0   1     ST8       #SPILL3(gr1,192)=gr20
   84| 0000FC add      7EC8B214   1     A         gr22=gr8,gr22
   87| 000100 ld       EAEE0270   1     L8        gr23=<s39:d624:l8>(gr14,624)
   84| 000104 add      7F9CB214   1     A         gr28=gr28,gr22
   85| 000108 ld       EAC20000   1     L8        gr22=.&&N&scratch(gr2,0)
   87| 00010C ld       EBD90CB0   1     L8        gr30=<s10:d3248:l8>(gr25,3248)
   85| 000110 ld       EA790B78   1     L8        gr19=<s10:d2936:l8>(gr25,2936)
   87| 000114 ld       EA390B98   1     L8        gr17=<s10:d2968:l8>(gr25,2968)
   87| 000118 ld       EA990CC8   1     L8        gr20=<s10:d3272:l8>(gr25,3272)
   85| 00011C ld       E9F90B60   1     L8        gr15=<s10:d2912:l8>(gr25,2912)
   87| 000120 ld       EA190BB0   1     L8        gr16=<s10:d2992:l8>(gr25,2992)
   85| 000124 ld       EB360518   1     L8        gr25=<s95:d1304:l8>(gr22,1304)
   87| 000128 ld       EACE0288   1     L8        gr22=<s39:d648:l8>(gr14,648)
   85| 00012C ld       E9950560   1     L8        gr12=<s95:d1376:l8>(gr21,1376)
   83| 000130 ld       E8650060   1     L8        gr3=<s39:d96:l8>(gr5,96)
   84| 000134 ld       E8A50200   1     L8        gr5=<s39:d512:l8>(gr5,512)
   85| 000138 ld       E8CE0268   1     L8        gr6=<s39:d616:l8>(gr14,616)
   84| 00013C ld       EBAE01A0   1     L8        gr29=<s39:d416:l8>(gr14,416)
   87| 000140 add      7EF7B214   1     A         gr23=gr23,gr22
   84| 000144 ld       EACE01D0   1     L8        gr22=<s39:d464:l8>(gr14,464)
   85| 000148 mulld    7F4061D2   1     M         gr26=gr0,gr12
   84| 00014C std      FAC100E0   1     ST8       #SPILL7(gr1,224)=gr22
   84| 000150 ld       EACE01E8   1     L8        gr22=<s39:d488:l8>(gr14,488)
   84| 000154 ld       EB6E01B8   1     L8        gr27=<s39:d440:l8>(gr14,440)
   87| 000158 addi     3BDEFFF8   1     AI        gr30=gr30,-8
   85| 00015C add      7F29CA14   1     A         gr25=gr9,gr25
   87| 000160 add      7E94F214   1     A         gr20=gr20,gr30
   87| 000164 ld       EBC20000   1     L8        gr30=.&&N&scratch(gr2,0)
   84| 000168 std      FAC100E8   1     ST8       #SPILL8(gr1,232)=gr22
   85| 00016C add      7F39D214   1     A         gr25=gr25,gr26
   84| 000170 mulld    7F4029D2   1     M         gr26=gr0,gr5
   85| 000174 ld       EACE0238   1     L8        gr22=<s39:d568:l8>(gr14,568)
   87| 000178 ld       EBF505C8   1     L8        gr31=<s95:d1480:l8>(gr21,1480)
   85| 00017C ld       EB0E0208   1     L8        gr24=<s39:d520:l8>(gr14,520)
   84| 000180 add      7FBDDA14   1     A         gr29=gr29,gr27
   85| 000184 ld       EAAE0220   1     L8        gr21=<s39:d544:l8>(gr14,544)
   85| 000188 mulld    7F6031D2   1     M         gr27=gr0,gr6
   85| 00018C std      FAC100F0   1     ST8       #SPILL9(gr1,240)=gr22
   87| 000190 ld       EBDE0580   1     L8        gr30=<s95:d1408:l8>(gr30,1408)
   83| 000194 ld       E90E0030   1     L8        gr8=<s39:d48:l8>(gr14,48)
   84| 000198 add      7FBAEA14   1     A         gr29=gr26,gr29
   83| 00019C ld       EB4E0018   1     L8        gr26=<s39:d24:l8>(gr14,24)
   85| 0001A0 ld       EACE0250   1     L8        gr22=<s39:d592:l8>(gr14,592)
   85| 0001A4 add      7F18AA14   1     A         gr24=gr24,gr21
   87| 0001A8 ld       E8EE02D0   1     L8        gr7=<s39:d720:l8>(gr14,720)
   83| 0001AC std      F90100D0   1     ST8       #SPILL5(gr1,208)=gr8
   85| 0001B0 add      7F7BC214   1     A         gr27=gr27,gr24
   87| 0001B4 add      7F0AF214   1     A         gr24=gr10,gr30
   85| 0001B8 std      FAC100F8   1     ST8       #SPILL10(gr1,248)=gr22
   83| 0001BC ld       EBCE0048   1     L8        gr30=<s39:d72:l8>(gr14,72)
    0| 0001C0 add      7EC4D214   1     A         gr22=gr4,gr26
   87| 0001C4 ld       EB4E02A0   1     L8        gr26=<s39:d672:l8>(gr14,672)
   87| 0001C8 ld       E90E02B8   1     L8        gr8=<s39:d696:l8>(gr14,696)
   84| 0001CC ld       E9C20000   1     L8        gr14=.&&N&scratch(gr2,0)
   81| 0001D0 addi     3AA00000   1     LI        gr21=0
   81| 0001D4 std      FAA100D8   1     ST8       #SPILL6(gr1,216)=gr21
   87| 0001D8 std      FB410100   1     ST8       #SPILL11(gr1,256)=gr26
   87| 0001DC std      F9010108   1     ST8       #SPILL12(gr1,264)=gr8
   84| 0001E0 ld       EAAE04C8   1     L8        gr21=<s95:d1224:l8>(gr14,1224)
   84| 0001E4 ld       EB4E04E0   1     L8        gr26=<s95:d1248:l8>(gr14,1248)
   85| 0001E8 ld       E90E0530   1     L8        gr8=<s95:d1328:l8>(gr14,1328)
   84| 0001EC std      FAA10110   1     ST8       #SPILL13(gr1,272)=gr21
   84| 0001F0 std      FB410118   1     ST8       #SPILL14(gr1,280)=gr26
   85| 0001F4 std      F9010120   1     ST8       #SPILL15(gr1,288)=gr8
   85| 0001F8 ld       EAAE0548   1     L8        gr21=<s95:d1352:l8>(gr14,1352)
   87| 0001FC ld       EB4E0598   1     L8        gr26=<s95:d1432:l8>(gr14,1432)
   87| 000200 ld       E90E05B0   1     L8        gr8=<s95:d1456:l8>(gr14,1456)
   85| 000204 std      FAA10128   1     ST8       #SPILL16(gr1,296)=gr21
   87| 000208 std      FB410130   1     ST8       #SPILL17(gr1,304)=gr26
   87| 00020C std      F9010138   1     ST8       #SPILL18(gr1,312)=gr8
    0| 000210 ld       E90100C8   1     L8        gr8=#SPILL4(gr1,200)
    0| 000214 bc       408103C8   1     BF        CL.57,cr0,0x2/gt,taken=20%(20,80)
    0| 000218 std      F88101A0   1     ST8       #SPILL31(gr1,416)=gr4
   84| 00021C subf     7FA5E850   1     S         gr29=gr29,gr5
   85| 000220 subf     7F66D850   1     S         gr27=gr27,gr6
   84| 000224 std      FBA10140   1     ST8       #SPILL19(gr1,320)=gr29
   85| 000228 std      FB610150   1     ST8       #SPILL21(gr1,336)=gr27
   87| 00022C mulld    7FA039D2   1     M         gr29=gr0,gr7
   87| 000230 mulld    7F60F9D2   1     M         gr27=gr0,gr31
   84| 000234 subf     7F8BE050   1     S         gr28=gr28,gr11
   85| 000238 subf     7F4CC850   1     S         gr26=gr25,gr12
   84| 00023C std      FB810148   1     ST8       #SPILL20(gr1,328)=gr28
   87| 000240 add      7F97EA14   1     A         gr28=gr23,gr29
   87| 000244 add      7FB8DA14   1     A         gr29=gr24,gr27
   85| 000248 std      FB410158   1     ST8       #SPILL22(gr1,344)=gr26
   87| 00024C subf     7F1FE850   1     S         gr24=gr29,gr31
   85| 000250 rldicr   781D1F24   1     SLL8      gr29=gr0,3
    0| 000254 mulld    7F4019D2   1     M         gr26=gr0,gr3
   85| 000258 addi     3BBDFFF8   1     AI        gr29=gr29,-8
   87| 00025C subf     7F27E050   1     S         gr25=gr28,gr7
   85| 000260 add      7F73EA14   1     A         gr27=gr19,gr29
   87| 000264 add      7FB1EA14   1     A         gr29=gr17,gr29
   87| 000268 ld       EA2100B8   1     L8        gr17=#SPILL2(gr1,184)
   85| 00026C add      7EEFDA14   1     A         gr23=gr15,gr27
   87| 000270 add      7E70EA14   1     A         gr19=gr16,gr29
    0| 000274 subf     7FA3D050   1     S         gr29=gr26,gr3
   87| 000278 std      FB210160   1     ST8       #SPILL23(gr1,352)=gr25
    0| 00027C add      7E16EA14   1     A         gr16=gr22,gr29
   87| 000280 rldicr   7A3B1F24   1     SLL8      gr27=gr17,3
    0| 000284 ld       E9E100D0   1     L8        gr15=#SPILL5(gr1,208)
   87| 000288 std      FB010168   1     ST8       #SPILL24(gr1,360)=gr24
   87| 00028C add      7ED4DA14   1     A         gr22=gr20,gr27
    0| 000290 ld       EA8100A8   1     L8        gr20=#SPILL0(gr1,168)
    0| 000294 subf     7FA09050   1     S         gr29=gr18,gr0
   85| 000298 std      FAE10170   1     ST8       #SPILL25(gr1,368)=gr23
    0| 00029C mulld    7F51F1D2   1     M         gr26=gr17,gr30
    0| 0002A0 mulld    7F6FA1D2   1     M         gr27=gr15,gr20
   87| 0002A4 std      FA610178   1     ST8       #SPILL26(gr1,376)=gr19
    0| 0002A8 std      FA010180   1     ST8       #SPILL27(gr1,384)=gr16
    0| 0002AC subfic   23800001   1     SFI       gr28=1,gr0,ca"
    0| 0002B0 addic.   341D0001   1     AI_R      gr0,cr0=gr29,1,ca"
    0| 0002B4 add      7F92E214   1     A         gr28=gr18,gr28
    0| 0002B8 ld       EBA20000   1     L8        gr29=.+CONSTANT_AREA(gr2,0)
    0| 0002BC rldicl   7B92F842   1     SRL8      gr18=gr28,1
   87| 0002C0 std      FAC10188   1     ST8       #SPILL28(gr1,392)=gr22
    0| 0002C4 cmpdi    2CB20000   1     C8        cr1=gr18,0
    0| 0002C8 mcrf     4F800000   1     LRCR      cr7=cr0
    0| 0002CC andi.    73800001   1     RN4_R     gr0,cr0=gr28,0,0x1
    0| 0002D0 subf     7C1E8050   1     S         gr0=gr16,gr30
    0| 0002D4 std      FA410190   1     ST8       #SPILL29(gr1,400)=gr18
    0| 0002D8 add      7C00DA14   1     A         gr0=gr0,gr27
    0| 0002DC lfs      C01D0000   1     LFS       fp0=+CONSTANT_AREA(gr29,0)
    0| 0002E0 add      7F80D214   1     A         gr28=gr0,gr26
    0| 0002E4 std      FB810198   1     ST8       #SPILL30(gr1,408)=gr28
   81|                              CL.58:
    0| 0002E8 ld       E80100A8   1     L8        gr0=#SPILL0(gr1,168)
    0| 0002EC ld       EB8100D8   1     L8        gr28=#SPILL6(gr1,216)
   82| 0002F0 addi     3BA00000   1     LI        gr29=0
    0| 0002F4 add      7F40E214   1     A         gr26=gr0,gr28
    0| 0002F8 bc       409D02B8   1     BF        CL.59,cr7,0x2/gt,taken=20%(20,80)
    0| 0002FC ld       EAC10130   1     L8        gr22=#SPILL17(gr1,304)
    0| 000300 ld       EA4100F0   1     L8        gr18=#SPILL9(gr1,240)
    0| 000304 ld       EAE100D0   1     L8        gr23=#SPILL5(gr1,208)
    0| 000308 ld       EA210168   1     L8        gr17=#SPILL24(gr1,360)
    0| 00030C ld       EA610100   1     L8        gr19=#SPILL11(gr1,256)
    0| 000310 ld       E9C10150   1     L8        gr14=#SPILL21(gr1,336)
    0| 000314 mulld    7F36D1D2   1     M         gr25=gr22,gr26
    0| 000318 mulld    7F12D1D2   1     M         gr24=gr18,gr26
    0| 00031C mulld    7C17D1D2   1     M         gr0=gr23,gr26
    0| 000320 ld       EAA10180   1     L8        gr21=#SPILL27(gr1,384)
    0| 000324 add      7E11CA14   1     A         gr16=gr17,gr25
    0| 000328 ld       E9E10120   1     L8        gr15=#SPILL15(gr1,288)
    0| 00032C std      FA0101B0   1     ST8       #SPILL33(gr1,432)=gr16
    0| 000330 add      7E0EC214   1     A         gr16=gr14,gr24
    0| 000334 ld       EB010110   1     L8        gr24=#SPILL13(gr1,272)
    0| 000338 ld       E9C100E0   1     L8        gr14=#SPILL7(gr1,224)
    0| 00033C add      7E80AA14   1     A         gr20=gr0,gr21
    0| 000340 mulld    7C13D1D2   1     M         gr0=gr19,gr26
    0| 000344 ld       EB210160   1     L8        gr25=#SPILL23(gr1,352)
    0| 000348 std      FA8101A8   1     ST8       #SPILL32(gr1,424)=gr20
   87| 00034C ld       EB810188   1     L8        gr28=#SPILL28(gr1,392)
   83| 000350 ld       EB610198   1     L8        gr27=#SPILL30(gr1,408)
    0| 000354 mulld    7E2FD1D2   1     M         gr17=gr15,gr26
    0| 000358 add      7C00CA14   1     A         gr0=gr0,gr25
    0| 00035C mulld    7DF8D1D2   1     M         gr15=gr24,gr26
    0| 000360 mulld    7DCED1D2   1     M         gr14=gr14,gr26
    0| 000364 ori      60210000   1     XNOP      
    0| 000368 ori      60210000   1     XNOP      
   82|                              CL.60:
   83| 00036C ld       EB2100B8   1     L8        gr25=#SPILL2(gr1,184)
   87| 000370 ld       EAE10108   1     L8        gr23=#SPILL12(gr1,264)
   85| 000374 ld       EA410128   1     L8        gr18=#SPILL16(gr1,296)
   84| 000378 ld       EAA100E8   1     L8        gr21=#SPILL8(gr1,232)
    0| 00037C ld       EB410190   1     L8        gr26=#SPILL29(gr1,400)
   87| 000380 ld       E88101B0   1     L8        gr4=#SPILL33(gr1,432)
   83| 000384 add      7E99EA14   1     A         gr20=gr25,gr29
   83| 000388 lfdux    7D5BF4EE   1     LFDU      fp10,gr27=d(gr27,gr30,0)
   87| 00038C mulld    7F14B9D2   1     M         gr24=gr20,gr23
   85| 000390 mulld    7E72A1D2   1     M         gr19=gr18,gr20
   84| 000394 ld       EAE10118   1     L8        gr23=#SPILL14(gr1,280)
   85| 000398 ld       EA4100F8   1     L8        gr18=#SPILL10(gr1,248)
   84| 00039C mulld    7ED4A9D2   1     M         gr22=gr20,gr21
   84| 0003A0 mulld    7EB4B9D2   1     M         gr21=gr20,gr23
   85| 0003A4 mulld    7EF2A1D2   1     M         gr23=gr18,gr20
   87| 0003A8 ld       EA410138   1     L8        gr18=#SPILL18(gr1,312)
   83| 0003AC mulld    7F34F1D2   1     M         gr25=gr20,gr30
   87| 0003B0 mulld    7E92A1D2   1     M         gr20=gr18,gr20
   83| 0003B4 ld       EA4101A8   1     L8        gr18=#SPILL32(gr1,424)
   87| 0003B8 add      7F180214   1     A         gr24=gr24,gr0
   87| 0003BC lfdu     CC3C0008   1     LFDU      fp1,gr28=g32b(gr28,8)
    0| 0003C0 mtspr    7F4903A6   1     LCTR      ctr=gr26
   87| 0003C4 ld       EB410178   1     L8        gr26=#SPILL26(gr1,376)
   85| 0003C8 add      7EF78214   1     A         gr23=gr23,gr16
   83| 0003CC add      7F399214   1     A         gr25=gr25,gr18
   84| 0003D0 ld       EA410140   1     L8        gr18=#SPILL19(gr1,320)
   87| 0003D4 add      7E942214   1     A         gr20=gr20,gr4
   84| 0003D8 add      7ED69214   1     A         gr22=gr22,gr18
   85| 0003DC ld       EA410158   1     L8        gr18=#SPILL22(gr1,344)
   84| 0003E0 add      7ECEB214   1     A         gr22=gr14,gr22
   85| 0003E4 add      7E739214   1     A         gr19=gr19,gr18
   84| 0003E8 ld       EA410148   1     L8        gr18=#SPILL20(gr1,328)
   85| 0003EC add      7E719A14   1     A         gr19=gr17,gr19
   84| 0003F0 add      7E52AA14   1     A         gr18=gr18,gr21
   85| 0003F4 ld       EAA10170   1     L8        gr21=#SPILL25(gr1,368)
   84| 0003F8 add      7E4F9214   1     A         gr18=gr15,gr18
    0| 0003FC bc       4182005C   1     BT        CL.481,cr0,0x4/eq,taken=50%(0,0)
   83| 000400 lfdux    7C591CEE   1     LFDU      fp2,gr25=d(gr25,gr3,0)
   87| 000404 lfdux    7CD83CEE   1     LFDU      fp6,gr24=v3(gr24,gr7,0)
   85| 000408 lfdux    7CF734EE   1     LFDU      fp7,gr23=v2(gr23,gr6,0)
   87| 00040C lfdu     CC7A0008   1     LFDU      fp3,gr26=g31b(gr26,8)
   84| 000410 lfdux    7D162CEE   1     LFDU      fp8,gr22=v1(gr22,gr5,0)
   85| 000414 lfdu     CC950008   1     LFDU      fp4,gr21=g2b(gr21,8)
   85| 000418 fadd     FCA2102A   1     AFL       fp5=fp2,fp2,fcr
   87| 00041C fmul     FCC60032   2     MFL       fp6=fp6,fp0,fcr
   85| 000420 fmul     FCE70032   2     MFL       fp7=fp7,fp0,fcr
   84| 000424 fadd     FD2A102A   2     AFL       fp9=fp10,fp2,fcr
   87| 000428 fmr      FD401090   2     LRFL      fp10=fp2
   84| 00042C fmul     FD080032   2     MFL       fp8=fp8,fp0,fcr
   87| 000430 fmul     FC4501B2   2     MFL       fp2=fp5,fp6,fcr
   85| 000434 fmul     FCA70172   2     MFL       fp5=fp7,fp5,fcr
   84| 000438 fmul     FCC80272   2     MFL       fp6=fp8,fp9,fcr
   87| 00043C fmul     FC4200F2   2     MFL       fp2=fp2,fp3,fcr
   85| 000440 fmul     FC650132   2     MFL       fp3=fp5,fp4,fcr
   84| 000444 stfdux   7CD25DEE   2     STFDU     gr18,w3da(gr18,gr11,0)=fp6
   87| 000448 fmul     FC420072   1     MFL       fp2=fp2,fp1,fcr
   85| 00044C stfdux   7C7365EE   2     STFDU     gr19,w3db(gr19,gr12,0)=fp3
   87| 000450 stfdux   7C54FDEE   1     STFDU     gr20,w3dc(gr20,gr31,0)=fp2
    0| 000454 bc       4186014C   1     BT        CL.419,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.481:
   83| 000458 lfdux    7C791CEE   1     LFDU      fp3,gr25=d(gr25,gr3,0)
   87| 00045C lfdux    7D183CEE   1     LFDU      fp8,gr24=v3(gr24,gr7,0)
   85| 000460 lfdux    7FF734EE   1     LFDU      fp31,gr23=v2(gr23,gr6,0)
   87| 000464 lfdu     CC9A0010   1     LFDU      fp4,gr26=g31b(gr26,16)
   84| 000468 lfdux    7D962CEE   1     LFDU      fp12,gr22=v1(gr22,gr5,0)
   85| 00046C lfdu     CCB50010   1     LFDU      fp5,gr21=g2b(gr21,16)
   87| 000470 lfd      C8DAFFF8   1     LFL       fp6=g31b(gr26,-8)
   85| 000474 fadd     FCE3182A   1     AFL       fp7=fp3,fp3,fcr
   83| 000478 lfdux    7C591CEE   1     LFDU      fp2,gr25=d(gr25,gr3,0)
   87| 00047C fmul     FD080032   1     MFL       fp8=fp8,fp0,fcr
   87| 000480 lfdux    7D383CEE   1     LFDU      fp9,gr24=v3(gr24,gr7,0)
   84| 000484 fadd     FD4A182A   1     AFL       fp10=fp10,fp3,fcr
   85| 000488 lfdux    7D7734EE   1     LFDU      fp11,gr23=v2(gr23,gr6,0)
   84| 00048C fmul     FD8C0032   1     MFL       fp12=fp12,fp0,fcr
   84| 000490 lfdux    7DB62CEE   1     LFDU      fp13,gr22=v1(gr22,gr5,0)
   85| 000494 fmul     FFFF0032   1     MFL       fp31=fp31,fp0,fcr
   85| 000498 lfd      CBD5FFF8   1     LFL       fp30=g2b(gr21,-8)
   85| 00049C fadd     FFA2102A   1     AFL       fp29=fp2,fp2,fcr
   87| 0004A0 fmul     FD290032   2     MFL       fp9=fp9,fp0,fcr
   87| 0004A4 fmul     FD070232   2     MFL       fp8=fp7,fp8,fcr
   85| 0004A8 fmul     FD6B0032   2     MFL       fp11=fp11,fp0,fcr
   84| 0004AC fmul     FD4C02B2   2     MFL       fp10=fp12,fp10,fcr
   85| 0004B0 fmul     FCFF01F2   2     MFL       fp7=fp31,fp7,fcr
   84| 0004B4 fmul     FD8D0032   2     MFL       fp12=fp13,fp0,fcr
   87| 0004B8 fmul     FDBD0272   2     MFL       fp13=fp29,fp9,fcr
   87| 0004BC fmul     FCC801B2   2     MFL       fp6=fp8,fp6,fcr
   84| 0004C0 fadd     FC63102A   2     AFL       fp3=fp3,fp2,fcr
   85| 0004C4 fmul     FD0B0772   2     MFL       fp8=fp11,fp29,fcr
   84| 0004C8 stfdux   7D525DEE   2     STFDU     gr18,w3da(gr18,gr11,0)=fp10
   85| 0004CC fmul     FD2707B2   1     MFL       fp9=fp7,fp30,fcr
   87| 0004D0 fmul     FC8D0132   2     MFL       fp4=fp13,fp4,fcr
   87| 0004D4 fmul     FCC60072   2     MFL       fp6=fp6,fp1,fcr
   84| 0004D8 fmul     FCEC00F2   2     MFL       fp7=fp12,fp3,fcr
   85| 0004DC fmul     FD480172   2     MFL       fp10=fp8,fp5,fcr
    0| 0004E0 bc       424000A8   1     BCF       ctr=CL.503,taken=0%(0,100)
    0|                              CL.504:
   85| 0004E4 lfdu     CC750010   1     LFDU      fp3,gr21=g2b(gr21,16)
   87| 0004E8 fmul     FC840072   1     MFL       fp4=fp4,fp1,fcr
   85| 0004EC stfdux   7D3365EE   2     STFDU     gr19,w3db(gr19,gr12,0)=fp9
   87| 0004F0 stfdux   7CD4FDEE   1     STFDU     gr20,w3dc(gr20,gr31,0)=fp6
   85| 0004F4 lfd      C8B5FFF8   1     LFL       fp5=g2b(gr21,-8)
   87| 0004F8 lfdu     CCDA0010   1     LFDU      fp6,gr26=g31b(gr26,16)
   84| 0004FC stfdux   7CF25DEE   1     STFDU     gr18,w3da(gr18,gr11,0)=fp7
   83| 000500 lfdux    7CF91CEE   1     LFDU      fp7,gr25=d(gr25,gr3,0)
   87| 000504 lfdux    7FF83CEE   1     LFDU      fp31,gr24=v3(gr24,gr7,0)
   87| 000508 lfd      C91AFFF8   1     LFL       fp8=g31b(gr26,-8)
   85| 00050C lfdux    7D3734EE   1     LFDU      fp9,gr23=v2(gr23,gr6,0)
   84| 000510 lfdux    7FD62CEE   1     LFDU      fp30,gr22=v1(gr22,gr5,0)
   85| 000514 stfdux   7D5365EE   1     STFDU     gr19,w3db(gr19,gr12,0)=fp10
   84| 000518 fadd     FD42382A   1     AFL       fp10=fp2,fp7,fcr
   83| 00051C lfdux    7C591CEE   1     LFDU      fp2,gr25=d(gr25,gr3,0)
   85| 000520 fadd     FD67382A   1     AFL       fp11=fp7,fp7,fcr
   85| 000524 lfdux    7D9734EE   1     LFDU      fp12,gr23=v2(gr23,gr6,0)
   87| 000528 lfdux    7DB83CEE   1     LFDU      fp13,gr24=v3(gr24,gr7,0)
   87| 00052C fmul     FFFF0032   1     MFL       fp31=fp31,fp0,fcr
   84| 000530 fmul     FFDE0032   2     MFL       fp30=fp30,fp0,fcr
   84| 000534 lfdux    7FB62CEE   1     LFDU      fp29,gr22=v1(gr22,gr5,0)
   85| 000538 fadd     FF82102A   1     AFL       fp28=fp2,fp2,fcr
   85| 00053C fmul     FD290032   2     MFL       fp9=fp9,fp0,fcr
   85| 000540 fmul     FD8C0032   2     MFL       fp12=fp12,fp0,fcr
   87| 000544 fmul     FDAD0032   2     MFL       fp13=fp13,fp0,fcr
   87| 000548 fmul     FFEB07F2   2     MFL       fp31=fp11,fp31,fcr
   87| 00054C stfdux   7C94FDEE   1     STFDU     gr20,w3dc(gr20,gr31,0)=fp4
   84| 000550 fmul     FC9D0032   1     MFL       fp4=fp29,fp0,fcr
   84| 000554 fadd     FCE7102A   2     AFL       fp7=fp7,fp2,fcr
   85| 000558 fmul     FD2902F2   2     MFL       fp9=fp9,fp11,fcr
   87| 00055C fmul     FD7C0372   2     MFL       fp11=fp28,fp13,fcr
   87| 000560 fmul     FD1F0232   2     MFL       fp8=fp31,fp8,fcr
   85| 000564 fmul     FD8C0732   2     MFL       fp12=fp12,fp28,fcr
   84| 000568 fmul     FDBE02B2   2     MFL       fp13=fp30,fp10,fcr
   84| 00056C fmul     FCE401F2   2     MFL       fp7=fp4,fp7,fcr
   85| 000570 fmul     FD290172   2     MFL       fp9=fp9,fp5,fcr
   87| 000574 fmul     FC8B01B2   2     MFL       fp4=fp11,fp6,fcr
   87| 000578 fmul     FCC80072   2     MFL       fp6=fp8,fp1,fcr
   85| 00057C fmul     FD4C00F2   2     MFL       fp10=fp12,fp3,fcr
   84| 000580 stfdux   7DB25DEE   2     STFDU     gr18,w3da(gr18,gr11,0)=fp13
    0| 000584 bc       4200FF60   1     BCT       ctr=CL.504,taken=100%(100,0)
    0|                              CL.503:
   87| 000588 stfdux   7CD4FDEE   1     STFDU     gr20,w3dc(gr20,gr31,0)=fp6
   87| 00058C fmul     FC240072   1     MFL       fp1=fp4,fp1,fcr
   85| 000590 stfdux   7D3365EE   2     STFDU     gr19,w3db(gr19,gr12,0)=fp9
   84| 000594 stfdux   7CF25DEE   1     STFDU     gr18,w3da(gr18,gr11,0)=fp7
   85| 000598 stfdux   7D5365EE   1     STFDU     gr19,w3db(gr19,gr12,0)=fp10
   87| 00059C stfdux   7C34FDEE   1     STFDU     gr20,w3dc(gr20,gr31,0)=fp1
    0|                              CL.419:
   90| 0005A0 ld       E88100C0   1     L8        gr4=#SPILL3(gr1,192)
   90| 0005A4 addi     3BBD0001   1     AI        gr29=gr29,1
   90| 0005A8 cmpld    7F3D2040   1     CL8       cr6=gr29,gr4
   90| 0005AC bc       4198FDC0   1     BT        CL.60,cr6,0x8/llt,taken=80%(80,20)
   90|                              CL.59:
   91| 0005B0 ld       E88100D8   1     L8        gr4=#SPILL6(gr1,216)
    0| 0005B4 ld       E80100D0   1     L8        gr0=#SPILL5(gr1,208)
    0| 0005B8 ld       EBA10198   1     L8        gr29=#SPILL30(gr1,408)
   91| 0005BC ld       EB8100B0   1     L8        gr28=#SPILL1(gr1,176)
   91| 0005C0 addi     38840001   1     AI        gr4=gr4,1
   91| 0005C4 std      F88100D8   1     ST8       #SPILL6(gr1,216)=gr4
    0| 0005C8 add      7FA0EA14   1     A         gr29=gr0,gr29
   91| 0005CC cmpld    7F24E040   1     CL8       cr6=gr4,gr28
    0| 0005D0 std      FBA10198   1     ST8       #SPILL30(gr1,408)=gr29
   91| 0005D4 bc       4198FD14   1     BT        CL.58,cr6,0x8/llt,taken=80%(80,20)
   91| 0005D8 ld       E88101A0   1     L8        gr4=#SPILL31(gr1,416)
   91|                              CL.57:
  102| 0005DC ld       E9820000   1     L8        gr12=.&&N&&config(gr2,0)
   95| 0005E0 ld       E9620000   1     L8        gr11=.&&N&&root(gr2,0)
    0| 0005E4 ld       EBE20000   1     L8        gr31=.&&N&scratch(gr2,0)
   95| 0005E8 addi     3BC00000   1     LI        gr30=0
  102| 0005EC lwz      800C0018   1     L4Z       gr0=<s177:d24:l4>(gr12,24)
   95| 0005F0 stw      93CB01FC   1     ST4Z      <s201:d508:l4>(gr11,508)=gr30
    0| 0005F4 ld       E87F05D0   1     L8        gr3=<s95:d1488:l8>(gr31,1488)
    0| 0005F8 ld       E8BF0638   1     L8        gr5=<s95:d1592:l8>(gr31,1592)
    0| 0005FC ld       E8DF0708   1     L8        gr6=<s95:d1800:l8>(gr31,1800)
    0| 000600 ld       E8FF06A0   1     L8        gr7=<s95:d1696:l8>(gr31,1696)
  102| 000604 cmpdi    2C200000   1     C8        cr0=gr0,0
  102| 000608 bc       418200B8   1     BT        CL.16,cr0,0x4/eq,taken=50%(0,0)
  103| 00060C ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  103| 000610 ld       E97F0840   1     L8        gr11=<s95:d2112:l8>(gr31,2112)
  103| 000614 ld       EBFF08A8   1     L8        gr31=<s95:d2216:l8>(gr31,2216)
  103| 000618 ld       E81D0410   1     L8        gr0=<s39:d1040:l8>(gr29,1040)
  103| 00061C std      F9610078   1     ST8       #MX_TEMP1(gr1,120)=gr11
  103| 000620 std      FBE10088   1     ST8       #MX_TEMP1(gr1,136)=gr31
  103| 000624 ld       E99D0820   1     L8        gr12=<s39:d2080:l8>(gr29,2080)
  103| 000628 std      F8010070   1     ST8       #MX_TEMP1(gr1,112)=gr0
  103| 00062C std      F9810080   1     ST8       #MX_TEMP1(gr1,128)=gr12
  103| 000630 bl       48000001   1     CALL      advx1,12,w3dd",gr3,d",gr4,w3de",gr5,w3dg",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,er",w3dh",abun",w4da",advx1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  103| 000634 ori      60000000   1
  107|                              CL.17:
    0| 000638 ld       E8620000   1     L8        gr3=.&&N&&bndry(gr2,0)
  157| 00063C ld       E9810350   1     L8        gr12=#stack(gr1,848)
  157| 000640 lfd      CBE10338   1     LFL       fp31=#stack(gr1,824)
  157| 000644 lfd      CBC10330   1     LFL       fp30=#stack(gr1,816)
  157| 000648 lfd      CBA10328   1     LFL       fp29=#stack(gr1,808)
  157| 00064C lfd      CB810320   1     LFL       fp28=#stack(gr1,800)
  115| 000650 stw      93C30328   1     ST4Z      bvstat[](gr3,808)=gr30
  115| 000654 stw      93C3032C   1     ST4Z      bvstat[](gr3,812)=gr30
  115| 000658 stw      93C30330   1     ST4Z      bvstat[](gr3,816)=gr30
  115| 00065C stw      93C30334   1     ST4Z      bvstat[](gr3,820)=gr30
  115| 000660 stw      93C30338   1     ST4Z      bvstat[](gr3,824)=gr30
  115| 000664 addi     38630324   1     AI        gr3=gr3,804
  157| 000668 addi     38210340   1     AI        gr1=gr1,832
  157| 00066C mtspr    7D8803A6   1     LLR       lr=gr12
  115| 000670 stwu     97C30018   1     ST4U      gr3,bvstat[](gr3,24)=gr30
  157| 000674 ld       E9C1FF50   1     L8        gr14=#stack(gr1,-176)
  157| 000678 ld       E9E1FF58   1     L8        gr15=#stack(gr1,-168)
  157| 00067C ld       EA01FF60   1     L8        gr16=#stack(gr1,-160)
  157| 000680 ld       EA21FF68   1     L8        gr17=#stack(gr1,-152)
  157| 000684 ld       EA41FF70   1     L8        gr18=#stack(gr1,-144)
  157| 000688 ld       EA61FF78   1     L8        gr19=#stack(gr1,-136)
  157| 00068C ld       EA81FF80   1     L8        gr20=#stack(gr1,-128)
  157| 000690 ld       EAA1FF88   1     L8        gr21=#stack(gr1,-120)
  157| 000694 ld       EAC1FF90   1     L8        gr22=#stack(gr1,-112)
  157| 000698 ld       EAE1FF98   1     L8        gr23=#stack(gr1,-104)
  157| 00069C ld       EB01FFA0   1     L8        gr24=#stack(gr1,-96)
  157| 0006A0 ld       EB21FFA8   1     L8        gr25=#stack(gr1,-88)
  157| 0006A4 ld       EB41FFB0   1     L8        gr26=#stack(gr1,-80)
  157| 0006A8 ld       EB61FFB8   1     L8        gr27=#stack(gr1,-72)
  157| 0006AC ld       EB81FFC0   1     L8        gr28=#stack(gr1,-64)
  157| 0006B0 ld       EBA1FFC8   1     L8        gr29=#stack(gr1,-56)
  157| 0006B4 ld       EBC1FFD0   1     L8        gr30=#stack(gr1,-48)
  157| 0006B8 ld       EBE1FFD8   1     L8        gr31=#stack(gr1,-40)
  157| 0006BC bclr     4E800020   1     BA        lr
  105|                              CL.16:
  106| 0006C0 bl       48000001   1     CALL      advx1,8,w3dd",gr3,d",gr4,w3de",gr5,w3dg",gr6,w3df",gr7,w3da",gr8,w3db",gr9,w3dc",gr10,advx1",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  106| 0006C4 ori      60000000   1
    0| 0006C8 b        4BFFFF70   1     B         CL.17,-1
  122|                              CL.2:
  123| 0006CC lwz      80030018   1     L4Z       gr0=<s177:d24:l4>(gr3,24)
    0| 0006D0 addi     38800000   1     LI        gr4=0
  123| 0006D4 cmpdi    2C200000   1     C8        cr0=gr0,0
  123| 0006D8 bc       41820394   1     BT        CL.23,cr0,0x4/eq,taken=50%(0,0)
  128| 0006DC ld       E8A20000   1     L8        gr5=.&&N&&grid(gr2,0)
  128| 0006E0 lwa      E8050016   1     L4A       gr0=<s183:d20:l4>(gr5,20)
  128| 0006E4 lwa      E8C50012   1     L4A       gr6=<s183:d16:l4>(gr5,16)
  128| 0006E8 subf     7C660050   1     S         gr3=gr0,gr6
  128| 0006EC addi     38030001   1     AI        gr0=gr3,1
  128| 0006F0 sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
  128| 0006F4 cmpdi    2F200000   1     C8        cr6=gr0,0
  128| 0006F8 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  128| 0006FC rldicr   78631764   1     SLL8      gr3=gr3,2
  128| 000700 subf     7CE30051   1     S_R       gr7,cr0=gr0,gr3
  128| 000704 crand    4C390A02   1     CR_N      cr0=cr[60],0x2/gt,0x2/gt,0x2/gt,cr0
  128| 000708 bc       40810104   1     BF        CL.109,cr0,0x2/gt,taken=50%(0,0)
  131| 00070C ld       EBA20000   1     L8        gr29=.&&N&field(gr2,0)
  131| 000710 ld       EB820000   1     L8        gr28=.&&N&scratch(gr2,0)
  129| 000714 lwa      EAA5000A   1     L4A       gr21=<s183:d8:l4>(gr5,8)
  129| 000718 lwa      EB65000E   1     L4A       gr27=<s183:d12:l4>(gr5,12)
  130| 00071C lwa      E9450002   1     L4A       gr10=<s183:d0:l4>(gr5,0)
  130| 000720 lwa      EB450006   1     L4A       gr26=<s183:d4:l4>(gr5,4)
  131| 000724 ld       E97D0440   1     L8        gr11=<s39:d1088:l8>(gr29,1088)
  131| 000728 ld       E99D0458   1     L8        gr12=<s39:d1112:l8>(gr29,1112)
  131| 00072C ld       E91D0470   1     L8        gr8=<s39:d1136:l8>(gr29,1136)
  131| 000730 ld       EBFC0870   1     L8        gr31=<s95:d2160:l8>(gr28,2160)
  131| 000734 ld       EBDC0888   1     L8        gr30=<s95:d2184:l8>(gr28,2184)
  131| 000738 ld       E93C08A0   1     L8        gr9=<s95:d2208:l8>(gr28,2208)
  131| 00073C ld       EA7D0410   1     L8        gr19=<s39:d1040:l8>(gr29,1040)
  131| 000740 ld       EA5C0840   1     L8        gr18=<s95:d2112:l8>(gr28,2112)
  129| 000744 subf     7F75D850   1     S         gr27=gr27,gr21
  131| 000748 ld       EBBD0428   1     L8        gr29=<s39:d1064:l8>(gr29,1064)
  131| 00074C ld       EB9C0858   1     L8        gr28=<s95:d2136:l8>(gr28,2136)
  129| 000750 addic.   377B0001   1     AI_R      gr27,cr0=gr27,1,ca"
    0| 000754 bc       40810888   1     BF        CL.315,cr0,0x2/gt,taken=50%(0,0)
    0| 000758 mulld    7F2951D2   1     M         gr25=gr9,gr10
    0| 00075C mulld    7F06F9D2   1     M         gr24=gr6,gr31
    0| 000760 mulld    7EF5F1D2   1     M         gr23=gr21,gr30
    0| 000764 mulld    7EC659D2   1     M         gr22=gr6,gr11
    0| 000768 mulld    7EB561D2   1     M         gr21=gr21,gr12
    0| 00076C mulld    7E8851D2   1     M         gr20=gr8,gr10
    0| 000770 subf     7E499050   1     S         gr18=gr18,gr9
    0| 000774 subf     7E689850   1     S         gr19=gr19,gr8
    0| 000778 add      7F9C9214   1     A         gr28=gr28,gr18
    0| 00077C add      7FBD9A14   1     A         gr29=gr29,gr19
    0| 000780 subfic   226A0001   1     SFI       gr19=1,gr10,ca"
    0| 000784 add      7F38CA14   1     A         gr25=gr24,gr25
    0| 000788 add      7F97E214   1     A         gr28=gr23,gr28
    0| 00078C add      7F15B214   1     A         gr24=gr21,gr22
    0| 000790 add      7FB4EA14   1     A         gr29=gr20,gr29
    0| 000794 subf     7D4AD050   1     S         gr10=gr26,gr10
    0| 000798 add      7F5A9A14   1     A         gr26=gr26,gr19
    0| 00079C add      7F99E214   1     A         gr28=gr25,gr28
    0| 0007A0 add      7FB8EA14   1     A         gr29=gr24,gr29
    0| 0007A4 addic.   354A0001   1     AI_R      gr10,cr0=gr10,1,ca"
  128|                              CL.104:
  129| 0007A8 addi     39400000   1     LI        gr10=0
    0| 0007AC bc       4081004C   1     BF        CL.108,cr0,0x2/gt,taken=20%(20,80)
    0| 0007B0 or       7FB7EB78   1     LR        gr23=gr29
    0| 0007B4 or       7F96E378   1     LR        gr22=gr28
  129|                              CL.105:
  131| 0007B8 or       7EF9BB78   1     LR        gr25=gr23
    0| 0007BC mtspr    7F4903A6   1     LCTR      ctr=gr26
  131| 0007C0 lfdux    7C1944EE   1     LFDU      fp0,gr25=er(gr25,gr8,0)
  131| 0007C4 or       7ED8B378   1     LR        gr24=gr22
    0| 0007C8 bc       42400018   1     BCF       ctr=CL.505,taken=0%(0,100)
    0| 0007CC ori      60210000   1     XNOP      
    0|                              CL.506:
  131| 0007D0 lfdux    7C3944EE   1     LFDU      fp1,gr25=er(gr25,gr8,0)
  131| 0007D4 stfdux   7C184DEE   1     STFDU     gr24,w3dh(gr24,gr9,0)=fp0
    0| 0007D8 fmr      FC000890   1     LRFL      fp0=fp1
    0| 0007DC bc       4200FFF4   1     BCT       ctr=CL.506,taken=100%(100,0)
    0|                              CL.505:
  133| 0007E0 addi     394A0001   1     AI        gr10=gr10,1
  131| 0007E4 stfdux   7C184DEE   1     STFDU     gr24,w3dh(gr24,gr9,0)=fp0
  133| 0007E8 cmpld    7CAAD840   1     CL8       cr1=gr10,gr27
    0| 0007EC add      7EECBA14   1     A         gr23=gr12,gr23
    0| 0007F0 add      7ED6F214   1     A         gr22=gr22,gr30
  133| 0007F4 bc       4184FFC4   1     BT        CL.105,cr1,0x8/llt,taken=80%(80,20)
  133|                              CL.108:
  134| 0007F8 addi     38840001   1     AI        gr4=gr4,1
    0| 0007FC add      7FABEA14   1     A         gr29=gr11,gr29
  134| 000800 cmpd     7CA72000   1     C8        cr1=gr7,gr4
    0| 000804 add      7F9CFA14   1     A         gr28=gr28,gr31
  134| 000808 bc       4185FFA0   1     BT        CL.104,cr1,0x2/gt,taken=80%(80,20)
  134|                              CL.109:
  128| 00080C cmpd     7CA03800   1     C8        cr1=gr0,gr7
  128| 000810 crand    4C392A02   1     CR_N      cr0=cr[61],0x2/gt,0x2/gt,0x2/gt,cr0
  128| 000814 bc       40810258   1     BF        CL.23,cr0,0x2/gt,taken=50%(0,0)
  131| 000818 ld       EB620000   1     L8        gr27=.&&N&field(gr2,0)
  131| 00081C ld       E9620000   1     L8        gr11=.&&N&scratch(gr2,0)
  129| 000820 lwa      E985000A   1     L4A       gr12=<s183:d8:l4>(gr5,8)
  129| 000824 lwa      EBE5000E   1     L4A       gr31=<s183:d12:l4>(gr5,12)
  130| 000828 lwa      E9450002   1     L4A       gr10=<s183:d0:l4>(gr5,0)
  134| 00082C addi     3B23FFFF   1     AI        gr25=gr3,-1
  131| 000830 ld       E91B0440   1     L8        gr8=<s39:d1088:l8>(gr27,1088)
  131| 000834 ld       E81B0458   1     L8        gr0=<s39:d1112:l8>(gr27,1112)
  131| 000838 ld       E87B0470   1     L8        gr3=<s39:d1136:l8>(gr27,1136)
  131| 00083C ld       E92B0870   1     L8        gr9=<s95:d2160:l8>(gr11,2160)
  131| 000840 ld       EBCB0888   1     L8        gr30=<s95:d2184:l8>(gr11,2184)
  131| 000844 ld       E88B08A0   1     L8        gr4=<s95:d2208:l8>(gr11,2208)
  129| 000848 subf     7FACF850   1     S         gr29=gr31,gr12
  131| 00084C ld       EBFB0410   1     L8        gr31=<s39:d1040:l8>(gr27,1040)
  129| 000850 addic.   37BD0001   1     AI_R      gr29,cr0=gr29,1,ca"
  131| 000854 ld       EB8B0840   1     L8        gr28=<s95:d2112:l8>(gr11,2112)
  134| 000858 sradi    7F391674   1     SRA8CA    gr25,ca=gr25,2
  128| 00085C addi     3B000000   1     LI        gr24=0
  131| 000860 ld       EB7B0428   1     L8        gr27=<s39:d1064:l8>(gr27,1064)
  128| 000864 std      FB0101B8   1     ST8       #SPILL34(gr1,440)=gr24
  131| 000868 ld       EB4B0858   1     L8        gr26=<s95:d2136:l8>(gr11,2136)
  130| 00086C lwa      E9650006   1     L4A       gr11=<s183:d4:l4>(gr5,4)
  134| 000870 addze    7CB90194   1     ADDE      gr5,ca=gr25,0,ca
    0| 000874 bc       408101F8   1     BF        CL.23,cr0,0x2/gt,taken=50%(0,0)
    0| 000878 mulld    7F2451D2   1     M         gr25=gr4,gr10
    0| 00087C mulld    7F0649D2   1     M         gr24=gr6,gr9
    0| 000880 mulld    7EECF1D2   1     M         gr23=gr12,gr30
    0| 000884 mulld    7CC641D2   1     M         gr6=gr6,gr8
    0| 000888 mulld    7D8C01D2   1     M         gr12=gr12,gr0
    0| 00088C mulld    7EC351D2   1     M         gr22=gr3,gr10
    0| 000890 subf     7FE3F850   1     S         gr31=gr31,gr3
    0| 000894 subf     7F84E050   1     S         gr28=gr28,gr4
    0| 000898 add      7F7BFA14   1     A         gr27=gr27,gr31
    0| 00089C add      7F9AE214   1     A         gr28=gr26,gr28
    0| 0008A0 mulld    7FE749D2   1     M         gr31=gr7,gr9
    0| 0008A4 mulld    7CE741D2   1     M         gr7=gr7,gr8
    0| 0008A8 add      7F38CA14   1     A         gr25=gr24,gr25
    0| 0008AC add      7F97E214   1     A         gr28=gr23,gr28
    0| 0008B0 add      7D866214   1     A         gr12=gr6,gr12
    0| 0008B4 add      7F76DA14   1     A         gr27=gr22,gr27
    0| 0008B8 subfic   234A0001   1     SFI       gr26=1,gr10,ca"
    0| 0008BC rldicr   79351764   1     SLL8      gr21=gr9,2
    0| 0008C0 add      7F99E214   1     A         gr28=gr25,gr28
    0| 0008C4 std      FAA101C0   1     ST8       #SPILL35(gr1,448)=gr21
    0| 0008C8 add      7F7B6214   1     A         gr27=gr27,gr12
    0| 0008CC add      7CCBD214   1     A         gr6=gr11,gr26
    0| 0008D0 rldicr   79141764   1     SLL8      gr20=gr8,2
    0| 0008D4 subf     7D8A5850   1     S         gr12=gr11,gr10
    0| 0008D8 std      FA8101C8   1     ST8       #SPILL36(gr1,456)=gr20
    0| 0008DC add      7F9CFA14   1     A         gr28=gr28,gr31
    0| 0008E0 subf     7FE9A850   1     S         gr31=gr21,gr9
    0| 0008E4 add      7F67DA14   1     A         gr27=gr7,gr27
    0| 0008E8 rldicr   792B0FA4   1     SLL8      gr11=gr9,1
    0| 0008EC rldicr   790A0FA4   1     SLL8      gr10=gr8,1
    0| 0008F0 subf     7CE8A050   1     S         gr7=gr20,gr8
    0| 0008F4 addic.   358C0001   1     AI_R      gr12,cr0=gr12,1,ca"
    0| 0008F8 rldicl   78DAF842   1     SRL8      gr26=gr6,1
    0| 0008FC add      7E69E214   1     A         gr19=gr9,gr28
    0| 000900 add      7D3CFA14   1     A         gr9=gr28,gr31
    0| 000904 std      FA6101D0   1     ST8       #SPILL37(gr1,464)=gr19
    0| 000908 std      F92101D8   1     ST8       #SPILL38(gr1,472)=gr9
    0| 00090C add      7EC8DA14   1     A         gr22=gr8,gr27
    0| 000910 addi     39050001   1     AI        gr8=gr5,1
    0| 000914 add      7F2BE214   1     A         gr25=gr11,gr28
    0| 000918 std      F90101E0   1     ST8       #SPILL39(gr1,480)=gr8
    0| 00091C add      7F0ADA14   1     A         gr24=gr10,gr27
    0| 000920 add      7EE7DA14   1     A         gr23=gr7,gr27
    0| 000924 mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000928 andi.    70C50001   1     RN4_R     gr5,cr0=gr6,0,0x1
    0| 00092C cmpdi    2FBA0000   1     C8        cr7=gr26,0
  128|                              CL.66:
  129| 000930 addi     38A00000   1     LI        gr5=0
    0| 000934 bc       408500E8   1     BF        CL.67,cr1,0x2/gt,taken=20%(20,80)
    0| 000938 or       7F75DB78   1     LR        gr21=gr27
    0| 00093C or       7EF4BB78   1     LR        gr20=gr23
    0| 000940 or       7ED3B378   1     LR        gr19=gr22
    0| 000944 or       7F12C378   1     LR        gr18=gr24
    0| 000948 or       7F91E378   1     LR        gr17=gr28
    0| 00094C ld       EA0101D8   1     L8        gr16=#SPILL38(gr1,472)
    0| 000950 or       7F2FCB78   1     LR        gr15=gr25
    0| 000954 ld       E9C101D0   1     L8        gr14=#SPILL37(gr1,464)
  129|                              CL.68:
  131| 000958 or       7EA6AB78   1     LR        gr6=gr21
  131| 00095C or       7E679B78   1     LR        gr7=gr19
  131| 000960 or       7E489378   1     LR        gr8=gr18
  131| 000964 or       7E89A378   1     LR        gr9=gr20
  131| 000968 or       7E2A8B78   1     LR        gr10=gr17
  131| 00096C or       7DCB7378   1     LR        gr11=gr14
  131| 000970 or       7DEC7B78   1     LR        gr12=gr15
  131| 000974 or       7E1F8378   1     LR        gr31=gr16
    0| 000978 mtspr    7F4903A6   1     LCTR      ctr=gr26
    0| 00097C bc       41820030   1     BT        CL.489,cr0,0x4/eq,taken=50%(0,0)
  131| 000980 lfdux    7C061CEE   1     LFDU      fp0,gr6=er(gr6,gr3,0)
  131| 000984 lfdux    7C271CEE   1     LFDU      fp1,gr7=er(gr7,gr3,0)
  131| 000988 lfdux    7C481CEE   1     LFDU      fp2,gr8=er(gr8,gr3,0)
  131| 00098C lfdux    7C691CEE   1     LFDU      fp3,gr9=er(gr9,gr3,0)
  131| 000990 stfdux   7C0A25EE   1     STFDU     gr10,w3dh(gr10,gr4,0)=fp0
  131| 000994 stfdux   7C2B25EE   1     STFDU     gr11,w3dh(gr11,gr4,0)=fp1
  131| 000998 stfdux   7C4C25EE   1     STFDU     gr12,w3dh(gr12,gr4,0)=fp2
  131| 00099C stfdux   7C7F25EE   1     STFDU     gr31,w3dh(gr31,gr4,0)=fp3
    0| 0009A0 bc       419E0050   1     BT        CL.425,cr7,0x4/eq,taken=20%(20,80)
    0| 0009A4 ori      60210000   1     XNOP      
    0| 0009A8 ori      60210000   1     XNOP      
    0|                              CL.489:
  131| 0009AC lfdux    7C061CEE   1     LFDU      fp0,gr6=er(gr6,gr3,0)
  131| 0009B0 lfdux    7C271CEE   1     LFDU      fp1,gr7=er(gr7,gr3,0)
  131| 0009B4 lfdux    7C481CEE   1     LFDU      fp2,gr8=er(gr8,gr3,0)
  131| 0009B8 lfdux    7C691CEE   1     LFDU      fp3,gr9=er(gr9,gr3,0)
  131| 0009BC lfdux    7C861CEE   1     LFDU      fp4,gr6=er(gr6,gr3,0)
  131| 0009C0 lfdux    7CA71CEE   1     LFDU      fp5,gr7=er(gr7,gr3,0)
  131| 0009C4 lfdux    7CC81CEE   1     LFDU      fp6,gr8=er(gr8,gr3,0)
  131| 0009C8 lfdux    7CE91CEE   1     LFDU      fp7,gr9=er(gr9,gr3,0)
  131| 0009CC stfdux   7C0A25EE   1     STFDU     gr10,w3dh(gr10,gr4,0)=fp0
  131| 0009D0 stfdux   7C2B25EE   1     STFDU     gr11,w3dh(gr11,gr4,0)=fp1
  131| 0009D4 stfdux   7C4C25EE   1     STFDU     gr12,w3dh(gr12,gr4,0)=fp2
  131| 0009D8 stfdux   7C7F25EE   1     STFDU     gr31,w3dh(gr31,gr4,0)=fp3
  131| 0009DC stfdux   7C8A25EE   1     STFDU     gr10,w3dh(gr10,gr4,0)=fp4
  131| 0009E0 stfdux   7CAB25EE   1     STFDU     gr11,w3dh(gr11,gr4,0)=fp5
  131| 0009E4 stfdux   7CCC25EE   1     STFDU     gr12,w3dh(gr12,gr4,0)=fp6
  131| 0009E8 stfdux   7CFF25EE   1     STFDU     gr31,w3dh(gr31,gr4,0)=fp7
    0| 0009EC bc       4200FFC0   1     BCT       ctr=CL.489,taken=100%(100,0)
    0|                              CL.425:
  133| 0009F0 addi     38A50001   1     AI        gr5=gr5,1
    0| 0009F4 add      7EA0AA14   1     A         gr21=gr0,gr21
  133| 0009F8 cmpld    7F25E840   1     CL8       cr6=gr5,gr29
    0| 0009FC add      7E80A214   1     A         gr20=gr0,gr20
    0| 000A00 add      7E609A14   1     A         gr19=gr0,gr19
    0| 000A04 add      7E409214   1     A         gr18=gr0,gr18
    0| 000A08 add      7E31F214   1     A         gr17=gr17,gr30
    0| 000A0C add      7E10F214   1     A         gr16=gr16,gr30
    0| 000A10 add      7DEFF214   1     A         gr15=gr15,gr30
    0| 000A14 add      7DCEF214   1     A         gr14=gr14,gr30
  133| 000A18 bc       4198FF40   1     BT        CL.68,cr6,0x8/llt,taken=80%(80,20)
  133|                              CL.67:
  134| 000A1C ld       E8A101B8   1     L8        gr5=#SPILL34(gr1,440)
    0| 000A20 ld       E8C101C0   1     L8        gr6=#SPILL35(gr1,448)
    0| 000A24 ld       E8E101D0   1     L8        gr7=#SPILL37(gr1,464)
  134| 000A28 ld       E90101E0   1     L8        gr8=#SPILL39(gr1,480)
    0| 000A2C ld       E92101D8   1     L8        gr9=#SPILL38(gr1,472)
    0| 000A30 ld       E94101C8   1     L8        gr10=#SPILL36(gr1,456)
  134| 000A34 addi     38A50001   1     AI        gr5=gr5,1
    0| 000A38 add      7F26CA14   1     A         gr25=gr6,gr25
  134| 000A3C std      F8A101B8   1     ST8       #SPILL34(gr1,440)=gr5
    0| 000A40 add      7CE63A14   1     A         gr7=gr6,gr7
  134| 000A44 cmpld    7F254040   1     CL8       cr6=gr5,gr8
    0| 000A48 add      7D264A14   1     A         gr9=gr6,gr9
    0| 000A4C std      F8E101D0   1     ST8       #SPILL37(gr1,464)=gr7
    0| 000A50 std      F92101D8   1     ST8       #SPILL38(gr1,472)=gr9
    0| 000A54 add      7F86E214   1     A         gr28=gr6,gr28
    0| 000A58 add      7F0AC214   1     A         gr24=gr10,gr24
    0| 000A5C add      7EEABA14   1     A         gr23=gr10,gr23
    0| 000A60 add      7ECAB214   1     A         gr22=gr10,gr22
    0| 000A64 add      7F6ADA14   1     A         gr27=gr10,gr27
  134| 000A68 bc       4198FEC8   1     BT        CL.66,cr6,0x8/llt,taken=80%(80,20)
  135|                              CL.23:
  139| 000A6C ld       E8620000   1     L8        gr3=.&&N&&config(gr2,0)
  139| 000A70 lwa      E8030026   1     L4A       gr0=<s177:d36:l4>(gr3,36)
  139| 000A74 cmpwi    2C000001   1     C4        cr0=gr0,1
  139| 000A78 bc       408104C4   1     BF        CL.36,cr0,0x2/gt,taken=50%(0,0)
   87| 000A7C sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
   87| 000A80 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  140| 000A84 rldicr   78631764   1     SLL8      gr3=gr3,2
  140| 000A88 subf     7D830051   1     S_R       gr12,cr0=gr0,gr3
  140| 000A8C bc       40810154   1     BF        CL.97,cr0,0x2/gt,taken=50%(0,0)
  141| 000A90 ld       EB420000   1     L8        gr26=.&&N&&grid(gr2,0)
  144| 000A94 ld       EB220000   1     L8        gr25=.&&N&field(gr2,0)
  144| 000A98 ld       EB020000   1     L8        gr24=.&&N&scratch(gr2,0)
  140| 000A9C addi     38800000   1     LI        gr4=0
  141| 000AA0 lwa      EA7A0012   1     L4A       gr19=<s183:d16:l4>(gr26,16)
  141| 000AA4 lwa      EBDA0016   1     L4A       gr30=<s183:d20:l4>(gr26,20)
  142| 000AA8 lwa      E8FA000A   1     L4A       gr7=<s183:d8:l4>(gr26,8)
  143| 000AAC lwa      E91A0002   1     L4A       gr8=<s183:d0:l4>(gr26,0)
  144| 000AB0 ld       E9390868   1     L8        gr9=<s39:d2152:l8>(gr25,2152)
  144| 000AB4 ld       E9590880   1     L8        gr10=<s39:d2176:l8>(gr25,2176)
  144| 000AB8 ld       E8B90898   1     L8        gr5=<s39:d2200:l8>(gr25,2200)
  144| 000ABC ld       E97808F0   1     L8        gr11=<s95:d2288:l8>(gr24,2288)
  144| 000AC0 ld       EBF80908   1     L8        gr31=<s95:d2312:l8>(gr24,2312)
  144| 000AC4 ld       E8D80920   1     L8        gr6=<s95:d2336:l8>(gr24,2336)
  144| 000AC8 ld       EA190820   1     L8        gr16=<s39:d2080:l8>(gr25,2080)
  144| 000ACC ld       EA3808A8   1     L8        gr17=<s95:d2216:l8>(gr24,2216)
  141| 000AD0 subf     7F93F050   1     S         gr28=gr30,gr19
  144| 000AD4 ld       EBD90838   1     L8        gr30=<s39:d2104:l8>(gr25,2104)
  144| 000AD8 ld       EBB808C0   1     L8        gr29=<s95:d2240:l8>(gr24,2240)
  141| 000ADC addic.   379C0001   1     AI_R      gr28,cr0=gr28,1,ca"
  142| 000AE0 lwa      EB7A000E   1     L4A       gr27=<s183:d12:l4>(gr26,12)
  143| 000AE4 lwa      EB5A0006   1     L4A       gr26=<s183:d4:l4>(gr26,4)
  144| 000AE8 ld       EB390850   1     L8        gr25=<s39:d2128:l8>(gr25,2128)
  144| 000AEC ld       EB1808D8   1     L8        gr24=<s95:d2264:l8>(gr24,2264)
    0| 000AF0 bc       408104D8   1     BF        CL.327,cr0,0x2/gt,taken=50%(0,0)
    0| 000AF4 mulld    7EE541D2   1     M         gr23=gr5,gr8
    0| 000AF8 mulld    7EC751D2   1     M         gr22=gr7,gr10
    0| 000AFC mulld    7EA999D2   1     M         gr21=gr9,gr19
    0| 000B00 mulld    7E8641D2   1     M         gr20=gr6,gr8
    0| 000B04 mulld    7E7359D2   1     M         gr19=gr19,gr11
    0| 000B08 mulld    7E47F9D2   1     M         gr18=gr7,gr31
    0| 000B0C subf     7E058050   1     S         gr16=gr16,gr5
    0| 000B10 subf     7E268850   1     S         gr17=gr17,gr6
    0| 000B14 add      7FDE8214   1     A         gr30=gr30,gr16
    0| 000B18 add      7FBD8A14   1     A         gr29=gr29,gr17
    0| 000B1C add      7EF6BA14   1     A         gr23=gr22,gr23
    0| 000B20 add      7FD5F214   1     A         gr30=gr21,gr30
    0| 000B24 add      7ED3A214   1     A         gr22=gr19,gr20
    0| 000B28 add      7FB2EA14   1     A         gr29=gr18,gr29
    0| 000B2C subf     7CE7D850   1     S         gr7=gr27,gr7
    0| 000B30 subfic   23680001   1     SFI       gr27=1,gr8,ca"
    0| 000B34 add      7FD7F214   1     A         gr30=gr23,gr30
    0| 000B38 add      7FB6EA14   1     A         gr29=gr22,gr29
    0| 000B3C addic.   34E70001   1     AI_R      gr7,cr0=gr7,1,ca"
    0| 000B40 subf     7D08D050   1     S         gr8=gr26,gr8
    0| 000B44 add      7E5ADA14   1     A         gr18=gr26,gr27
    0| 000B48 add      7F59F214   1     A         gr26=gr25,gr30
    0| 000B4C add      7EF8EA14   1     A         gr23=gr24,gr29
    0| 000B50 mcrf     4F000000   1     LRCR      cr6=cr0
    0| 000B54 addic.   35080001   1     AI_R      gr8,cr0=gr8,1,ca"
  140|                              CL.90:
  141| 000B58 addi     39000000   1     LI        gr8=0
    0| 000B5C bc       40990070   1     BF        CL.96,cr6,0x2/gt,taken=20%(20,80)
    0| 000B60 or       7EF6BB78   1     LR        gr22=gr23
    0| 000B64 or       7F55D378   1     LR        gr21=gr26
  141|                              CL.91:
  142| 000B68 addi     3BC00000   1     LI        gr30=0
    0| 000B6C bc       4081004C   1     BF        CL.95,cr0,0x2/gt,taken=20%(20,80)
    0| 000B70 or       7EB4AB78   1     LR        gr20=gr21
    0| 000B74 or       7ED3B378   1     LR        gr19=gr22
  142|                              CL.92:
  144| 000B78 or       7E9DA378   1     LR        gr29=gr20
    0| 000B7C mtspr    7E4903A6   1     LCTR      ctr=gr18
  144| 000B80 lfdux    7C1D2CEE   1     LFDU      fp0,gr29=abun(gr29,gr5,0)
  144| 000B84 or       7E7B9B78   1     LR        gr27=gr19
    0| 000B88 bc       42400018   1     BCF       ctr=CL.508,taken=0%(0,100)
    0| 000B8C ori      60210000   1     XNOP      
    0|                              CL.509:
  144| 000B90 lfdux    7C3D2CEE   1     LFDU      fp1,gr29=abun(gr29,gr5,0)
  144| 000B94 stfdux   7C1B35EE   1     STFDU     gr27,w4da(gr27,gr6,0)=fp0
    0| 000B98 fmr      FC000890   1     LRFL      fp0=fp1
    0| 000B9C bc       4200FFF4   1     BCT       ctr=CL.509,taken=100%(100,0)
    0|                              CL.508:
  146| 000BA0 addi     3BDE0001   1     AI        gr30=gr30,1
  144| 000BA4 stfdux   7C1B35EE   1     STFDU     gr27,w4da(gr27,gr6,0)=fp0
  146| 000BA8 cmpld    7CBE3840   1     CL8       cr1=gr30,gr7
    0| 000BAC add      7E8AA214   1     A         gr20=gr10,gr20
    0| 000BB0 add      7E73FA14   1     A         gr19=gr19,gr31
  146| 000BB4 bc       4184FFC4   1     BT        CL.92,cr1,0x8/llt,taken=80%(80,20)
  146|                              CL.95:
  147| 000BB8 addi     39080001   1     AI        gr8=gr8,1
    0| 000BBC add      7ECBB214   1     A         gr22=gr11,gr22
  147| 000BC0 cmpld    7CA8E040   1     CL8       cr1=gr8,gr28
    0| 000BC4 add      7EA9AA14   1     A         gr21=gr9,gr21
  147| 000BC8 bc       4184FFA0   1     BT        CL.91,cr1,0x8/llt,taken=80%(80,20)
  147|                              CL.96:
  148| 000BCC addi     38840001   1     AI        gr4=gr4,1
    0| 000BD0 add      7EF7C214   1     A         gr23=gr23,gr24
  148| 000BD4 cmpd     7CAC2000   1     C8        cr1=gr12,gr4
    0| 000BD8 add      7F59D214   1     A         gr26=gr25,gr26
  148| 000BDC bc       4185FF7C   1     BT        CL.90,cr1,0x2/gt,taken=80%(80,20)
  148|                              CL.97:
  140| 000BE0 cmpd     7C206000   1     C8        cr0=gr0,gr12
  140| 000BE4 bc       40810358   1     BF        CL.36,cr0,0x2/gt,taken=50%(0,0)
  144| 000BE8 ld       EBE20000   1     L8        gr31=.&&N&field(gr2,0)
  144| 000BEC ld       E9620000   1     L8        gr11=.&&N&scratch(gr2,0)
  141| 000BF0 ld       E9420000   1     L8        gr10=.&&N&&grid(gr2,0)
  148| 000BF4 addi     3803FFFF   1     AI        gr0=gr3,-1
  140| 000BF8 addi     3A400000   1     LI        gr18=0
  148| 000BFC sradi    7C051674   1     SRA8CA    gr5,ca=gr0,2
  140| 000C00 std      FA4101E8   1     ST8       #SPILL40(gr1,488)=gr18
  144| 000C04 ld       EA3F0868   1     L8        gr17=<s39:d2152:l8>(gr31,2152)
  144| 000C08 ld       E81F0880   1     L8        gr0=<s39:d2176:l8>(gr31,2176)
  141| 000C0C lwa      EAEA0012   1     L4A       gr23=<s183:d16:l4>(gr10,16)
  142| 000C10 lwa      E8CA000A   1     L4A       gr6=<s183:d8:l4>(gr10,8)
  143| 000C14 lwa      E92A0002   1     L4A       gr9=<s183:d0:l4>(gr10,0)
  144| 000C18 ld       E87F0898   1     L8        gr3=<s39:d2200:l8>(gr31,2200)
  144| 000C1C std      FA2101F0   1     ST8       #SPILL41(gr1,496)=gr17
  144| 000C20 ld       EA0B08F0   1     L8        gr16=<s95:d2288:l8>(gr11,2288)
  144| 000C24 ld       EBCB0908   1     L8        gr30=<s95:d2312:l8>(gr11,2312)
  144| 000C28 ld       E88B0920   1     L8        gr4=<s95:d2336:l8>(gr11,2336)
  144| 000C2C ld       EA9F0820   1     L8        gr20=<s39:d2080:l8>(gr31,2080)
  144| 000C30 ld       EAAB08A8   1     L8        gr21=<s95:d2216:l8>(gr11,2216)
  141| 000C34 lwa      EA6A0016   1     L4A       gr19=<s183:d20:l4>(gr10,20)
  144| 000C38 std      FA0101F8   1     ST8       #SPILL42(gr1,504)=gr16
  144| 000C3C ld       E91F0850   1     L8        gr8=<s39:d2128:l8>(gr31,2128)
  144| 000C40 ld       E8EB08D8   1     L8        gr7=<s95:d2264:l8>(gr11,2264)
  144| 000C44 ld       EADF0838   1     L8        gr22=<s39:d2104:l8>(gr31,2104)
  144| 000C48 ld       E96B08C0   1     L8        gr11=<s95:d2240:l8>(gr11,2240)
    0| 000C4C mulld    7FA349D2   1     M         gr29=gr3,gr9
    0| 000C50 mulld    7F8031D2   1     M         gr28=gr0,gr6
    0| 000C54 mulld    7F71B9D2   1     M         gr27=gr17,gr23
    0| 000C58 mulld    7F4449D2   1     M         gr26=gr4,gr9
    0| 000C5C mulld    7F30B9D2   1     M         gr25=gr16,gr23
    0| 000C60 mulld    7F06F1D2   1     M         gr24=gr6,gr30
    0| 000C64 subf     7E83A050   1     S         gr20=gr20,gr3
    0| 000C68 subf     7EA4A850   1     S         gr21=gr21,gr4
  141| 000C6C subf     7EF79850   1     S         gr23=gr19,gr23
  148| 000C70 addze    7CA50194   1     ADDE      gr5,ca=gr5,0,ca
  142| 000C74 lwa      EBEA000E   1     L4A       gr31=<s183:d12:l4>(gr10,12)
    0| 000C78 add      7ED6A214   1     A         gr22=gr22,gr20
    0| 000C7C add      7EABAA14   1     A         gr21=gr11,gr21
  141| 000C80 addic.   35F70001   1     AI_R      gr15,cr0=gr23,1,ca"
  143| 000C84 lwa      E94A0006   1     L4A       gr10=<s183:d4:l4>(gr10,4)
  141| 000C88 std      F9E10200   1     ST8       #SPILL43(gr1,512)=gr15
    0| 000C8C mulld    7D6861D2   1     M         gr11=gr8,gr12
    0| 000C90 mulld    7D8C39D2   1     M         gr12=gr12,gr7
    0| 000C94 add      7FBCEA14   1     A         gr29=gr28,gr29
    0| 000C98 bc       408102A4   1     BF        CL.36,cr0,0x2/gt,taken=20%(20,80)
    0| 000C9C add      7F7BB214   1     A         gr27=gr27,gr22
    0| 000CA0 add      7F59D214   1     A         gr26=gr25,gr26
    0| 000CA4 add      7F35C214   1     A         gr25=gr21,gr24
    0| 000CA8 subfic   23890001   1     SFI       gr28=1,gr9,ca"
    0| 000CAC subf     7FE6F850   1     S         gr31=gr31,gr6
    0| 000CB0 rldicr   79181764   1     SLL8      gr24=gr8,2
    0| 000CB4 add      7F7BEA14   1     A         gr27=gr27,gr29
    0| 000CB8 std      FB010208   1     ST8       #SPILL44(gr1,520)=gr24
    0| 000CBC rldicr   78F71764   1     SLL8      gr23=gr7,2
    0| 000CC0 add      7F39D214   1     A         gr25=gr25,gr26
    0| 000CC4 std      FAE10210   1     ST8       #SPILL45(gr1,528)=gr23
    0| 000CC8 add      7CCAE214   1     A         gr6=gr10,gr28
    0| 000CCC addic.   37BF0001   1     AI_R      gr29,cr0=gr31,1,ca"
    0| 000CD0 subf     7F895050   1     S         gr28=gr10,gr9
    0| 000CD4 add      7FEBDA14   1     A         gr31=gr11,gr27
    0| 000CD8 rldicr   791A0FA4   1     SLL8      gr26=gr8,1
    0| 000CDC subf     7F68C050   1     S         gr27=gr24,gr8
    0| 000CE0 add      7D2CCA14   1     A         gr9=gr12,gr25
    0| 000CE4 rldicr   78EB0FA4   1     SLL8      gr11=gr7,1
    0| 000CE8 subf     7D47B850   1     S         gr10=gr23,gr7
    0| 000CEC mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000CF0 addic.   359C0001   1     AI_R      gr12,cr0=gr28,1,ca"
    0| 000CF4 rldicl   78DCF842   1     SRL8      gr28=gr6,1
    0| 000CF8 add      7EC8FA14   1     A         gr22=gr8,gr31
    0| 000CFC add      7D1AFA14   1     A         gr8=gr26,gr31
    0| 000D00 std      FAC10218   1     ST8       #SPILL46(gr1,536)=gr22
    0| 000D04 std      F9010220   1     ST8       #SPILL47(gr1,544)=gr8
    0| 000D08 add      7EBBFA14   1     A         gr21=gr27,gr31
    0| 000D0C add      7E98FA14   1     A         gr20=gr24,gr31
    0| 000D10 std      FAA10228   1     ST8       #SPILL48(gr1,552)=gr21
    0| 000D14 std      FA810230   1     ST8       #SPILL49(gr1,560)=gr20
    0| 000D18 add      7E674A14   1     A         gr19=gr7,gr9
    0| 000D1C add      7CE95A14   1     A         gr7=gr9,gr11
    0| 000D20 std      FA610238   1     ST8       #SPILL50(gr1,568)=gr19
    0| 000D24 std      F8E10240   1     ST8       #SPILL51(gr1,576)=gr7
    0| 000D28 add      7DC95214   1     A         gr14=gr9,gr10
    0| 000D2C add      7F29BA14   1     A         gr25=gr9,gr23
    0| 000D30 std      F9C10248   1     ST8       #SPILL52(gr1,584)=gr14
    0| 000D34 std      FB210250   1     ST8       #SPILL53(gr1,592)=gr25
    0| 000D38 addi     3BE50001   1     AI        gr31=gr5,1
    0| 000D3C mcrf     4F000000   1     LRCR      cr6=cr0
    0| 000D40 std      FBE10258   1     ST8       #SPILL54(gr1,600)=gr31
    0| 000D44 andi.    70C50001   1     RN4_R     gr5,cr0=gr6,0,0x1
    0| 000D48 cmpdi    2EBC0000   1     C8        cr5=gr28,0
  140|                              CL.72:
  141| 000D4C addi     38A00000   1     LI        gr5=0
  141| 000D50 std      F8A10260   1     ST8       #SPILL55(gr1,608)=gr5
    0| 000D54 bc       40850168   1     BF        CL.73,cr1,0x2/gt,taken=20%(20,80)
    0| 000D58 ld       E8C10240   1     L8        gr6=#SPILL51(gr1,576)
    0| 000D5C ld       E8E10248   1     L8        gr7=#SPILL52(gr1,584)
    0| 000D60 ld       EB610250   1     L8        gr27=#SPILL53(gr1,592)
    0| 000D64 ld       EB410238   1     L8        gr26=#SPILL50(gr1,568)
    0| 000D68 ld       EB210220   1     L8        gr25=#SPILL47(gr1,544)
    0| 000D6C ld       EB010228   1     L8        gr24=#SPILL48(gr1,552)
    0| 000D70 std      F8C10268   1     ST8       #SPILL56(gr1,616)=gr6
    0| 000D74 std      F8E10270   1     ST8       #SPILL57(gr1,624)=gr7
    0| 000D78 ld       EAE10230   1     L8        gr23=#SPILL49(gr1,560)
    0| 000D7C ld       EAC10218   1     L8        gr22=#SPILL46(gr1,536)
  141|                              CL.74:
  142| 000D80 addi     38A00000   1     LI        gr5=0
    0| 000D84 bc       409900E8   1     BF        CL.75,cr6,0x2/gt,taken=20%(20,80)
    0| 000D88 or       7F15C378   1     LR        gr21=gr24
    0| 000D8C or       7EF4BB78   1     LR        gr20=gr23
    0| 000D90 or       7ED3B378   1     LR        gr19=gr22
    0| 000D94 or       7F32CB78   1     LR        gr18=gr25
    0| 000D98 ld       EA210270   1     L8        gr17=#SPILL57(gr1,624)
    0| 000D9C or       7F70DB78   1     LR        gr16=gr27
    0| 000DA0 or       7F4FD378   1     LR        gr15=gr26
    0| 000DA4 ld       E9C10268   1     L8        gr14=#SPILL56(gr1,616)
  142|                              CL.76:
  144| 000DA8 or       7E669B78   1     LR        gr6=gr19
  144| 000DAC or       7E479378   1     LR        gr7=gr18
  144| 000DB0 or       7EA8AB78   1     LR        gr8=gr21
  144| 000DB4 or       7E89A378   1     LR        gr9=gr20
  144| 000DB8 or       7DEA7B78   1     LR        gr10=gr15
  144| 000DBC or       7DCB7378   1     LR        gr11=gr14
  144| 000DC0 or       7E2C8B78   1     LR        gr12=gr17
  144| 000DC4 or       7E1F8378   1     LR        gr31=gr16
    0| 000DC8 mtspr    7F8903A6   1     LCTR      ctr=gr28
    0| 000DCC bc       41820030   1     BT        CL.498,cr0,0x4/eq,taken=50%(0,0)
  144| 000DD0 lfdux    7C061CEE   1     LFDU      fp0,gr6=abun(gr6,gr3,0)
  144| 000DD4 lfdux    7C271CEE   1     LFDU      fp1,gr7=abun(gr7,gr3,0)
  144| 000DD8 lfdux    7C481CEE   1     LFDU      fp2,gr8=abun(gr8,gr3,0)
  144| 000DDC lfdux    7C691CEE   1     LFDU      fp3,gr9=abun(gr9,gr3,0)
  144| 000DE0 stfdux   7C0A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp0
  144| 000DE4 stfdux   7C2B25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp1
  144| 000DE8 stfdux   7C4C25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp2
  144| 000DEC stfdux   7C7F25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp3
    0| 000DF0 bc       41960050   1     BT        CL.431,cr5,0x4/eq,taken=20%(20,80)
    0| 000DF4 ori      60210000   1     XNOP      
    0| 000DF8 ori      60210000   1     XNOP      
    0|                              CL.498:
  144| 000DFC lfdux    7C061CEE   1     LFDU      fp0,gr6=abun(gr6,gr3,0)
  144| 000E00 lfdux    7C271CEE   1     LFDU      fp1,gr7=abun(gr7,gr3,0)
  144| 000E04 lfdux    7C481CEE   1     LFDU      fp2,gr8=abun(gr8,gr3,0)
  144| 000E08 lfdux    7C691CEE   1     LFDU      fp3,gr9=abun(gr9,gr3,0)
  144| 000E0C lfdux    7C861CEE   1     LFDU      fp4,gr6=abun(gr6,gr3,0)
  144| 000E10 lfdux    7CA71CEE   1     LFDU      fp5,gr7=abun(gr7,gr3,0)
  144| 000E14 lfdux    7CC81CEE   1     LFDU      fp6,gr8=abun(gr8,gr3,0)
  144| 000E18 lfdux    7CE91CEE   1     LFDU      fp7,gr9=abun(gr9,gr3,0)
  144| 000E1C stfdux   7C0A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp0
  144| 000E20 stfdux   7C2B25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp1
  144| 000E24 stfdux   7C4C25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp2
  144| 000E28 stfdux   7C7F25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp3
  144| 000E2C stfdux   7C8A25EE   1     STFDU     gr10,w4da(gr10,gr4,0)=fp4
  144| 000E30 stfdux   7CAB25EE   1     STFDU     gr11,w4da(gr11,gr4,0)=fp5
  144| 000E34 stfdux   7CCC25EE   1     STFDU     gr12,w4da(gr12,gr4,0)=fp6
  144| 000E38 stfdux   7CFF25EE   1     STFDU     gr31,w4da(gr31,gr4,0)=fp7
    0| 000E3C bc       4200FFC0   1     BCT       ctr=CL.498,taken=100%(100,0)
    0|                              CL.431:
  146| 000E40 addi     38A50001   1     AI        gr5=gr5,1
    0| 000E44 add      7EA0AA14   1     A         gr21=gr0,gr21
  146| 000E48 cmpld    7FA5E840   1     CL8       cr7=gr5,gr29
    0| 000E4C add      7E80A214   1     A         gr20=gr0,gr20
    0| 000E50 add      7E609A14   1     A         gr19=gr0,gr19
    0| 000E54 add      7E409214   1     A         gr18=gr0,gr18
    0| 000E58 add      7E31F214   1     A         gr17=gr17,gr30
    0| 000E5C add      7E10F214   1     A         gr16=gr16,gr30
    0| 000E60 add      7DEFF214   1     A         gr15=gr15,gr30
    0| 000E64 add      7DCEF214   1     A         gr14=gr14,gr30
  146| 000E68 bc       419CFF40   1     BT        CL.76,cr7,0x8/llt,taken=80%(80,20)
  146|                              CL.75:
  147| 000E6C ld       E8A10260   1     L8        gr5=#SPILL55(gr1,608)
    0| 000E70 ld       E8C101F8   1     L8        gr6=#SPILL42(gr1,504)
    0| 000E74 ld       E8E10268   1     L8        gr7=#SPILL56(gr1,616)
  147| 000E78 ld       E9010200   1     L8        gr8=#SPILL43(gr1,512)
    0| 000E7C ld       E9210270   1     L8        gr9=#SPILL57(gr1,624)
    0| 000E80 ld       E94101F0   1     L8        gr10=#SPILL41(gr1,496)
  147| 000E84 addi     38A50001   1     AI        gr5=gr5,1
    0| 000E88 add      7F66DA14   1     A         gr27=gr6,gr27
  147| 000E8C std      F8A10260   1     ST8       #SPILL55(gr1,608)=gr5
    0| 000E90 add      7CE63A14   1     A         gr7=gr6,gr7
  147| 000E94 cmpld    7FA54040   1     CL8       cr7=gr5,gr8
    0| 000E98 add      7D264A14   1     A         gr9=gr6,gr9
    0| 000E9C std      F8E10268   1     ST8       #SPILL56(gr1,616)=gr7
    0| 000EA0 std      F9210270   1     ST8       #SPILL57(gr1,624)=gr9
    0| 000EA4 add      7F46D214   1     A         gr26=gr6,gr26
    0| 000EA8 add      7F2ACA14   1     A         gr25=gr10,gr25
    0| 000EAC add      7F0AC214   1     A         gr24=gr10,gr24
    0| 000EB0 add      7EEABA14   1     A         gr23=gr10,gr23
    0| 000EB4 add      7ECAB214   1     A         gr22=gr10,gr22
  147| 000EB8 bc       419CFEC8   1     BT        CL.74,cr7,0x8/llt,taken=80%(80,20)
  147|                              CL.73:
  148| 000EBC ld       E8A101E8   1     L8        gr5=#SPILL40(gr1,488)
    0| 000EC0 ld       E8C10208   1     L8        gr6=#SPILL44(gr1,520)
    0| 000EC4 ld       E8E10218   1     L8        gr7=#SPILL46(gr1,536)
  148| 000EC8 ld       E9010258   1     L8        gr8=#SPILL54(gr1,600)
    0| 000ECC ld       E9210220   1     L8        gr9=#SPILL47(gr1,544)
    0| 000ED0 ld       E9410228   1     L8        gr10=#SPILL48(gr1,552)
    0| 000ED4 ld       E9610230   1     L8        gr11=#SPILL49(gr1,560)
    0| 000ED8 ld       E9810210   1     L8        gr12=#SPILL45(gr1,528)
    0| 000EDC ld       EBE10238   1     L8        gr31=#SPILL50(gr1,568)
    0| 000EE0 ld       EB610240   1     L8        gr27=#SPILL51(gr1,576)
    0| 000EE4 ld       EB410248   1     L8        gr26=#SPILL52(gr1,584)
    0| 000EE8 ld       EB210250   1     L8        gr25=#SPILL53(gr1,592)
  148| 000EEC addi     38A50001   1     AI        gr5=gr5,1
    0| 000EF0 add      7CE63A14   1     A         gr7=gr6,gr7
  148| 000EF4 std      F8A101E8   1     ST8       #SPILL40(gr1,488)=gr5
    0| 000EF8 std      F8E10218   1     ST8       #SPILL46(gr1,536)=gr7
  148| 000EFC cmpld    7FA54040   1     CL8       cr7=gr5,gr8
    0| 000F00 add      7D264A14   1     A         gr9=gr6,gr9
    0| 000F04 add      7D465214   1     A         gr10=gr6,gr10
    0| 000F08 std      F9210220   1     ST8       #SPILL47(gr1,544)=gr9
    0| 000F0C std      F9410228   1     ST8       #SPILL48(gr1,552)=gr10
    0| 000F10 add      7D665A14   1     A         gr11=gr6,gr11
    0| 000F14 add      7FECFA14   1     A         gr31=gr12,gr31
    0| 000F18 std      F9610230   1     ST8       #SPILL49(gr1,560)=gr11
    0| 000F1C std      FBE10238   1     ST8       #SPILL50(gr1,568)=gr31
    0| 000F20 add      7F6CDA14   1     A         gr27=gr12,gr27
    0| 000F24 add      7F4CD214   1     A         gr26=gr12,gr26
    0| 000F28 std      FB610240   1     ST8       #SPILL51(gr1,576)=gr27
    0| 000F2C std      FB410248   1     ST8       #SPILL52(gr1,584)=gr26
    0| 000F30 add      7F2CCA14   1     A         gr25=gr12,gr25
    0| 000F34 std      FB210250   1     ST8       #SPILL53(gr1,592)=gr25
  148| 000F38 bc       419CFE14   1     BT        CL.72,cr7,0x8/llt,taken=80%(80,20)
  149|                              CL.36:
    0| 000F3C ld       E8620000   1     L8        gr3=.&&N&&bndry(gr2,0)
  152| 000F40 addi     38000000   1     LI        gr0=0
  157| 000F44 ld       E9C10290   1     L8        gr14=#stack(gr1,656)
  157| 000F48 ld       E9E10298   1     L8        gr15=#stack(gr1,664)
  157| 000F4C ld       EA0102A0   1     L8        gr16=#stack(gr1,672)
  157| 000F50 ld       EA2102A8   1     L8        gr17=#stack(gr1,680)
  152| 000F54 stw      90030388   1     ST4Z      bvstat[](gr3,904)=gr0
  153| 000F58 stw      900303A8   1     ST4Z      bvstat[](gr3,936)=gr0
  152| 000F5C stw      9003038C   1     ST4Z      bvstat[](gr3,908)=gr0
  153| 000F60 stw      900303AC   1     ST4Z      bvstat[](gr3,940)=gr0
  152| 000F64 stw      90030390   1     ST4Z      bvstat[](gr3,912)=gr0
  153| 000F68 stw      900303B0   1     ST4Z      bvstat[](gr3,944)=gr0
  152| 000F6C stw      90030394   1     ST4Z      bvstat[](gr3,916)=gr0
  153| 000F70 stw      900303B4   1     ST4Z      bvstat[](gr3,948)=gr0
  152| 000F74 stw      90030398   1     ST4Z      bvstat[](gr3,920)=gr0
  153| 000F78 stw      900303B8   1     ST4Z      bvstat[](gr3,952)=gr0
  152| 000F7C stw      9003039C   1     ST4Z      bvstat[](gr3,924)=gr0
  153| 000F80 addi     386303A4   1     AI        gr3=gr3,932
  157| 000F84 ld       EA4102B0   1     L8        gr18=#stack(gr1,688)
  157| 000F88 ld       EA6102B8   1     L8        gr19=#stack(gr1,696)
  157| 000F8C ld       EA8102C0   1     L8        gr20=#stack(gr1,704)
  157| 000F90 ld       EAA102C8   1     L8        gr21=#stack(gr1,712)
  157| 000F94 ld       EAC102D0   1     L8        gr22=#stack(gr1,720)
  157| 000F98 ld       EAE102D8   1     L8        gr23=#stack(gr1,728)
  157| 000F9C ld       EB0102E0   1     L8        gr24=#stack(gr1,736)
  157| 000FA0 ld       EB2102E8   1     L8        gr25=#stack(gr1,744)
  157| 000FA4 ld       EB4102F0   1     L8        gr26=#stack(gr1,752)
  157| 000FA8 ld       EB6102F8   1     L8        gr27=#stack(gr1,760)
  157| 000FAC ld       EB810300   1     L8        gr28=#stack(gr1,768)
  157| 000FB0 ld       EBA10308   1     L8        gr29=#stack(gr1,776)
  157| 000FB4 ld       EBC10310   1     L8        gr30=#stack(gr1,784)
  157| 000FB8 ld       EBE10318   1     L8        gr31=#stack(gr1,792)
  157| 000FBC addi     38210340   1     AI        gr1=gr1,832
  153| 000FC0 stwu     94030018   1     ST4U      gr3,bvstat[](gr3,24)=gr0
  157| 000FC4 bclr     4E800020   1     BA        lr
  147|                              CL.327:
    0| 000FC8 mtspr    7D8903A6   1     LCTR      ctr=gr12
  147|                              CL.244:
  148| 000FCC addi     38840001   1     AI        gr4=gr4,1
  148| 000FD0 cmpd     7C246000   1     C8        cr0=gr4,gr12
  148| 000FD4 bc       4100FFF8   1     BCTT      ctr=CL.244,cr0,0x1/lt,taken=80%(80,20)
    0| 000FD8 b        4BFFFC08   1     B         CL.97,-1
  133|                              CL.315:
    0| 000FDC mtspr    7CE903A6   1     LCTR      ctr=gr7
  133|                              CL.283:
  134| 000FE0 addi     38840001   1     AI        gr4=gr4,1
  134| 000FE4 cmpd     7CA43800   1     C8        cr1=gr4,gr7
  134| 000FE8 bc       4104FFF8   1     BCTT      ctr=CL.283,cr1,0x1/lt,taken=80%(80,20)
    0| 000FEC b        4BFFF820   1     B         CL.109,-1
     |               Tag Table
     | 000FF0        00000000 00012201 84120000 00000FF0
     |               Instruction count         1020
     |               Straight-line exec time   1066
     |               Constant Area
     | 000000        3F000000

 
 
>>>>> COMPILATION UNIT EPILOGUE SECTION <<<<<
 
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    1           0         0           0          1            0
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    transprt_1D.f90             07/08/15   15:48:39
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    1           0         0           0          1            0
 
 
    Source records read.......................................     164
1501-510  Compilation successful for file transprt_1D.f90.
1501-543  Object file created.
