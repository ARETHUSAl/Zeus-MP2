IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- msendrec.f90 07/08/15 15:48:20
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** msendrec_bnd   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-536 (I) Loop (loop index 1) at msendrec.f90 <line 115> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*($$LoopIV9 + 1ll) + (#4)*((long long) (.isy->isy - 1) + ((long long) (.isy->isy - 2) - (long long) (.isy->isy - 1)) * (long long) ((unsigned int) $$LoopIVA)) + (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 1) at msendrec.f90 <line 115> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 2) at msendrec.f90 <line 114> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 3) at msendrec.f90 <line 113> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 7) at msendrec.f90 <line 117> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*($$LoopIV6 + 1ll) + (#4)*((long long) (.iey->iey + 1) + ((long long) (.iey->iey + 2) - (long long) (.iey->iey + 1)) * (long long) ((unsigned int) $$LoopIV7)) + (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 7) at msendrec.f90 <line 117> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 8) at msendrec.f90 <line 114> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 9) at msendrec.f90 <line 113> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 13) at msendrec.f90 <line 159> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 14) at msendrec.f90 <line 160> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 15) at msendrec.f90 <line 161> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*((long long) (.isz->isz - 1) + ((long long) (.isz->isz - 2) - (long long) (.isz->isz - 1)) * (long long) ((unsigned int) $$LoopIV3)) + (#4)*($$LoopIV4 + 1ll) + (8ll)*($$LoopIV5 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 15) at msendrec.f90 <line 161> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 16) at msendrec.f90 <line 159> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 17) at msendrec.f90 <line 160> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 18) at msendrec.f90 <line 163> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*((long long) (.iez->iez + 1) + ((long long) (.iez->iez + 2) - (long long) (.iez->iez + 1)) * (long long) ((unsigned int) $$LoopIV0)) + (#4)*($$LoopIV1 + 1ll) + (8ll)*($$LoopIV2 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 18) at msendrec.f90 <line 163> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 19) at msendrec.f90 <line 113> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 20) at msendrec.f90 <line 114> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(2ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.isy->isy - 1) + ((long long) (.isy->isy - 2) - (long long) (.isy->isy - 1)) * (long long) ((unsigned int) $$LoopIVA)) + (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(4ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.isy->isy - 1) + ((long long) (.isy->isy - 2) - (long long) (.isy->isy - 1)) * (long long) ((unsigned int) $$LoopIVA)) + (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(1ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.isy->isy - 1) + ((long long) (.isy->isy - 2) - (long long) (.isy->isy - 1)) * (long long) ((unsigned int) $$LoopIVA)) + (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(3ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.isy->isy - 1) + ((long long) (.isy->isy - 2) - (long long) (.isy->isy - 1)) * (long long) ((unsigned int) $$LoopIVA)) + (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 21) at msendrec.f90 <line 115> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 22) at msendrec.f90 <line 113> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 23) at msendrec.f90 <line 114> was not SIMD vectorized because the loop is not the innermost loop.
1586-536 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(4ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.iey->iey + 1) + ((long long) (.iey->iey + 2) - (long long) (.iey->iey + 1)) * (long long) ((unsigned int) $$LoopIV7)) + (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(2ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.iey->iey + 1) + ((long long) (.iey->iey + 2) - (long long) (.iey->iey + 1)) * (long long) ((unsigned int) $$LoopIV7)) + (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(1ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.iey->iey + 1) + ((long long) (.iey->iey + 2) - (long long) (.iey->iey + 1)) * (long long) ((unsigned int) $$LoopIV7)) + (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (#5 + #4) + (#5)*(3ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + (#4)*((long long) (.iey->iey + 1) + ((long long) (.iey->iey + 2) - (long long) (.iey->iey + 1)) * (long long) ((unsigned int) $$LoopIV7)) + (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable alignment.
1586-554 (I) Loop (loop index 24) at msendrec.f90 <line 117> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 25) at msendrec.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 26) at msendrec.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-533 (I) Loop (loop index 27) at msendrec.f90 <line 67> was not SIMD vectorized because its number of iterations is too small.
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][4ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][4ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) .iex->iex + ((long long) (.iex->iex - 1) - (long long) .iex->iex) * (long long) $$DCIVE][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][2ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][2ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) .iex->iex + ((long long) (.iex->iex - 1) - (long long) .iex->iex) * (long long) $$DCIVE][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][3ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][3ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) .iex->iex + ((long long) (.iex->iex - 1) - (long long) .iex->iex) * (long long) $$DCIVE][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][1ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][1ll + ($$CIVC * 4ll + (long long) kn % 4ll)][$$DCIVD + 1ll][(long long) .iex->iex + ((long long) (.iex->iex - 1) - (long long) .iex->iex) * (long long) $$DCIVE][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-540 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at msendrec.f90 <line 69> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-534 (I) Loop (loop index 28) at msendrec.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 29) at msendrec.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-533 (I) Loop (loop index 30) at msendrec.f90 <line 67> was not SIMD vectorized because its number of iterations is too small.
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][2ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][2ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) .isx->isx + ((long long) (.isx->isx + 1) - (long long) .isx->isx) * (long long) $$DCIV11][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][3ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][3ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) .isx->isx + ((long long) (.isx->isx + 1) - (long long) .isx->isx) * (long long) $$DCIV11][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][4ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][4ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) .isx->isx + ((long long) (.isx->isx + 1) - (long long) .isx->isx) * (long long) $$DCIV11][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][1ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][1ll + ($$CIVD * 4ll + (long long) kn % 4ll)][$$DCIV10 + 1ll][(long long) .isx->isx + ((long long) (.isx->isx + 1) - (long long) .isx->isx) * (long long) $$DCIV11][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(2ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(3ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(4ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-540 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 30) at msendrec.f90 <line 71> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*(1ll + ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-534 (I) Loop (loop index 34) at msendrec.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 35) at msendrec.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-533 (I) Loop (loop index 36) at msendrec.f90 <line 67> was not SIMD vectorized because its number of iterations is too small.
1586-540 (I) Loop (loop index 36) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][$$DCIVF + 1ll][$$DCIV10 + 1ll][(long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][$$DCIVF + 1ll][$$DCIV10 + 1ll][(long long) .isx->isx + ((long long) (.isx->isx + 1) - (long long) .isx->isx) * (long long) $$DCIV11][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 36) at msendrec.f90 <line 71> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*($$DCIVF + 1ll) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 36) at msendrec.f90 <line 71> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 36) at msendrec.f90 <line 71> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*($$DCIVF + 1ll) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-534 (I) Loop (loop index 40) at msendrec.f90 <line 67> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 41) at msendrec.f90 <line 68> was not SIMD vectorized because the loop is not the innermost loop.
1586-533 (I) Loop (loop index 42) at msendrec.f90 <line 67> was not SIMD vectorized because its number of iterations is too small.
1586-540 (I) Loop (loop index 42) at msendrec.f90 <line 67> was not SIMD vectorized because it contains memory references ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][$$DCIVC + 1ll][$$DCIVD + 1ll][(long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE][1ll][1ll] = ((double *)((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll)))->x[][$$DCIVC + 1ll][$$DCIVD + 1ll][(long long) .iex->iex + ((long long) (.iex->iex - 1) - (long long) .iex->iex) * (long long) $$DCIVE][1ll][1ll]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 42) at msendrec.f90 <line 69> was not SIMD vectorized because it contains memory references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*($$DCIVC + 1ll) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 42) at msendrec.f90 <line 69> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 42) at msendrec.f90 <line 69> was not SIMD vectorized because it contains non-stride-one store references ((char *).x  + -24ll - (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * (max((long long) jn,0ll) * max((long long) in,0ll)))*($$DCIVC + 1ll) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"10">. Total number of the innermost loops SIMD vectorized <"0">.


    14|         SUBROUTINE msendrec_bnd (isx, iex, isy, iey, isz, iez, x)
    14|           #4 = max(int(in),0) * 8
                  #5 = 8 * (max(int(jn),0) * max(int(in),0))
    32|           nelm = 1
    35|           IF ((ntiles[].off132 > 1)) THEN
    36|             IF ((0 <> ((coords[].off120 <> 0  .OR.  periodic[].off108)  &
                &     .AND.  1))) THEN
    37|               nreq = nreq + 1
    38|               T_2 = nsub + 16300
                      CALL mpi_isend((x + ((-24) - (8 * (max(int(%VAL(jn)),0) * &
                &       max(int(%VAL(in)),0)) + max(int(%VAL(in)),0) * 8)) + (8 * &
                &       (max(int(%VAL(jn)),0) * max(int(%VAL(in)),0)))*(1) + (max(&
                &       int(%VAL(in)),0) * 8)*(1) + (8)*(int(isx)) + (8)*(1) + (8)&
                &       *(1)),nelm,ilsm_slice,n1m,T_2,comm3d,(req + (-4) + (4)*(&
                &       int(%VAL(nreq)))),ierr)
    41|             ENDIF
    42|             IF ((0 <> ((ntiles[].off132 - coords[].off120 <> 1  .OR.  &
                &     periodic[].off108)  .AND.  1))) THEN
    44|               nreq = nreq + 1
    45|               T_3 = nsub + 16300
                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))) + (&
                &       %VAL(#5))*(1) + (%VAL(#4))*(1) + (8)*(int((iex + 1))) + (&
                &       8)*(1) + (8)*(1)),nelm,ilsm_slice,n1p,T_3,comm3d,(req + (&
                &       -4) + (4)*(int(%VAL(nreq)))),ierr)
    48|             ENDIF
    51|             IF ((0 <> ((ntiles[].off132 - coords[].off120 <> 1  .OR.  &
                &     periodic[].off108)  .AND.  1))) THEN
    53|               nreq = nreq + 1
    54|               T_4 = nsub + 16400
                      CALL mpi_isend((x + ((-24) - (%VAL(#5) + %VAL(#4))) + (&
                &       %VAL(#5))*(1) + (%VAL(#4))*(1) + (8)*(int(iex)) + (8)*(1) &
                &       + (8)*(1)),nelm,ilsm_slice,n1p,T_4,comm3d,(req + (-4) + (&
                &       4)*(int(%VAL(nreq)))),ierr)
    57|             ENDIF
    58|             IF ((0 <> ((coords[].off120 <> 0  .OR.  periodic[].off108)  &
                &     .AND.  1))) THEN
    60|               nreq = nreq + 1
    61|               T_5 = nsub + 16400
                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))) + (&
                &       %VAL(#5))*(1) + (%VAL(#4))*(1) + (8)*(int((isx - 1))) + (&
                &       8)*(1) + (8)*(1)),nelm,ilsm_slice,n1m,T_5,comm3d,(req + (&
                &       -4) + (4)*(int(%VAL(nreq)))),ierr)
    65|             ELSE
                      lab_1
    66|               IF ((0 <> (periodic[].off108  .AND.  1))) THEN
    67|                 IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
                          $$DCIVC = 0
       Id=40              DO $$DCIVC = $$DCIVC, MOD(int(kn), int(4))-1
                            ! DIR_INDEPENDENT loopId = 0 
    68|                     IF ((int(jn) > 0)) THEN
                              $$DCIVD = 0
       Id=41                  DO $$DCIVD = $$DCIVD, int(int(jn))-1
    67|                         IF (.FALSE.) GOTO lab_157
                                $$DCIVE = 0
       Id=42                    DO $$DCIVE = $$DCIVE, 1
    69|                           x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,$$DCIVC &
                &                   + 1) = x(1,1,int(iex) + (int((iex - 1)) - int(&
                &                   iex)) * int($$DCIVE),$$DCIVD + 1,$$DCIVC + 1)
    67|                         ENDDO
                                lab_157
    68|                       ENDDO
                            ENDIF
    67|                   ENDDO
                        ENDIF
                        IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))) &
                &         THEN
                          $$CIVC = int(0)
       Id=25              DO $$CIVC = $$CIVC, int((((int(kn) - MOD(int(kn), 4)&
                &             ) - 1) / 4 + 1))-1
                            ! DIR_INDEPENDENT loopId = 0 
                            ! DIR_NEW loopId = 25, varId = 185 
                            ! DIR_NEW loopId = 25, varId = 186 
    68|                     IF ((int(jn) > 0)) THEN
                              $$DCIVD = 0
       Id=26                  DO $$DCIVD = $$DCIVD, int(int(jn))-1
    67|                         IF (.FALSE.) GOTO lab_103
                                $$DCIVE = 0
       Id=27                    DO $$DCIVE = $$DCIVE, 1
    69|                           x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,1 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,1 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
                                  x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,2 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,2 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
                                  x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,3 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,3 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
                                  x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,4 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,4 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
    67|                         ENDDO
                                lab_103
    68|                       ENDDO
                            ENDIF
    67|                   ENDDO
                        ENDIF
                        IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
                          $$DCIVF = 0
       Id=34              DO $$DCIVF = $$DCIVF, MOD(int(kn), int(4))-1
                            ! DIR_INDEPENDENT loopId = 0 
    68|                     IF ((int(jn) > 0)) THEN
                              $$DCIV10 = 0
       Id=35                  DO $$DCIV10 = $$DCIV10, int(int(jn))-1
    67|                         IF (.FALSE.) GOTO lab_145
                                $$DCIV11 = 0
       Id=36                    DO $$DCIV11 = $$DCIV11, 1
    71|                           x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                   iex + 1))) * int($$DCIV11),$$DCIV10 + 1,&
                &                   $$DCIVF + 1) = x(1,1,int(isx) + (int((isx + 1)&
                &                   ) - int(isx)) * int($$DCIV11),$$DCIV10 + 1,&
                &                   $$DCIVF + 1)
    67|                         ENDDO
                                lab_145
    68|                       ENDDO
                            ENDIF
    67|                   ENDDO
                        ENDIF
                        IF (.NOT.(int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))&
                &         ) GOTO lab_105
                        $$CIVD = int(0)
       Id=28            DO $$CIVD = $$CIVD, int((((int(kn) - MOD(int(kn), 4)) &
                &           - 1) / 4 + 1))-1
                          ! DIR_INDEPENDENT loopId = 0 
                          ! DIR_NEW loopId = 28, varId = 188 
                          ! DIR_NEW loopId = 28, varId = 189 
    68|                   IF ((int(jn) > 0)) THEN
                            $$DCIV10 = 0
       Id=29                DO $$DCIV10 = $$DCIV10, int(int(jn))-1
    67|                       IF (.FALSE.) GOTO lab_109
                              $$DCIV11 = 0
       Id=30                  DO $$DCIV11 = $$DCIV11, 1
    71|                         x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,1 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,1 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
                                x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,2 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,2 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
                                x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,3 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,3 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
                                x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,4 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,4 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
    67|                       ENDDO
                              lab_109
    68|                     ENDDO
                          ENDIF
    67|                 ENDDO
                        lab_105
    75|                 lab_7
    76|                 lab_6
    78|                 IF ((ldimen > 1)) THEN
    80|                   IF ((ntiles[].off136 > 1)) THEN
    81|                     IF ((0 <> ((coords[].off124 <> 0  .OR.  &
                &             periodic[].off112)  .AND.  1))) THEN
    83|                       nreq = nreq + 1
    84|                       T_6 = nsub + 16500
                              CALL mpi_isend((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int(isy)) + (8)*(&
                &               1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,n2m,T_6,&
                &               comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
    87|                     ENDIF
    88|                     IF ((0 <> ((ntiles[].off136 - coords[].off124 <> 1  &
                &             .OR.  periodic[].off112)  .AND.  1))) THEN
    90|                       nreq = nreq + 1
    91|                       T_7 = nsub + 16500
                              CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int((iey + 1))) &
                &               + (8)*(1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,&
                &               n2p,T_7,comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))&
                &               ),ierr)
    94|                     ENDIF
    97|                     IF ((0 <> ((ntiles[].off136 - coords[].off124 <> 1  &
                &             .OR.  periodic[].off112)  .AND.  1))) THEN
    99|                       nreq = nreq + 1
   100|                       T_8 = nsub + 16600
                              CALL mpi_isend((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int(iey)) + (8)*(&
                &               1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,n2p,T_8,&
                &               comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   103|                     ENDIF
   104|                     IF ((0 <> ((coords[].off124 <> 0  .OR.  &
                &             periodic[].off112)  .AND.  1))) THEN
   106|                       nreq = nreq + 1
   107|                       T_9 = nsub + 16600
                              CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int((isy - 1))) &
                &               + (8)*(1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,&
                &               n2m,T_9,comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))&
                &               ),ierr)
   111|                     ELSE
                              lab_17
   112|                       IF ((0 <> (periodic[].off112  .AND.  1))) THEN
   113|                         IF ((MOD(int(kn), 4) > 0  .AND.  0 < int(kn))) &
                &                 THEN
                                  $$LoopIV9 = 0
       Id=3                       DO $$LoopIV9 = $$LoopIV9, MOD(int(kn), int(&
                &                     4))-1
                                    ! DIR_INDEPENDENT loopId = 0 
   114|                             IF (.FALSE.) GOTO lab_134
                                    $$LoopIVA = 0
       Id=2                         DO $$LoopIVA = $$LoopIVA, 1
   113|                               IF ((0 < int(in))) THEN
                                        $$LoopIVB = 0
       Id=1                             DO $$LoopIVB = $$LoopIVB, int(int(in))&
                &                           -1
   115|                                   x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),$$LoopIV9 + 1) = &
                &                           x(1,1,$$LoopIVB + 1,int(iey) + (int((&
                &                           iey - 1)) - int(iey)) * int(int(&
                &                           $$LoopIVA)),$$LoopIV9 + 1)
   113|                                 ENDDO
                                      ENDIF
   114|                             ENDDO
                                    lab_134
   113|                           ENDDO
                                ENDIF
                                IF ((0 < int(kn)  .AND.  int(kn) > MOD(int(kn), &
                &                 4))) THEN
                                  $$CIVE = int(0)
       Id=19                      DO $$CIVE = $$CIVE, int((((int(kn) - MOD(&
                &                     int(kn), 4)) - 1) / 4 + 1))-1
                                    ! DIR_INDEPENDENT loopId = 0 
                                    ! DIR_NEW loopId = 19, varId = 201 
                                    ! DIR_NEW loopId = 19, varId = 202 
   114|                             IF (.FALSE.) GOTO lab_88
                                    $$LoopIVA = 0
       Id=20                        DO $$LoopIVA = $$LoopIVA, 1
   113|                               IF ((0 < int(in))) THEN
                                        $$LoopIVB = 0
       Id=21                            DO $$LoopIVB = $$LoopIVB, int(int(in))&
                &                           -1
   115|                                   x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),1 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),1 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
                                          x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),2 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),2 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
                                          x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),3 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),3 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
                                          x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),4 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),4 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
   113|                                 ENDDO
                                      ENDIF
   114|                             ENDDO
                                    lab_88
   113|                           ENDDO
                                ENDIF
                                IF ((MOD(int(kn), 4) > 0  .AND.  0 < int(kn))) &
                &                 THEN
                                  $$LoopIV6 = 0
       Id=9                       DO $$LoopIV6 = $$LoopIV6, MOD(int(kn), int(&
                &                     4))-1
                                    ! DIR_INDEPENDENT loopId = 0 
   114|                             IF (.FALSE.) GOTO lab_122
                                    $$LoopIV7 = 0
       Id=8                         DO $$LoopIV7 = $$LoopIV7, 1
   113|                               IF ((0 < int(in))) THEN
                                        $$LoopIV8 = 0
       Id=7                             DO $$LoopIV8 = $$LoopIV8, int(int(in))&
                &                           -1
   117|                                   x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                           int((iey + 2)) - int((iey + 1))) * &
                &                           int(int($$LoopIV7)),$$LoopIV6 + 1) = &
                &                           x(1,1,$$LoopIV8 + 1,int(isy) + (int((&
                &                           isy + 1)) - int(isy)) * int(int(&
                &                           $$LoopIV7)),$$LoopIV6 + 1)
   113|                                 ENDDO
                                      ENDIF
   114|                             ENDDO
                                    lab_122
   113|                           ENDDO
                                ENDIF
                                IF (.NOT.(0 < int(kn)  .AND.  int(kn) > MOD(int(&
                &                 kn), 4))) GOTO lab_92
                                $$CIVF = int(0)
       Id=22                    DO $$CIVF = $$CIVF, int((((int(kn) - MOD(int(&
                &                   kn), 4)) - 1) / 4 + 1))-1
                                  ! DIR_INDEPENDENT loopId = 0 
                                  ! DIR_NEW loopId = 22, varId = 198 
                                  ! DIR_NEW loopId = 22, varId = 199 
   114|                           IF (.FALSE.) GOTO lab_94
                                  $$LoopIV7 = 0
       Id=23                      DO $$LoopIV7 = $$LoopIV7, 1
   113|                             IF ((0 < int(in))) THEN
                                      $$LoopIV8 = 0
       Id=24                          DO $$LoopIV8 = $$LoopIV8, int(int(in))&
                &                         -1
   117|                                 x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),1 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),1 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
                                        x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),2 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),2 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
                                        x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),3 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),3 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
                                        x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),4 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),4 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
   113|                               ENDDO
                                    ENDIF
   114|                           ENDDO
                                  lab_94
   113|                         ENDDO
                                lab_92
   121|                         lab_23
   122|                         lab_22
   124|                         IF ((ldimen > 2)) THEN
   126|                           IF ((ntiles[].off140 > 1)) THEN
   127|                             IF ((0 <> ((ntiles[].off140 - &
                &                     coords[].off128 <> 1  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   129|                               nreq = nreq + 1
   130|                               T_10 = nsub + 16700
                                      CALL mpi_isend((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int(iez)) + (&
                &                       %VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)*(&
                &                       1)),nelm,klsm_slice,n3p,T_10,comm3d,(req &
                &                       + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   133|                             ENDIF
   134|                             IF ((0 <> ((coords[].off128 <> 0  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   136|                               nreq = nreq + 1
   137|                               T_11 = nsub + 16700
                                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int((isz - 1))) &
                &                       + (%VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)&
                &                       *(1)),nelm,klsm_slice,n3m,T_11,comm3d,(&
                &                       req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   140|                             ENDIF
   143|                             IF ((0 <> ((coords[].off128 <> 0  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   145|                               nreq = nreq + 1
   146|                               T_12 = nsub + 16800
                                      CALL mpi_isend((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int(isz)) + (&
                &                       %VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)*(&
                &                       1)),nelm,klsm_slice,n3m,T_12,comm3d,(req &
                &                       + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   149|                             ENDIF
   150|                             IF ((0 <> ((ntiles[].off140 - &
                &                     coords[].off128 <> 1  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   152|                               nreq = nreq + 1
   153|                               T_13 = nsub + 16800
                                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int((iez + 1))) &
                &                       + (%VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)&
                &                       *(1)),nelm,klsm_slice,n3p,T_13,comm3d,(&
                &                       req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   157|                             ELSE
                                      lab_33
   158|                               IF ((0 <> (periodic[].off116  .AND.  1))) &
                &                       THEN
   159|                                 IF (.FALSE.) GOTO lab_73
                                        $$LoopIV3 = 0
       Id=13                            DO $$LoopIV3 = $$LoopIV3, 1
   160|                                   IF ((0 < int(jn))) THEN
                                            $$LoopIV4 = 0
       Id=14                                DO $$LoopIV4 = $$LoopIV4, int(int(&
                &                               jn))-1
   159|                                       IF ((0 < int(in))) THEN
                                                $$LoopIV5 = 0
       Id=15                                    DO $$LoopIV5 = $$LoopIV5, int(&
                &                                   int(in))-1
                                                  ! DIR_INDEPENDENT loopId = 0 
   161|                                           x(1,1,$$LoopIV5 + 1,$$LoopIV4 &
                &                                   + 1,int((isz - 1)) + (int((&
                &                                   isz - 2)) - int((isz - 1))) * &
                &                                   int(int($$LoopIV3))) = x(1,1,&
                &                                   $$LoopIV5 + 1,$$LoopIV4 + 1,&
                &                                   int(iez) + (int((iez - 1)) - &
                &                                   int(iez)) * int(int($$LoopIV3)&
                &                                   ))
   159|                                         ENDDO
                                              ENDIF
   160|                                     ENDDO
                                          ENDIF
   159|                                 ENDDO
                                        lab_73
                                        IF (.FALSE.) GOTO lab_79
                                        $$LoopIV0 = 0
       Id=16                            DO $$LoopIV0 = $$LoopIV0, 1
   160|                                   IF ((0 < int(jn))) THEN
                                            $$LoopIV1 = 0
       Id=17                                DO $$LoopIV1 = $$LoopIV1, int(int(&
                &                               jn))-1
   159|                                       IF ((0 < int(in))) THEN
                                                $$LoopIV2 = 0
       Id=18                                    DO $$LoopIV2 = $$LoopIV2, int(&
                &                                   int(in))-1
                                                  ! DIR_INDEPENDENT loopId = 0 
   163|                                           x(1,1,$$LoopIV2 + 1,$$LoopIV1 &
                &                                   + 1,int((iez + 1)) + (int((&
                &                                   iez + 2)) - int((iez + 1))) * &
                &                                   int(int($$LoopIV0))) = x(1,1,&
                &                                   $$LoopIV2 + 1,$$LoopIV1 + 1,&
                &                                   int(isz) + (int((isz + 1)) - &
                &                                   int(isz)) * int(int($$LoopIV0)&
                &                                   ))
   159|                                         ENDDO
                                              ENDIF
   160|                                     ENDDO
                                          ENDIF
   159|                                 ENDDO
                                        lab_79
   167|                                 lab_39
   170|                                 lab_32
   171|                                 lab_16
   174|                                 lab_60
                                        RETURN
                                      END SUBROUTINE msendrec_bnd


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            67            40    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            68            41    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            67            42    Loop was not SIMD vectorized because its number of 
                                          iterations is too small.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*($$DCIVC + 
                                          1ll) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 
                                          1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long 
                                          long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) 
                                          * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)) 
                                          with  non-vectorizable strides.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*($$DCIVC + 1ll) + (max((long long) in,0ll) 
                                          * 8ll)*($$DCIVD + 1ll) + (8ll)*((long long) 
                                          (.isx->isx - 1) + ((long long) (.isx->isx - 2) - 
                                          (long long) (.isx->isx - 1)) * (long long) $$DCIVE) + 
                                          (8ll)*(1ll) + (8ll)*(1ll)).
         0            67            25    Outer loop has been unrolled 4 time(s).
         0            67            25    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            68            26    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            67            27    Loop has been rolled.
         0            67            27    Loop was not SIMD vectorized because its number of 
                                          iterations is too small.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(1ll + 
                                          ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long 
                                          long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) 
                                          - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) 
                                          + (8ll)*(1ll) + (8ll)*(1ll)) with  non-vectorizable 
                                          strides.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(1ll + ($$CIVC * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 
                                          1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long 
                                          long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) 
                                          * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(2ll + 
                                          ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long 
                                          long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) 
                                          - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) 
                                          + (8ll)*(1ll) + (8ll)*(1ll)) with  non-vectorizable 
                                          strides.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(2ll + ($$CIVC * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 
                                          1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long 
                                          long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) 
                                          * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(3ll + 
                                          ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long 
                                          long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) 
                                          - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) 
                                          + (8ll)*(1ll) + (8ll)*(1ll)) with  non-vectorizable 
                                          strides.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(3ll + ($$CIVC * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 
                                          1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long 
                                          long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) 
                                          * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            69                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(4ll + 
                                          ($$CIVC * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIVD + 1ll) + (8ll)*((long 
                                          long) (.isx->isx - 1) + ((long long) (.isx->isx - 2) 
                                          - (long long) (.isx->isx - 1)) * (long long) $$DCIVE) 
                                          + (8ll)*(1ll) + (8ll)*(1ll)) with  non-vectorizable 
                                          strides.
         0            69                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            69                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(4ll + ($$CIVC * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIVD + 
                                          1ll) + (8ll)*((long long) (.isx->isx - 1) + ((long 
                                          long) (.isx->isx - 2) - (long long) (.isx->isx - 1)) 
                                          * (long long) $$DCIVE) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            67            34    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            68            35    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            67            36    Loop was not SIMD vectorized because its number of 
                                          iterations is too small.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*($$DCIVF + 
                                          1ll) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 
                                          1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long 
                                          long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) 
                                          * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) 
                                          with  non-vectorizable strides.
         0            71                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*($$DCIVF + 1ll) + (max((long long) in,0ll) 
                                          * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long long) 
                                          (.iex->iex + 1) + ((long long) (.iex->iex + 2) - 
                                          (long long) (.iex->iex + 1)) * (long long) $$DCIV11) 
                                          + (8ll)*(1ll) + (8ll)*(1ll)).
         0            67            28    Outer loop has been unrolled 4 time(s).
         0            67            28    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            68            29    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            67            30    Loop has been rolled.
         0            67            30    Loop was not SIMD vectorized because its number of 
                                          iterations is too small.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(1ll + 
                                          ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long 
                                          long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) 
                                          - (long long) (.iex->iex + 1)) * (long long) 
                                          $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with  
                                          non-vectorizable strides.
         0            71                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(1ll + ($$CIVD * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 
                                          1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long 
                                          long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) 
                                          * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            71                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(2ll + 
                                          ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long 
                                          long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) 
                                          - (long long) (.iex->iex + 1)) * (long long) 
                                          $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with  
                                          non-vectorizable strides.
         0            71                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(2ll + ($$CIVD * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 
                                          1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long 
                                          long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) 
                                          * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            71                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(3ll + 
                                          ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long 
                                          long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) 
                                          - (long long) (.iex->iex + 1)) * (long long) 
                                          $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with  
                                          non-vectorizable strides.
         0            71                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(3ll + ($$CIVD * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 
                                          1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long 
                                          long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) 
                                          * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
         0            71                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (8ll * 
                                          (max((long long) jn,0ll) * max((long long) in,0ll)) + 
                                          max((long long) in,0ll) * 8ll) + (8ll * (max((long 
                                          long) jn,0ll) * max((long long) in,0ll)))*(4ll + 
                                          ($$CIVD * 4ll + (long long) kn % 4ll)) + (max((long 
                                          long) in,0ll) * 8ll)*($$DCIV10 + 1ll) + (8ll)*((long 
                                          long) (.iex->iex + 1) + ((long long) (.iex->iex + 2) 
                                          - (long long) (.iex->iex + 1)) * (long long) 
                                          $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)) with  
                                          non-vectorizable strides.
         0            71                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            71                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *).x  + -24ll 
                                          - (8ll * (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)) + max((long long) in,0ll) * 8ll) + (8ll * 
                                          (max((long long) jn,0ll) * max((long long) 
                                          in,0ll)))*(4ll + ($$CIVD * 4ll + (long long) kn % 
                                          4ll)) + (max((long long) in,0ll) * 8ll)*($$DCIV10 + 
                                          1ll) + (8ll)*((long long) (.iex->iex + 1) + ((long 
                                          long) (.iex->iex + 2) - (long long) (.iex->iex + 1)) 
                                          * (long long) $$DCIV11) + (8ll)*(1ll) + (8ll)*(1ll)).
         0           113             3    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           114             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           115                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*($$LoopIV9 + 1ll) + (#4)*((long long) (.isy->isy 
                                          - 1) + ((long long) (.isy->isy - 2) - (long long) 
                                          (.isy->isy - 1)) * (long long) ((unsigned int) 
                                          $$LoopIVA)) + (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + 
                                          (8ll)*(1ll))  with non-vectorizable alignment.
         0           115                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           113            19    Loop interchanging applied to loop nest.
         0           113            19    Outer loop has been unrolled 4 time(s).
         0           113            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           114            20    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           113            21    Loop has been rolled.
         0           115                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(1ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.isy->isy - 1) + ((long long) 
                                          (.isy->isy - 2) - (long long) (.isy->isy - 1)) * 
                                          (long long) ((unsigned int) $$LoopIVA)) + 
                                          (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           115                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           115                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(2ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.isy->isy - 1) + ((long long) 
                                          (.isy->isy - 2) - (long long) (.isy->isy - 1)) * 
                                          (long long) ((unsigned int) $$LoopIVA)) + 
                                          (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           115                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           115                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(3ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.isy->isy - 1) + ((long long) 
                                          (.isy->isy - 2) - (long long) (.isy->isy - 1)) * 
                                          (long long) ((unsigned int) $$LoopIVA)) + 
                                          (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           115                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           115                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(4ll + ($$CIVE * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.isy->isy - 1) + ((long long) 
                                          (.isy->isy - 2) - (long long) (.isy->isy - 1)) * 
                                          (long long) ((unsigned int) $$LoopIVA)) + 
                                          (8ll)*($$LoopIVB + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           115                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           113             9    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           114             8    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           117                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*($$LoopIV6 + 1ll) + (#4)*((long long) (.iey->iey 
                                          + 1) + ((long long) (.iey->iey + 2) - (long long) 
                                          (.iey->iey + 1)) * (long long) ((unsigned int) 
                                          $$LoopIV7)) + (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + 
                                          (8ll)*(1ll))  with non-vectorizable alignment.
         0           117                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           113            22    Loop interchanging applied to loop nest.
         0           113            22    Outer loop has been unrolled 4 time(s).
         0           113            22    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           114            23    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           113            24    Loop has been rolled.
         0           117                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(1ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.iey->iey + 1) + ((long long) 
                                          (.iey->iey + 2) - (long long) (.iey->iey + 1)) * 
                                          (long long) ((unsigned int) $$LoopIV7)) + 
                                          (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           117                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           117                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(2ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.iey->iey + 1) + ((long long) 
                                          (.iey->iey + 2) - (long long) (.iey->iey + 1)) * 
                                          (long long) ((unsigned int) $$LoopIV7)) + 
                                          (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           117                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           117                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(3ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.iey->iey + 1) + ((long long) 
                                          (.iey->iey + 2) - (long long) (.iey->iey + 1)) * 
                                          (long long) ((unsigned int) $$LoopIV7)) + 
                                          (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           117                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           117                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*(4ll + ($$CIVF * 4ll + (long long) kn % 4ll)) + 
                                          (#4)*((long long) (.iey->iey + 1) + ((long long) 
                                          (.iey->iey + 2) - (long long) (.iey->iey + 1)) * 
                                          (long long) ((unsigned int) $$LoopIV7)) + 
                                          (8ll)*($$LoopIV8 + 1ll) + (8ll)*(1ll) + (8ll)*(1ll))  
                                          with non-vectorizable alignment.
         0           117                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           159            13    Loop interchanging applied to loop nest.
         0           159            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           160            14    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           159            15    Loop has been rolled.
         0           161                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*((long long) (.isz->isz - 1) + ((long long) 
                                          (.isz->isz - 2) - (long long) (.isz->isz - 1)) * 
                                          (long long) ((unsigned int) $$LoopIV3)) + 
                                          (#4)*($$LoopIV4 + 1ll) + (8ll)*($$LoopIV5 + 1ll) + 
                                          (8ll)*(1ll) + (8ll)*(1ll))  with non-vectorizable 
                                          alignment.
         0           161                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           159            16    Loop interchanging applied to loop nest.
         0           159            16    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           160            17    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           159            18    Loop has been rolled.
         0           163                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *).x  + -24ll - (#5 + #4) + 
                                          (#5)*((long long) (.iez->iez + 1) + ((long long) 
                                          (.iez->iez + 2) - (long long) (.iez->iez + 1)) * 
                                          (long long) ((unsigned int) $$LoopIV0)) + 
                                          (#4)*($$LoopIV1 + 1ll) + (8ll)*($$LoopIV2 + 1ll) + 
                                          (8ll)*(1ll) + (8ll)*(1ll))  with non-vectorizable 
                                          alignment.
         0           163                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.


    14|         SUBROUTINE msendrec_bnd (isx, iex, isy, iey, isz, iez, x)
    14|           #4 = max(int(in),0) * 8
                  #5 = 8 * (max(int(jn),0) * max(int(in),0))
    32|           nelm = 1
    35|           IF ((ntiles[].off132 > 1)) THEN
    36|             IF ((0 <> ((coords[].off120 <> 0  .OR.  periodic[].off108)  &
                &     .AND.  1))) THEN
    37|               nreq = nreq + 1
    38|               T_2 = nsub + 16300
                      CALL mpi_isend((x + ((-24) - (8 * (max(int(%VAL(jn)),0) * &
                &       max(int(%VAL(in)),0)) + max(int(%VAL(in)),0) * 8)) + (8 * &
                &       (max(int(%VAL(jn)),0) * max(int(%VAL(in)),0)))*(1) + (max(&
                &       int(%VAL(in)),0) * 8)*(1) + (8)*(int(isx)) + (8)*(1) + (8)&
                &       *(1)),nelm,ilsm_slice,n1m,T_2,comm3d,(req + (-4) + (4)*(&
                &       int(%VAL(nreq)))),ierr)
    41|             ENDIF
    42|             IF ((0 <> ((ntiles[].off132 - coords[].off120 <> 1  .OR.  &
                &     periodic[].off108)  .AND.  1))) THEN
    44|               nreq = nreq + 1
    45|               T_3 = nsub + 16300
                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))) + (&
                &       %VAL(#5))*(1) + (%VAL(#4))*(1) + (8)*(int((iex + 1))) + (&
                &       8)*(1) + (8)*(1)),nelm,ilsm_slice,n1p,T_3,comm3d,(req + (&
                &       -4) + (4)*(int(%VAL(nreq)))),ierr)
    48|             ENDIF
    51|             IF ((0 <> ((ntiles[].off132 - coords[].off120 <> 1  .OR.  &
                &     periodic[].off108)  .AND.  1))) THEN
    53|               nreq = nreq + 1
    54|               T_4 = nsub + 16400
                      CALL mpi_isend((x + ((-24) - (%VAL(#5) + %VAL(#4))) + (&
                &       %VAL(#5))*(1) + (%VAL(#4))*(1) + (8)*(int(iex)) + (8)*(1) &
                &       + (8)*(1)),nelm,ilsm_slice,n1p,T_4,comm3d,(req + (-4) + (&
                &       4)*(int(%VAL(nreq)))),ierr)
    57|             ENDIF
    58|             IF ((0 <> ((coords[].off120 <> 0  .OR.  periodic[].off108)  &
                &     .AND.  1))) THEN
    60|               nreq = nreq + 1
    61|               T_5 = nsub + 16400
                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))) + (&
                &       %VAL(#5))*(1) + (%VAL(#4))*(1) + (8)*(int((isx - 1))) + (&
                &       8)*(1) + (8)*(1)),nelm,ilsm_slice,n1m,T_5,comm3d,(req + (&
                &       -4) + (4)*(int(%VAL(nreq)))),ierr)
    65|             ELSE
                      lab_1
    66|               IF ((0 <> (periodic[].off108  .AND.  1))) THEN
    67|                 IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
                          $$DCIVC = 0
       Id=40              DO $$DCIVC = $$DCIVC, MOD(int(kn), int(4))-1
                            ! DIR_INDEPENDENT loopId = 0 
    68|                     IF ((int(jn) > 0)) THEN
                              $$DCIVD = 0
       Id=41                  DO $$DCIVD = $$DCIVD, int(int(jn))-1
    67|                         IF (.FALSE.) GOTO lab_157
                                $$DCIVE = 0
       Id=42                    DO $$DCIVE = $$DCIVE, 1
    69|                           x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,$$DCIVC &
                &                   + 1) = x(1,1,int(iex) + (int((iex - 1)) - int(&
                &                   iex)) * int($$DCIVE),$$DCIVD + 1,$$DCIVC + 1)
    67|                         ENDDO
                                lab_157
    68|                       ENDDO
                            ENDIF
    67|                   ENDDO
                        ENDIF
                        IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))) &
                &         THEN
                          $$CIVC = int(0)
       Id=25              DO $$CIVC = $$CIVC, int((((int(kn) - MOD(int(kn), 4)&
                &             ) - 1) / 4 + 1))-1
                            ! DIR_INDEPENDENT loopId = 0 
                            ! DIR_NEW loopId = 25, varId = 185 
                            ! DIR_NEW loopId = 25, varId = 186 
    68|                     IF ((int(jn) > 0)) THEN
                              $$DCIVD = 0
       Id=26                  DO $$DCIVD = $$DCIVD, int(int(jn))-1
    67|                         IF (.FALSE.) GOTO lab_103
                                $$DCIVE = 0
       Id=27                    DO $$DCIVE = $$DCIVE, 1
    69|                           x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,1 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,1 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
                                  x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,2 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,2 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
                                  x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,3 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,3 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
                                  x(1,1,int((isx - 1)) + (int((isx - 2)) - int((&
                &                   isx - 1))) * int($$DCIVE),$$DCIVD + 1,4 + (&
                &                   $$CIVC * 4 + MOD(int(kn), 4))) = x(1,1,int(&
                &                   iex) + (int((iex - 1)) - int(iex)) * int(&
                &                   $$DCIVE),$$DCIVD + 1,4 + ($$CIVC * 4 + MOD(&
                &                   int(kn), 4)))
    67|                         ENDDO
                                lab_103
    68|                       ENDDO
                            ENDIF
    67|                   ENDDO
                        ENDIF
                        IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
                          $$DCIVF = 0
       Id=34              DO $$DCIVF = $$DCIVF, MOD(int(kn), int(4))-1
                            ! DIR_INDEPENDENT loopId = 0 
    68|                     IF ((int(jn) > 0)) THEN
                              $$DCIV10 = 0
       Id=35                  DO $$DCIV10 = $$DCIV10, int(int(jn))-1
    67|                         IF (.FALSE.) GOTO lab_145
                                $$DCIV11 = 0
       Id=36                    DO $$DCIV11 = $$DCIV11, 1
    71|                           x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                   iex + 1))) * int($$DCIV11),$$DCIV10 + 1,&
                &                   $$DCIVF + 1) = x(1,1,int(isx) + (int((isx + 1)&
                &                   ) - int(isx)) * int($$DCIV11),$$DCIV10 + 1,&
                &                   $$DCIVF + 1)
    67|                         ENDDO
                                lab_145
    68|                       ENDDO
                            ENDIF
    67|                   ENDDO
                        ENDIF
                        IF (.NOT.(int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))&
                &         ) GOTO lab_105
                        $$CIVD = int(0)
       Id=28            DO $$CIVD = $$CIVD, int((((int(kn) - MOD(int(kn), 4)) &
                &           - 1) / 4 + 1))-1
                          ! DIR_INDEPENDENT loopId = 0 
                          ! DIR_NEW loopId = 28, varId = 188 
                          ! DIR_NEW loopId = 28, varId = 189 
    68|                   IF ((int(jn) > 0)) THEN
                            $$DCIV10 = 0
       Id=29                DO $$DCIV10 = $$DCIV10, int(int(jn))-1
    67|                       IF (.FALSE.) GOTO lab_109
                              $$DCIV11 = 0
       Id=30                  DO $$DCIV11 = $$DCIV11, 1
    71|                         x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,1 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,1 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
                                x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,2 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,2 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
                                x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,3 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,3 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
                                x(1,1,int((iex + 1)) + (int((iex + 2)) - int((&
                &                 iex + 1))) * int($$DCIV11),$$DCIV10 + 1,4 + (&
                &                 $$CIVD * 4 + MOD(int(kn), 4))) = x(1,1,int(isx) &
                &                 + (int((isx + 1)) - int(isx)) * int($$DCIV11),&
                &                 $$DCIV10 + 1,4 + ($$CIVD * 4 + MOD(int(kn), 4)))&
                &                 
    67|                       ENDDO
                              lab_109
    68|                     ENDDO
                          ENDIF
    67|                 ENDDO
                        lab_105
    75|                 lab_7
    76|                 lab_6
    78|                 IF ((ldimen > 1)) THEN
    80|                   IF ((ntiles[].off136 > 1)) THEN
    81|                     IF ((0 <> ((coords[].off124 <> 0  .OR.  &
                &             periodic[].off112)  .AND.  1))) THEN
    83|                       nreq = nreq + 1
    84|                       T_6 = nsub + 16500
                              CALL mpi_isend((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int(isy)) + (8)*(&
                &               1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,n2m,T_6,&
                &               comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
    87|                     ENDIF
    88|                     IF ((0 <> ((ntiles[].off136 - coords[].off124 <> 1  &
                &             .OR.  periodic[].off112)  .AND.  1))) THEN
    90|                       nreq = nreq + 1
    91|                       T_7 = nsub + 16500
                              CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int((iey + 1))) &
                &               + (8)*(1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,&
                &               n2p,T_7,comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))&
                &               ),ierr)
    94|                     ENDIF
    97|                     IF ((0 <> ((ntiles[].off136 - coords[].off124 <> 1  &
                &             .OR.  periodic[].off112)  .AND.  1))) THEN
    99|                       nreq = nreq + 1
   100|                       T_8 = nsub + 16600
                              CALL mpi_isend((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int(iey)) + (8)*(&
                &               1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,n2p,T_8,&
                &               comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   103|                     ENDIF
   104|                     IF ((0 <> ((coords[].off124 <> 0  .OR.  &
                &             periodic[].off112)  .AND.  1))) THEN
   106|                       nreq = nreq + 1
   107|                       T_9 = nsub + 16600
                              CALL mpi_irecv((x + ((-24) - (%VAL(#5) + %VAL(#4))&
                &               ) + (%VAL(#5))*(1) + (%VAL(#4))*(int((isy - 1))) &
                &               + (8)*(1) + (8)*(1) + (8)*(1)),nelm,jlsm_slice,&
                &               n2m,T_9,comm3d,(req + (-4) + (4)*(int(%VAL(nreq)))&
                &               ),ierr)
   111|                     ELSE
                              lab_17
   112|                       IF ((0 <> (periodic[].off112  .AND.  1))) THEN
   113|                         IF ((MOD(int(kn), 4) > 0  .AND.  0 < int(kn))) &
                &                 THEN
                                  $$LoopIV9 = 0
       Id=3                       DO $$LoopIV9 = $$LoopIV9, MOD(int(kn), int(&
                &                     4))-1
                                    ! DIR_INDEPENDENT loopId = 0 
   114|                             IF (.FALSE.) GOTO lab_134
                                    $$LoopIVA = 0
       Id=2                         DO $$LoopIVA = $$LoopIVA, 1
   113|                               IF ((0 < int(in))) THEN
                                        $$LoopIVB = 0
       Id=1                             DO $$LoopIVB = $$LoopIVB, int(int(in))&
                &                           -1
   115|                                   x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),$$LoopIV9 + 1) = &
                &                           x(1,1,$$LoopIVB + 1,int(iey) + (int((&
                &                           iey - 1)) - int(iey)) * int(int(&
                &                           $$LoopIVA)),$$LoopIV9 + 1)
   113|                                 ENDDO
                                      ENDIF
   114|                             ENDDO
                                    lab_134
   113|                           ENDDO
                                ENDIF
                                IF ((0 < int(kn)  .AND.  int(kn) > MOD(int(kn), &
                &                 4))) THEN
                                  $$CIVE = int(0)
       Id=19                      DO $$CIVE = $$CIVE, int((((int(kn) - MOD(&
                &                     int(kn), 4)) - 1) / 4 + 1))-1
                                    ! DIR_INDEPENDENT loopId = 0 
                                    ! DIR_NEW loopId = 19, varId = 201 
                                    ! DIR_NEW loopId = 19, varId = 202 
   114|                             IF (.FALSE.) GOTO lab_88
                                    $$LoopIVA = 0
       Id=20                        DO $$LoopIVA = $$LoopIVA, 1
   113|                               IF ((0 < int(in))) THEN
                                        $$LoopIVB = 0
       Id=21                            DO $$LoopIVB = $$LoopIVB, int(int(in))&
                &                           -1
   115|                                   x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),1 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),1 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
                                          x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),2 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),2 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
                                          x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),3 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),3 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
                                          x(1,1,$$LoopIVB + 1,int((isy - 1)) + (&
                &                           int((isy - 2)) - int((isy - 1))) * &
                &                           int(int($$LoopIVA)),4 + ($$CIVE * 4 + &
                &                           MOD(int(kn), 4))) = x(1,1,$$LoopIVB + &
                &                           1,int(iey) + (int((iey - 1)) - int(&
                &                           iey)) * int(int($$LoopIVA)),4 + (&
                &                           $$CIVE * 4 + MOD(int(kn), 4)))
   113|                                 ENDDO
                                      ENDIF
   114|                             ENDDO
                                    lab_88
   113|                           ENDDO
                                ENDIF
                                IF ((MOD(int(kn), 4) > 0  .AND.  0 < int(kn))) &
                &                 THEN
                                  $$LoopIV6 = 0
       Id=9                       DO $$LoopIV6 = $$LoopIV6, MOD(int(kn), int(&
                &                     4))-1
                                    ! DIR_INDEPENDENT loopId = 0 
   114|                             IF (.FALSE.) GOTO lab_122
                                    $$LoopIV7 = 0
       Id=8                         DO $$LoopIV7 = $$LoopIV7, 1
   113|                               IF ((0 < int(in))) THEN
                                        $$LoopIV8 = 0
       Id=7                             DO $$LoopIV8 = $$LoopIV8, int(int(in))&
                &                           -1
   117|                                   x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                           int((iey + 2)) - int((iey + 1))) * &
                &                           int(int($$LoopIV7)),$$LoopIV6 + 1) = &
                &                           x(1,1,$$LoopIV8 + 1,int(isy) + (int((&
                &                           isy + 1)) - int(isy)) * int(int(&
                &                           $$LoopIV7)),$$LoopIV6 + 1)
   113|                                 ENDDO
                                      ENDIF
   114|                             ENDDO
                                    lab_122
   113|                           ENDDO
                                ENDIF
                                IF (.NOT.(0 < int(kn)  .AND.  int(kn) > MOD(int(&
                &                 kn), 4))) GOTO lab_92
                                $$CIVF = int(0)
       Id=22                    DO $$CIVF = $$CIVF, int((((int(kn) - MOD(int(&
                &                   kn), 4)) - 1) / 4 + 1))-1
                                  ! DIR_INDEPENDENT loopId = 0 
                                  ! DIR_NEW loopId = 22, varId = 198 
                                  ! DIR_NEW loopId = 22, varId = 199 
   114|                           IF (.FALSE.) GOTO lab_94
                                  $$LoopIV7 = 0
       Id=23                      DO $$LoopIV7 = $$LoopIV7, 1
   113|                             IF ((0 < int(in))) THEN
                                      $$LoopIV8 = 0
       Id=24                          DO $$LoopIV8 = $$LoopIV8, int(int(in))&
                &                         -1
   117|                                 x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),1 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),1 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
                                        x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),2 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),2 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
                                        x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),3 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),3 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
                                        x(1,1,$$LoopIV8 + 1,int((iey + 1)) + (&
                &                         int((iey + 2)) - int((iey + 1))) * int(&
                &                         int($$LoopIV7)),4 + ($$CIVF * 4 + MOD(&
                &                         int(kn), 4))) = x(1,1,$$LoopIV8 + 1,int(&
                &                         isy) + (int((isy + 1)) - int(isy)) * &
                &                         int(int($$LoopIV7)),4 + ($$CIVF * 4 + &
                &                         MOD(int(kn), 4)))
   113|                               ENDDO
                                    ENDIF
   114|                           ENDDO
                                  lab_94
   113|                         ENDDO
                                lab_92
   121|                         lab_23
   122|                         lab_22
   124|                         IF ((ldimen > 2)) THEN
   126|                           IF ((ntiles[].off140 > 1)) THEN
   127|                             IF ((0 <> ((ntiles[].off140 - &
                &                     coords[].off128 <> 1  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   129|                               nreq = nreq + 1
   130|                               T_10 = nsub + 16700
                                      CALL mpi_isend((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int(iez)) + (&
                &                       %VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)*(&
                &                       1)),nelm,klsm_slice,n3p,T_10,comm3d,(req &
                &                       + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   133|                             ENDIF
   134|                             IF ((0 <> ((coords[].off128 <> 0  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   136|                               nreq = nreq + 1
   137|                               T_11 = nsub + 16700
                                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int((isz - 1))) &
                &                       + (%VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)&
                &                       *(1)),nelm,klsm_slice,n3m,T_11,comm3d,(&
                &                       req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   140|                             ENDIF
   143|                             IF ((0 <> ((coords[].off128 <> 0  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   145|                               nreq = nreq + 1
   146|                               T_12 = nsub + 16800
                                      CALL mpi_isend((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int(isz)) + (&
                &                       %VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)*(&
                &                       1)),nelm,klsm_slice,n3m,T_12,comm3d,(req &
                &                       + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   149|                             ENDIF
   150|                             IF ((0 <> ((ntiles[].off140 - &
                &                     coords[].off128 <> 1  .OR.  &
                &                     periodic[].off116)  .AND.  1))) THEN
   152|                               nreq = nreq + 1
   153|                               T_13 = nsub + 16800
                                      CALL mpi_irecv((x + ((-24) - (%VAL(#5) + &
                &                       %VAL(#4))) + (%VAL(#5))*(int((iez + 1))) &
                &                       + (%VAL(#4))*(1) + (8)*(1) + (8)*(1) + (8)&
                &                       *(1)),nelm,klsm_slice,n3p,T_13,comm3d,(&
                &                       req + (-4) + (4)*(int(%VAL(nreq)))),ierr)
   157|                             ELSE
                                      lab_33
   158|                               IF ((0 <> (periodic[].off116  .AND.  1))) &
                &                       THEN
   159|                                 IF (.FALSE.) GOTO lab_73
                                        $$LoopIV3 = 0
       Id=13                            DO $$LoopIV3 = $$LoopIV3, 1
   160|                                   IF ((0 < int(jn))) THEN
                                            $$LoopIV4 = 0
       Id=14                                DO $$LoopIV4 = $$LoopIV4, int(int(&
                &                               jn))-1
   159|                                       IF ((0 < int(in))) THEN
                                                $$LoopIV5 = 0
       Id=15                                    DO $$LoopIV5 = $$LoopIV5, int(&
                &                                   int(in))-1
                                                  ! DIR_INDEPENDENT loopId = 0 
   161|                                           x(1,1,$$LoopIV5 + 1,$$LoopIV4 &
                &                                   + 1,int((isz - 1)) + (int((&
                &                                   isz - 2)) - int((isz - 1))) * &
                &                                   int(int($$LoopIV3))) = x(1,1,&
                &                                   $$LoopIV5 + 1,$$LoopIV4 + 1,&
                &                                   int(iez) + (int((iez - 1)) - &
                &                                   int(iez)) * int(int($$LoopIV3)&
                &                                   ))
   159|                                         ENDDO
                                              ENDIF
   160|                                     ENDDO
                                          ENDIF
   159|                                 ENDDO
                                        lab_73
                                        IF (.FALSE.) GOTO lab_79
                                        $$LoopIV0 = 0
       Id=16                            DO $$LoopIV0 = $$LoopIV0, 1
   160|                                   IF ((0 < int(jn))) THEN
                                            $$LoopIV1 = 0
       Id=17                                DO $$LoopIV1 = $$LoopIV1, int(int(&
                &                               jn))-1
   159|                                       IF ((0 < int(in))) THEN
                                                $$LoopIV2 = 0
       Id=18                                    DO $$LoopIV2 = $$LoopIV2, int(&
                &                                   int(in))-1
                                                  ! DIR_INDEPENDENT loopId = 0 
   163|                                           x(1,1,$$LoopIV2 + 1,$$LoopIV1 &
                &                                   + 1,int((iez + 1)) + (int((&
                &                                   iez + 2)) - int((iez + 1))) * &
                &                                   int(int($$LoopIV0))) = x(1,1,&
                &                                   $$LoopIV2 + 1,$$LoopIV1 + 1,&
                &                                   int(isz) + (int((isz + 1)) - &
                &                                   int(isz)) * int(int($$LoopIV0)&
                &                                   ))
   159|                                         ENDDO
                                              ENDIF
   160|                                     ENDDO
                                          ENDIF
   159|                                 ENDDO
                                        lab_79
   167|                                 lab_39
   170|                                 lab_32
   171|                                 lab_16
   174|                                 lab_60
                                        RETURN
                                      END SUBROUTINE msendrec_bnd

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- ---- ----
 CCR's set/used:   ss-- ssss
     | 000000                           PDEF     msendrec_bnd
   14|                                  PROC      .isx,.iex,.isy,.iey,.isz,.iez,.x,gr3-gr9
    0| 000000 std      FBE1FFF8   1     ST8       #stack(gr1,-8)=gr31
    0| 000004 std      FBC1FFF0   1     ST8       #stack(gr1,-16)=gr30
    0| 000008 std      FBA1FFE8   1     ST8       #stack(gr1,-24)=gr29
    0| 00000C std      FB81FFE0   1     ST8       #stack(gr1,-32)=gr28
    0| 000010 std      FB61FFD8   1     ST8       #stack(gr1,-40)=gr27
    0| 000014 std      FB41FFD0   1     ST8       #stack(gr1,-48)=gr26
    0| 000018 std      FB21FFC8   1     ST8       #stack(gr1,-56)=gr25
    0| 00001C std      FB01FFC0   1     ST8       #stack(gr1,-64)=gr24
    0| 000020 std      FAE1FFB8   1     ST8       #stack(gr1,-72)=gr23
    0| 000024 std      FAC1FFB0   1     ST8       #stack(gr1,-80)=gr22
    0| 000028 std      FAA1FFA8   1     ST8       #stack(gr1,-88)=gr21
    0| 00002C std      FA81FFA0   1     ST8       #stack(gr1,-96)=gr20
    0| 000030 std      FA61FF98   1     ST8       #stack(gr1,-104)=gr19
    0| 000034 std      FA41FF90   1     ST8       #stack(gr1,-112)=gr18
    0| 000038 std      FA21FF88   1     ST8       #stack(gr1,-120)=gr17
    0| 00003C std      FA01FF80   1     ST8       #stack(gr1,-128)=gr16
    0| 000040 std      F9E1FF78   1     ST8       #stack(gr1,-136)=gr15
    0| 000044 std      F9C1FF70   1     ST8       #stack(gr1,-144)=gr14
    0| 000048 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 00004C mfcr     7D800026   1     LFCR      gr12=cr4,4
    0| 000050 stw      91810008   1     ST4A      #stack(gr1,8)=gr12
    0| 000054 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000058 stdu     F821FE21   1     ST8U      gr1,#stack(gr1,-480)=gr1
    0| 00005C std      F88100C8   1     ST8       #SPILL1(gr1,200)=gr4
    0| 000060 ld       E9420000   1     L8        gr10=.&&N&&param(gr2,0)
   32| 000064 addi     38000001   1     LI        gr0=1
    0| 000068 or       7CBF2B78   1     LR        gr31=gr5
   32| 00006C stw      90010080   1     ST4Z      nelm(gr1,128)=gr0
    0| 000070 std      F8C100D0   1     ST8       #SPILL2(gr1,208)=gr6
   35| 000074 ld       E9620000   1     L8        gr11=.&&N&&mpipar(gr2,0)
    0| 000078 lwa      E88A0002   1     L4A       gr4=<s16:d0:l4>(gr10,0)
    0| 00007C lwa      E80A0006   1     L4A       gr0=<s16:d4:l4>(gr10,4)
    0| 000080 std      F86100C0   1     ST8       #SPILL0(gr1,192)=gr3
    0| 000084 std      F8E100D8   1     ST8       #SPILL3(gr1,216)=gr7
    0| 000088 std      F90100F8   1     ST8       #SPILL7(gr1,248)=gr8
   35| 00008C lwa      E86B0086   1     L4A       gr3=<s33:d132:l4>(gr11,132)
    0| 000090 sradi    7C86FE76   1     SRA8      gr6=gr4,63,ca"
    0| 000094 sradi    7C05FE76   1     SRA8      gr5=gr0,63,ca"
    0| 000098 andc     7C8C3078   1     ANDC      gr12=gr4,gr6
    0| 00009C andc     7C1D2878   1     ANDC      gr29=gr0,gr5
    0| 0000A0 mulld    7F8CE9D2   1     M         gr28=gr12,gr29
   35| 0000A4 cmpwi    2C030001   1     C4        cr0=gr3,1
    0| 0000A8 rldicr   7B9B1F24   1     SLL8      gr27=gr28,3
    0| 0000AC std      F98100E0   1     ST8       #SPILL4(gr1,224)=gr12
    0| 0000B0 std      FBA100E8   1     ST8       #SPILL5(gr1,232)=gr29
    0| 0000B4 std      FB8100F0   1     ST8       #SPILL6(gr1,240)=gr28
    0| 0000B8 std      F9210100   1     ST8       #SPILL8(gr1,256)=gr9
    0| 0000BC rldicr   799E1F24   1     SLL8      gr30=gr12,3
    0| 0000C0 std      FB610108   1     ST8       #SPILL9(gr1,264)=gr27
    0| 0000C4 lwz      808B006C   1     L4Z       gr4=<s33:d108:l4>(gr11,108)
   35| 0000C8 bc       40811018   1     BF        CL.1,cr0,0x2/gt,taken=50%(0,0)
   36| 0000CC lwa      E80B007A   1     L4A       gr0=<s33:d120:l4>(gr11,120)
   36| 0000D0 cntlzw   7C050034   1     CNTLZ4    gr5=gr0
   36| 0000D4 addi     38A5FFE0   1     AI        gr5=gr5,-32
   36| 0000D8 rlwinm   54A50FFE   1     SRL4      gr5=gr5,31
   36| 0000DC or       7C852B78   1     O         gr5=gr4,gr5
   36| 0000E0 rldicl   78A507E1   1     RN8_R     gr5,cr0=gr5,0,0x1
   36| 0000E4 bc       41820074   1     BT        CL.2,cr0,0x4/eq,taken=60%(60,40)
   38| 0000E8 ld       EBA100C0   1     L8        gr29=#SPILL0(gr1,192)
   37| 0000EC lwz      808B0018   1     L4Z       gr4=<s33:d24:l4>(gr11,24)
   38| 0000F0 lwz      80AB001C   1     L4Z       gr5=<s33:d28:l4>(gr11,28)
   38| 0000F4 ld       E8C20000   1     L8        gr6=.&&N&&mpiyes(gr2,0)
   38| 0000F8 addi     3869FFF8   1     AI        gr3=gr9,-8
   38| 0000FC addi     394B0014   1     AI        gr10=gr11,20
   38| 000100 lwa      E81D0002   1     L4A       gr0=isx(gr29,0)
   37| 000104 addi     38840001   1     AI        gr4=gr4,1
   38| 000108 addi     38A53FAC   1     AI        gr5=gr5,16300
   37| 00010C stw      908B0018   1     ST4Z      <s33:d24:l4>(gr11,24)=gr4
   38| 000110 extsw    7C8407B4   1     EXTS4     gr4=gr4
   38| 000114 addi     38C6176C   1     AI        gr6=gr6,5996
   38| 000118 rldicr   78841764   1     SLL8      gr4=gr4,2
   38| 00011C rldicr   78001F24   1     SLL8      gr0=gr0,3
   38| 000120 stw      90A10084   1     ST4Z      T_2(gr1,132)=gr5
   38| 000124 add      7D243214   1     A         gr9=gr4,gr6
   38| 000128 addi     390B0020   1     AI        gr8=gr11,32
   38| 00012C addi     38E10084   1     AI        gr7=gr1,132
   38| 000130 addi     38CB0024   1     AI        gr6=gr11,36
   38| 000134 addi     38AB0060   1     AI        gr5=gr11,96
   38| 000138 addi     38810080   1     AI        gr4=gr1,128
   38| 00013C add      7C601A14   1     A         gr3=gr0,gr3
   38| 000140 bl       48000001   1     CALL      mpi_isend,8,x[]",gr3,nelm",gr4,ilsm_slice",gr5,n1m",gr6,T_2",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,#ProcAlias",mpi_isend",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   38| 000144 ori      60000000   1
    0| 000148 ld       E9620000   1     L8        gr11=.&&N&&mpipar(gr2,0)
    0| 00014C lwa      E86B0086   1     L4A       gr3=<s33:d132:l4>(gr11,132)
    0| 000150 lwa      E80B007A   1     L4A       gr0=<s33:d120:l4>(gr11,120)
    0| 000154 lwa      E88B006E   1     L4A       gr4=<s33:d108:l4>(gr11,108)
   41|                              CL.2:
   42| 000158 subf     7C601850   1     S         gr3=gr3,gr0
   42| 00015C addi     3863FFFF   1     AI        gr3=gr3,-1
   42| 000160 cntlzw   7C630034   1     CNTLZ4    gr3=gr3
   42| 000164 addi     3863FFE0   1     AI        gr3=gr3,-32
   42| 000168 rlwinm   54630FFE   1     SRL4      gr3=gr3,31
   42| 00016C or       7C632378   1     O         gr3=gr3,gr4
   42| 000170 rldicl   786307E1   1     RN8_R     gr3,cr0=gr3,0,0x1
   42| 000174 bc       41820100   1     BT        CL.4,cr0,0x4/eq,taken=60%(60,40)
   45| 000178 ld       EB4100C8   1     L8        gr26=#SPILL1(gr1,200)
   44| 00017C lwz      808B0018   1     L4Z       gr4=<s33:d24:l4>(gr11,24)
   45| 000180 lwz      80AB001C   1     L4Z       gr5=<s33:d28:l4>(gr11,28)
   45| 000184 ld       E8C20000   1     L8        gr6=.&&N&&mpiyes(gr2,0)
   45| 000188 ld       EB210100   1     L8        gr25=#SPILL8(gr1,256)
   45| 00018C addi     38E10088   1     AI        gr7=gr1,136
   45| 000190 lwz      807A0000   1     L4Z       gr3=iex(gr26,0)
   44| 000194 addi     38840001   1     AI        gr4=gr4,1
   45| 000198 addi     38A53FAC   1     AI        gr5=gr5,16300
   44| 00019C stw      908B0018   1     ST4Z      <s33:d24:l4>(gr11,24)=gr4
   45| 0001A0 stw      90A10088   1     ST4Z      T_3(gr1,136)=gr5
   45| 0001A4 extsw    7C8407B4   1     EXTS4     gr4=gr4
   45| 0001A8 addi     38030001   1     AI        gr0=gr3,1
   45| 0001AC addi     3BB9FFF8   1     AI        gr29=gr25,-8
   45| 0001B0 extsw    7C0007B4   1     EXTS4     gr0=gr0
   45| 0001B4 rldicr   78841764   1     SLL8      gr4=gr4,2
   45| 0001B8 addi     3B86176C   1     AI        gr28=gr6,5996
   45| 0001BC rldicr   78001F24   1     SLL8      gr0=gr0,3
   45| 0001C0 extsw    7C7B07B4   1     EXTS4     gr27=gr3
   45| 0001C4 add      7D24E214   1     A         gr9=gr4,gr28
   45| 0001C8 add      7C60EA14   1     A         gr3=gr0,gr29
   45| 0001CC addi     38810080   1     AI        gr4=gr1,128
   45| 0001D0 addi     38AB0060   1     AI        gr5=gr11,96
   45| 0001D4 addi     38CB0028   1     AI        gr6=gr11,40
   45| 0001D8 addi     390B0020   1     AI        gr8=gr11,32
   45| 0001DC addi     394B0014   1     AI        gr10=gr11,20
   45| 0001E0 bl       48000001   1     CALL      mpi_irecv,8,x[]",gr3,nelm",gr4,ilsm_slice",gr5,n1p",gr6,T_3",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,mpi_irecv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   45| 0001E4 ori      60000000   1
    0| 0001E8 ld       E9620000   1     L8        gr11=.&&N&&mpipar(gr2,0)
    0| 0001EC lwz      806B0084   1     L4Z       gr3=<s33:d132:l4>(gr11,132)
    0| 0001F0 lwz      800B0078   1     L4Z       gr0=<s33:d120:l4>(gr11,120)
    0| 0001F4 lwz      808B006C   1     L4Z       gr4=<s33:d108:l4>(gr11,108)
    0| 0001F8 subf     7C601850   1     S         gr3=gr3,gr0
    0| 0001FC extsw    7C0007B4   1     EXTS4     gr0=gr0
    0| 000200 addi     3863FFFF   1     AI        gr3=gr3,-1
    0| 000204 cntlzw   7C630034   1     CNTLZ4    gr3=gr3
    0| 000208 addi     3863FFE0   1     AI        gr3=gr3,-32
    0| 00020C rlwinm   54630FFE   1     SRL4      gr3=gr3,31
    0| 000210 or       7C632378   1     O         gr3=gr3,gr4
   51| 000214 rldicl   786307E1   1     RN8_R     gr3,cr0=gr3,0,0x1
   51| 000218 bc       4182005C   1     BT        CL.4,cr0,0x4/eq,taken=60%(60,40)
   53| 00021C lwz      80AB0018   1     L4Z       gr5=<s33:d24:l4>(gr11,24)
   54| 000220 rldicr   7B601F24   1     SLL8      gr0=gr27,3
   54| 000224 lwz      80CB001C   1     L4Z       gr6=<s33:d28:l4>(gr11,28)
   54| 000228 add      7C60EA14   1     A         gr3=gr0,gr29
   54| 00022C addi     38E1008C   1     AI        gr7=gr1,140
   54| 000230 addi     38810080   1     AI        gr4=gr1,128
   53| 000234 addi     38050001   1     AI        gr0=gr5,1
   54| 000238 addi     390B0020   1     AI        gr8=gr11,32
   53| 00023C stw      900B0018   1     ST4Z      <s33:d24:l4>(gr11,24)=gr0
   54| 000240 extsw    7C0007B4   1     EXTS4     gr0=gr0
   54| 000244 addi     38A64010   1     AI        gr5=gr6,16400
   54| 000248 rldicr   78001764   1     SLL8      gr0=gr0,2
   54| 00024C stw      90A1008C   1     ST4Z      T_4(gr1,140)=gr5
   54| 000250 addi     38AB0060   1     AI        gr5=gr11,96
   54| 000254 add      7D20E214   1     A         gr9=gr0,gr28
   54| 000258 addi     38CB0028   1     AI        gr6=gr11,40
   54| 00025C addi     394B0014   1     AI        gr10=gr11,20
   54| 000260 bl       48000001   1     CALL      mpi_isend,8,x[]",gr3,nelm",gr4,ilsm_slice",gr5,n1p",gr6,T_4",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,#ProcAlias",mpi_isend",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   54| 000264 ori      60000000   1
    0| 000268 ld       E9620000   1     L8        gr11=.&&N&&mpipar(gr2,0)
    0| 00026C lwa      E80B007A   1     L4A       gr0=<s33:d120:l4>(gr11,120)
    0| 000270 lwa      E88B006E   1     L4A       gr4=<s33:d108:l4>(gr11,108)
   57|                              CL.4:
   58| 000274 cntlzw   7C030034   1     CNTLZ4    gr3=gr0
   58| 000278 addi     3803FFE0   1     AI        gr0=gr3,-32
   58| 00027C rlwinm   54000FFE   1     SRL4      gr0=gr0,31
   58| 000280 or       7C002378   1     O         gr0=gr0,gr4
   58| 000284 rldicl   780007E1   1     RN8_R     gr0,cr0=gr0,0,0x1
   58| 000288 bc       41820070   1     BT        CL.6,cr0,0x4/eq,taken=60%(60,40)
   61| 00028C ld       EBA100C0   1     L8        gr29=#SPILL0(gr1,192)
   60| 000290 lwz      808B0018   1     L4Z       gr4=<s33:d24:l4>(gr11,24)
   61| 000294 lwz      80CB001C   1     L4Z       gr6=<s33:d28:l4>(gr11,28)
   61| 000298 ld       E8E20000   1     L8        gr7=.&&N&&mpiyes(gr2,0)
   61| 00029C ld       EB810100   1     L8        gr28=#SPILL8(gr1,256)
   61| 0002A0 addi     394B0014   1     AI        gr10=gr11,20
   61| 0002A4 lwz      80BD0000   1     L4Z       gr5=isx(gr29,0)
   60| 0002A8 addi     38040001   1     AI        gr0=gr4,1
   61| 0002AC addi     390B0020   1     AI        gr8=gr11,32
   60| 0002B0 stw      900B0018   1     ST4Z      <s33:d24:l4>(gr11,24)=gr0
   61| 0002B4 addi     387CFFF8   1     AI        gr3=gr28,-8
   61| 0002B8 addi     3885FFFF   1     AI        gr4=gr5,-1
   61| 0002BC extsw    7C0507B4   1     EXTS4     gr5=gr0
   61| 0002C0 extsw    7C8007B4   1     EXTS4     gr0=gr4
   61| 0002C4 addi     38864010   1     AI        gr4=gr6,16400
   61| 0002C8 rldicr   78A51764   1     SLL8      gr5=gr5,2
   61| 0002CC stw      90810090   1     ST4Z      T_5(gr1,144)=gr4
   61| 0002D0 addi     38C7176C   1     AI        gr6=gr7,5996
   61| 0002D4 rldicr   78001F24   1     SLL8      gr0=gr0,3
   61| 0002D8 add      7D253214   1     A         gr9=gr5,gr6
   61| 0002DC addi     38E10090   1     AI        gr7=gr1,144
   61| 0002E0 addi     38CB0024   1     AI        gr6=gr11,36
   61| 0002E4 addi     38AB0060   1     AI        gr5=gr11,96
   61| 0002E8 addi     38810080   1     AI        gr4=gr1,128
   61| 0002EC add      7C601A14   1     A         gr3=gr0,gr3
   61| 0002F0 bl       48000001   1     CALL      mpi_irecv,8,x[]",gr3,nelm",gr4,ilsm_slice",gr5,n1m",gr6,T_5",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,mpi_irecv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   61| 0002F4 ori      60000000   1
   76|                              CL.6:
   78| 0002F8 ld       EBA20000   1     L8        gr29=.&&N&&config(gr2,0)
   78| 0002FC lwa      E81D0006   1     L4A       gr0=<s54:d4:l4>(gr29,4)
   78| 000300 cmpwi    2C000001   1     C4        cr0=gr0,1
   78| 000304 bc       4081043C   1     BF        CL.160,cr0,0x2/gt,taken=30%(30,70)
   80| 000308 ld       E8A20000   1     L8        gr5=.&&N&&mpipar(gr2,0)
   80| 00030C lwa      E885008A   1     L4A       gr4=<s33:d136:l4>(gr5,136)
    0| 000310 lwz      80650070   1     L4Z       gr3=<s33:d112:l4>(gr5,112)
   80| 000314 cmpwi    2C040001   1     C4        cr0=gr4,1
   80| 000318 bc       40810828   1     BF        CL.17,cr0,0x2/gt,taken=50%(0,0)
   81| 00031C lwa      E805007E   1     L4A       gr0=<s33:d124:l4>(gr5,124)
   81| 000320 cntlzw   7C050034   1     CNTLZ4    gr5=gr0
   81| 000324 addi     38A5FFE0   1     AI        gr5=gr5,-32
   81| 000328 rlwinm   54A50FFE   1     SRL4      gr5=gr5,31
   81| 00032C or       7C652B78   1     O         gr5=gr3,gr5
   81| 000330 rldicl   78A507E1   1     RN8_R     gr5,cr0=gr5,0,0x1
   81| 000334 bc       41820074   1     BT        CL.18,cr0,0x4/eq,taken=60%(60,40)
   83| 000338 ld       EB820000   1     L8        gr28=.&&N&&mpipar(gr2,0)
   84| 00033C lwa      E87F0002   1     L4A       gr3=isy(gr31,0)
   84| 000340 ld       E8C20000   1     L8        gr6=.&&N&&mpiyes(gr2,0)
   84| 000344 ld       EB610100   1     L8        gr27=#SPILL8(gr1,256)
   84| 000348 addi     38E10094   1     AI        gr7=gr1,148
   83| 00034C lwz      809C0018   1     L4Z       gr4=<s33:d24:l4>(gr28,24)
   84| 000350 lwz      80BC001C   1     L4Z       gr5=<s33:d28:l4>(gr28,28)
   84| 000354 mulld    7C63F1D2   1     M         gr3=gr3,gr30
   83| 000358 addi     38840001   1     AI        gr4=gr4,1
   84| 00035C subf     7C1ED850   1     S         gr0=gr27,gr30
   83| 000360 stw      909C0018   1     ST4Z      <s33:d24:l4>(gr28,24)=gr4
   84| 000364 extsw    7C8407B4   1     EXTS4     gr4=gr4
   84| 000368 addi     38C6176C   1     AI        gr6=gr6,5996
   84| 00036C rldicr   78841764   1     SLL8      gr4=gr4,2
   84| 000370 addi     38A54074   1     AI        gr5=gr5,16500
   84| 000374 add      7D243214   1     A         gr9=gr4,gr6
   84| 000378 addi     38810080   1     AI        gr4=gr1,128
   84| 00037C add      7C630214   1     A         gr3=gr3,gr0
   84| 000380 stw      90A10094   1     ST4Z      T_6(gr1,148)=gr5
   84| 000384 addi     395C0014   1     AI        gr10=gr28,20
   84| 000388 addi     391C0020   1     AI        gr8=gr28,32
   84| 00038C addi     38DC002C   1     AI        gr6=gr28,44
   84| 000390 addi     38BC0064   1     AI        gr5=gr28,100
   84| 000394 bl       48000001   1     CALL      mpi_isend,8,x[]",gr3,nelm",gr4,jlsm_slice",gr5,n2m",gr6,T_6",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,#ProcAlias",mpi_isend",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   84| 000398 ori      60000000   1
    0| 00039C lwa      E89C008A   1     L4A       gr4=<s33:d136:l4>(gr28,136)
    0| 0003A0 lwa      E81C007E   1     L4A       gr0=<s33:d124:l4>(gr28,124)
    0| 0003A4 lwa      E87C0072   1     L4A       gr3=<s33:d112:l4>(gr28,112)
   87|                              CL.18:
   88| 0003A8 subf     7C802050   1     S         gr4=gr4,gr0
   88| 0003AC addi     3884FFFF   1     AI        gr4=gr4,-1
   88| 0003B0 cntlzw   7C840034   1     CNTLZ4    gr4=gr4
   88| 0003B4 addi     3884FFE0   1     AI        gr4=gr4,-32
   88| 0003B8 rlwinm   54840FFE   1     SRL4      gr4=gr4,31
   88| 0003BC or       7C642378   1     O         gr4=gr3,gr4
   88| 0003C0 rldicl   788407E1   1     RN8_R     gr4,cr0=gr4,0,0x1
   88| 0003C4 bc       418200FC   1     BT        CL.20,cr0,0x4/eq,taken=60%(60,40)
   91| 0003C8 ld       EB2100D0   1     L8        gr25=#SPILL2(gr1,208)
   90| 0003CC ld       EB020000   1     L8        gr24=.&&N&&mpipar(gr2,0)
   91| 0003D0 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
   91| 0003D4 ld       EAE10100   1     L8        gr23=#SPILL8(gr1,256)
   91| 0003D8 addi     38E10098   1     AI        gr7=gr1,152
   91| 0003DC lwz      80790000   1     L4Z       gr3=iey(gr25,0)
   90| 0003E0 lwz      80980018   1     L4Z       gr4=<s33:d24:l4>(gr24,24)
   91| 0003E4 lwz      80D8001C   1     L4Z       gr6=<s33:d28:l4>(gr24,28)
   91| 0003E8 addi     3B65176C   1     AI        gr27=gr5,5996
   91| 0003EC subf     7F9EB850   1     S         gr28=gr23,gr30
   91| 0003F0 addi     38B80064   1     AI        gr5=gr24,100
   91| 0003F4 addi     38030001   1     AI        gr0=gr3,1
   90| 0003F8 addi     38840001   1     AI        gr4=gr4,1
   91| 0003FC extsw    7C0007B4   1     EXTS4     gr0=gr0
   90| 000400 stw      90980018   1     ST4Z      <s33:d24:l4>(gr24,24)=gr4
   91| 000404 mulld    7C00F1D2   1     M         gr0=gr0,gr30
   91| 000408 extsw    7C8407B4   1     EXTS4     gr4=gr4
   91| 00040C addi     38C64074   1     AI        gr6=gr6,16500
   91| 000410 rldicr   78841764   1     SLL8      gr4=gr4,2
   91| 000414 stw      90C10098   1     ST4Z      T_7(gr1,152)=gr6
   91| 000418 add      7D24DA14   1     A         gr9=gr4,gr27
   91| 00041C addi     38810080   1     AI        gr4=gr1,128
   91| 000420 extsw    7C7A07B4   1     EXTS4     gr26=gr3
   91| 000424 add      7C60E214   1     A         gr3=gr0,gr28
   91| 000428 addi     38D80030   1     AI        gr6=gr24,48
   91| 00042C addi     39180020   1     AI        gr8=gr24,32
   91| 000430 addi     39580014   1     AI        gr10=gr24,20
   91| 000434 bl       48000001   1     CALL      mpi_irecv,8,x[]",gr3,nelm",gr4,jlsm_slice",gr5,n2p",gr6,T_7",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,mpi_irecv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   91| 000438 ori      60000000   1
    0| 00043C lwz      80980088   1     L4Z       gr4=<s33:d136:l4>(gr24,136)
    0| 000440 lwz      8018007C   1     L4Z       gr0=<s33:d124:l4>(gr24,124)
    0| 000444 lwz      80780070   1     L4Z       gr3=<s33:d112:l4>(gr24,112)
    0| 000448 subf     7C802050   1     S         gr4=gr4,gr0
    0| 00044C extsw    7C0007B4   1     EXTS4     gr0=gr0
    0| 000450 addi     3884FFFF   1     AI        gr4=gr4,-1
    0| 000454 cntlzw   7C840034   1     CNTLZ4    gr4=gr4
    0| 000458 addi     3884FFE0   1     AI        gr4=gr4,-32
    0| 00045C rlwinm   54840FFE   1     SRL4      gr4=gr4,31
    0| 000460 or       7C642378   1     O         gr4=gr3,gr4
   97| 000464 rldicl   788407E1   1     RN8_R     gr4,cr0=gr4,0,0x1
   97| 000468 bc       41820058   1     BT        CL.20,cr0,0x4/eq,taken=60%(60,40)
   99| 00046C lwz      80780018   1     L4Z       gr3=<s33:d24:l4>(gr24,24)
  100| 000470 lwz      8098001C   1     L4Z       gr4=<s33:d28:l4>(gr24,28)
  100| 000474 mulld    7C1AF1D2   1     M         gr0=gr26,gr30
   99| 000478 addi     38630001   1     AI        gr3=gr3,1
  100| 00047C addi     388440D8   1     AI        gr4=gr4,16600
   99| 000480 stw      90780018   1     ST4Z      <s33:d24:l4>(gr24,24)=gr3
  100| 000484 stw      9081009C   1     ST4Z      T_8(gr1,156)=gr4
  100| 000488 extsw    7C6307B4   1     EXTS4     gr3=gr3
  100| 00048C addi     38E1009C   1     AI        gr7=gr1,156
  100| 000490 rldicr   78631764   1     SLL8      gr3=gr3,2
  100| 000494 addi     38810080   1     AI        gr4=gr1,128
  100| 000498 add      7D23DA14   1     A         gr9=gr3,gr27
  100| 00049C add      7C60E214   1     A         gr3=gr0,gr28
  100| 0004A0 addi     38B80064   1     AI        gr5=gr24,100
  100| 0004A4 addi     38D80030   1     AI        gr6=gr24,48
  100| 0004A8 addi     39180020   1     AI        gr8=gr24,32
  100| 0004AC addi     39580014   1     AI        gr10=gr24,20
  100| 0004B0 bl       48000001   1     CALL      mpi_isend,8,x[]",gr3,nelm",gr4,jlsm_slice",gr5,n2p",gr6,T_8",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,#ProcAlias",mpi_isend",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  100| 0004B4 ori      60000000   1
    0| 0004B8 lwa      E818007E   1     L4A       gr0=<s33:d124:l4>(gr24,124)
    0| 0004BC lwa      E8780072   1     L4A       gr3=<s33:d112:l4>(gr24,112)
  103|                              CL.20:
  104| 0004C0 cntlzw   7C040034   1     CNTLZ4    gr4=gr0
  104| 0004C4 addi     3804FFE0   1     AI        gr0=gr4,-32
  104| 0004C8 rlwinm   54000FFE   1     SRL4      gr0=gr0,31
  104| 0004CC or       7C001B78   1     O         gr0=gr0,gr3
  104| 0004D0 rldicl   780007E1   1     RN8_R     gr0,cr0=gr0,0,0x1
  104| 0004D4 bc       4082000C   1     BF        CL.1125,cr0,0x4/eq,taken=40%(40,60)
    0| 0004D8 lwa      E81D0006   1     L4A       gr0=<s54:d4:l4>(gr29,4)
  111| 0004DC b        48000074   1     B         CL.22,-1
    0|                              CL.1125:
  107| 0004E0 lwz      807F0000   1     L4Z       gr3=isy(gr31,0)
  106| 0004E4 ld       EBE20000   1     L8        gr31=.&&N&&mpipar(gr2,0)
  107| 0004E8 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  107| 0004EC ld       EB810100   1     L8        gr28=#SPILL8(gr1,256)
  107| 0004F0 addi     3863FFFF   1     AI        gr3=gr3,-1
  106| 0004F4 lwz      80DF0018   1     L4Z       gr6=<s33:d24:l4>(gr31,24)
  107| 0004F8 extsw    7C6307B4   1     EXTS4     gr3=gr3
  107| 0004FC lwz      809F001C   1     L4Z       gr4=<s33:d28:l4>(gr31,28)
  107| 000500 mulld    7C63F1D2   1     M         gr3=gr3,gr30
  106| 000504 addi     38C60001   1     AI        gr6=gr6,1
  107| 000508 subf     7C1EE050   1     S         gr0=gr28,gr30
  106| 00050C stw      90DF0018   1     ST4Z      <s33:d24:l4>(gr31,24)=gr6
  107| 000510 extsw    7CC707B4   1     EXTS4     gr7=gr6
  107| 000514 addi     388440D8   1     AI        gr4=gr4,16600
  107| 000518 rldicr   78E61764   1     SLL8      gr6=gr7,2
  107| 00051C stw      908100A0   1     ST4Z      T_9(gr1,160)=gr4
  107| 000520 addi     38A5176C   1     AI        gr5=gr5,5996
  107| 000524 addi     395F0014   1     AI        gr10=gr31,20
  107| 000528 add      7D253214   1     A         gr9=gr5,gr6
  107| 00052C addi     391F0020   1     AI        gr8=gr31,32
  107| 000530 addi     38E100A0   1     AI        gr7=gr1,160
  107| 000534 addi     38DF002C   1     AI        gr6=gr31,44
  107| 000538 addi     38BF0064   1     AI        gr5=gr31,100
  107| 00053C addi     38810080   1     AI        gr4=gr1,128
  107| 000540 add      7C630214   1     A         gr3=gr3,gr0
  107| 000544 bl       48000001   1     CALL      mpi_irecv,8,x[]",gr3,nelm",gr4,jlsm_slice",gr5,n2m",gr6,T_9",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,mpi_irecv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000548 ori      60000000   1
    0| 00054C lwa      E81D0006   1     L4A       gr0=<s54:d4:l4>(gr29,4)
  122|                              CL.22:
  124| 000550 cmpwi    2C000002   1     C4        cr0=gr0,2
  124| 000554 bc       408101EC   1     BF        CL.160,cr0,0x2/gt,taken=30%(30,70)
  126| 000558 ld       E8820000   1     L8        gr4=.&&N&&mpipar(gr2,0)
  126| 00055C lwz      8064008C   1     L4Z       gr3=<s33:d140:l4>(gr4,140)
    0| 000560 lwz      80040074   1     L4Z       gr0=<s33:d116:l4>(gr4,116)
  126| 000564 cmpwi    2C030001   1     C4        cr0=gr3,1
  126| 000568 bc       408102B0   1     BF        CL.33,cr0,0x2/gt,taken=50%(0,0)
  127| 00056C lwz      80840080   1     L4Z       gr4=<s33:d128:l4>(gr4,128)
  127| 000570 subf     7C641850   1     S         gr3=gr3,gr4
  127| 000574 extsw    7C8407B4   1     EXTS4     gr4=gr4
  127| 000578 addi     3863FFFF   1     AI        gr3=gr3,-1
  127| 00057C cntlzw   7C630034   1     CNTLZ4    gr3=gr3
  127| 000580 addi     3863FFE0   1     AI        gr3=gr3,-32
  127| 000584 rlwinm   54630FFE   1     SRL4      gr3=gr3,31
  127| 000588 or       7C031B78   1     O         gr3=gr0,gr3
  127| 00058C rldicl   786307E1   1     RN8_R     gr3,cr0=gr3,0,0x1
  127| 000590 bc       41820078   1     BT        CL.34,cr0,0x4/eq,taken=60%(60,40)
  130| 000594 ld       EBE100F8   1     L8        gr31=#SPILL7(gr1,248)
  129| 000598 ld       EBC20000   1     L8        gr30=.&&N&&mpipar(gr2,0)
  130| 00059C ld       EB810108   1     L8        gr28=#SPILL9(gr1,264)
  130| 0005A0 ld       E8C20000   1     L8        gr6=.&&N&&mpiyes(gr2,0)
  130| 0005A4 ld       EBA10100   1     L8        gr29=#SPILL8(gr1,256)
  130| 0005A8 addi     38E100A4   1     AI        gr7=gr1,164
  130| 0005AC lwa      E87F0002   1     L4A       gr3=iez(gr31,0)
  129| 0005B0 lwz      809E0018   1     L4Z       gr4=<s33:d24:l4>(gr30,24)
  130| 0005B4 lwz      80BE001C   1     L4Z       gr5=<s33:d28:l4>(gr30,28)
  130| 0005B8 addi     38C6176C   1     AI        gr6=gr6,5996
  130| 0005BC subf     7C1CE850   1     S         gr0=gr29,gr28
  130| 0005C0 addi     395E0014   1     AI        gr10=gr30,20
  130| 0005C4 mulld    7C63E1D2   1     M         gr3=gr3,gr28
  129| 0005C8 addi     38840001   1     AI        gr4=gr4,1
  130| 0005CC addi     38A5413C   1     AI        gr5=gr5,16700
  129| 0005D0 stw      909E0018   1     ST4Z      <s33:d24:l4>(gr30,24)=gr4
  130| 0005D4 stw      90A100A4   1     ST4Z      T_10(gr1,164)=gr5
  130| 0005D8 extsw    7C8407B4   1     EXTS4     gr4=gr4
  130| 0005DC addi     391E0020   1     AI        gr8=gr30,32
  130| 0005E0 rldicr   78841764   1     SLL8      gr4=gr4,2
  130| 0005E4 addi     38BE0068   1     AI        gr5=gr30,104
  130| 0005E8 add      7D243214   1     A         gr9=gr4,gr6
  130| 0005EC addi     38810080   1     AI        gr4=gr1,128
  130| 0005F0 addi     38DE0038   1     AI        gr6=gr30,56
  130| 0005F4 add      7C630214   1     A         gr3=gr3,gr0
  130| 0005F8 bl       48000001   1     CALL      mpi_isend,8,x[]",gr3,nelm",gr4,klsm_slice",gr5,n3p",gr6,T_10",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,#ProcAlias",mpi_isend",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  130| 0005FC ori      60000000   1
    0| 000600 lwa      E89E0082   1     L4A       gr4=<s33:d128:l4>(gr30,128)
    0| 000604 lwa      E81E0076   1     L4A       gr0=<s33:d116:l4>(gr30,116)
  133|                              CL.34:
  134| 000608 cntlzw   7C830034   1     CNTLZ4    gr3=gr4
  134| 00060C addi     3863FFE0   1     AI        gr3=gr3,-32
  134| 000610 rlwinm   54630FFE   1     SRL4      gr3=gr3,31
  134| 000614 or       7C031B78   1     O         gr3=gr0,gr3
  134| 000618 rldicl   786307E1   1     RN8_R     gr3,cr0=gr3,0,0x1
  134| 00061C bc       418200F0   1     BT        CL.36,cr0,0x4/eq,taken=60%(60,40)
  137| 000620 ld       EB8100D8   1     L8        gr28=#SPILL3(gr1,216)
  136| 000624 ld       EB620000   1     L8        gr27=.&&N&&mpipar(gr2,0)
  137| 000628 ld       EB210108   1     L8        gr25=#SPILL9(gr1,264)
  137| 00062C ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  137| 000630 ld       EB410100   1     L8        gr26=#SPILL8(gr1,256)
  137| 000634 addi     38E100A8   1     AI        gr7=gr1,168
  137| 000638 lwz      807C0000   1     L4Z       gr3=isz(gr28,0)
  136| 00063C lwz      809B0018   1     L4Z       gr4=<s33:d24:l4>(gr27,24)
  137| 000640 lwz      80DB001C   1     L4Z       gr6=<s33:d28:l4>(gr27,28)
  137| 000644 addi     3BC5176C   1     AI        gr30=gr5,5996
  137| 000648 subf     7FF9D050   1     S         gr31=gr26,gr25
  137| 00064C addi     38BB0068   1     AI        gr5=gr27,104
  137| 000650 addi     3803FFFF   1     AI        gr0=gr3,-1
  136| 000654 addi     38840001   1     AI        gr4=gr4,1
  137| 000658 extsw    7C0007B4   1     EXTS4     gr0=gr0
  136| 00065C stw      909B0018   1     ST4Z      <s33:d24:l4>(gr27,24)=gr4
  137| 000660 mulld    7C00C9D2   1     M         gr0=gr0,gr25
  137| 000664 extsw    7C8407B4   1     EXTS4     gr4=gr4
  137| 000668 addi     38C6413C   1     AI        gr6=gr6,16700
  137| 00066C rldicr   78841764   1     SLL8      gr4=gr4,2
  137| 000670 stw      90C100A8   1     ST4Z      T_11(gr1,168)=gr6
  137| 000674 add      7D24F214   1     A         gr9=gr4,gr30
  137| 000678 addi     38810080   1     AI        gr4=gr1,128
  137| 00067C extsw    7C7D07B4   1     EXTS4     gr29=gr3
  137| 000680 add      7C60FA14   1     A         gr3=gr0,gr31
  137| 000684 addi     38DB0034   1     AI        gr6=gr27,52
  137| 000688 addi     391B0020   1     AI        gr8=gr27,32
  137| 00068C addi     395B0014   1     AI        gr10=gr27,20
  137| 000690 bl       48000001   1     CALL      mpi_irecv,8,x[]",gr3,nelm",gr4,klsm_slice",gr5,n3m",gr6,T_11",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,mpi_irecv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  137| 000694 ori      60000000   1
    0| 000698 lwa      E89B0082   1     L4A       gr4=<s33:d128:l4>(gr27,128)
    0| 00069C lwz      801B0074   1     L4Z       gr0=<s33:d116:l4>(gr27,116)
    0| 0006A0 cntlzw   7C830034   1     CNTLZ4    gr3=gr4
    0| 0006A4 addi     3863FFE0   1     AI        gr3=gr3,-32
    0| 0006A8 rlwinm   54630FFE   1     SRL4      gr3=gr3,31
    0| 0006AC or       7C031B78   1     O         gr3=gr0,gr3
  143| 0006B0 rldicl   786307E1   1     RN8_R     gr3,cr0=gr3,0,0x1
  143| 0006B4 bc       41820058   1     BT        CL.36,cr0,0x4/eq,taken=60%(60,40)
  145| 0006B8 lwz      807B0018   1     L4Z       gr3=<s33:d24:l4>(gr27,24)
  146| 0006BC lwz      809B001C   1     L4Z       gr4=<s33:d28:l4>(gr27,28)
  146| 0006C0 mulld    7C19E9D2   1     M         gr0=gr25,gr29
  145| 0006C4 addi     38630001   1     AI        gr3=gr3,1
  146| 0006C8 addi     388441A0   1     AI        gr4=gr4,16800
  145| 0006CC stw      907B0018   1     ST4Z      <s33:d24:l4>(gr27,24)=gr3
  146| 0006D0 stw      908100AC   1     ST4Z      T_12(gr1,172)=gr4
  146| 0006D4 extsw    7C6307B4   1     EXTS4     gr3=gr3
  146| 0006D8 addi     38810080   1     AI        gr4=gr1,128
  146| 0006DC rldicr   78631764   1     SLL8      gr3=gr3,2
  146| 0006E0 addi     38E100AC   1     AI        gr7=gr1,172
  146| 0006E4 add      7D23F214   1     A         gr9=gr3,gr30
  146| 0006E8 add      7C60FA14   1     A         gr3=gr0,gr31
  146| 0006EC addi     38BB0068   1     AI        gr5=gr27,104
  146| 0006F0 addi     38DB0034   1     AI        gr6=gr27,52
  146| 0006F4 addi     391B0020   1     AI        gr8=gr27,32
  146| 0006F8 addi     395B0014   1     AI        gr10=gr27,20
  146| 0006FC bl       48000001   1     CALL      mpi_isend,8,x[]",gr3,nelm",gr4,klsm_slice",gr5,n3m",gr6,T_12",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,#ProcAlias",mpi_isend",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  146| 000700 ori      60000000   1
    0| 000704 lwa      E89B0082   1     L4A       gr4=<s33:d128:l4>(gr27,128)
    0| 000708 lwa      E81B0076   1     L4A       gr0=<s33:d116:l4>(gr27,116)
  149|                              CL.36:
  150| 00070C ld       E8A20000   1     L8        gr5=.&&N&&mpipar(gr2,0)
  150| 000710 lwz      8065008C   1     L4Z       gr3=<s33:d140:l4>(gr5,140)
  150| 000714 subf     7C641850   1     S         gr3=gr3,gr4
  150| 000718 addi     3863FFFF   1     AI        gr3=gr3,-1
  150| 00071C cntlzw   7C630034   1     CNTLZ4    gr3=gr3
  150| 000720 addi     3863FFE0   1     AI        gr3=gr3,-32
  150| 000724 rlwinm   54630FFE   1     SRL4      gr3=gr3,31
  150| 000728 or       7C001B78   1     O         gr0=gr0,gr3
  150| 00072C rldicl   780007E1   1     RN8_R     gr0,cr0=gr0,0,0x1
  150| 000730 bc       40820070   1     BF        CL.1128,cr0,0x4/eq,taken=40%(40,60)
    0| 000734 ori      60210000   1     XNOP      
    0| 000738 ori      60210000   1     XNOP      
    0| 00073C ori      60210000   1     XNOP      
  174|                              CL.160:
  174| 000740 ld       E80101F0   1     L8        gr0=#stack(gr1,496)
  174| 000744 lwa      E98101EA   1     L4A       gr12=#stack(gr1,488)
  174| 000748 addi     382101E0   1     AI        gr1=gr1,480
  174| 00074C ld       E9C1FF70   1     L8        gr14=#stack(gr1,-144)
  174| 000750 ld       E9E1FF78   1     L8        gr15=#stack(gr1,-136)
  174| 000754 mtspr    7C0803A6   1     LLR       lr=gr0
  174| 000758 mtcrf    7D808120   1     MTCRF     cr4=gr12
  174| 00075C ld       EA01FF80   1     L8        gr16=#stack(gr1,-128)
  174| 000760 ld       EA21FF88   1     L8        gr17=#stack(gr1,-120)
  174| 000764 ld       EA41FF90   1     L8        gr18=#stack(gr1,-112)
  174| 000768 ld       EA61FF98   1     L8        gr19=#stack(gr1,-104)
  174| 00076C ld       EA81FFA0   1     L8        gr20=#stack(gr1,-96)
  174| 000770 ld       EAA1FFA8   1     L8        gr21=#stack(gr1,-88)
  174| 000774 ld       EAC1FFB0   1     L8        gr22=#stack(gr1,-80)
  174| 000778 ld       EAE1FFB8   1     L8        gr23=#stack(gr1,-72)
  174| 00077C ld       EB01FFC0   1     L8        gr24=#stack(gr1,-64)
  174| 000780 ld       EB21FFC8   1     L8        gr25=#stack(gr1,-56)
  174| 000784 ld       EB41FFD0   1     L8        gr26=#stack(gr1,-48)
  174| 000788 ld       EB61FFD8   1     L8        gr27=#stack(gr1,-40)
  174| 00078C ld       EB81FFE0   1     L8        gr28=#stack(gr1,-32)
  174| 000790 ld       EBA1FFE8   1     L8        gr29=#stack(gr1,-24)
  174| 000794 ld       EBC1FFF0   1     L8        gr30=#stack(gr1,-16)
  174| 000798 ld       EBE1FFF8   1     L8        gr31=#stack(gr1,-8)
  174| 00079C bclr     4E800020   1     BA        lr
    0|                              CL.1128:
  153| 0007A0 ld       EBE100F8   1     L8        gr31=#SPILL7(gr1,248)
  153| 0007A4 ld       EBA10108   1     L8        gr29=#SPILL9(gr1,264)
  152| 0007A8 lwz      80C50018   1     L4Z       gr6=<s33:d24:l4>(gr5,24)
  153| 0007AC ld       EB820000   1     L8        gr28=.&&N&&mpipar(gr2,0)
  153| 0007B0 lwz      8085001C   1     L4Z       gr4=<s33:d28:l4>(gr5,28)
  153| 0007B4 ld       E8A20000   1     L8        gr5=.&&N&&mpiyes(gr2,0)
  153| 0007B8 lwz      807F0000   1     L4Z       gr3=iez(gr31,0)
  153| 0007BC ld       EBC10100   1     L8        gr30=#SPILL8(gr1,256)
  152| 0007C0 addi     38C60001   1     AI        gr6=gr6,1
  153| 0007C4 addi     395C0014   1     AI        gr10=gr28,20
  152| 0007C8 stw      90DC0018   1     ST4Z      <s33:d24:l4>(gr28,24)=gr6
  153| 0007CC extsw    7CC607B4   1     EXTS4     gr6=gr6
  153| 0007D0 addi     38630001   1     AI        gr3=gr3,1
  153| 0007D4 subf     7C1DF050   1     S         gr0=gr30,gr29
  153| 0007D8 extsw    7C6307B4   1     EXTS4     gr3=gr3
  153| 0007DC addi     388441A0   1     AI        gr4=gr4,16800
  153| 0007E0 mulld    7C63E9D2   1     M         gr3=gr3,gr29
  153| 0007E4 stw      908100B0   1     ST4Z      T_13(gr1,176)=gr4
  153| 0007E8 rldicr   78C61764   1     SLL8      gr6=gr6,2
  153| 0007EC addi     38A5176C   1     AI        gr5=gr5,5996
  153| 0007F0 addi     391C0020   1     AI        gr8=gr28,32
  153| 0007F4 add      7D253214   1     A         gr9=gr5,gr6
  153| 0007F8 addi     38E100B0   1     AI        gr7=gr1,176
  153| 0007FC addi     38DC0038   1     AI        gr6=gr28,56
  153| 000800 addi     38BC0068   1     AI        gr5=gr28,104
  153| 000804 addi     38810080   1     AI        gr4=gr1,128
  153| 000808 add      7C630214   1     A         gr3=gr3,gr0
  153| 00080C bl       48000001   1     CALL      mpi_irecv,8,x[]",gr3,nelm",gr4,klsm_slice",gr5,n3p",gr6,T_13",gr7,comm3d",gr8,req[]",gr9,ierr",gr10,mpi_irecv",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  153| 000810 ori      60000000   1
  157| 000814 b        4BFFFF2C   1     B         CL.160,-1
  157|                              CL.33:
  158| 000818 andi.    70000001   1     RN4_R     gr0,cr0=gr0,0,0x1
  158| 00081C bc       4182FF24   1     BT        CL.160,cr0,0x4/eq,taken=30%(30,70)
  160| 000820 ld       E8620000   1     L8        gr3=.&&N&&param(gr2,0)
  160| 000824 lwa      E8030006   1     L4A       gr0=<s16:d4:l4>(gr3,4)
  159| 000828 lwa      E9030002   1     L4A       gr8=<s16:d0:l4>(gr3,0)
    0| 00082C cmpwi    2C000000   1     C4        cr0=gr0,0
  160| 000830 bc       4081FF10   1     BF        CL.160,cr0,0x2/gt,taken=30%(30,70)
    0| 000834 cmpwi    2F880000   1     C4        cr7=gr8,0
  160| 000838 addi     38600000   1     LI        gr3=0
    0| 00083C bc       409DFF04   1     BF        CL.160,cr7,0x2/gt,taken=40%(40,60)
    0| 000840 ld       E8E100D8   1     L8        gr7=#SPILL3(gr1,216)
    0| 000844 ld       EB410100   1     L8        gr26=#SPILL8(gr1,256)
    0| 000848 ld       EB6100F8   1     L8        gr27=#SPILL7(gr1,248)
    0| 00084C ld       EB210108   1     L8        gr25=#SPILL9(gr1,264)
    0| 000850 rldicl   7909E8C2   1     SRL8      gr9=gr8,3
    0| 000854 andi.    711D0007   1     RN4_R     gr29,cr0=gr8,0,0x7
    0| 000858 lwz      80870000   1     L4Z       gr4=isz(gr7,0)
    0| 00085C addi     38DAFFF8   1     AI        gr6=gr26,-8
    0| 000860 lwa      E8BB0002   1     L4A       gr5=iez(gr27,0)
    0| 000864 subf     7D593050   1     S         gr10=gr6,gr25
    0| 000868 subf     7D793050   1     S         gr11=gr6,gr25
    0| 00086C cmpdi    2CA90000   1     C8        cr1=gr9,0
    0| 000870 addi     38C4FFFF   1     AI        gr6=gr4,-1
    0| 000874 extsw    7CDF07B4   1     EXTS4     gr31=gr6
    0| 000878 mulld    7D85C9D2   1     M         gr12=gr5,gr25
    0| 00087C mulld    7F99F9D2   1     M         gr28=gr25,gr31
  160|                              CL.76:
    0| 000880 addi     38630001   1     AI        gr3=gr3,1
  161| 000884 add      7CCBE214   1     A         gr6=gr11,gr28
  161| 000888 add      7CEB6214   1     A         gr7=gr11,gr12
    0| 00088C bc       41820018   1     BT        CL.687,cr0,0x4/eq,taken=50%(0,0)
    0| 000890 mtspr    7FA903A6   1     LCTR      ctr=gr29
    0|                              CL.827:
  161| 000894 lfdu     CC070008   1     LFDU      fp0,gr7=x[](gr7,8)
  161| 000898 stfdu    DC060008   1     STFDU     gr6,x[](gr6,8)=fp0
    0| 00089C bc       4200FFF8   1     BCT       ctr=CL.827,taken=100%(100,0)
    0| 0008A0 bc       41860050   1     BT        CL.688,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.687:
    0| 0008A4 mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 0008A8 ori      60210000   1     XNOP      
    0|                              CL.834:
  161| 0008AC lfd      C8070008   1     LFL       fp0=x[](gr7,8)
  161| 0008B0 stfd     D8060008   1     STFL      x[](gr6,8)=fp0
  161| 0008B4 lfd      C8070010   1     LFL       fp0=x[](gr7,16)
  161| 0008B8 stfd     D8060010   1     STFL      x[](gr6,16)=fp0
  161| 0008BC lfd      C8070018   1     LFL       fp0=x[](gr7,24)
  161| 0008C0 stfd     D8060018   1     STFL      x[](gr6,24)=fp0
  161| 0008C4 lfd      C8070020   1     LFL       fp0=x[](gr7,32)
  161| 0008C8 stfd     D8060020   1     STFL      x[](gr6,32)=fp0
  161| 0008CC lfd      C8070028   1     LFL       fp0=x[](gr7,40)
  161| 0008D0 stfd     D8060028   1     STFL      x[](gr6,40)=fp0
  161| 0008D4 lfd      C8070030   1     LFL       fp0=x[](gr7,48)
  161| 0008D8 stfd     D8060030   1     STFL      x[](gr6,48)=fp0
  161| 0008DC lfd      C8070038   1     LFL       fp0=x[](gr7,56)
  161| 0008E0 stfd     D8060038   1     STFL      x[](gr6,56)=fp0
  161| 0008E4 lfdu     CC070040   1     LFDU      fp0,gr7=x[](gr7,64)
  161| 0008E8 stfdu    DC060040   1     STFDU     gr6,x[](gr6,64)=fp0
    0| 0008EC bc       4200FFC0   1     BCT       ctr=CL.834,taken=100%(100,0)
    0|                              CL.688:
  160| 0008F0 cmpld    7F230040   1     CL8       cr6=gr3,gr0
    0| 0008F4 add      7D6BF214   1     A         gr11=gr11,gr30
  160| 0008F8 bc       4198FF88   1     BT        CL.76,cr6,0x8/llt,taken=80%(80,20)
    0| 0008FC addi     3865FFFF   1     AI        gr3=gr5,-1
    0| 000900 extsw    7CA507B4   1     EXTS4     gr5=gr5
    0| 000904 extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 000908 addi     3884FFFE   1     AI        gr4=gr4,-2
    0| 00090C subf     7C651850   1     S         gr3=gr3,gr5
    0| 000910 extsw    7C8607B4   1     EXTS4     gr6=gr4
    0| 000914 mulld    7C85C9D2   1     M         gr4=gr5,gr25
    0| 000918 subf     7CFF3050   1     S         gr7=gr6,gr31
    0| 00091C mulld    7CA3C9D2   1     M         gr5=gr3,gr25
    0| 000920 or       7D264B78   1     LR        gr6=gr9
  160| 000924 addi     38600000   1     LI        gr3=0
    0| 000928 mulld    7CE7C9D2   1     M         gr7=gr7,gr25
    0| 00092C add      7D242A14   1     A         gr9=gr4,gr5
    0| 000930 andi.    710B0007   1     RN4_R     gr11,cr0=gr8,0,0x7
    0| 000934 cmpdi    2CA60000   1     C8        cr1=gr6,0
  160|                              CL.363:
  161| 000938 add      7CA75214   1     A         gr5=gr7,gr10
    0| 00093C addi     38630001   1     AI        gr3=gr3,1
  161| 000940 add      7C895214   1     A         gr4=gr9,gr10
  161| 000944 add      7CA5E214   1     A         gr5=gr5,gr28
    0| 000948 bc       4182001C   1     BT        CL.690,cr0,0x4/eq,taken=50%(0,0)
    0| 00094C mtspr    7D6903A6   1     LCTR      ctr=gr11
    0| 000950 ori      60210000   1     XNOP      
    0|                              CL.828:
  161| 000954 lfdu     CC040008   1     LFDU      fp0,gr4=x[](gr4,8)
  161| 000958 stfdu    DC050008   1     STFDU     gr5,x[](gr5,8)=fp0
    0| 00095C bc       4200FFF8   1     BCT       ctr=CL.828,taken=100%(100,0)
    0| 000960 bc       41860050   1     BT        CL.691,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.690:
    0| 000964 mtspr    7CC903A6   1     LCTR      ctr=gr6
    0| 000968 ori      60210000   1     XNOP      
    0|                              CL.833:
  161| 00096C lfd      C8040008   1     LFL       fp0=x[](gr4,8)
  161| 000970 stfd     D8050008   1     STFL      x[](gr5,8)=fp0
  161| 000974 lfd      C8040010   1     LFL       fp0=x[](gr4,16)
  161| 000978 stfd     D8050010   1     STFL      x[](gr5,16)=fp0
  161| 00097C lfd      C8040018   1     LFL       fp0=x[](gr4,24)
  161| 000980 stfd     D8050018   1     STFL      x[](gr5,24)=fp0
  161| 000984 lfd      C8040020   1     LFL       fp0=x[](gr4,32)
  161| 000988 stfd     D8050020   1     STFL      x[](gr5,32)=fp0
  161| 00098C lfd      C8040028   1     LFL       fp0=x[](gr4,40)
  161| 000990 stfd     D8050028   1     STFL      x[](gr5,40)=fp0
  161| 000994 lfd      C8040030   1     LFL       fp0=x[](gr4,48)
  161| 000998 stfd     D8050030   1     STFL      x[](gr5,48)=fp0
  161| 00099C lfd      C8040038   1     LFL       fp0=x[](gr4,56)
  161| 0009A0 stfd     D8050038   1     STFL      x[](gr5,56)=fp0
  161| 0009A4 lfdu     CC040040   1     LFDU      fp0,gr4=x[](gr4,64)
  161| 0009A8 stfdu    DC050040   1     STFDU     gr5,x[](gr5,64)=fp0
    0| 0009AC bc       4200FFC0   1     BCT       ctr=CL.833,taken=100%(100,0)
    0|                              CL.691:
  160| 0009B0 cmpld    7F230040   1     CL8       cr6=gr3,gr0
    0| 0009B4 add      7D4AF214   1     A         gr10=gr10,gr30
  160| 0009B8 bc       4198FF80   1     BT        CL.363,cr6,0x8/llt,taken=80%(80,20)
    0| 0009BC lwz      809B0000   1     L4Z       gr4=iez(gr27,0)
    0| 0009C0 ld       EB6100D8   1     L8        gr27=#SPILL3(gr1,216)
    0| 0009C4 addi     38DAFFF8   1     AI        gr6=gr26,-8
    0| 0009C8 rldicl   7909E8C2   1     SRL8      gr9=gr8,3
    0| 0009CC subf     7D593050   1     S         gr10=gr6,gr25
    0| 0009D0 subf     7D793050   1     S         gr11=gr6,gr25
    0| 0009D4 addi     38C40001   1     AI        gr6=gr4,1
    0| 0009D8 lwa      E8BB0002   1     L4A       gr5=isz(gr27,0)
    0| 0009DC extsw    7CCC07B4   1     EXTS4     gr12=gr6
  160| 0009E0 addi     38600000   1     LI        gr3=0
    0| 0009E4 andi.    711F0007   1     RN4_R     gr31,cr0=gr8,0,0x7
    0| 0009E8 mulld    7FACC9D2   1     M         gr29=gr12,gr25
    0| 0009EC mulld    7F85C9D2   1     M         gr28=gr5,gr25
    0| 0009F0 cmpdi    2CA90000   1     C8        cr1=gr9,0
  160|                              CL.82:
    0| 0009F4 addi     38630001   1     AI        gr3=gr3,1
  163| 0009F8 add      7CCBEA14   1     A         gr6=gr11,gr29
  163| 0009FC add      7CEBE214   1     A         gr7=gr11,gr28
    0| 000A00 bc       41820024   1     BT        CL.693,cr0,0x4/eq,taken=50%(0,0)
    0| 000A04 mtspr    7FE903A6   1     LCTR      ctr=gr31
    0| 000A08 ori      60210000   1     XNOP      
    0| 000A0C ori      60210000   1     XNOP      
    0| 000A10 ori      60210000   1     XNOP      
    0|                              CL.829:
  163| 000A14 lfdu     CC070008   1     LFDU      fp0,gr7=x[](gr7,8)
  163| 000A18 stfdu    DC060008   1     STFDU     gr6,x[](gr6,8)=fp0
    0| 000A1C bc       4200FFF8   1     BCT       ctr=CL.829,taken=100%(100,0)
    0| 000A20 bc       41860050   1     BT        CL.694,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.693:
    0| 000A24 mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 000A28 ori      60210000   1     XNOP      
    0|                              CL.832:
  163| 000A2C lfd      C8070008   1     LFL       fp0=x[](gr7,8)
  163| 000A30 stfd     D8060008   1     STFL      x[](gr6,8)=fp0
  163| 000A34 lfd      C8070010   1     LFL       fp0=x[](gr7,16)
  163| 000A38 stfd     D8060010   1     STFL      x[](gr6,16)=fp0
  163| 000A3C lfd      C8070018   1     LFL       fp0=x[](gr7,24)
  163| 000A40 stfd     D8060018   1     STFL      x[](gr6,24)=fp0
  163| 000A44 lfd      C8070020   1     LFL       fp0=x[](gr7,32)
  163| 000A48 stfd     D8060020   1     STFL      x[](gr6,32)=fp0
  163| 000A4C lfd      C8070028   1     LFL       fp0=x[](gr7,40)
  163| 000A50 stfd     D8060028   1     STFL      x[](gr6,40)=fp0
  163| 000A54 lfd      C8070030   1     LFL       fp0=x[](gr7,48)
  163| 000A58 stfd     D8060030   1     STFL      x[](gr6,48)=fp0
  163| 000A5C lfd      C8070038   1     LFL       fp0=x[](gr7,56)
  163| 000A60 stfd     D8060038   1     STFL      x[](gr6,56)=fp0
  163| 000A64 lfdu     CC070040   1     LFDU      fp0,gr7=x[](gr7,64)
  163| 000A68 stfdu    DC060040   1     STFDU     gr6,x[](gr6,64)=fp0
    0| 000A6C bc       4200FFC0   1     BCT       ctr=CL.832,taken=100%(100,0)
    0|                              CL.694:
  160| 000A70 cmpld    7F230040   1     CL8       cr6=gr3,gr0
    0| 000A74 add      7D6BF214   1     A         gr11=gr11,gr30
  160| 000A78 bc       4198FF7C   1     BT        CL.82,cr6,0x8/llt,taken=80%(80,20)
    0| 000A7C addi     38650001   1     AI        gr3=gr5,1
    0| 000A80 extsw    7CA507B4   1     EXTS4     gr5=gr5
    0| 000A84 extsw    7C6307B4   1     EXTS4     gr3=gr3
    0| 000A88 addi     38840002   1     AI        gr4=gr4,2
    0| 000A8C subf     7C651850   1     S         gr3=gr3,gr5
    0| 000A90 extsw    7C8407B4   1     EXTS4     gr4=gr4
    0| 000A94 mulld    7CA5C9D2   1     M         gr5=gr5,gr25
    0| 000A98 subf     7C8C2050   1     S         gr4=gr4,gr12
    0| 000A9C mulld    7CC3C9D2   1     M         gr6=gr3,gr25
    0| 000AA0 or       7D274B78   1     LR        gr7=gr9
  160| 000AA4 addi     38600000   1     LI        gr3=0
    0| 000AA8 mulld    7C84C9D2   1     M         gr4=gr4,gr25
    0| 000AAC add      7CA53214   1     A         gr5=gr5,gr6
    0| 000AB0 andi.    71090007   1     RN4_R     gr9,cr0=gr8,0,0x7
    0| 000AB4 cmpdi    2CA70000   1     C8        cr1=gr7,0
  160|                              CL.357:
  163| 000AB8 add      7D045214   1     A         gr8=gr4,gr10
    0| 000ABC addi     38630001   1     AI        gr3=gr3,1
  163| 000AC0 add      7CC55214   1     A         gr6=gr5,gr10
  163| 000AC4 add      7D08EA14   1     A         gr8=gr8,gr29
    0| 000AC8 bc       4182001C   1     BT        CL.696,cr0,0x4/eq,taken=50%(0,0)
    0| 000ACC mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 000AD0 ori      60210000   1     XNOP      
    0|                              CL.830:
  163| 000AD4 lfdu     CC060008   1     LFDU      fp0,gr6=x[](gr6,8)
  163| 000AD8 stfdu    DC080008   1     STFDU     gr8,x[](gr8,8)=fp0
    0| 000ADC bc       4200FFF8   1     BCT       ctr=CL.830,taken=100%(100,0)
    0| 000AE0 bc       41860050   1     BT        CL.697,cr1,0x4/eq,taken=20%(20,80)
    0|                              CL.696:
    0| 000AE4 mtspr    7CE903A6   1     LCTR      ctr=gr7
    0| 000AE8 ori      60210000   1     XNOP      
    0|                              CL.831:
  163| 000AEC lfd      C8060008   1     LFL       fp0=x[](gr6,8)
  163| 000AF0 stfd     D8080008   1     STFL      x[](gr8,8)=fp0
  163| 000AF4 lfd      C8060010   1     LFL       fp0=x[](gr6,16)
  163| 000AF8 stfd     D8080010   1     STFL      x[](gr8,16)=fp0
  163| 000AFC lfd      C8060018   1     LFL       fp0=x[](gr6,24)
  163| 000B00 stfd     D8080018   1     STFL      x[](gr8,24)=fp0
  163| 000B04 lfd      C8060020   1     LFL       fp0=x[](gr6,32)
  163| 000B08 stfd     D8080020   1     STFL      x[](gr8,32)=fp0
  163| 000B0C lfd      C8060028   1     LFL       fp0=x[](gr6,40)
  163| 000B10 stfd     D8080028   1     STFL      x[](gr8,40)=fp0
  163| 000B14 lfd      C8060030   1     LFL       fp0=x[](gr6,48)
  163| 000B18 stfd     D8080030   1     STFL      x[](gr8,48)=fp0
  163| 000B1C lfd      C8060038   1     LFL       fp0=x[](gr6,56)
  163| 000B20 stfd     D8080038   1     STFL      x[](gr8,56)=fp0
  163| 000B24 lfdu     CC060040   1     LFDU      fp0,gr6=x[](gr6,64)
  163| 000B28 stfdu    DC080040   1     STFDU     gr8,x[](gr8,64)=fp0
    0| 000B2C bc       4200FFC0   1     BCT       ctr=CL.831,taken=100%(100,0)
    0|                              CL.697:
  160| 000B30 cmpld    7F230040   1     CL8       cr6=gr3,gr0
    0| 000B34 add      7D4AF214   1     A         gr10=gr10,gr30
  160| 000B38 bc       4098FC08   1     BF        CL.160,cr6,0x8/llt,taken=20%(20,80)
  160| 000B3C b        4BFFFF7C   1     B         CL.357,-1
  111|                              CL.17:
  112| 000B40 andi.    70630001   1     RN4_R     gr3,cr0=gr3,0,0x1
  112| 000B44 bc       4182FA0C   1     BT        CL.22,cr0,0x4/eq,taken=50%(0,0)
  113| 000B48 ld       E8C20000   1     L8        gr6=.&&N&&param(gr2,0)
    0| 000B4C ld       E8E10100   1     L8        gr7=#SPILL8(gr1,256)
  113| 000B50 lwa      E906000A   1     L4A       gr8=<s16:d8:l4>(gr6,8)
    0| 000B54 addi     3887FFF8   1     AI        gr4=gr7,-8
  113| 000B58 sradi    7D031674   1     SRA8CA    gr3,ca=gr8,2
  113| 000B5C cmpwi    2F880000   1     C4        cr7=gr8,0
  113| 000B60 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
  113| 000B64 rldicr   78701764   1     SLL8      gr16=gr3,2
  113| 000B68 subf     7F904051   1     S_R       gr28,cr0=gr8,gr16
  113| 000B6C crand    4C3D0A02   1     CR_N      cr0=cr[70],0x2/gt,0x2/gt,0x2/gt,cr0
  113| 000B70 bc       408100B4   1     BF        CL.135,cr0,0x2/gt,taken=20%(20,80)
  113| 000B74 lwa      E9260002   1     L4A       gr9=<s16:d0:l4>(gr6,0)
  113| 000B78 addi     38600000   1     LI        gr3=0
    0| 000B7C subf     7D5E2050   1     S         gr10=gr4,gr30
  115| 000B80 ld       EB4100D0   1     L8        gr26=#SPILL2(gr1,208)
    0| 000B84 cmpdi    2CA90000   1     C8        cr1=gr9,0
  113|                              CL.130:
  113| 000B88 bc       40850088   1     BF        CL.346,cr1,0x2/gt,taken=20%(20,80)
  115| 000B8C lwz      809F0000   1     L4Z       gr4=isy(gr31,0)
  115| 000B90 lwz      80BA0000   1     L4Z       gr5=iey(gr26,0)
    0| 000B94 mtspr    7D2903A6   1     LCTR      ctr=gr9
  115| 000B98 addi     38C4FFFF   1     AI        gr6=gr4,-1
  115| 000B9C extsw    7CAB07B4   1     EXTS4     gr11=gr5
  115| 000BA0 extsw    7CCC07B4   1     EXTS4     gr12=gr6
  115| 000BA4 mulld    7FABF1D2   1     M         gr29=gr11,gr30
  115| 000BA8 mulld    7CECF1D2   1     M         gr7=gr12,gr30
  115| 000BAC add      7CCAEA14   1     A         gr6=gr10,gr29
  115| 000BB0 add      7F675214   1     A         gr27=gr7,gr10
  115| 000BB4 add      7CE75214   1     A         gr7=gr7,gr10
    0| 000BB8 ori      60210000   1     XNOP      
    0| 000BBC ori      60210000   1     XNOP      
    0| 000BC0 ori      60210000   1     XNOP      
    0|                              CL.820:
  115| 000BC4 lfdu     CC060008   1     LFDU      fp0,gr6=x[](gr6,8)
  115| 000BC8 stfdu    DC070008   1     STFDU     gr7,x[](gr7,8)=fp0
    0| 000BCC bc       4200FFF8   1     BCT       ctr=CL.820,taken=100%(100,0)
  115| 000BD0 addi     38A5FFFF   1     AI        gr5=gr5,-1
  115| 000BD4 addi     3884FFFE   1     AI        gr4=gr4,-2
  115| 000BD8 extsw    7CA507B4   1     EXTS4     gr5=gr5
  115| 000BDC extsw    7C8407B4   1     EXTS4     gr4=gr4
  115| 000BE0 subf     7CAB2850   1     S         gr5=gr5,gr11
  115| 000BE4 subf     7C8C2050   1     S         gr4=gr4,gr12
  115| 000BE8 mulld    7CA5F1D2   1     M         gr5=gr5,gr30
  115| 000BEC mulld    7C84F1D2   1     M         gr4=gr4,gr30
  115| 000BF0 add      7CA55214   1     A         gr5=gr5,gr10
  115| 000BF4 add      7C84DA14   1     A         gr4=gr4,gr27
  115| 000BF8 add      7CA5EA14   1     A         gr5=gr5,gr29
    0| 000BFC mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 000C00 ori      60210000   1     XNOP      
    0|                              CL.819:
  115| 000C04 lfdu     CC050008   1     LFDU      fp0,gr5=x[](gr5,8)
  115| 000C08 stfdu    DC040008   1     STFDU     gr4,x[](gr4,8)=fp0
    0| 000C0C bc       4200FFF8   1     BCT       ctr=CL.819,taken=100%(100,0)
  113|                              CL.346:
    0| 000C10 ld       E8810108   1     L8        gr4=#SPILL9(gr1,264)
  113| 000C14 addi     38630001   1     AI        gr3=gr3,1
  113| 000C18 cmpd     7F3C1800   1     C8        cr6=gr28,gr3
    0| 000C1C add      7D445214   1     A         gr10=gr4,gr10
  113| 000C20 bc       4199FF68   1     BT        CL.130,cr6,0x2/gt,taken=80%(80,20)
  113|                              CL.135:
    0| 000C24 ld       E8810108   1     L8        gr4=#SPILL9(gr1,264)
  113| 000C28 cmpd     7CA8E000   1     C8        cr1=gr8,gr28
  113| 000C2C crand    4E3D2A02   1     CR_N      cr4=cr[71],0x2/gt,0x2/gt,0x2/gt,cr4
    0| 000C30 mulld    7C64E1D2   1     M         gr3=gr4,gr28
  113| 000C34 bc       409101F8   1     BF        CL.86,cr4,0x2/gt,taken=20%(20,80)
  113| 000C38 ld       E9220000   1     L8        gr9=.&&N&&param(gr2,0)
    0| 000C3C ld       E9410100   1     L8        gr10=#SPILL8(gr1,256)
    0| 000C40 ld       E96100F0   1     L8        gr11=#SPILL6(gr1,240)
    0| 000C44 ld       E9810108   1     L8        gr12=#SPILL9(gr1,264)
  113| 000C48 addi     3890FFFF   1     AI        gr4=gr16,-1
  115| 000C4C ld       E9E100D0   1     L8        gr15=#SPILL2(gr1,208)
  113| 000C50 lwa      E9090002   1     L4A       gr8=<s16:d0:l4>(gr9,0)
    0| 000C54 addi     38AAFFF8   1     AI        gr5=gr10,-8
  113| 000C58 sradi    7C841674   1     SRA8CA    gr4,ca=gr4,2
    0| 000C5C rldicr   797B2EA4   1     SLL8      gr27=gr11,5
    0| 000C60 subf     7CBE2850   1     S         gr5=gr5,gr30
  113| 000C64 addze    7C840194   1     ADDE      gr4,ca=gr4,0,ca
    0| 000C68 add      7F432A14   1     A         gr26=gr3,gr5
    0| 000C6C rldicr   796726E4   1     SLL8      gr7=gr11,4
    0| 000C70 subf     7CCCD850   1     S         gr6=gr27,gr12
    0| 000C74 rldicl   7919F842   1     SRL8      gr25=gr8,1
    0| 000C78 rlwinm   550507FE   1     RN4       gr5=gr8,0,0x1
  113| 000C7C addi     38600000   1     LI        gr3=0
    0| 000C80 cmpwi    2C880000   1     C4        cr1=gr8,0
    0| 000C84 add      7F0CD214   1     A         gr24=gr12,gr26
    0| 000C88 add      7EE7D214   1     A         gr23=gr7,gr26
    0| 000C8C add      7EC6D214   1     A         gr22=gr6,gr26
    0| 000C90 addi     3AA40001   1     AI        gr21=gr4,1
    0| 000C94 cmpdi    2F390000   1     C8        cr6=gr25,0
    0| 000C98 cmpdi    2FA50000   1     C8        cr7=gr5,0
  113|                              CL.87:
  113| 000C9C bc       40850174   1     BF        CL.349,cr1,0x2/gt,taken=50%(0,0)
  115| 000CA0 lwz      808F0000   1     L4Z       gr4=iey(gr15,0)
  115| 000CA4 lwz      80BF0000   1     L4Z       gr5=isy(gr31,0)
    0| 000CA8 mtspr    7F2903A6   1     LCTR      ctr=gr25
  115| 000CAC extsw    7C9407B4   1     EXTS4     gr20=gr4
  115| 000CB0 addi     38C5FFFF   1     AI        gr6=gr5,-1
  115| 000CB4 mulld    7E74F1D2   1     M         gr19=gr20,gr30
  115| 000CB8 extsw    7CD207B4   1     EXTS4     gr18=gr6
  115| 000CBC add      7CD3D214   1     A         gr6=gr19,gr26
  115| 000CC0 mulld    7E32F1D2   1     M         gr17=gr18,gr30
  115| 000CC4 add      7CF3C214   1     A         gr7=gr19,gr24
  115| 000CC8 add      7D11D214   1     A         gr8=gr17,gr26
  115| 000CCC add      7D31C214   1     A         gr9=gr17,gr24
  115| 000CD0 add      7D53BA14   1     A         gr10=gr19,gr23
  115| 000CD4 add      7D71BA14   1     A         gr11=gr17,gr23
  115| 000CD8 add      7D93B214   1     A         gr12=gr19,gr22
  115| 000CDC add      7FB1B214   1     A         gr29=gr17,gr22
    0| 000CE0 bc       419E002C   1     BT        CL.792,cr7,0x4/eq,taken=50%(0,0)
  115| 000CE4 lfdu     CC060008   1     LFDU      fp0,gr6=x[](gr6,8)
  115| 000CE8 stfdu    DC080008   1     STFDU     gr8,x[](gr8,8)=fp0
  115| 000CEC lfdu     CC070008   1     LFDU      fp0,gr7=x[](gr7,8)
  115| 000CF0 stfdu    DC090008   1     STFDU     gr9,x[](gr9,8)=fp0
  115| 000CF4 lfdu     CC0A0008   1     LFDU      fp0,gr10=x[](gr10,8)
  115| 000CF8 stfdu    DC0B0008   1     STFDU     gr11,x[](gr11,8)=fp0
  115| 000CFC lfdu     CC0C0008   1     LFDU      fp0,gr12=x[](gr12,8)
  115| 000D00 stfdu    DC1D0008   1     STFDU     gr29,x[](gr29,8)=fp0
    0| 000D04 bc       419A004C   1     BT        CL.673,cr6,0x4/eq,taken=20%(20,80)
    0| 000D08 ori      60210000   1     XNOP      
    0|                              CL.792:
  115| 000D0C lfd      C8060008   1     LFL       fp0=x[](gr6,8)
  115| 000D10 stfd     D8080008   1     STFL      x[](gr8,8)=fp0
  115| 000D14 lfd      C8070008   1     LFL       fp0=x[](gr7,8)
  115| 000D18 stfd     D8090008   1     STFL      x[](gr9,8)=fp0
  115| 000D1C lfd      C80A0008   1     LFL       fp0=x[](gr10,8)
  115| 000D20 stfd     D80B0008   1     STFL      x[](gr11,8)=fp0
  115| 000D24 lfd      C80C0008   1     LFL       fp0=x[](gr12,8)
  115| 000D28 stfd     D81D0008   1     STFL      x[](gr29,8)=fp0
  115| 000D2C lfdu     CC060010   1     LFDU      fp0,gr6=x[](gr6,16)
  115| 000D30 stfdu    DC080010   1     STFDU     gr8,x[](gr8,16)=fp0
  115| 000D34 lfdu     CC070010   1     LFDU      fp0,gr7=x[](gr7,16)
  115| 000D38 stfdu    DC090010   1     STFDU     gr9,x[](gr9,16)=fp0
  115| 000D3C lfdu     CC0A0010   1     LFDU      fp0,gr10=x[](gr10,16)
  115| 000D40 stfdu    DC0B0010   1     STFDU     gr11,x[](gr11,16)=fp0
  115| 000D44 lfdu     CC0C0010   1     LFDU      fp0,gr12=x[](gr12,16)
  115| 000D48 stfdu    DC1D0010   1     STFDU     gr29,x[](gr29,16)=fp0
    0| 000D4C bc       4200FFC0   1     BCT       ctr=CL.792,taken=100%(100,0)
    0|                              CL.673:
  115| 000D50 addi     3884FFFF   1     AI        gr4=gr4,-1
  115| 000D54 addi     38A5FFFE   1     AI        gr5=gr5,-2
  115| 000D58 extsw    7C8407B4   1     EXTS4     gr4=gr4
  115| 000D5C extsw    7CA507B4   1     EXTS4     gr5=gr5
  115| 000D60 subf     7C942050   1     S         gr4=gr4,gr20
  115| 000D64 subf     7CB22850   1     S         gr5=gr5,gr18
  115| 000D68 mulld    7C84F1D2   1     M         gr4=gr4,gr30
  115| 000D6C mulld    7CA5F1D2   1     M         gr5=gr5,gr30
  115| 000D70 add      7C849A14   1     A         gr4=gr4,gr19
  115| 000D74 add      7D658A14   1     A         gr11=gr5,gr17
  115| 000D78 add      7CA4D214   1     A         gr5=gr4,gr26
  115| 000D7C add      7CCBD214   1     A         gr6=gr11,gr26
  115| 000D80 add      7CE4C214   1     A         gr7=gr4,gr24
  115| 000D84 add      7D0BC214   1     A         gr8=gr11,gr24
  115| 000D88 add      7D24BA14   1     A         gr9=gr4,gr23
  115| 000D8C add      7D4BBA14   1     A         gr10=gr11,gr23
  115| 000D90 add      7C84B214   1     A         gr4=gr4,gr22
  115| 000D94 add      7D6BB214   1     A         gr11=gr11,gr22
    0| 000D98 mtspr    7F2903A6   1     LCTR      ctr=gr25
    0| 000D9C bc       419E0030   1     BT        CL.790,cr7,0x4/eq,taken=50%(0,0)
  115| 000DA0 lfdu     CC050008   1     LFDU      fp0,gr5=x[](gr5,8)
  115| 000DA4 stfdu    DC060008   1     STFDU     gr6,x[](gr6,8)=fp0
  115| 000DA8 lfdu     CC070008   1     LFDU      fp0,gr7=x[](gr7,8)
  115| 000DAC stfdu    DC080008   1     STFDU     gr8,x[](gr8,8)=fp0
  115| 000DB0 lfdu     CC090008   1     LFDU      fp0,gr9=x[](gr9,8)
  115| 000DB4 stfdu    DC0A0008   1     STFDU     gr10,x[](gr10,8)=fp0
  115| 000DB8 lfdu     CC040008   1     LFDU      fp0,gr4=x[](gr4,8)
  115| 000DBC stfdu    DC0B0008   1     STFDU     gr11,x[](gr11,8)=fp0
    0| 000DC0 bc       419A0050   1     BT        CL.349,cr6,0x4/eq,taken=20%(20,80)
    0| 000DC4 ori      60210000   1     XNOP      
    0| 000DC8 ori      60210000   1     XNOP      
    0|                              CL.790:
  115| 000DCC lfd      C8050008   1     LFL       fp0=x[](gr5,8)
  115| 000DD0 stfd     D8060008   1     STFL      x[](gr6,8)=fp0
  115| 000DD4 lfd      C8070008   1     LFL       fp0=x[](gr7,8)
  115| 000DD8 stfd     D8080008   1     STFL      x[](gr8,8)=fp0
  115| 000DDC lfd      C8090008   1     LFL       fp0=x[](gr9,8)
  115| 000DE0 stfd     D80A0008   1     STFL      x[](gr10,8)=fp0
  115| 000DE4 lfd      C8040008   1     LFL       fp0=x[](gr4,8)
  115| 000DE8 stfd     D80B0008   1     STFL      x[](gr11,8)=fp0
  115| 000DEC lfdu     CC050010   1     LFDU      fp0,gr5=x[](gr5,16)
  115| 000DF0 stfdu    DC060010   1     STFDU     gr6,x[](gr6,16)=fp0
  115| 000DF4 lfdu     CC070010   1     LFDU      fp0,gr7=x[](gr7,16)
  115| 000DF8 stfdu    DC080010   1     STFDU     gr8,x[](gr8,16)=fp0
  115| 000DFC lfdu     CC090010   1     LFDU      fp0,gr9=x[](gr9,16)
  115| 000E00 stfdu    DC0A0010   1     STFDU     gr10,x[](gr10,16)=fp0
  115| 000E04 lfdu     CC040010   1     LFDU      fp0,gr4=x[](gr4,16)
  115| 000E08 stfdu    DC0B0010   1     STFDU     gr11,x[](gr11,16)=fp0
    0| 000E0C bc       4200FFC0   1     BCT       ctr=CL.790,taken=100%(100,0)
  113|                              CL.349:
  113| 000E10 addi     38630001   1     AI        gr3=gr3,1
    0| 000E14 add      7F18DA14   1     A         gr24=gr24,gr27
  113| 000E18 cmpld    7EA3A840   1     CL8       cr5=gr3,gr21
    0| 000E1C add      7F5ADA14   1     A         gr26=gr26,gr27
    0| 000E20 add      7EF7DA14   1     A         gr23=gr23,gr27
    0| 000E24 add      7ED6DA14   1     A         gr22=gr22,gr27
  113| 000E28 bc       4194FE74   1     BT        CL.87,cr5,0x8/llt,taken=80%(80,20)
  113|                              CL.86:
  113| 000E2C bc       408100B8   1     BF        CL.123,cr0,0x2/gt,taken=20%(20,80)
  113| 000E30 ld       E8A20000   1     L8        gr5=.&&N&&param(gr2,0)
    0| 000E34 ld       E8C10100   1     L8        gr6=#SPILL8(gr1,256)
  113| 000E38 addi     38600000   1     LI        gr3=0
  117| 000E3C ld       EB6100D0   1     L8        gr27=#SPILL2(gr1,208)
  113| 000E40 lwa      E9050002   1     L4A       gr8=<s16:d0:l4>(gr5,0)
    0| 000E44 addi     3886FFF8   1     AI        gr4=gr6,-8
    0| 000E48 subf     7D3E2050   1     S         gr9=gr4,gr30
    0| 000E4C cmpdi    2C280000   1     C8        cr0=gr8,0
  113|                              CL.118:
  113| 000E50 bc       40810080   1     BF        CL.352,cr0,0x2/gt,taken=20%(20,80)
  117| 000E54 lwz      809B0000   1     L4Z       gr4=iey(gr27,0)
  117| 000E58 lwz      80BF0000   1     L4Z       gr5=isy(gr31,0)
    0| 000E5C mtspr    7D0903A6   1     LCTR      ctr=gr8
  117| 000E60 addi     38C40001   1     AI        gr6=gr4,1
  117| 000E64 extsw    7CAA07B4   1     EXTS4     gr10=gr5
  117| 000E68 extsw    7CCB07B4   1     EXTS4     gr11=gr6
  117| 000E6C mulld    7D8AF1D2   1     M         gr12=gr10,gr30
  117| 000E70 mulld    7CEBF1D2   1     M         gr7=gr11,gr30
  117| 000E74 add      7CC96214   1     A         gr6=gr9,gr12
  117| 000E78 add      7FA74A14   1     A         gr29=gr7,gr9
  117| 000E7C add      7CE74A14   1     A         gr7=gr7,gr9
    0| 000E80 ori      60210000   1     XNOP      
    0|                              CL.824:
  117| 000E84 lfdu     CC060008   1     LFDU      fp0,gr6=x[](gr6,8)
  117| 000E88 stfdu    DC070008   1     STFDU     gr7,x[](gr7,8)=fp0
    0| 000E8C bc       4200FFF8   1     BCT       ctr=CL.824,taken=100%(100,0)
  117| 000E90 addi     38A50001   1     AI        gr5=gr5,1
  117| 000E94 addi     38840002   1     AI        gr4=gr4,2
  117| 000E98 extsw    7CA507B4   1     EXTS4     gr5=gr5
  117| 000E9C extsw    7C8407B4   1     EXTS4     gr4=gr4
  117| 000EA0 subf     7CAA2850   1     S         gr5=gr5,gr10
  117| 000EA4 subf     7C8B2050   1     S         gr4=gr4,gr11
  117| 000EA8 mulld    7CA5F1D2   1     M         gr5=gr5,gr30
  117| 000EAC mulld    7C84F1D2   1     M         gr4=gr4,gr30
  117| 000EB0 add      7CA54A14   1     A         gr5=gr5,gr9
  117| 000EB4 add      7C84EA14   1     A         gr4=gr4,gr29
  117| 000EB8 add      7CA56214   1     A         gr5=gr5,gr12
    0| 000EBC mtspr    7D0903A6   1     LCTR      ctr=gr8
    0| 000EC0 ori      60210000   1     XNOP      
    0|                              CL.823:
  117| 000EC4 lfdu     CC050008   1     LFDU      fp0,gr5=x[](gr5,8)
  117| 000EC8 stfdu    DC040008   1     STFDU     gr4,x[](gr4,8)=fp0
    0| 000ECC bc       4200FFF8   1     BCT       ctr=CL.823,taken=100%(100,0)
  113|                              CL.352:
    0| 000ED0 ld       E8810108   1     L8        gr4=#SPILL9(gr1,264)
  113| 000ED4 addi     38630001   1     AI        gr3=gr3,1
  113| 000ED8 cmpd     7CBC1800   1     C8        cr1=gr28,gr3
    0| 000EDC add      7D244A14   1     A         gr9=gr4,gr9
  113| 000EE0 bc       4185FF70   1     BT        CL.118,cr1,0x2/gt,taken=80%(80,20)
  113|                              CL.123:
  113| 000EE4 bc       4091F66C   1     BF        CL.22,cr4,0x2/gt,taken=20%(20,80)
    0| 000EE8 ld       E9410108   1     L8        gr10=#SPILL9(gr1,264)
  113| 000EEC ld       E9220000   1     L8        gr9=.&&N&&param(gr2,0)
    0| 000EF0 ld       E9610100   1     L8        gr11=#SPILL8(gr1,256)
    0| 000EF4 ld       E98100F0   1     L8        gr12=#SPILL6(gr1,240)
  113| 000EF8 addi     3890FFFF   1     AI        gr4=gr16,-1
  117| 000EFC ld       EA2100D0   1     L8        gr17=#SPILL2(gr1,208)
    0| 000F00 mulld    7C6AE1D2   1     M         gr3=gr10,gr28
  113| 000F04 lwa      E8C90002   1     L4A       gr6=<s16:d0:l4>(gr9,0)
    0| 000F08 addi     38ABFFF8   1     AI        gr5=gr11,-8
  113| 000F0C sradi    7C841674   1     SRA8CA    gr4,ca=gr4,2
    0| 000F10 rldicr   799C2EA4   1     SLL8      gr28=gr12,5
    0| 000F14 subf     7CBE2850   1     S         gr5=gr5,gr30
  113| 000F18 addze    7C840194   1     ADDE      gr4,ca=gr4,0,ca
    0| 000F1C add      7F632A14   1     A         gr27=gr3,gr5
    0| 000F20 rldicr   798826E4   1     SLL8      gr8=gr12,4
    0| 000F24 subf     7CEAE050   1     S         gr7=gr28,gr10
    0| 000F28 rlwinm   54C507FE   1     RN4       gr5=gr6,0,0x1
    0| 000F2C rldicl   78DAF843   1     SRL8_R    gr26,cr0=gr6,1
  113| 000F30 addi     38600000   1     LI        gr3=0
    0| 000F34 cmpwi    2C860000   1     C4        cr1=gr6,0
    0| 000F38 add      7F2ADA14   1     A         gr25=gr10,gr27
    0| 000F3C add      7F08DA14   1     A         gr24=gr8,gr27
    0| 000F40 add      7EE7DA14   1     A         gr23=gr7,gr27
    0| 000F44 addi     3AC40001   1     AI        gr22=gr4,1
    0| 000F48 cmpdi    2F250000   1     C8        cr6=gr5,0
  113|                              CL.93:
  113| 000F4C bc       40850174   1     BF        CL.355,cr1,0x2/gt,taken=50%(0,0)
  117| 000F50 lwz      809F0000   1     L4Z       gr4=isy(gr31,0)
  117| 000F54 lwz      80B10000   1     L4Z       gr5=iey(gr17,0)
    0| 000F58 mtspr    7F4903A6   1     LCTR      ctr=gr26
  117| 000F5C extsw    7C9507B4   1     EXTS4     gr21=gr4
  117| 000F60 addi     38C50001   1     AI        gr6=gr5,1
  117| 000F64 mulld    7E95F1D2   1     M         gr20=gr21,gr30
  117| 000F68 extsw    7CD307B4   1     EXTS4     gr19=gr6
  117| 000F6C add      7CD4DA14   1     A         gr6=gr20,gr27
  117| 000F70 mulld    7E53F1D2   1     M         gr18=gr19,gr30
  117| 000F74 add      7CF4CA14   1     A         gr7=gr20,gr25
  117| 000F78 add      7D12DA14   1     A         gr8=gr18,gr27
  117| 000F7C add      7D32CA14   1     A         gr9=gr18,gr25
  117| 000F80 add      7D54C214   1     A         gr10=gr20,gr24
  117| 000F84 add      7D72C214   1     A         gr11=gr18,gr24
  117| 000F88 add      7D94BA14   1     A         gr12=gr20,gr23
  117| 000F8C add      7FB2BA14   1     A         gr29=gr18,gr23
    0| 000F90 bc       419A002C   1     BT        CL.800,cr6,0x4/eq,taken=50%(0,0)
  117| 000F94 lfdu     CC060008   1     LFDU      fp0,gr6=x[](gr6,8)
  117| 000F98 stfdu    DC080008   1     STFDU     gr8,x[](gr8,8)=fp0
  117| 000F9C lfdu     CC070008   1     LFDU      fp0,gr7=x[](gr7,8)
  117| 000FA0 stfdu    DC090008   1     STFDU     gr9,x[](gr9,8)=fp0
  117| 000FA4 lfdu     CC0A0008   1     LFDU      fp0,gr10=x[](gr10,8)
  117| 000FA8 stfdu    DC0B0008   1     STFDU     gr11,x[](gr11,8)=fp0
  117| 000FAC lfdu     CC0C0008   1     LFDU      fp0,gr12=x[](gr12,8)
  117| 000FB0 stfdu    DC1D0008   1     STFDU     gr29,x[](gr29,8)=fp0
    0| 000FB4 bc       4182004C   1     BT        CL.685,cr0,0x4/eq,taken=20%(20,80)
    0| 000FB8 ori      60210000   1     XNOP      
    0|                              CL.800:
  117| 000FBC lfd      C8060008   1     LFL       fp0=x[](gr6,8)
  117| 000FC0 stfd     D8080008   1     STFL      x[](gr8,8)=fp0
  117| 000FC4 lfd      C8070008   1     LFL       fp0=x[](gr7,8)
  117| 000FC8 stfd     D8090008   1     STFL      x[](gr9,8)=fp0
  117| 000FCC lfd      C80A0008   1     LFL       fp0=x[](gr10,8)
  117| 000FD0 stfd     D80B0008   1     STFL      x[](gr11,8)=fp0
  117| 000FD4 lfd      C80C0008   1     LFL       fp0=x[](gr12,8)
  117| 000FD8 stfd     D81D0008   1     STFL      x[](gr29,8)=fp0
  117| 000FDC lfdu     CC060010   1     LFDU      fp0,gr6=x[](gr6,16)
  117| 000FE0 stfdu    DC080010   1     STFDU     gr8,x[](gr8,16)=fp0
  117| 000FE4 lfdu     CC070010   1     LFDU      fp0,gr7=x[](gr7,16)
  117| 000FE8 stfdu    DC090010   1     STFDU     gr9,x[](gr9,16)=fp0
  117| 000FEC lfdu     CC0A0010   1     LFDU      fp0,gr10=x[](gr10,16)
  117| 000FF0 stfdu    DC0B0010   1     STFDU     gr11,x[](gr11,16)=fp0
  117| 000FF4 lfdu     CC0C0010   1     LFDU      fp0,gr12=x[](gr12,16)
  117| 000FF8 stfdu    DC1D0010   1     STFDU     gr29,x[](gr29,16)=fp0
    0| 000FFC bc       4200FFC0   1     BCT       ctr=CL.800,taken=100%(100,0)
    0|                              CL.685:
  117| 001000 addi     38840001   1     AI        gr4=gr4,1
  117| 001004 addi     38A50002   1     AI        gr5=gr5,2
  117| 001008 extsw    7C8407B4   1     EXTS4     gr4=gr4
  117| 00100C extsw    7CA507B4   1     EXTS4     gr5=gr5
  117| 001010 subf     7C952050   1     S         gr4=gr4,gr21
  117| 001014 subf     7CB32850   1     S         gr5=gr5,gr19
  117| 001018 mulld    7C84F1D2   1     M         gr4=gr4,gr30
  117| 00101C mulld    7CA5F1D2   1     M         gr5=gr5,gr30
  117| 001020 add      7D64A214   1     A         gr11=gr4,gr20
  117| 001024 add      7D459214   1     A         gr10=gr5,gr18
  117| 001028 add      7C8BDA14   1     A         gr4=gr11,gr27
  117| 00102C add      7CAADA14   1     A         gr5=gr10,gr27
  117| 001030 add      7CCBCA14   1     A         gr6=gr11,gr25
  117| 001034 add      7CEACA14   1     A         gr7=gr10,gr25
  117| 001038 add      7D0BC214   1     A         gr8=gr11,gr24
  117| 00103C add      7D2AC214   1     A         gr9=gr10,gr24
  117| 001040 add      7D6BBA14   1     A         gr11=gr11,gr23
  117| 001044 add      7D4ABA14   1     A         gr10=gr10,gr23
    0| 001048 mtspr    7F4903A6   1     LCTR      ctr=gr26
    0| 00104C bc       419A0030   1     BT        CL.798,cr6,0x4/eq,taken=50%(0,0)
  117| 001050 lfdu     CC040008   1     LFDU      fp0,gr4=x[](gr4,8)
  117| 001054 stfdu    DC050008   1     STFDU     gr5,x[](gr5,8)=fp0
  117| 001058 lfdu     CC060008   1     LFDU      fp0,gr6=x[](gr6,8)
  117| 00105C stfdu    DC070008   1     STFDU     gr7,x[](gr7,8)=fp0
  117| 001060 lfdu     CC080008   1     LFDU      fp0,gr8=x[](gr8,8)
  117| 001064 stfdu    DC090008   1     STFDU     gr9,x[](gr9,8)=fp0
  117| 001068 lfdu     CC0B0008   1     LFDU      fp0,gr11=x[](gr11,8)
  117| 00106C stfdu    DC0A0008   1     STFDU     gr10,x[](gr10,8)=fp0
    0| 001070 bc       41820050   1     BT        CL.355,cr0,0x4/eq,taken=20%(20,80)
    0| 001074 ori      60210000   1     XNOP      
    0| 001078 ori      60210000   1     XNOP      
    0|                              CL.798:
  117| 00107C lfd      C8040008   1     LFL       fp0=x[](gr4,8)
  117| 001080 stfd     D8050008   1     STFL      x[](gr5,8)=fp0
  117| 001084 lfd      C8060008   1     LFL       fp0=x[](gr6,8)
  117| 001088 stfd     D8070008   1     STFL      x[](gr7,8)=fp0
  117| 00108C lfd      C8080008   1     LFL       fp0=x[](gr8,8)
  117| 001090 stfd     D8090008   1     STFL      x[](gr9,8)=fp0
  117| 001094 lfd      C80B0008   1     LFL       fp0=x[](gr11,8)
  117| 001098 stfd     D80A0008   1     STFL      x[](gr10,8)=fp0
  117| 00109C lfdu     CC040010   1     LFDU      fp0,gr4=x[](gr4,16)
  117| 0010A0 stfdu    DC050010   1     STFDU     gr5,x[](gr5,16)=fp0
  117| 0010A4 lfdu     CC060010   1     LFDU      fp0,gr6=x[](gr6,16)
  117| 0010A8 stfdu    DC070010   1     STFDU     gr7,x[](gr7,16)=fp0
  117| 0010AC lfdu     CC080010   1     LFDU      fp0,gr8=x[](gr8,16)
  117| 0010B0 stfdu    DC090010   1     STFDU     gr9,x[](gr9,16)=fp0
  117| 0010B4 lfdu     CC0B0010   1     LFDU      fp0,gr11=x[](gr11,16)
  117| 0010B8 stfdu    DC0A0010   1     STFDU     gr10,x[](gr10,16)=fp0
    0| 0010BC bc       4200FFC0   1     BCT       ctr=CL.798,taken=100%(100,0)
  113|                              CL.355:
  113| 0010C0 addi     38630001   1     AI        gr3=gr3,1
    0| 0010C4 add      7F39E214   1     A         gr25=gr25,gr28
  113| 0010C8 cmpld    7FA3B040   1     CL8       cr7=gr3,gr22
    0| 0010CC add      7F7BE214   1     A         gr27=gr27,gr28
    0| 0010D0 add      7F18E214   1     A         gr24=gr24,gr28
    0| 0010D4 add      7EF7E214   1     A         gr23=gr23,gr28
  113| 0010D8 bc       409CF478   1     BF        CL.22,cr7,0x8/llt,taken=20%(20,80)
  113| 0010DC b        4BFFFE70   1     B         CL.93,-1
   65|                              CL.1:
   66| 0010E0 andi.    70830001   1     RN4_R     gr3,cr0=gr4,0,0x1
   66| 0010E4 bc       4182F214   1     BT        CL.6,cr0,0x4/eq,taken=50%(0,0)
   67| 0010E8 lwa      E90A000A   1     L4A       gr8=<s16:d8:l4>(gr10,8)
   67| 0010EC sradi    7D031674   1     SRA8CA    gr3,ca=gr8,2
   67| 0010F0 cmpwi    2E080000   1     C4        cr4=gr8,0
   67| 0010F4 addze    7C630194   1     ADDE      gr3,ca=gr3,0,ca
   67| 0010F8 rldicr   78641764   1     SLL8      gr4=gr3,2
   67| 0010FC subf     7CA44051   1     S_R       gr5,cr0=gr8,gr4
   67| 001100 std      F8810110   1     ST8       #SPILL10(gr1,272)=gr4
   67| 001104 crand    4FA18A02   1     CR_N      cr7=cr[04],0x2/gt,0x2/gt,0x2/gt,cr7
   67| 001108 std      F8A10118   1     ST8       #SPILL11(gr1,280)=gr5
   67| 00110C bc       409D00A4   1     BF        CL.159,cr7,0x2/gt,taken=50%(0,0)
    0| 001110 cmpwi    2C000000   1     C4        cr0=gr0,0
   67| 001114 addi     38600000   1     LI        gr3=0
    0| 001118 bc       40810098   1     BF        CL.159,cr0,0x2/gt,taken=40%(40,60)
    0| 00111C ld       EB4100C0   1     L8        gr26=#SPILL0(gr1,192)
    0| 001120 ld       EB2100C8   1     L8        gr25=#SPILL1(gr1,200)
    0| 001124 ld       EB010100   1     L8        gr24=#SPILL8(gr1,256)
    0| 001128 subfic   215EFFF8   1     SFI       gr10=-8,gr30,ca"
    0| 00112C mulld    7D3DF1D2   1     M         gr9=gr29,gr30
    0| 001130 lwz      80BA0000   1     L4Z       gr5=isx(gr26,0)
    0| 001134 lwz      80D90000   1     L4Z       gr6=iex(gr25,0)
    0| 001138 add      7D4AC214   1     A         gr10=gr10,gr24
    0| 00113C addi     3965FFFF   1     AI        gr11=gr5,-1
    0| 001140 addi     3886FFFF   1     AI        gr4=gr6,-1
    0| 001144 addi     38A5FFFE   1     AI        gr5=gr5,-2
    0| 001148 extsw    7CC707B4   1     EXTS4     gr7=gr6
    0| 00114C extsw    7D6607B4   1     EXTS4     gr6=gr11
    0| 001150 extsw    7C8407B4   1     EXTS4     gr4=gr4
    0| 001154 extsw    7CA507B4   1     EXTS4     gr5=gr5
    0| 001158 rldicr   78EB1F24   1     SLL8      gr11=gr7,3
    0| 00115C rldicr   78CC1F24   1     SLL8      gr12=gr6,3
    0| 001160 rldicr   78BC1F24   1     SLL8      gr28=gr5,3
    0| 001164 rldicr   789D1F24   1     SLL8      gr29=gr4,3
   67|                              CL.154:
    0| 001168 addi     38630001   1     AI        gr3=gr3,1
   69| 00116C add      7C8AE214   1     A         gr4=gr10,gr28
   69| 001170 add      7CAA5A14   1     A         gr5=gr10,gr11
   69| 001174 add      7CCA6214   1     A         gr6=gr10,gr12
   69| 001178 add      7CEAEA14   1     A         gr7=gr10,gr29
    0| 00117C mtspr    7C0903A6   1     LCTR      ctr=gr0
    0| 001180 ori      60210000   1     XNOP      
    0| 001184 ori      60210000   1     XNOP      
    0| 001188 ori      60210000   1     XNOP      
    0|                              CL.815:
   69| 00118C lfdux    7C05F4EE   1     LFDU      fp0,gr5=x[](gr5,gr30,0)
   69| 001190 stfdux   7C06F5EE   1     STFDU     gr6,x[](gr6,gr30,0)=fp0
   69| 001194 lfdux    7C07F4EE   1     LFDU      fp0,gr7=x[](gr7,gr30,0)
   69| 001198 stfdux   7C04F5EE   1     STFDU     gr4,x[](gr4,gr30,0)=fp0
    0| 00119C bc       4200FFF0   1     BCT       ctr=CL.815,taken=100%(100,0)
   67| 0011A0 ld       E8810118   1     L8        gr4=#SPILL11(gr1,280)
    0| 0011A4 add      7D495214   1     A         gr10=gr9,gr10
   67| 0011A8 cmpd     7C241800   1     C8        cr0=gr4,gr3
   67| 0011AC bc       4181FFBC   1     BT        CL.154,cr0,0x2/gt,taken=80%(80,20)
   67|                              CL.159:
   67| 0011B0 ld       E8610118   1     L8        gr3=#SPILL11(gr1,280)
   67| 0011B4 cmpd     7C281800   1     C8        cr0=gr8,gr3
   67| 0011B8 crand    4E218A02   1     CR_N      cr4=cr[04],0x2/gt,0x2/gt,0x2/gt,cr4
   67| 0011BC bc       40910164   1     BF        CL.99,cr4,0x2/gt,taken=50%(0,0)
    0| 0011C0 ld       E8C100E0   1     L8        gr6=#SPILL4(gr1,224)
   67| 0011C4 ld       E9010110   1     L8        gr8=#SPILL10(gr1,272)
    0| 0011C8 ld       E8A100E8   1     L8        gr5=#SPILL5(gr1,232)
    0| 0011CC cmpwi    2C000000   1     C4        cr0=gr0,0
    0| 0011D0 rldicr   78C31764   1     SLL8      gr3=gr6,2
   67| 0011D4 addi     3888FFFF   1     AI        gr4=gr8,-1
    0| 0011D8 subf     7C661850   1     S         gr3=gr3,gr6
   67| 0011DC sradi    7C841674   1     SRA8CA    gr4,ca=gr4,2
    0| 0011E0 mulld    7D65F1D2   1     M         gr11=gr5,gr30
    0| 0011E4 rldicr   78CA26E4   1     SLL8      gr10=gr6,4
    0| 0011E8 rldicr   78691F24   1     SLL8      gr9=gr3,3
   67| 0011EC addi     38600000   1     LI        gr3=0
   67| 0011F0 addze    7CE40194   1     ADDE      gr7,ca=gr4,0,ca
    0| 0011F4 bc       4081012C   1     BF        CL.99,cr0,0x2/gt,taken=20%(20,80)
    0| 0011F8 ld       EB210118   1     L8        gr25=#SPILL11(gr1,280)
    0| 0011FC ld       EB6100C0   1     L8        gr27=#SPILL0(gr1,192)
    0| 001200 ld       EB4100C8   1     L8        gr26=#SPILL1(gr1,200)
    0| 001204 ld       EBA10100   1     L8        gr29=#SPILL8(gr1,256)
    0| 001208 or       7CB72B78   1     LR        gr23=gr5
    0| 00120C ld       EB0100E0   1     L8        gr24=#SPILL4(gr1,224)
    0| 001210 mulld    7D0BC9D2   1     M         gr8=gr11,gr25
    0| 001214 lwz      80BB0000   1     L4Z       gr5=isx(gr27,0)
    0| 001218 lwz      80DA0000   1     L4Z       gr6=iex(gr26,0)
    0| 00121C subfic   209EFFF8   1     SFI       gr4=-8,gr30,ca"
    0| 001220 rldicr   7B0C2EA4   1     SLL8      gr12=gr24,5
    0| 001224 add      7F84EA14   1     A         gr28=gr4,gr29
    0| 001228 mulld    7D4AB9D2   1     M         gr10=gr10,gr23
    0| 00122C mulld    7D29B9D2   1     M         gr9=gr9,gr23
    0| 001230 addi     3BA5FFFF   1     AI        gr29=gr5,-1
    0| 001234 addi     3886FFFF   1     AI        gr4=gr6,-1
    0| 001238 addi     38A5FFFE   1     AI        gr5=gr5,-2
    0| 00123C add      7EC8E214   1     A         gr22=gr8,gr28
    0| 001240 extsw    7CC807B4   1     EXTS4     gr8=gr6
    0| 001244 extsw    7FA607B4   1     EXTS4     gr6=gr29
    0| 001248 extsw    7C8407B4   1     EXTS4     gr4=gr4
    0| 00124C extsw    7CA507B4   1     EXTS4     gr5=gr5
    0| 001250 add      7E8BB214   1     A         gr20=gr11,gr22
    0| 001254 addi     39670001   1     AI        gr11=gr7,1
    0| 001258 mulld    7EACB9D2   1     M         gr21=gr12,gr23
    0| 00125C std      F9610120   1     ST8       #SPILL12(gr1,288)=gr11
    0| 001260 add      7E6AB214   1     A         gr19=gr10,gr22
    0| 001264 add      7E49B214   1     A         gr18=gr9,gr22
    0| 001268 rldicr   79111F24   1     SLL8      gr17=gr8,3
    0| 00126C rldicr   78D01F24   1     SLL8      gr16=gr6,3
    0| 001270 rldicr   78AF1F24   1     SLL8      gr15=gr5,3
    0| 001274 rldicr   788E1F24   1     SLL8      gr14=gr4,3
   67|                              CL.100:
   69| 001278 add      7C8FB214   1     A         gr4=gr15,gr22
   69| 00127C add      7CAFA214   1     A         gr5=gr15,gr20
   69| 001280 add      7CCF9A14   1     A         gr6=gr15,gr19
   69| 001284 add      7CEE9214   1     A         gr7=gr14,gr18
   69| 001288 add      7D109214   1     A         gr8=gr16,gr18
   69| 00128C add      7D2F9214   1     A         gr9=gr15,gr18
   69| 001290 add      7D519A14   1     A         gr10=gr17,gr19
   69| 001294 add      7D719214   1     A         gr11=gr17,gr18
   69| 001298 add      7D8EB214   1     A         gr12=gr14,gr22
   69| 00129C add      7FB0B214   1     A         gr29=gr16,gr22
   69| 0012A0 add      7F91B214   1     A         gr28=gr17,gr22
   69| 0012A4 add      7F6EA214   1     A         gr27=gr14,gr20
   69| 0012A8 add      7F4E9A14   1     A         gr26=gr14,gr19
   69| 0012AC add      7F30A214   1     A         gr25=gr16,gr20
   69| 0012B0 add      7F109A14   1     A         gr24=gr16,gr19
   69| 0012B4 add      7EF1A214   1     A         gr23=gr17,gr20
    0| 0012B8 mtspr    7C0903A6   1     LCTR      ctr=gr0
    0|                              CL.816:
   69| 0012BC lfdux    7C1CF4EE   1     LFDU      fp0,gr28=x[](gr28,gr30,0)
   69| 0012C0 stfdux   7C1DF5EE   1     STFDU     gr29,x[](gr29,gr30,0)=fp0
   69| 0012C4 lfdux    7C17F4EE   1     LFDU      fp0,gr23=x[](gr23,gr30,0)
   69| 0012C8 stfdux   7C19F5EE   1     STFDU     gr25,x[](gr25,gr30,0)=fp0
   69| 0012CC lfdux    7C0AF4EE   1     LFDU      fp0,gr10=x[](gr10,gr30,0)
   69| 0012D0 stfdux   7C18F5EE   1     STFDU     gr24,x[](gr24,gr30,0)=fp0
   69| 0012D4 lfdux    7C0BF4EE   1     LFDU      fp0,gr11=x[](gr11,gr30,0)
   69| 0012D8 stfdux   7C08F5EE   1     STFDU     gr8,x[](gr8,gr30,0)=fp0
   69| 0012DC lfdux    7C0CF4EE   1     LFDU      fp0,gr12=x[](gr12,gr30,0)
   69| 0012E0 stfdux   7C04F5EE   1     STFDU     gr4,x[](gr4,gr30,0)=fp0
   69| 0012E4 lfdux    7C1BF4EE   1     LFDU      fp0,gr27=x[](gr27,gr30,0)
   69| 0012E8 stfdux   7C05F5EE   1     STFDU     gr5,x[](gr5,gr30,0)=fp0
   69| 0012EC lfdux    7C1AF4EE   1     LFDU      fp0,gr26=x[](gr26,gr30,0)
   69| 0012F0 stfdux   7C06F5EE   1     STFDU     gr6,x[](gr6,gr30,0)=fp0
   69| 0012F4 lfdux    7C07F4EE   1     LFDU      fp0,gr7=x[](gr7,gr30,0)
   69| 0012F8 stfdux   7C09F5EE   1     STFDU     gr9,x[](gr9,gr30,0)=fp0
    0| 0012FC bc       4200FFC0   1     BCT       ctr=CL.816,taken=100%(100,0)
   67| 001300 ld       E8810120   1     L8        gr4=#SPILL12(gr1,288)
   67| 001304 addi     38630001   1     AI        gr3=gr3,1
    0| 001308 add      7ED5B214   1     A         gr22=gr21,gr22
    0| 00130C add      7E94AA14   1     A         gr20=gr20,gr21
    0| 001310 add      7E73AA14   1     A         gr19=gr19,gr21
    0| 001314 add      7E52AA14   1     A         gr18=gr18,gr21
   67| 001318 cmpld    7C232040   1     CL8       cr0=gr3,gr4
   67| 00131C bc       4180FF5C   1     BT        CL.100,cr0,0x8/llt,taken=80%(80,20)
   67|                              CL.99:
   67| 001320 bc       409D00A0   1     BF        CL.147,cr7,0x2/gt,taken=50%(0,0)
    0| 001324 cmpwi    2C000000   1     C4        cr0=gr0,0
   67| 001328 addi     38600000   1     LI        gr3=0
    0| 00132C bc       40810094   1     BF        CL.147,cr0,0x2/gt,taken=40%(40,60)
    0| 001330 ld       EB8100C0   1     L8        gr28=#SPILL0(gr1,192)
    0| 001334 ld       EB6100C8   1     L8        gr27=#SPILL1(gr1,200)
    0| 001338 ld       EB4100E8   1     L8        gr26=#SPILL5(gr1,232)
    0| 00133C ld       EB210100   1     L8        gr25=#SPILL8(gr1,256)
    0| 001340 subfic   213EFFF8   1     SFI       gr9=-8,gr30,ca"
    0| 001344 lwz      80DC0000   1     L4Z       gr6=isx(gr28,0)
    0| 001348 lwz      80BB0000   1     L4Z       gr5=iex(gr27,0)
    0| 00134C mulld    7D1AF1D2   1     M         gr8=gr26,gr30
    0| 001350 addi     39450001   1     AI        gr10=gr5,1
    0| 001354 addi     38860001   1     AI        gr4=gr6,1
    0| 001358 addi     38A50002   1     AI        gr5=gr5,2
    0| 00135C extsw    7CC707B4   1     EXTS4     gr7=gr6
    0| 001360 extsw    7D4607B4   1     EXTS4     gr6=gr10
    0| 001364 extsw    7C8407B4   1     EXTS4     gr4=gr4
    0| 001368 extsw    7CA507B4   1     EXTS4     gr5=gr5
    0| 00136C add      7D29CA14   1     A         gr9=gr9,gr25
    0| 001370 rldicr   78EA1F24   1     SLL8      gr10=gr7,3
    0| 001374 rldicr   78CB1F24   1     SLL8      gr11=gr6,3
    0| 001378 rldicr   78BD1F24   1     SLL8      gr29=gr5,3
    0| 00137C rldicr   788C1F24   1     SLL8      gr12=gr4,3
   67|                              CL.142:
    0| 001380 addi     38630001   1     AI        gr3=gr3,1
   71| 001384 add      7C89EA14   1     A         gr4=gr9,gr29
   71| 001388 add      7CA95214   1     A         gr5=gr9,gr10
   71| 00138C add      7CC95A14   1     A         gr6=gr9,gr11
   71| 001390 add      7CE96214   1     A         gr7=gr9,gr12
    0| 001394 mtspr    7C0903A6   1     LCTR      ctr=gr0
    0| 001398 ori      60210000   1     XNOP      
    0|                              CL.817:
   71| 00139C lfdux    7C05F4EE   1     LFDU      fp0,gr5=x[](gr5,gr30,0)
   71| 0013A0 stfdux   7C06F5EE   1     STFDU     gr6,x[](gr6,gr30,0)=fp0
   71| 0013A4 lfdux    7C07F4EE   1     LFDU      fp0,gr7=x[](gr7,gr30,0)
   71| 0013A8 stfdux   7C04F5EE   1     STFDU     gr4,x[](gr4,gr30,0)=fp0
    0| 0013AC bc       4200FFF0   1     BCT       ctr=CL.817,taken=100%(100,0)
   67| 0013B0 ld       E8810118   1     L8        gr4=#SPILL11(gr1,280)
    0| 0013B4 add      7D284A14   1     A         gr9=gr8,gr9
   67| 0013B8 cmpd     7C241800   1     C8        cr0=gr4,gr3
   67| 0013BC bc       4181FFC4   1     BT        CL.142,cr0,0x2/gt,taken=80%(80,20)
   67|                              CL.147:
   67| 0013C0 bc       4091EF38   1     BF        CL.6,cr4,0x2/gt,taken=50%(0,0)
    0| 0013C4 ld       E8C100E0   1     L8        gr6=#SPILL4(gr1,224)
   67| 0013C8 ld       E9210110   1     L8        gr9=#SPILL10(gr1,272)
    0| 0013CC ld       E8A100E8   1     L8        gr5=#SPILL5(gr1,232)
    0| 0013D0 cmpwi    2C000000   1     C4        cr0=gr0,0
    0| 0013D4 rldicr   78C31764   1     SLL8      gr3=gr6,2
   67| 0013D8 addi     3889FFFF   1     AI        gr4=gr9,-1
    0| 0013DC subf     7C661850   1     S         gr3=gr3,gr6
   67| 0013E0 sradi    7C841674   1     SRA8CA    gr4,ca=gr4,2
    0| 0013E4 mulld    7CE5F1D2   1     M         gr7=gr5,gr30
    0| 0013E8 rldicr   78CA26E4   1     SLL8      gr10=gr6,4
    0| 0013EC rldicr   78681F24   1     SLL8      gr8=gr3,3
   67| 0013F0 addi     38600000   1     LI        gr3=0
   67| 0013F4 addze    7C840194   1     ADDE      gr4,ca=gr4,0,ca
    0| 0013F8 bc       4081EF00   1     BF        CL.6,cr0,0x2/gt,taken=20%(20,80)
    0| 0013FC ld       EB210118   1     L8        gr25=#SPILL11(gr1,280)
    0| 001400 ld       EB6100C0   1     L8        gr27=#SPILL0(gr1,192)
    0| 001404 ld       EB4100C8   1     L8        gr26=#SPILL1(gr1,200)
    0| 001408 ld       EAE10100   1     L8        gr23=#SPILL8(gr1,256)
    0| 00140C or       7CAE2B78   1     LR        gr14=gr5
    0| 001410 ld       EB0100E0   1     L8        gr24=#SPILL4(gr1,224)
    0| 001414 mulld    7D27C9D2   1     M         gr9=gr7,gr25
    0| 001418 lwz      80BB0000   1     L4Z       gr5=isx(gr27,0)
    0| 00141C lwz      80DA0000   1     L4Z       gr6=iex(gr26,0)
    0| 001420 subfic   23BEFFF8   1     SFI       gr29=-8,gr30,ca"
    0| 001424 rldicr   7B0C2EA4   1     SLL8      gr12=gr24,5
    0| 001428 add      7F97EA14   1     A         gr28=gr23,gr29
    0| 00142C mulld    7D6A71D2   1     M         gr11=gr10,gr14
    0| 001430 mulld    7D4871D2   1     M         gr10=gr8,gr14
    0| 001434 addi     39060001   1     AI        gr8=gr6,1
    0| 001438 addi     3BA50001   1     AI        gr29=gr5,1
    0| 00143C addi     38C60002   1     AI        gr6=gr6,2
    0| 001440 add      7EC9E214   1     A         gr22=gr9,gr28
    0| 001444 extsw    7CA907B4   1     EXTS4     gr9=gr5
    0| 001448 extsw    7D0807B4   1     EXTS4     gr8=gr8
    0| 00144C extsw    7FA507B4   1     EXTS4     gr5=gr29
    0| 001450 extsw    7CC607B4   1     EXTS4     gr6=gr6
    0| 001454 add      7E87B214   1     A         gr20=gr7,gr22
    0| 001458 addi     38E40001   1     AI        gr7=gr4,1
    0| 00145C mulld    7EAC71D2   1     M         gr21=gr12,gr14
    0| 001460 std      F8E10128   1     ST8       #SPILL13(gr1,296)=gr7
    0| 001464 add      7E6BB214   1     A         gr19=gr11,gr22
    0| 001468 add      7E4AB214   1     A         gr18=gr10,gr22
    0| 00146C rldicr   79311F24   1     SLL8      gr17=gr9,3
    0| 001470 rldicr   79101F24   1     SLL8      gr16=gr8,3
    0| 001474 rldicr   78CF1F24   1     SLL8      gr15=gr6,3
    0| 001478 rldicr   78AE1F24   1     SLL8      gr14=gr5,3
   67|                              CL.106:
   71| 00147C add      7C8F9A14   1     A         gr4=gr15,gr19
   71| 001480 add      7CAEB214   1     A         gr5=gr14,gr22
   71| 001484 add      7CCF9214   1     A         gr6=gr15,gr18
   71| 001488 add      7CF0B214   1     A         gr7=gr16,gr22
   71| 00148C add      7D0E9214   1     A         gr8=gr14,gr18
   71| 001490 add      7D319214   1     A         gr9=gr17,gr18
   71| 001494 add      7D509214   1     A         gr10=gr16,gr18
   71| 001498 add      7D71B214   1     A         gr11=gr17,gr22
   71| 00149C add      7D8FA214   1     A         gr12=gr15,gr20
   71| 0014A0 add      7FAFB214   1     A         gr29=gr15,gr22
   71| 0014A4 add      7F8E9A14   1     A         gr28=gr14,gr19
   71| 0014A8 add      7F6EA214   1     A         gr27=gr14,gr20
   71| 0014AC add      7F519A14   1     A         gr26=gr17,gr19
   71| 0014B0 add      7F31A214   1     A         gr25=gr17,gr20
   71| 0014B4 add      7F109A14   1     A         gr24=gr16,gr19
   71| 0014B8 add      7EF0A214   1     A         gr23=gr16,gr20
    0| 0014BC mtspr    7C0903A6   1     LCTR      ctr=gr0
    0| 0014C0 ori      60210000   1     XNOP      
    0| 0014C4 ori      60210000   1     XNOP      
    0| 0014C8 ori      60210000   1     XNOP      
    0|                              CL.818:
   71| 0014CC lfdux    7C0BF4EE   1     LFDU      fp0,gr11=x[](gr11,gr30,0)
   71| 0014D0 stfdux   7C07F5EE   1     STFDU     gr7,x[](gr7,gr30,0)=fp0
   71| 0014D4 lfdux    7C19F4EE   1     LFDU      fp0,gr25=x[](gr25,gr30,0)
   71| 0014D8 stfdux   7C17F5EE   1     STFDU     gr23,x[](gr23,gr30,0)=fp0
   71| 0014DC lfdux    7C1AF4EE   1     LFDU      fp0,gr26=x[](gr26,gr30,0)
   71| 0014E0 stfdux   7C18F5EE   1     STFDU     gr24,x[](gr24,gr30,0)=fp0
   71| 0014E4 lfdux    7C09F4EE   1     LFDU      fp0,gr9=x[](gr9,gr30,0)
   71| 0014E8 stfdux   7C0AF5EE   1     STFDU     gr10,x[](gr10,gr30,0)=fp0
   71| 0014EC lfdux    7C05F4EE   1     LFDU      fp0,gr5=x[](gr5,gr30,0)
   71| 0014F0 stfdux   7C1DF5EE   1     STFDU     gr29,x[](gr29,gr30,0)=fp0
   71| 0014F4 lfdux    7C1BF4EE   1     LFDU      fp0,gr27=x[](gr27,gr30,0)
   71| 0014F8 stfdux   7C0CF5EE   1     STFDU     gr12,x[](gr12,gr30,0)=fp0
   71| 0014FC lfdux    7C1CF4EE   1     LFDU      fp0,gr28=x[](gr28,gr30,0)
   71| 001500 stfdux   7C04F5EE   1     STFDU     gr4,x[](gr4,gr30,0)=fp0
   71| 001504 lfdux    7C08F4EE   1     LFDU      fp0,gr8=x[](gr8,gr30,0)
   71| 001508 stfdux   7C06F5EE   1     STFDU     gr6,x[](gr6,gr30,0)=fp0
    0| 00150C bc       4200FFC0   1     BCT       ctr=CL.818,taken=100%(100,0)
   67| 001510 ld       E8810128   1     L8        gr4=#SPILL13(gr1,296)
   67| 001514 addi     38630001   1     AI        gr3=gr3,1
    0| 001518 add      7E94AA14   1     A         gr20=gr20,gr21
    0| 00151C add      7ED5B214   1     A         gr22=gr21,gr22
    0| 001520 add      7E73AA14   1     A         gr19=gr19,gr21
    0| 001524 add      7E52AA14   1     A         gr18=gr18,gr21
   67| 001528 cmpld    7C232040   1     CL8       cr0=gr3,gr4
   67| 00152C bc       4080EDCC   1     BF        CL.6,cr0,0x8/llt,taken=20%(20,80)
   67| 001530 b        4BFFFF4C   1     B         CL.106,-1
     |               Tag Table
     | 001534        00000000 00012203 80120000 00001534
     |               Instruction count         1357
     |               Straight-line exec time   1357

 
 
>>>>> COMPILATION UNIT EPILOGUE SECTION <<<<<
 
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    msendrec.f90                07/08/15   15:48:20
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................     179
1501-510  Compilation successful for file msendrec.f90.
1501-543  Object file created.
