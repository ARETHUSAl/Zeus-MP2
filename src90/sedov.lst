IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- sedov.f90 07/08/15 15:48:48
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** sedov   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at sedov.f90 <line 59> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at sedov.f90 <line 60> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] = d0; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns4.[2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns5.[1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] = ((d0 *  1.3806599982553653E-016) * t0) / ((gamm1 * mmw) *  1.6605299201696573E-024); with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns4.[1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] = d0; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns5.[2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)][$$CIV1 + 1ll][$$CIV0 + 1ll] = ((d0 *  1.3806599982553653E-016) * t0) / ((gamm1 * mmw) *  1.6605299201696573E-024); with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 64> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 65> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 63> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 62> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at sedov.f90 <line 66> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*(2ll + ($$CIV6 * 2ll + (long long) kn % 2ll)) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
1586-534 (I) Loop (loop index 4) at sedov.f90 <line 74> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at sedov.f90 <line 75> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 6) at sedov.f90 <line 76> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 6) at sedov.f90 <line 76> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 7) at sedov.f90 <line 74> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 7) at sedov.f90 <line 74> was not SIMD vectorized because it contains unsupported loop structure.
1586-534 (I) Loop (loop index 10) at sedov.f90 <line 74> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 11) at sedov.f90 <line 75> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 12) at sedov.f90 <line 76> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 12) at sedov.f90 <line 76> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 13) at sedov.f90 <line 75> was not SIMD vectorized because the loop is not the innermost loop.
1586-538 (I) Loop (loop index 14) at sedov.f90 <line 76> was not SIMD vectorized because it contains unsupported loop structure.
1586-552 (I) Loop (loop index 14) at sedov.f90 <line 76> was not SIMD vectorized because it contains control flow.
1586-534 (I) Loop (loop index 18) at sedov.f90 <line 59> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 19) at sedov.f90 <line 60> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] = d0; with non-vectorizable strides.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns2.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns4.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 61> was not SIMD vectorized because it contains memory references ((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns5.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] = ((d0 *  1.3806599982553653E-016) * t0) / ((gamm1 * mmw) *  1.6605299201696573E-024); with non-vectorizable strides.
1586-536 (I) Loop (loop index 20) at sedov.f90 <line 64> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 64> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at sedov.f90 <line 64> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 20) at sedov.f90 <line 64> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 20) at sedov.f90 <line 62> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 62> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at sedov.f90 <line 62> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 20) at sedov.f90 <line 62> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 20) at sedov.f90 <line 63> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 63> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at sedov.f90 <line 63> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 20) at sedov.f90 <line 63> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 20) at sedov.f90 <line 65> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 65> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at sedov.f90 <line 65> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 20) at sedov.f90 <line 65> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 20) at sedov.f90 <line 66> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 20) at sedov.f90 <line 66> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 20) at sedov.f90 <line 66> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 20) at sedov.f90 <line 66> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"5">. Total number of the innermost loops SIMD vectorized <"0">.


    10|         SUBROUTINE sedov ()
    34|           d0 =  1.0000000000000000E-008
    35|           t0 =  5.0000000000000000E+001
    36|           eblast =  1.0000000000000000E+050
    37|           rblast =  1.0000000000000000E+012
    39|           IF ((myid == 0)) THEN
    10|             |pgen%nlitems%type.off32 = 4
                    |pgen%nlitems%kind.off40 = 
                    |pgen%nlitems%size.off48 = 
                    |pgen%nlitems%name_addr.off56 = &
                &     "pgend0t0eblastrblastsedov.f90" + 4
                    |pgen%nlitems%name_len.off64 = 2
                    |pgen%nlitems%item_addr.off72 = loc(d0)
                    |pgen%nlitems%type.off80 = 4
                    |pgen%nlitems%kind.off88 = 
                    |pgen%nlitems%size.off96 = 
                    |pgen%nlitems%name_addr.off104 = &
                &     "pgend0t0eblastrblastsedov.f90" + 6
                    |pgen%nlitems%name_len.off112 = 2
                    |pgen%nlitems%item_addr.off120 = loc(t0)
                    |pgen%nlitems%type.off128 = 4
                    |pgen%nlitems%kind.off136 = 
                    |pgen%nlitems%size.off144 = 
                    |pgen%nlitems%name_addr.off152 = &
                &     "pgend0t0eblastrblastsedov.f90" + 8
                    |pgen%nlitems%name_len.off160 = 6
                    |pgen%nlitems%item_addr.off168 = loc(eblast)
                    |pgen%version = 129
                    |pgen%name_addr = "pgend0t0eblastrblastsedov.f90"
                    |pgen%name_len = 4
                    |pgen%num_of_items = 4
                    |pgen%nlitems%type.off176 = 4
                    |pgen%nlitems%kind.off184 = 
                    |pgen%nlitems%size.off192 = 
                    |pgen%nlitems%name_addr.off200 = &
                &     "pgend0t0eblastrblastsedov.f90" + 14
                    |pgen%nlitems%name_len.off208 = 6
                    |pgen%nlitems%item_addr.off216 = loc(rblast)
    40|             |pgen%name_flags = 0
                    #2 = _xlfBeginIO(1,2,#1,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#2))
    41|             |pgen%name_flags = 0
                    #4 = _xlfBeginIO(2,258,#3,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#4))
    42|             buf_in[].off1744 = d0
    43|             buf_in[].off1752 = t0
    44|             buf_in[].off1760 = eblast
    45|             buf_in[].off1768 = rblast
    46|           ENDIF
    47|           T_2 = 4
                  T_3 = 1275070495
                  T_4 = 0
                  CALL mpi_bcast(buf_in,T_2,T_3,T_4,comm3d,ierr)
    49|           IF ((myid <> 0)) THEN
    50|             d0 = buf_in[].off1744
    51|             t0 = buf_in[].off1752
    52|             eblast = buf_in[].off1760
    53|             rblast = buf_in[].off1768
    54|           ENDIF
    59|           IF ((MOD(int(kn), 2) > 0  .AND.  int(kn) > 0)) THEN
                    $$CIV2 = 0
       Id=18        DO $$CIV2 = $$CIV2, MOD(int(kn), int(2))-1
    60|               IF ((int(jn) > 0)) THEN
                        $$CIV1 = 0
       Id=19            DO $$CIV1 = $$CIV1, int(int(jn))-1
    61|                   IF ((int(in) > 0)) THEN
                            $$CIV0 = 0
       Id=20                DO $$CIV0 = $$CIV0, int(int(in))-1
    62|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = d0
    63|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    64|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    65|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    66|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = ((&
                &               d0 *  1.3806599982553653E-016) * t0) / ((gamm1 * &
                &               mmw) *  1.6605299201696573E-024)
    67|                     ENDDO
                          ENDIF
    68|                 ENDDO
                      ENDIF
    69|             ENDDO
                  ENDIF
    59|           IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 2))) THEN
                    $$CIV6 = int(0)
       Id=1         DO $$CIV6 = $$CIV6, int((((int(kn) - MOD(int(kn), 2)) - 1)&
                &        / 2 + 1))-1
    60|               IF ((int(jn) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int(int(jn))-1
    61|                   IF ((int(in) > 0)) THEN
                            $$CIV0 = 0
       Id=3                 DO $$CIV0 = $$CIV0, int(int(in))-1
    62|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = d0
    63|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    64|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    65|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    66|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = ((d0 *  &
                &               1.3806599982553653E-016) * t0) / ((gamm1 * mmw) * &
                &                1.6605299201696573E-024)
    62|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = d0
    63|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    64|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    65|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    66|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = ((d0 *  &
                &               1.3806599982553653E-016) * t0) / ((gamm1 * mmw) * &
                &                1.6605299201696573E-024)
    67|                     ENDDO
                          ENDIF
    68|                 ENDDO
                      ENDIF
    69|             ENDDO
                  ENDIF
    74|           IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
    73|             eb0 = ( 3.0000000000000000E+000 * eblast) / ( &
                &     1.2566370964050292E+001 * (rblast ** 3))
    74|             $$CIV5 = 0
       Id=10        DO $$CIV5 = $$CIV5, MOD(int(kn), int(4))-1
    75|               IF ((int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=11            DO $$CIV4 = $$CIV4, int(int(jn))-1
    76|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=12                DO $$CIV3 = $$CIV3, int(int(in))-1
    77|                       IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = &
                &                 d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) + &
                &                 eb0
                              ENDIF
    78|                     ENDDO
                          ENDIF
    79|                 ENDDO
                      ENDIF
    80|             ENDDO
                  ENDIF
    74|           IF ((int(kn) > MOD(int(kn), 4)  .AND.  int(kn) > 0)) THEN
    73|             eb0 = ( 3.0000000000000000E+000 * eblast) / ( &
                &     1.2566370964050292E+001 * (rblast ** 3))
    74|             $$CIV5 = MOD(int(kn), int(4))
       Id=7         DO $$CIV5 = $$CIV5, int(int(kn))
    75|               IF ((MOD(int(jn), 2) > 0  .AND.  int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=13            DO $$CIV4 = $$CIV4, MOD(int(jn), int(2))-1
    76|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=14                DO $$CIV3 = $$CIV3, int(int(in))-1
    77|                       IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = &
                &                 d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) + &
                &                 eb0
                              ENDIF
    78|                     ENDDO
                          ENDIF
    79|                 ENDDO
                      ENDIF
    80|             ENDDO
                  ENDIF
    74|           IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))) THEN
    73|             eb0 = ( 3.0000000000000000E+000 * eblast) / ( &
                &     1.2566370964050292E+001 * (rblast ** 3))
    74|             $$CIV8 = int(0)
       Id=4         DO $$CIV8 = $$CIV8, int((((int(kn) - MOD(int(kn), 4)) - 1)&
                &        / 4 + 1))-1
    75|               IF ((int(jn) > 0  .AND.  int(jn) > MOD(int(jn), 2))) THEN
                        $$CIV7 = int(0)
       Id=5             DO $$CIV7 = $$CIV7, int((((int(jn) - MOD(int(jn), 2)) &
                &           - 1) / 2 + 1))-1
    76|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(int(in))-1
    77|                       IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
    78|                     ENDDO
                          ENDIF
    79|                 ENDDO
                      ENDIF
    80|             ENDDO
                  ENDIF
    83|           RETURN
                END SUBROUTINE sedov


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            59            18    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            60            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            62                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
         0            63                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            63                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            63                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            63                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 
                                          1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
         0            64                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            64                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            64                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            64                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 
                                          1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
         0            65                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            65                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            65                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            65                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 
                                          1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
         0            66                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            66                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            66                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            66                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-e%addr  + 
                                          d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
         0            59             1    Outer loop has been unrolled 2 time(s).
         0            59             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            60             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            62                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*(1ll + ($$CIV6 * 
                                          2ll + (long long) kn % 2ll)) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
         0            63                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            63                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            63                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            63                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + 
                                          ($$CIV6 * 2ll + (long long) kn % 2ll)) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
         0            64                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            64                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            64                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            64                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + 
                                          ($$CIV6 * 2ll + (long long) kn % 2ll)) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
         0            65                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            65                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            65                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            65                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + 
                                          ($$CIV6 * 2ll + (long long) kn % 2ll)) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
         0            66                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            66                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*(1ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            66                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            66                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-e%addr  + 
                                          d-e%rvo + (d-e%bounds%mult[].off152)*(1ll + ($$CIV6 * 
                                          2ll + (long long) kn % 2ll)) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
         0            62                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            62                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            62                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*(2ll + ($$CIV6 * 
                                          2ll + (long long) kn % 2ll)) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
         0            63                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            63                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            63                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            63                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + 
                                          ($$CIV6 * 2ll + (long long) kn % 2ll)) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
         0            64                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            64                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            64                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            64                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + 
                                          ($$CIV6 * 2ll + (long long) kn % 2ll)) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
         0            65                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            65                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            65                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            65                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + 
                                          ($$CIV6 * 2ll + (long long) kn % 2ll)) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
         0            66                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            66                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*(2ll + ($$CIV6 * 2ll + 
                                          (long long) kn % 2ll)) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            66                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            66                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-e%addr  + 
                                          d-e%rvo + (d-e%bounds%mult[].off152)*(2ll + ($$CIV6 * 
                                          2ll + (long long) kn % 2ll)) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
         0            74            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            75            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            76            12    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            76            12    Loop was not SIMD vectorized because it contains 
                                          control flow.
         0            74             7    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            74             7    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            75            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            76            14    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            76            14    Loop was not SIMD vectorized because it contains 
                                          control flow.
         0            74             4    Outer loop has been unrolled 4 time(s).
         0            74             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            75             5    Outer loop has been unrolled 2 time(s).
         0            75             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            76             6    Loop was not SIMD vectorized because it contains 
                                          unsupported loop structure.
         0            76             6    Loop was not SIMD vectorized because it contains 
                                          control flow.


    10|         SUBROUTINE sedov ()
    34|           d0 =  1.0000000000000000E-008
    35|           t0 =  5.0000000000000000E+001
    36|           eblast =  1.0000000000000000E+050
    37|           rblast =  1.0000000000000000E+012
    39|           IF ((myid == 0)) THEN
    10|             |pgen%nlitems%type.off32 = 4
                    |pgen%nlitems%kind.off40 = 
                    |pgen%nlitems%size.off48 = 
                    |pgen%nlitems%name_addr.off56 = &
                &     "pgend0t0eblastrblastsedov.f90" + 4
                    |pgen%nlitems%name_len.off64 = 2
                    |pgen%nlitems%item_addr.off72 = loc(d0)
                    |pgen%nlitems%type.off80 = 4
                    |pgen%nlitems%kind.off88 = 
                    |pgen%nlitems%size.off96 = 
                    |pgen%nlitems%name_addr.off104 = &
                &     "pgend0t0eblastrblastsedov.f90" + 6
                    |pgen%nlitems%name_len.off112 = 2
                    |pgen%nlitems%item_addr.off120 = loc(t0)
                    |pgen%nlitems%type.off128 = 4
                    |pgen%nlitems%kind.off136 = 
                    |pgen%nlitems%size.off144 = 
                    |pgen%nlitems%name_addr.off152 = &
                &     "pgend0t0eblastrblastsedov.f90" + 8
                    |pgen%nlitems%name_len.off160 = 6
                    |pgen%nlitems%item_addr.off168 = loc(eblast)
                    |pgen%version = 129
                    |pgen%name_addr = "pgend0t0eblastrblastsedov.f90"
                    |pgen%name_len = 4
                    |pgen%num_of_items = 4
                    |pgen%nlitems%type.off176 = 4
                    |pgen%nlitems%kind.off184 = 
                    |pgen%nlitems%size.off192 = 
                    |pgen%nlitems%name_addr.off200 = &
                &     "pgend0t0eblastrblastsedov.f90" + 14
                    |pgen%nlitems%name_len.off208 = 6
                    |pgen%nlitems%item_addr.off216 = loc(rblast)
    40|             |pgen%name_flags = 0
                    #2 = _xlfBeginIO(1,2,#1,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#2))
    41|             |pgen%name_flags = 0
                    #4 = _xlfBeginIO(2,258,#3,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#4))
    42|             buf_in[].off1744 = d0
    43|             buf_in[].off1752 = t0
    44|             buf_in[].off1760 = eblast
    45|             buf_in[].off1768 = rblast
    46|           ENDIF
    47|           T_2 = 4
                  T_3 = 1275070495
                  T_4 = 0
                  CALL mpi_bcast(buf_in,T_2,T_3,T_4,comm3d,ierr)
    49|           IF ((myid <> 0)) THEN
    50|             d0 = buf_in[].off1744
    51|             t0 = buf_in[].off1752
    52|             eblast = buf_in[].off1760
    53|             rblast = buf_in[].off1768
    54|           ENDIF
    59|           IF ((MOD(int(kn), 2) > 0  .AND.  int(kn) > 0)) THEN
                    $$CIV2 = 0
       Id=18        DO $$CIV2 = $$CIV2, MOD(int(kn), int(2))-1
    60|               IF ((int(jn) > 0)) THEN
                        $$CIV1 = 0
       Id=19            DO $$CIV1 = $$CIV1, int(int(jn))-1
    61|                   IF ((int(in) > 0)) THEN
                            $$CIV0 = 0
       Id=20                DO $$CIV0 = $$CIV0, int(int(in))-1
    62|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = d0
    63|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    64|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    65|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    66|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = ((&
                &               d0 *  1.3806599982553653E-016) * t0) / ((gamm1 * &
                &               mmw) *  1.6605299201696573E-024)
    67|                     ENDDO
                          ENDIF
    68|                 ENDDO
                      ENDIF
    69|             ENDDO
                  ENDIF
    59|           IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 2))) THEN
                    $$CIV6 = int(0)
       Id=1         DO $$CIV6 = $$CIV6, int((((int(kn) - MOD(int(kn), 2)) - 1)&
                &        / 2 + 1))-1
    60|               IF ((int(jn) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int(int(jn))-1
    61|                   IF ((int(in) > 0)) THEN
                            $$CIV0 = 0
       Id=3                 DO $$CIV0 = $$CIV0, int(int(in))-1
    62|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = d0
    63|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    64|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    65|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    66|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,1 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = ((d0 *  &
                &               1.3806599982553653E-016) * t0) / ((gamm1 * mmw) * &
                &                1.6605299201696573E-024)
    62|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = d0
    63|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    64|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    65|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * &
                &               2 + MOD(int(kn), 2))) =  0.0000000000000000E+000
    66|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,2 + ($$CIV6 * 2 &
                &               + MOD(int(kn), 2))) = ((d0 *  &
                &               1.3806599982553653E-016) * t0) / ((gamm1 * mmw) * &
                &                1.6605299201696573E-024)
    67|                     ENDDO
                          ENDIF
    68|                 ENDDO
                      ENDIF
    69|             ENDDO
                  ENDIF
    74|           IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
    73|             eb0 = ( 3.0000000000000000E+000 * eblast) / ( &
                &     1.2566370964050292E+001 * (rblast ** 3))
    74|             $$CIV5 = 0
       Id=10        DO $$CIV5 = $$CIV5, MOD(int(kn), int(4))-1
    75|               IF ((int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=11            DO $$CIV4 = $$CIV4, int(int(jn))-1
    76|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=12                DO $$CIV3 = $$CIV3, int(int(in))-1
    77|                       IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = &
                &                 d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) + &
                &                 eb0
                              ENDIF
    78|                     ENDDO
                          ENDIF
    79|                 ENDDO
                      ENDIF
    80|             ENDDO
                  ENDIF
    74|           $$csx0 = int(kn) > MOD(int(kn), 4)  .AND.  int(kn) > 0
                  IF ($$csx0) THEN
    73|             eb0 = ( 3.0000000000000000E+000 * eblast) / ( &
                &     1.2566370964050292E+001 * (rblast ** 3))
    74|             $$CIV5 = MOD(int(kn), int(4))
       Id=7         DO $$CIV5 = $$CIV5, int(int(kn))
    75|               IF ((MOD(int(jn), 2) > 0  .AND.  int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=13            DO $$CIV4 = $$CIV4, MOD(int(jn), int(2))-1
    76|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=14                DO $$CIV3 = $$CIV3, int(int(in))-1
    77|                       IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = &
                &                 d-e%addr%e($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) + &
                &                 eb0
                              ENDIF
    78|                     ENDDO
                          ENDIF
    79|                 ENDDO
                      ENDIF
    80|             ENDDO
                  ENDIF
    74|           IF ($$csx0) THEN
    73|             eb0 = ( 3.0000000000000000E+000 * eblast) / ( &
                &     1.2566370964050292E+001 * (rblast ** 3))
    74|             $$CIV8 = int(0)
       Id=4         DO $$CIV8 = $$CIV8, int((((int(kn) - MOD(int(kn), 4)) - 1)&
                &        / 4 + 1))-1
    75|               IF ((int(jn) > 0  .AND.  int(jn) > MOD(int(jn), 2))) THEN
                        $$CIV7 = int(0)
       Id=5             DO $$CIV7 = $$CIV7, int((((int(jn) - MOD(int(jn), 2)) &
                &           - 1) / 2 + 1))-1
    76|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(int(in))-1
    77|                       IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),1 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),2 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),3 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,1 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
                              IF ((d-x1b%addr%x1b($$CIV3 + 1) < rblast)) THEN
                                d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) = &
                &                 d-e%addr%e($$CIV3 + 1,2 + ($$CIV7 * 2 + MOD(int(&
                &                 jn), 2)),4 + ($$CIV8 * 4 + MOD(int(kn), 4))) + &
                &                 eb0
                              ENDIF
    78|                     ENDDO
                          ENDIF
    79|                 ENDDO
                      ENDIF
    80|             ENDDO
                  ENDIF
    83|           RETURN
                END SUBROUTINE sedov

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- ---- ssss
 CCR's set/used:   ss-- -sss
     | 000000                           PDEF     sedov
   10|                                  PROC      
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 std      FBE1FFD8   1     ST8       #stack(gr1,-40)=gr31
    0| 000014 std      FBC1FFD0   1     ST8       #stack(gr1,-48)=gr30
    0| 000018 std      FBA1FFC8   1     ST8       #stack(gr1,-56)=gr29
    0| 00001C std      FB81FFC0   1     ST8       #stack(gr1,-64)=gr28
    0| 000020 std      FB61FFB8   1     ST8       #stack(gr1,-72)=gr27
    0| 000024 std      FB41FFB0   1     ST8       #stack(gr1,-80)=gr26
    0| 000028 std      FB21FFA8   1     ST8       #stack(gr1,-88)=gr25
    0| 00002C std      FB01FFA0   1     ST8       #stack(gr1,-96)=gr24
    0| 000030 std      FAE1FF98   1     ST8       #stack(gr1,-104)=gr23
    0| 000034 std      FAC1FF90   1     ST8       #stack(gr1,-112)=gr22
    0| 000038 std      FAA1FF88   1     ST8       #stack(gr1,-120)=gr21
    0| 00003C std      FA81FF80   1     ST8       #stack(gr1,-128)=gr20
    0| 000040 std      FA61FF78   1     ST8       #stack(gr1,-136)=gr19
    0| 000044 std      FA41FF70   1     ST8       #stack(gr1,-144)=gr18
    0| 000048 std      FA21FF68   1     ST8       #stack(gr1,-152)=gr17
    0| 00004C std      FA01FF60   1     ST8       #stack(gr1,-160)=gr16
    0| 000050 std      F9E1FF58   1     ST8       #stack(gr1,-168)=gr15
    0| 000054 std      F9C1FF50   1     ST8       #stack(gr1,-176)=gr14
    0| 000058 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 00005C std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000060 stdu     F821FC41   1     ST8U      gr1,#stack(gr1,-960)=gr1
   39| 000064 ld       EBE20000   1     L8        gr31=.&&N&&mpipar(gr2,0)
   34| 000068 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
   35| 00006C addi     38004049   1     LI        gr0=16457
   35| 000070 rldicr   780083C6   1     SLL8      gr0=gr0,48
   35| 000074 std      F8010098   1     ST8       t0(gr1,152)=gr0
   39| 000078 lwz      801F0004   1     L4Z       gr0=<s79:d4:l4>(gr31,4)
   34| 00007C lfd      CBE30030   1     LFL       fp31=+CONSTANT_AREA(gr3,48)
   36| 000080 lfd      CBC30040   1     LFL       fp30=+CONSTANT_AREA(gr3,64)
   37| 000084 lfd      CBA30048   1     LFL       fp29=+CONSTANT_AREA(gr3,72)
   35| 000088 lfs      C3830038   1     LFS       fp28=+CONSTANT_AREA(gr3,56)
   39| 00008C cmpdi    2C200000   1     C8        cr0=gr0,0
   34| 000090 stfd     DBE10090   1     STFL      d0(gr1,144)=fp31
   36| 000094 stfd     DBC100A0   1     STFL      eblast(gr1,160)=fp30
   37| 000098 stfd     DBA100A8   1     STFL      rblast(gr1,168)=fp29
   39| 00009C bc       40820138   1     BF        CL.1,cr0,0x4/eq,taken=60%(60,40)
   40| 0000A0 addi     38A00000   1     LI        gr5=0
    0| 0000A4 addi     38000004   1     LI        gr0=4
   40| 0000A8 stw      90A100C4   1     ST4Z      <a1:d196:l4>(gr1,196)=gr5
    0| 0000AC std      F80100E0   1     ST8       <a1:d224:l8>(gr1,224)=gr0
    0| 0000B0 std      F8010110   1     ST8       <a1:d272:l8>(gr1,272)=gr0
    0| 0000B4 std      F8010140   1     ST8       <a1:d320:l8>(gr1,320)=gr0
    0| 0000B8 std      F80100D0   1     ST8       <a1:d208:l8>(gr1,208)=gr0
    0| 0000BC std      F80100D8   1     ST8       <a1:d216:l8>(gr1,216)=gr0
    0| 0000C0 std      F8010170   1     ST8       <a1:d368:l8>(gr1,368)=gr0
    0| 0000C4 or       7C6A1B78   1     LR        gr10=gr3
    0| 0000C8 addi     38630010   1     AI        gr3=gr3,16
   40| 0000CC ld       EBA20000   1     L8        gr29=.$STATIC(gr2,0)
    0| 0000D0 std      F86100F8   1     ST8       <a1:d248:l8>(gr1,248)=gr3
    0| 0000D4 addi     38000008   1     LI        gr0=8
    0| 0000D8 addi     388A001A   1     AI        gr4=gr10,26
    0| 0000DC std      F80100E8   1     ST8       <a1:d232:l8>(gr1,232)=gr0
    0| 0000E0 std      F80100F0   1     ST8       <a1:d240:l8>(gr1,240)=gr0
    0| 0000E4 std      F8010118   1     ST8       <a1:d280:l8>(gr1,280)=gr0
    0| 0000E8 std      F8010120   1     ST8       <a1:d288:l8>(gr1,288)=gr0
    0| 0000EC std      F8010148   1     ST8       <a1:d328:l8>(gr1,328)=gr0
    0| 0000F0 std      F8010150   1     ST8       <a1:d336:l8>(gr1,336)=gr0
    0| 0000F4 std      F8010178   1     ST8       <a1:d376:l8>(gr1,376)=gr0
    0| 0000F8 addi     386100A8   1     AI        gr3=gr1,168
    0| 0000FC std      F8010180   1     ST8       <a1:d384:l8>(gr1,384)=gr0
    0| 000100 std      F8810188   1     ST8       <a1:d392:l8>(gr1,392)=gr4
   40| 000104 ori      60BE8000   1     OIL       gr30=gr5,0x8000
    0| 000108 addi     38A10090   1     AI        gr5=gr1,144
    0| 00010C std      F8610198   1     ST8       <a1:d408:l8>(gr1,408)=gr3
    0| 000110 addi     38000002   1     LI        gr0=2
    0| 000114 std      F8A10108   1     ST8       <a1:d264:l8>(gr1,264)=gr5
    0| 000118 addi     386A0012   1     AI        gr3=gr10,18
    0| 00011C std      F8010130   1     ST8       <a1:d304:l8>(gr1,304)=gr0
    0| 000120 addi     38810098   1     AI        gr4=gr1,152
    0| 000124 addi     392A0014   1     AI        gr9=gr10,20
    0| 000128 std      F8610128   1     ST8       <a1:d296:l8>(gr1,296)=gr3
    0| 00012C addi     38A00006   1     LI        gr5=6
    0| 000130 addi     38C100A0   1     AI        gr6=gr1,160
    0| 000134 std      F8810138   1     ST8       <a1:d312:l8>(gr1,312)=gr4
    0| 000138 addi     38E00081   1     LI        gr7=129
    0| 00013C addi     390A000C   1     AI        gr8=gr10,12
    0| 000140 std      F8010100   1     ST8       <a1:d256:l8>(gr1,256)=gr0
    0| 000144 std      F8A10190   1     ST8       <a1:d400:l8>(gr1,400)=gr5
    0| 000148 std      F9210158   1     ST8       <a1:d344:l8>(gr1,344)=gr9
    0| 00014C std      F8A10160   1     ST8       <a1:d352:l8>(gr1,352)=gr5
    0| 000150 std      F8C10168   1     ST8       <a1:d360:l8>(gr1,360)=gr6
    0| 000154 stw      90E100C0   1     ST4Z      <a1:d192:l4>(gr1,192)=gr7
    0| 000158 std      F90100C8   1     ST8       <a1:d200:l8>(gr1,200)=gr8
   40| 00015C addi     38600001   1     LI        gr3=1
   40| 000160 addi     38800002   1     LI        gr4=2
   40| 000164 or       7FA5EB78   1     LR        gr5=gr29
   40| 000168 or       7FC6F378   1     LR        gr6=gr30
   40| 00016C addi     38E00000   1     LI        gr7=0
   40| 000170 addi     39000000   1     LI        gr8=0
   40| 000174 addi     392100C0   1     AI        gr9=gr1,192
   40| 000178 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#1",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|pgen,gr9,#def_xlfBeginIO11",_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   40| 00017C ori      60000000   1
   40| 000180 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   40| 000184 ori      60000000   1
   41| 000188 addi     38BD0040   1     AI        gr5=gr29,64
   41| 00018C addi     38600002   1     LI        gr3=2
   41| 000190 addi     38800102   1     LI        gr4=258
   41| 000194 or       7FC6F378   1     LR        gr6=gr30
   41| 000198 addi     38E00000   1     LI        gr7=0
   41| 00019C addi     39000000   1     LI        gr8=0
   41| 0001A0 addi     392100C0   1     AI        gr9=gr1,192
   41| 0001A4 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#3",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|pgen,gr9,#use_xlfBeginIO21,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   41| 0001A8 ori      60000000   1
   41| 0001AC bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   41| 0001B0 ori      60000000   1
   42| 0001B4 lfd      CBE10090   1     LFL       fp31=d0(gr1,144)
   43| 0001B8 lfd      CB810098   1     LFL       fp28=t0(gr1,152)
   44| 0001BC lfd      CBC100A0   1     LFL       fp30=eblast(gr1,160)
   45| 0001C0 lfd      CBA100A8   1     LFL       fp29=rblast(gr1,168)
   42| 0001C4 stfd     DBFF06D0   1     STFL      <s79:d1744:l8>(gr31,1744)=fp31
   43| 0001C8 stfd     DB9F06D8   1     STFL      <s79:d1752:l8>(gr31,1752)=fp28
   44| 0001CC stfd     DBDF06E0   1     STFL      <s79:d1760:l8>(gr31,1760)=fp30
   45| 0001D0 stfd     DBBF06E8   1     STFL      <s79:d1768:l8>(gr31,1768)=fp29
   46|                              CL.1:
   47| 0001D4 addis    3C604C00   1     LIU       gr3=19456
   47| 0001D8 addi     38000004   1     LI        gr0=4
   47| 0001DC addi     3863081F   1     AI        gr3=gr3,2079
   47| 0001E0 stw      90010080   1     ST4Z      T_2(gr1,128)=gr0
   47| 0001E4 addi     38800000   1     LI        gr4=0
   47| 0001E8 stw      90610084   1     ST4Z      T_3(gr1,132)=gr3
   47| 0001EC stw      90810088   1     ST4Z      T_4(gr1,136)=gr4
   47| 0001F0 addi     391F0014   1     AI        gr8=gr31,20
   47| 0001F4 addi     38FF0020   1     AI        gr7=gr31,32
   47| 0001F8 addi     38C10088   1     AI        gr6=gr1,136
   47| 0001FC addi     38A10084   1     AI        gr5=gr1,132
   47| 000200 addi     38810080   1     AI        gr4=gr1,128
   47| 000204 addi     387F06D0   1     AI        gr3=gr31,1744
   47| 000208 bl       48000001   1     CALL      mpi_bcast,6,buf_in[]",gr3,T_2",gr4,T_3",gr5,T_4",gr6,comm3d",gr7,ierr",gr8,#ProcAlias",mpi_bcast",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   47| 00020C ori      60000000   1
   49| 000210 lwz      801F0004   1     L4Z       gr0=<s79:d4:l4>(gr31,4)
   49| 000214 cmpdi    2C200000   1     C8        cr0=gr0,0
   49| 000218 bc       41820014   1     BT        CL.2,cr0,0x4/eq,taken=50%(0,0)
   50| 00021C lfd      CBFF06D0   1     LFL       fp31=<s79:d1744:l8>(gr31,1744)
   51| 000220 lfd      CB9F06D8   1     LFL       fp28=<s79:d1752:l8>(gr31,1752)
   52| 000224 lfd      CBDF06E0   1     LFL       fp30=<s79:d1760:l8>(gr31,1760)
   53| 000228 lfd      CBBF06E8   1     LFL       fp29=<s79:d1768:l8>(gr31,1768)
   54|                              CL.2:
   54| 00022C ld       E8620000   1     L8        gr3=.&&N&&root(gr2,0)
   54| 000230 ld       E8820000   1     L8        gr4=.&&N&&cons(gr2,0)
   54| 000234 ld       E8A20000   1     L8        gr5=.+CONSTANT_AREA(gr2,0)
   59| 000238 ld       E8C20000   1     L8        gr6=.&&N&&param(gr2,0)
   54| 00023C lfd      C84300E8   1     LFL       fp2=<s101:d232:l8>(gr3,232)
   54| 000240 lfd      C8640008   1     LFL       fp3=<s104:d8:l8>(gr4,8)
   54| 000244 lfs      C0050050   1     LFS       fp0=+CONSTANT_AREA(gr5,80)
   54| 000248 lfs      C0250054   1     LFS       fp1=+CONSTANT_AREA(gr5,84)
   59| 00024C lwa      E8E6000A   1     L4A       gr7=<s107:d8:l4>(gr6,8)
   54| 000250 fmul     FC4200F2   1     MFL       fp2=fp2,fp3,fcr
   54| 000254 fmul     FC1F0032   2     MFL       fp0=fp31,fp0,fcr
   59| 000258 std      F8E101C8   1     ST8       #SPILL0(gr1,456)=gr7
   59| 00025C sradi    7CE00E74   1     SRA8CA    gr0,ca=gr7,1
   59| 000260 cmpwi    2C870000   1     C4        cr1=gr7,0
   59| 000264 addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
   54| 000268 fmul     FC220072   1     MFL       fp1=fp2,fp1,fcr
   59| 00026C rldicr   78080FA4   2     SLL8      gr8=gr0,1
   54| 000270 fmul     FC1C0032   1     MFL       fp0=fp28,fp0,fcr
   59| 000274 subf     7D283851   1     S_R       gr9,cr0=gr7,gr8
   59| 000278 std      F90101D0   1     ST8       #SPILL1(gr1,464)=gr8
   59| 00027C crand    4C250A02   1     CR_N      cr0=cr[10],0x2/gt,0x2/gt,0x2/gt,cr0
   59| 000280 std      F92101D8   1     ST8       #SPILL2(gr1,472)=gr9
   54| 000284 fdiv     FC200824   1     DFL       fp1=fp0,fp1,fcr
   59| 000288 bc       4081018C   1     BF        CL.89,cr0,0x2/gt,taken=50%(0,0)
   62| 00028C ld       E9020000   1     L8        gr8=.&&N&field(gr2,0)
   60| 000290 lwa      E8060006   1     L4A       gr0=<s107:d4:l4>(gr6,4)
   61| 000294 lwa      EBA60002   1     L4A       gr29=<s107:d0:l4>(gr6,0)
   59| 000298 addi     38600000   1     LI        gr3=0
   66| 00029C ld       EA680098   1     L8        gr19=<s22:d152:l8>(gr8,152)
   62| 0002A0 ld       EA480030   1     L8        gr18=<s22:d48:l8>(gr8,48)
   63| 0002A4 ld       EA2801D0   1     L8        gr17=<s22:d464:l8>(gr8,464)
   64| 0002A8 ld       EA080238   1     L8        gr16=<s22:d568:l8>(gr8,568)
   65| 0002AC ld       E9E802A0   1     L8        gr15=<s22:d672:l8>(gr8,672)
   64| 0002B0 ld       E9280208   1     L8        gr9=<s22:d520:l8>(gr8,520)
   66| 0002B4 ld       EB6800B0   1     L8        gr27=<s22:d176:l8>(gr8,176)
   62| 0002B8 std      FA4101E8   1     ST8       #SPILL4(gr1,488)=gr18
   65| 0002BC ld       E9480270   1     L8        gr10=<s22:d624:l8>(gr8,624)
   64| 0002C0 ld       EBE80220   1     L8        gr31=<s22:d544:l8>(gr8,544)
   65| 0002C4 ld       EBC80288   1     L8        gr30=<s22:d648:l8>(gr8,648)
   63| 0002C8 std      FA2101F0   1     ST8       #SPILL5(gr1,496)=gr17
   66| 0002CC ld       EAE80068   1     L8        gr23=<s22:d104:l8>(gr8,104)
   66| 0002D0 ld       EAC80080   1     L8        gr22=<s22:d128:l8>(gr8,128)
    0| 0002D4 cmpwi    2F000000   1     C4        cr6=gr0,0
   62| 0002D8 ld       EA880000   1     L8        gr20=<s22:d0:l8>(gr8,0)
   63| 0002DC ld       EAA801A0   1     L8        gr21=<s22:d416:l8>(gr8,416)
   62| 0002E0 ld       E9680018   1     L8        gr11=<s22:d24:l8>(gr8,24)
   63| 0002E4 ld       E98801B8   1     L8        gr12=<s22:d440:l8>(gr8,440)
   62| 0002E8 ld       EB880048   1     L8        gr28=<s22:d72:l8>(gr8,72)
   66| 0002EC std      FA6101E0   1     ST8       #SPILL3(gr1,480)=gr19
   63| 0002F0 ld       EB4801E8   1     L8        gr26=<s22:d488:l8>(gr8,488)
   64| 0002F4 ld       EB280250   1     L8        gr25=<s22:d592:l8>(gr8,592)
   65| 0002F8 ld       EB0802B8   1     L8        gr24=<s22:d696:l8>(gr8,696)
   64| 0002FC std      FA0101F8   1     ST8       #SPILL6(gr1,504)=gr16
   65| 000300 std      F9E10200   1     ST8       #SPILL7(gr1,512)=gr15
   62| 000304 ld       E8880060   1     L8        gr4=<s22:d96:l8>(gr8,96)
   66| 000308 ld       E8A800C8   1     L8        gr5=<s22:d200:l8>(gr8,200)
   63| 00030C ld       E8C80200   1     L8        gr6=<s22:d512:l8>(gr8,512)
   64| 000310 ld       E8E80268   1     L8        gr7=<s22:d616:l8>(gr8,616)
   65| 000314 ld       E90802D0   1     L8        gr8=<s22:d720:l8>(gr8,720)
    0| 000318 bc       40990968   1     BF        CL.264,cr6,0x2/gt,taken=40%(40,60)
    0| 00031C or       7E6E9B78   1     LR        gr14=gr19
    0| 000320 add      7E6AF214   1     A         gr19=gr10,gr30
    0| 000324 add      7D29FA14   1     A         gr9=gr9,gr31
    0| 000328 add      7FF6BA14   1     A         gr31=gr22,gr23
    0| 00032C add      7FCEDA14   1     A         gr30=gr14,gr27
    0| 000330 add      7D4CAA14   1     A         gr10=gr12,gr21
    0| 000334 add      7EFEFA14   1     A         gr23=gr30,gr31
    0| 000338 ld       EBE20000   1     L8        gr31=.+CONSTANT_AREA(gr2,0)
    0| 00033C add      7D6BA214   1     A         gr11=gr11,gr20
    0| 000340 add      7D93C214   1     A         gr12=gr19,gr24
    0| 000344 add      7D29CA14   1     A         gr9=gr9,gr25
    0| 000348 add      7D4AD214   1     A         gr10=gr10,gr26
    0| 00034C add      7D6BE214   1     A         gr11=gr11,gr28
    0| 000350 add      7ECC7A14   1     A         gr22=gr12,gr15
    0| 000354 add      7EA98214   1     A         gr21=gr9,gr16
    0| 000358 add      7E8A8A14   1     A         gr20=gr10,gr17
    0| 00035C add      7E6B9214   1     A         gr19=gr11,gr18
    0| 000360 cmpdi    2C3D0000   1     C8        cr0=gr29,0
    0| 000364 lfs      C01F0058   1     LFS       fp0=+CONSTANT_AREA(gr31,88)
   59|                              CL.84:
   60| 000368 addi     39200000   1     LI        gr9=0
    0| 00036C bc       40810070   1     BF        CL.88,cr0,0x2/gt,taken=50%(0,0)
    0| 000370 or       7E729B78   1     LR        gr18=gr19
    0| 000374 or       7E91A378   1     LR        gr17=gr20
    0| 000378 or       7EB0AB78   1     LR        gr16=gr21
    0| 00037C or       7ECFB378   1     LR        gr15=gr22
    0| 000380 or       7EEEBB78   1     LR        gr14=gr23
   60|                              CL.85:
    0| 000384 addi     39290001   1     AI        gr9=gr9,1
   66| 000388 or       7DCA7378   1     LR        gr10=gr14
   65| 00038C or       7DEB7B78   1     LR        gr11=gr15
   64| 000390 or       7E0C8378   1     LR        gr12=gr16
   63| 000394 or       7E3F8B78   1     LR        gr31=gr17
   62| 000398 or       7E5E9378   1     LR        gr30=gr18
    0| 00039C mtspr    7FA903A6   1     LCTR      ctr=gr29
    0| 0003A0 ori      60210000   1     XNOP      
    0| 0003A4 ori      60210000   1     XNOP      
    0|                              CL.394:
   62| 0003A8 stfdux   7FFE25EE   1     STFDU     gr30,d(gr30,gr4,0)=fp31
   63| 0003AC stfdux   7C1F35EE   1     STFDU     gr31,v1(gr31,gr6,0)=fp0
   64| 0003B0 stfdux   7C0C3DEE   1     STFDU     gr12,v2(gr12,gr7,0)=fp0
   65| 0003B4 stfdux   7C0B45EE   1     STFDU     gr11,v3(gr11,gr8,0)=fp0
   66| 0003B8 stfdux   7C2A2DEE   1     STFDU     gr10,e(gr10,gr5,0)=fp1
    0| 0003BC bc       4200FFEC   1     BCT       ctr=CL.394,taken=100%(100,0)
   68| 0003C0 cmpld    7F290040   1     CL8       cr6=gr9,gr0
    0| 0003C4 add      7E52E214   1     A         gr18=gr18,gr28
    0| 0003C8 add      7E31D214   1     A         gr17=gr17,gr26
    0| 0003CC add      7E10CA14   1     A         gr16=gr16,gr25
    0| 0003D0 add      7DEFC214   1     A         gr15=gr15,gr24
    0| 0003D4 add      7DCEDA14   1     A         gr14=gr14,gr27
   68| 0003D8 bc       4198FFAC   1     BT        CL.85,cr6,0x8/llt,taken=80%(80,20)
   68|                              CL.88:
   69| 0003DC ld       E94101D8   1     L8        gr10=#SPILL2(gr1,472)
    0| 0003E0 ld       E92101E8   1     L8        gr9=#SPILL4(gr1,488)
    0| 0003E4 ld       E96101F0   1     L8        gr11=#SPILL5(gr1,496)
    0| 0003E8 ld       E98101F8   1     L8        gr12=#SPILL6(gr1,504)
    0| 0003EC ld       EBE10200   1     L8        gr31=#SPILL7(gr1,512)
    0| 0003F0 ld       EBC101E0   1     L8        gr30=#SPILL3(gr1,480)
   69| 0003F4 addi     38630001   1     AI        gr3=gr3,1
    0| 0003F8 add      7E699A14   1     A         gr19=gr9,gr19
   69| 0003FC cmpd     7F2A1800   1     C8        cr6=gr10,gr3
    0| 000400 add      7E8BA214   1     A         gr20=gr11,gr20
    0| 000404 add      7EACAA14   1     A         gr21=gr12,gr21
    0| 000408 add      7ED6FA14   1     A         gr22=gr22,gr31
    0| 00040C add      7EF7F214   1     A         gr23=gr23,gr30
   69| 000410 bc       4199FF58   1     BT        CL.84,cr6,0x2/gt,taken=80%(80,20)
   69|                              CL.89:
   59| 000414 ld       E80101C8   1     L8        gr0=#SPILL0(gr1,456)
   59| 000418 ld       E86101D8   1     L8        gr3=#SPILL2(gr1,472)
   59| 00041C cmpd     7F201800   1     C8        cr6=gr0,gr3
   59| 000420 crand    4C25CA02   1     CR_N      cr0=cr[16],0x2/gt,0x2/gt,0x2/gt,cr0
   59| 000424 bc       40810330   1     BF        CL.28,cr0,0x2/gt,taken=50%(0,0)
   62| 000428 ld       E8E20000   1     L8        gr7=.&&N&field(gr2,0)
   60| 00042C ld       EAC20000   1     L8        gr22=.&&N&&param(gr2,0)
   69| 000430 ld       EA8101D0   1     L8        gr20=#SPILL1(gr1,464)
    0| 000434 ld       E9C101D8   1     L8        gr14=#SPILL2(gr1,472)
   59| 000438 addi     3A400000   1     LI        gr18=0
   59| 00043C std      FA410218   1     ST8       #SPILL10(gr1,536)=gr18
   61| 000440 lwa      EA760002   1     L4A       gr19=<s107:d0:l4>(gr22,0)
   64| 000444 ld       E9070238   1     L8        gr8=<s22:d568:l8>(gr7,568)
   63| 000448 ld       E8A701D0   1     L8        gr5=<s22:d464:l8>(gr7,464)
   64| 00044C ld       E9470208   1     L8        gr10=<s22:d520:l8>(gr7,520)
   64| 000450 ld       EB670220   1     L8        gr27=<s22:d544:l8>(gr7,544)
   62| 000454 ld       E8670030   1     L8        gr3=<s22:d48:l8>(gr7,48)
   61| 000458 std      FA610210   1     ST8       #SPILL9(gr1,528)=gr19
   66| 00045C ld       EB870098   1     L8        gr28=<s22:d152:l8>(gr7,152)
   65| 000460 ld       E92702A0   1     L8        gr9=<s22:d672:l8>(gr7,672)
   60| 000464 lwa      EAB60006   1     L4A       gr21=<s107:d4:l4>(gr22,4)
   65| 000468 ld       EBE70270   1     L8        gr31=<s22:d624:l8>(gr7,624)
   65| 00046C ld       EB470288   1     L8        gr26=<s22:d648:l8>(gr7,648)
    0| 000470 rldicr   79130FA4   1     SLL8      gr19=gr8,1
   62| 000474 ld       E8870000   1     L8        gr4=<s22:d0:l8>(gr7,0)
    0| 000478 std      FA610240   1     ST8       #SPILL15(gr1,576)=gr19
   60| 00047C std      FAA10208   1     ST8       #SPILL8(gr1,520)=gr21
   66| 000480 ld       E9870068   1     L8        gr12=<s22:d104:l8>(gr7,104)
   62| 000484 ld       EBC70018   1     L8        gr30=<s22:d24:l8>(gr7,24)
   66| 000488 ld       EAE70080   1     L8        gr23=<s22:d128:l8>(gr7,128)
   65| 00048C ld       EB0702B8   1     L8        gr24=<s22:d696:l8>(gr7,696)
   69| 000490 addi     3814FFFF   1     AI        gr0=gr20,-1
    0| 000494 add      7D4ADA14   1     A         gr10=gr10,gr27
    0| 000498 rldicr   78BB0FA4   1     SLL8      gr27=gr5,1
   69| 00049C sradi    7C0B0E74   1     SRA8CA    gr11,ca=gr0,1
    0| 0004A0 std      FB610248   1     ST8       #SPILL16(gr1,584)=gr27
   63| 0004A4 ld       E8C701A0   1     L8        gr6=<s22:d416:l8>(gr7,416)
   63| 0004A8 ld       EBA701B8   1     L8        gr29=<s22:d440:l8>(gr7,440)
   66| 0004AC ld       E80700B0   1     L8        gr0=<s22:d176:l8>(gr7,176)
   64| 0004B0 ld       EB270250   1     L8        gr25=<s22:d592:l8>(gr7,592)
    0| 0004B4 add      7FFFD214   1     A         gr31=gr31,gr26
    0| 0004B8 rldicr   787A0FA4   1     SLL8      gr26=gr3,1
   63| 0004BC ld       EA0701E8   1     L8        gr16=<s22:d488:l8>(gr7,488)
    0| 0004C0 std      FB410250   1     ST8       #SPILL17(gr1,592)=gr26
    0| 0004C4 add      7D8CBA14   1     A         gr12=gr12,gr23
    0| 0004C8 add      7C84F214   1     A         gr4=gr4,gr30
    0| 0004CC mulld    7FCEE1D2   1     M         gr30=gr14,gr28
   63| 0004D0 std      FA010228   1     ST8       #SPILL12(gr1,552)=gr16
    0| 0004D4 mulld    7EE971D2   1     M         gr23=gr9,gr14
    0| 0004D8 mulld    7EC871D2   1     M         gr22=gr8,gr14
    0| 0004DC mulld    7EA571D2   1     M         gr21=gr5,gr14
    0| 0004E0 mulld    7E8371D2   1     M         gr20=gr3,gr14
    0| 0004E4 rldicr   792E0FA4   1     SLL8      gr14=gr9,1
    0| 0004E8 add      7FF8FA14   1     A         gr31=gr24,gr31
    0| 0004EC std      F9C10238   1     ST8       #SPILL14(gr1,568)=gr14
    0| 0004F0 rldicr   7B8F0FA4   1     SLL8      gr15=gr28,1
    0| 0004F4 add      7F6EFA14   1     A         gr27=gr14,gr31
    0| 0004F8 std      F9E10230   1     ST8       #SPILL13(gr1,560)=gr15
    0| 0004FC add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 000500 ld       E9210240   1     L8        gr9=#SPILL15(gr1,576)
    0| 000504 add      7CC6EA14   1     A         gr6=gr6,gr29
    0| 000508 add      7FAC7A14   1     A         gr29=gr12,gr15
    0| 00050C add      7F9C0214   1     A         gr28=gr28,gr0
    0| 000510 add      7D4ACA14   1     A         gr10=gr10,gr25
   62| 000514 ld       EA270048   1     L8        gr17=<s22:d72:l8>(gr7,72)
    0| 000518 add      7CC68214   1     A         gr6=gr6,gr16
   69| 00051C addze    7E6B0194   1     ADDE      gr19,ca=gr11,0,ca
    0| 000520 ld       E9610208   1     L8        gr11=#SPILL8(gr1,520)
    0| 000524 add      7F40EA14   1     A         gr26=gr0,gr29
    0| 000528 add      7FACE214   1     A         gr29=gr12,gr28
   62| 00052C std      FA210220   1     ST8       #SPILL11(gr1,544)=gr17
    0| 000530 add      7D885214   1     A         gr12=gr8,gr10
    0| 000534 ld       E9010248   1     L8        gr8=#SPILL16(gr1,584)
    0| 000538 add      7F895214   1     A         gr28=gr9,gr10
    0| 00053C add      7D453214   1     A         gr10=gr5,gr6
    0| 000540 ld       E8A10250   1     L8        gr5=#SPILL17(gr1,592)
    0| 000544 add      7C848A14   1     A         gr4=gr4,gr17
    0| 000548 cmpwi    2C0B0000   1     C4        cr0=gr11,0
    0| 00054C add      7D664214   1     A         gr11=gr6,gr8
    0| 000550 add      7D032214   1     A         gr8=gr3,gr4
   62| 000554 ld       E8670060   1     L8        gr3=<s22:d96:l8>(gr7,96)
    0| 000558 add      7D242A14   1     A         gr9=gr4,gr5
   66| 00055C ld       E88700C8   1     L8        gr4=<s22:d200:l8>(gr7,200)
   63| 000560 ld       E8A70200   1     L8        gr5=<s22:d512:l8>(gr7,512)
   64| 000564 ld       E8C70268   1     L8        gr6=<s22:d616:l8>(gr7,616)
   65| 000568 ld       E8E702D0   1     L8        gr7=<s22:d720:l8>(gr7,720)
    0| 00056C bc       408101E8   1     BF        CL.28,cr0,0x2/gt,taken=20%(20,80)
    0| 000570 add      7F5AF214   1     A         gr26=gr26,gr30
    0| 000574 add      7FBDF214   1     A         gr29=gr29,gr30
    0| 000578 std      FB410258   1     ST8       #SPILL18(gr1,600)=gr26
    0| 00057C std      FBA10260   1     ST8       #SPILL19(gr1,608)=gr29
    0| 000580 add      7FD7DA14   1     A         gr30=gr23,gr27
    0| 000584 add      7F77FA14   1     A         gr27=gr23,gr31
    0| 000588 std      FBC10268   1     ST8       #SPILL20(gr1,616)=gr30
    0| 00058C std      FB610270   1     ST8       #SPILL21(gr1,624)=gr27
    0| 000590 add      7FF6E214   1     A         gr31=gr22,gr28
    0| 000594 add      7F8CB214   1     A         gr28=gr12,gr22
    0| 000598 std      FBE10278   1     ST8       #SPILL22(gr1,632)=gr31
    0| 00059C std      FB810280   1     ST8       #SPILL23(gr1,640)=gr28
    0| 0005A0 add      7D8BAA14   1     A         gr12=gr11,gr21
    0| 0005A4 std      F9810288   1     ST8       #SPILL24(gr1,648)=gr12
    0| 0005A8 std      F80102B0   1     ST8       #SPILL29(gr1,688)=gr0
    0| 0005AC ld       EAE10210   1     L8        gr23=#SPILL9(gr1,528)
    0| 0005B0 ld       EAC20000   1     L8        gr22=.+CONSTANT_AREA(gr2,0)
    0| 0005B4 add      7D6AAA14   1     A         gr11=gr10,gr21
    0| 0005B8 add      7D49A214   1     A         gr10=gr9,gr20
    0| 0005BC std      F9610290   1     ST8       #SPILL25(gr1,656)=gr11
    0| 0005C0 add      7D28A214   1     A         gr9=gr8,gr20
    0| 0005C4 std      F9410298   1     ST8       #SPILL26(gr1,664)=gr10
    0| 0005C8 addi     39130001   1     AI        gr8=gr19,1
    0| 0005CC std      F92102A0   1     ST8       #SPILL27(gr1,672)=gr9
    0| 0005D0 std      F90102A8   1     ST8       #SPILL28(gr1,680)=gr8
    0| 0005D4 cmpdi    2C370000   1     C8        cr0=gr23,0
    0| 0005D8 lfs      C0160058   1     LFS       fp0=+CONSTANT_AREA(gr22,88)
   59|                              CL.29:
   60| 0005DC addi     39000000   1     LI        gr8=0
    0| 0005E0 bc       408100D0   1     BF        CL.30,cr0,0x2/gt,taken=20%(20,80)
    0| 0005E4 ld       EAE102A0   1     L8        gr23=#SPILL27(gr1,672)
    0| 0005E8 ld       EAC10298   1     L8        gr22=#SPILL26(gr1,664)
    0| 0005EC ld       EAA10290   1     L8        gr21=#SPILL25(gr1,656)
    0| 0005F0 ld       EA810288   1     L8        gr20=#SPILL24(gr1,648)
    0| 0005F4 ld       EA610280   1     L8        gr19=#SPILL23(gr1,640)
    0| 0005F8 ld       EA410278   1     L8        gr18=#SPILL22(gr1,632)
    0| 0005FC ld       EA210270   1     L8        gr17=#SPILL21(gr1,624)
    0| 000600 ld       EA010268   1     L8        gr16=#SPILL20(gr1,616)
    0| 000604 ld       E9E10260   1     L8        gr15=#SPILL19(gr1,608)
    0| 000608 ld       E9C10258   1     L8        gr14=#SPILL18(gr1,600)
   60|                              CL.31:
    0| 00060C ld       E8010210   1     L8        gr0=#SPILL9(gr1,528)
    0| 000610 addi     39080001   1     AI        gr8=gr8,1
   66| 000614 or       7DC97378   1     LR        gr9=gr14
   66| 000618 or       7DEA7B78   1     LR        gr10=gr15
   65| 00061C or       7E0B8378   1     LR        gr11=gr16
   65| 000620 or       7E2C8B78   1     LR        gr12=gr17
   64| 000624 or       7E5F9378   1     LR        gr31=gr18
   64| 000628 or       7E7E9B78   1     LR        gr30=gr19
   63| 00062C or       7E9DA378   1     LR        gr29=gr20
   63| 000630 or       7EBCAB78   1     LR        gr28=gr21
   62| 000634 or       7EDBB378   1     LR        gr27=gr22
   62| 000638 or       7EFABB78   1     LR        gr26=gr23
    0| 00063C mtspr    7C0903A6   1     LCTR      ctr=gr0
    0| 000640 ori      60210000   1     XNOP      
    0|                              CL.395:
   62| 000644 stfdux   7FFA1DEE   1     STFDU     gr26,d(gr26,gr3,0)=fp31
   62| 000648 stfdux   7FFB1DEE   1     STFDU     gr27,d(gr27,gr3,0)=fp31
   63| 00064C stfdux   7C1C2DEE   1     STFDU     gr28,v1(gr28,gr5,0)=fp0
   63| 000650 stfdux   7C1D2DEE   1     STFDU     gr29,v1(gr29,gr5,0)=fp0
   64| 000654 stfdux   7C1E35EE   1     STFDU     gr30,v2(gr30,gr6,0)=fp0
   64| 000658 stfdux   7C1F35EE   1     STFDU     gr31,v2(gr31,gr6,0)=fp0
   65| 00065C stfdux   7C0C3DEE   1     STFDU     gr12,v3(gr12,gr7,0)=fp0
   65| 000660 stfdux   7C0B3DEE   1     STFDU     gr11,v3(gr11,gr7,0)=fp0
   66| 000664 stfdux   7C2A25EE   1     STFDU     gr10,e(gr10,gr4,0)=fp1
   66| 000668 stfdux   7C2925EE   1     STFDU     gr9,e(gr9,gr4,0)=fp1
    0| 00066C bc       4200FFD8   1     BCT       ctr=CL.395,taken=100%(100,0)
   68| 000670 ld       E9210208   1     L8        gr9=#SPILL8(gr1,520)
    0| 000674 ld       E80102B0   1     L8        gr0=#SPILL29(gr1,688)
    0| 000678 ld       E9410220   1     L8        gr10=#SPILL11(gr1,544)
    0| 00067C ld       E9610228   1     L8        gr11=#SPILL12(gr1,552)
    0| 000680 add      7E73CA14   1     A         gr19=gr19,gr25
    0| 000684 add      7E52CA14   1     A         gr18=gr18,gr25
   68| 000688 cmpld    7F284840   1     CL8       cr6=gr8,gr9
    0| 00068C add      7E31C214   1     A         gr17=gr17,gr24
    0| 000690 add      7EEABA14   1     A         gr23=gr10,gr23
    0| 000694 add      7ECAB214   1     A         gr22=gr10,gr22
    0| 000698 add      7EABAA14   1     A         gr21=gr11,gr21
    0| 00069C add      7E8BA214   1     A         gr20=gr11,gr20
    0| 0006A0 add      7E10C214   1     A         gr16=gr16,gr24
    0| 0006A4 add      7DE07A14   1     A         gr15=gr0,gr15
    0| 0006A8 add      7DC07214   1     A         gr14=gr0,gr14
   68| 0006AC bc       4198FF60   1     BT        CL.31,cr6,0x8/llt,taken=80%(80,20)
   68|                              CL.30:
   69| 0006B0 ld       E9010218   1     L8        gr8=#SPILL10(gr1,536)
    0| 0006B4 ld       E8010230   1     L8        gr0=#SPILL13(gr1,560)
    0| 0006B8 ld       E9210258   1     L8        gr9=#SPILL18(gr1,600)
   69| 0006BC ld       E94102A8   1     L8        gr10=#SPILL28(gr1,680)
    0| 0006C0 ld       E9610260   1     L8        gr11=#SPILL19(gr1,608)
    0| 0006C4 ld       E9810238   1     L8        gr12=#SPILL14(gr1,568)
    0| 0006C8 ld       EBE10268   1     L8        gr31=#SPILL20(gr1,616)
    0| 0006CC ld       EBC10270   1     L8        gr30=#SPILL21(gr1,624)
    0| 0006D0 ld       EBA10240   1     L8        gr29=#SPILL15(gr1,576)
    0| 0006D4 ld       EB810278   1     L8        gr28=#SPILL22(gr1,632)
    0| 0006D8 ld       EB610280   1     L8        gr27=#SPILL23(gr1,640)
    0| 0006DC ld       EB410248   1     L8        gr26=#SPILL16(gr1,584)
    0| 0006E0 ld       EAE10288   1     L8        gr23=#SPILL24(gr1,648)
    0| 0006E4 ld       EAC10290   1     L8        gr22=#SPILL25(gr1,656)
    0| 0006E8 ld       EAA10250   1     L8        gr21=#SPILL17(gr1,592)
    0| 0006EC ld       EA810298   1     L8        gr20=#SPILL26(gr1,664)
    0| 0006F0 ld       EA6102A0   1     L8        gr19=#SPILL27(gr1,672)
   69| 0006F4 addi     39080001   1     AI        gr8=gr8,1
    0| 0006F8 add      7D204A14   1     A         gr9=gr0,gr9
   69| 0006FC std      F9010218   1     ST8       #SPILL10(gr1,536)=gr8
    0| 000700 std      F9210258   1     ST8       #SPILL18(gr1,600)=gr9
   69| 000704 cmpld    7F285040   1     CL8       cr6=gr8,gr10
    0| 000708 add      7D605A14   1     A         gr11=gr0,gr11
    0| 00070C add      7FECFA14   1     A         gr31=gr12,gr31
    0| 000710 std      F9610260   1     ST8       #SPILL19(gr1,608)=gr11
    0| 000714 std      FBE10268   1     ST8       #SPILL20(gr1,616)=gr31
    0| 000718 add      7FCCF214   1     A         gr30=gr12,gr30
    0| 00071C add      7F9CEA14   1     A         gr28=gr28,gr29
    0| 000720 std      FBC10270   1     ST8       #SPILL21(gr1,624)=gr30
    0| 000724 std      FB810278   1     ST8       #SPILL22(gr1,632)=gr28
    0| 000728 add      7F7BEA14   1     A         gr27=gr27,gr29
    0| 00072C add      7EF7D214   1     A         gr23=gr23,gr26
    0| 000730 std      FB610280   1     ST8       #SPILL23(gr1,640)=gr27
    0| 000734 std      FAE10288   1     ST8       #SPILL24(gr1,648)=gr23
    0| 000738 add      7ED6D214   1     A         gr22=gr22,gr26
    0| 00073C add      7E94AA14   1     A         gr20=gr20,gr21
    0| 000740 std      FAC10290   1     ST8       #SPILL25(gr1,656)=gr22
    0| 000744 std      FA810298   1     ST8       #SPILL26(gr1,664)=gr20
    0| 000748 add      7E73AA14   1     A         gr19=gr19,gr21
    0| 00074C std      FA6102A0   1     ST8       #SPILL27(gr1,672)=gr19
   69| 000750 bc       4198FE8C   1     BT        CL.29,cr6,0x8/llt,taken=80%(80,20)
   69|                              CL.28:
   69| 000754 ld       E86101C8   1     L8        gr3=#SPILL0(gr1,456)
   69| 000758 sradi    7C601674   1     SRA8CA    gr0,ca=gr3,2
   69| 00075C addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
   74| 000760 rldicr   78061764   1     SLL8      gr6=gr0,2
   74| 000764 subf     7CE61851   1     S_R       gr7,cr0=gr3,gr6
   74| 000768 crand    4C250A02   1     CR_N      cr0=cr[10],0x2/gt,0x2/gt,0x2/gt,cr0
   74| 00076C bc       408100F4   1     BF        CL.65,cr0,0x2/gt,taken=50%(0,0)
   73| 000770 fmul     FC1D0772   1     MFL       fp0=fp29,fp29,fcr
   73| 000774 ld       EBC20000   1     L8        gr30=.+CONSTANT_AREA(gr2,0)
   75| 000778 ld       EBA20000   1     L8        gr29=.&&N&&param(gr2,0)
   77| 00077C ld       E9020000   1     L8        gr8=.&&N&field(gr2,0)
   77| 000780 ld       E8620000   1     L8        gr3=.&&N&grid(gr2,0)
   73| 000784 fmul     FC1D0032   1     MFL       fp0=fp29,fp0,fcr
   73| 000788 lfs      C03E005C   1     LFS       fp1=+CONSTANT_AREA(gr30,92)
   73| 00078C lfs      C05E0060   1     LFS       fp2=+CONSTANT_AREA(gr30,96)
   75| 000790 lwa      E81D0006   1     L4A       gr0=<s107:d4:l4>(gr29,4)
   77| 000794 ld       E8880068   1     L8        gr4=<s22:d104:l8>(gr8,104)
   77| 000798 ld       E8A80080   1     L8        gr5=<s22:d128:l8>(gr8,128)
   77| 00079C ld       E9480098   1     L8        gr10=<s22:d152:l8>(gr8,152)
   77| 0007A0 ld       E96800B0   1     L8        gr11=<s22:d176:l8>(gr8,176)
   73| 0007A4 fmul     FC3E0072   1     MFL       fp1=fp30,fp1,fcr
   77| 0007A8 ld       E98800C8   1     L8        gr12=<s22:d200:l8>(gr8,200)
   73| 0007AC fmul     FC0000B2   1     MFL       fp0=fp0,fp2,fcr
    0| 0007B0 cmpwi    2C000000   1     C4        cr0=gr0,0
   76| 0007B4 lwa      E93D0002   1     L4A       gr9=<s107:d0:l4>(gr29,0)
   77| 0007B8 ld       E90306C8   1     L8        gr8=<s69:d1736:l8>(gr3,1736)
   77| 0007BC ld       EBE306E0   1     L8        gr31=<s69:d1760:l8>(gr3,1760)
   74| 0007C0 addi     38600000   1     LI        gr3=0
   73| 0007C4 fdiv     FC210024   1     DFL       fp1=fp1,fp0,fcr
    0| 0007C8 bc       408104A4   1     BF        CL.274,cr0,0x2/gt,taken=40%(40,60)
    0| 0007CC add      7CA42A14   1     A         gr5=gr4,gr5
    0| 0007D0 add      7C8A5A14   1     A         gr4=gr10,gr11
    0| 0007D4 add      7CA56214   1     A         gr5=gr5,gr12
   77| 0007D8 add      7FA8FA14   1     A         gr29=gr8,gr31
    0| 0007DC add      7FE42A14   1     A         gr31=gr4,gr5
    0| 0007E0 cmpdi    2C290000   1     C8        cr0=gr9,0
   74|                              CL.59:
   75| 0007E4 addi     38800000   1     LI        gr4=0
    0| 0007E8 bc       40810068   1     BF        CL.64,cr0,0x2/gt,taken=20%(20,80)
    0| 0007EC or       7FFEFB78   1     LR        gr30=gr31
   75|                              CL.60:
   77| 0007F0 or       7FA5EB78   1     LR        gr5=gr29
    0| 0007F4 or       7FC8F378   1     LR        gr8=gr30
   77| 0007F8 lfdu     CC050008   1     LFDU      fp0,gr5=x1b(gr5,8)
    0| 0007FC mtspr    7D2903A6   1     LCTR      ctr=gr9
   77| 000800 fcmpu    FF00E800   1     CFL       cr6=fp0,fp29
   77| 000804 bc       40980010   1     BF        CL.396,cr6,0x20/flt,taken=50%(0,0)
   77| 000808 lfd      C81E0000   1     LFL       fp0=e(gr30,0)
   77| 00080C fadd     FC00082A   1     AFL       fp0=fp0,fp1,fcr
   77| 000810 stfd     D81E0000   1     STFL      e(gr30,0)=fp0
   77|                              CL.396:
    0| 000814 bc       4240002C   1     BCF       ctr=CL.397,taken=0%(0,100)
    0| 000818 ori      60210000   1     XNOP      
    0| 00081C ori      60210000   1     XNOP      
    0|                              CL.398:
   77| 000820 lfdu     CC050008   1     LFDU      fp0,gr5=x1b(gr5,8)
   77| 000824 fcmpu    FF00E800   1     CFL       cr6=fp0,fp29
    0| 000828 add      7D086214   1     A         gr8=gr8,gr12
   77| 00082C bc       40980010   1     BF        CL.399,cr6,0x20/flt,taken=50%(0,0)
   77| 000830 lfd      C8080000   1     LFL       fp0=e(gr8,0)
   77| 000834 fadd     FC00082A   1     AFL       fp0=fp0,fp1,fcr
   77| 000838 stfd     D8080000   1     STFL      e(gr8,0)=fp0
   77|                              CL.399:
    0| 00083C bc       4200FFE4   1     BCT       ctr=CL.398,taken=100%(100,0)
    0|                              CL.397:
   79| 000840 addi     38840001   1     AI        gr4=gr4,1
    0| 000844 add      7FCBF214   1     A         gr30=gr11,gr30
   79| 000848 cmpld    7F240040   1     CL8       cr6=gr4,gr0
   79| 00084C bc       4198FFA4   1     BT        CL.60,cr6,0x8/llt,taken=80%(80,20)
   79|                              CL.64:
   80| 000850 addi     38630001   1     AI        gr3=gr3,1
    0| 000854 add      7FFF5214   1     A         gr31=gr31,gr10
   80| 000858 cmpd     7F271800   1     C8        cr6=gr7,gr3
   80| 00085C bc       4199FF88   1     BT        CL.59,cr6,0x2/gt,taken=80%(80,20)
   80|                              CL.65:
   74| 000860 ld       E80101C8   1     L8        gr0=#SPILL0(gr1,456)
   74| 000864 cmpd     7C203800   1     C8        cr0=gr0,gr7
   74| 000868 crand    4CA12A02   1     CR_N      cr1=cr[01],0x2/gt,0x2/gt,0x2/gt,cr1
   74| 00086C bc       40850398   1     BF        CL.103,cr1,0x2/gt,taken=30%(30,70)
   73| 000870 fmul     FC1D0772   1     MFL       fp0=fp29,fp29,fcr
   73| 000874 ld       EB220000   1     L8        gr25=.+CONSTANT_AREA(gr2,0)
   75| 000878 ld       EB020000   1     L8        gr24=.&&N&&param(gr2,0)
   77| 00087C ld       E9820000   1     L8        gr12=.&&N&field(gr2,0)
   77| 000880 ld       EBA20000   1     L8        gr29=.&&N&grid(gr2,0)
   74| 000884 or       7CE83B78   1     LR        gr8=gr7
   73| 000888 fmul     FC1D0032   1     MFL       fp0=fp29,fp0,fcr
   73| 00088C lfs      C039005C   1     LFS       fp1=+CONSTANT_AREA(gr25,92)
   73| 000890 lfs      C0590060   1     LFS       fp2=+CONSTANT_AREA(gr25,96)
   75| 000894 lwa      E9380006   1     L4A       gr9=<s107:d4:l4>(gr24,4)
   77| 000898 ld       E8AC0098   1     L8        gr5=<s22:d152:l8>(gr12,152)
   77| 00089C ld       E94C0068   1     L8        gr10=<s22:d104:l8>(gr12,104)
   77| 0008A0 ld       E96C0080   1     L8        gr11=<s22:d128:l8>(gr12,128)
   77| 0008A4 ld       E88C00B0   1     L8        gr4=<s22:d176:l8>(gr12,176)
   73| 0008A8 fmul     FC3E0072   1     MFL       fp1=fp30,fp1,fcr
   77| 0008AC sradi    7D200E74   2     SRA8CA    gr0,ca=gr9,1
   73| 0008B0 fmul     FC0000B2   1     MFL       fp0=fp0,fp2,fcr
   77| 0008B4 addze    7C600194   1     ADDE      gr3,ca=gr0,0,ca
   77| 0008B8 ld       E80C00C8   1     L8        gr0=<s22:d200:l8>(gr12,200)
    0| 0008BC add      7F6A5A14   1     A         gr27=gr10,gr11
   75| 0008C0 rldicr   78630FA4   1     SLL8      gr3=gr3,1
   77| 0008C4 ld       E99D06C8   1     L8        gr12=<s69:d1736:l8>(gr29,1736)
   73| 0008C8 fdiv     FC010024   1     DFL       fp0=fp1,fp0,fcr
   76| 0008CC lwa      EBF80002   1     L4A       gr31=<s107:d0:l4>(gr24,0)
   77| 0008D0 ld       EB9D06E0   1     L8        gr28=<s69:d1760:l8>(gr29,1760)
    0| 0008D4 mulld    7FA539D2   1     M         gr29=gr5,gr7
    0| 0008D8 add      7F60DA14   1     A         gr27=gr0,gr27
    0| 0008DC add      7F442A14   1     A         gr26=gr4,gr5
   75| 0008E0 subf     7FC34851   1     S_R       gr30,cr0=gr9,gr3
   75| 0008E4 cmpwi    2F890000   1     C4        cr7=gr9,0
    0| 0008E8 add      7F7ADA14   1     A         gr27=gr26,gr27
   75| 0008EC crand    4C3D0A02   1     CR_N      cr0=cr[70],0x2/gt,0x2/gt,0x2/gt,cr0
    0| 0008F0 bc       40810094   1     BF        CL.180,cr0,0x2/gt,taken=20%(20,80)
   77| 0008F4 add      7EECE214   1     A         gr23=gr12,gr28
    0| 0008F8 add      7F3BEA14   1     A         gr25=gr27,gr29
    0| 0008FC cmpdi    2C3F0000   1     C8        cr0=gr31,0
   74|                              CL.52:
   75| 000900 addi     3BA00000   1     LI        gr29=0
    0| 000904 bc       4081006C   1     BF        CL.70,cr0,0x2/gt,taken=20%(20,80)
    0| 000908 or       7F38CB78   1     LR        gr24=gr25
   75|                              CL.66:
   77| 00090C or       7EFBBB78   1     LR        gr27=gr23
    0| 000910 or       7F1AC378   1     LR        gr26=gr24
   77| 000914 lfdu     CC3B0008   1     LFDU      fp1,gr27=x1b(gr27,8)
    0| 000918 mtspr    7FE903A6   1     LCTR      ctr=gr31
   77| 00091C fcmpu    FF01E800   1     CFL       cr6=fp1,fp29
   77| 000920 bc       40980010   1     BF        CL.400,cr6,0x20/flt,taken=50%(0,0)
   77| 000924 lfd      C8380000   1     LFL       fp1=e(gr24,0)
   77| 000928 fadd     FC21002A   1     AFL       fp1=fp1,fp0,fcr
   77| 00092C stfd     D8380000   1     STFL      e(gr24,0)=fp1
   77|                              CL.400:
    0| 000930 bc       42400030   1     BCF       ctr=CL.401,taken=0%(0,100)
    0| 000934 ori      60210000   1     XNOP      
    0| 000938 ori      60210000   1     XNOP      
    0| 00093C ori      60210000   1     XNOP      
    0|                              CL.402:
   77| 000940 lfdu     CC3B0008   1     LFDU      fp1,gr27=x1b(gr27,8)
   77| 000944 fcmpu    FF01E800   1     CFL       cr6=fp1,fp29
    0| 000948 add      7F40D214   1     A         gr26=gr0,gr26
   77| 00094C bc       40980010   1     BF        CL.403,cr6,0x20/flt,taken=50%(0,0)
   77| 000950 lfd      C83A0000   1     LFL       fp1=e(gr26,0)
   77| 000954 fadd     FC21002A   1     AFL       fp1=fp1,fp0,fcr
   77| 000958 stfd     D83A0000   1     STFL      e(gr26,0)=fp1
   77|                              CL.403:
    0| 00095C bc       4200FFE4   1     BCT       ctr=CL.402,taken=100%(100,0)
    0|                              CL.401:
   79| 000960 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 000964 add      7F04C214   1     A         gr24=gr4,gr24
   79| 000968 cmpd     7F3DF000   1     C8        cr6=gr29,gr30
   79| 00096C bc       4198FFA0   1     BT        CL.66,cr6,0x1/lt,taken=80%(80,20)
   79|                              CL.70:
   80| 000970 ld       EBA101C8   1     L8        gr29=#SPILL0(gr1,456)
   80| 000974 addi     39080001   1     AI        gr8=gr8,1
    0| 000978 add      7F392A14   1     A         gr25=gr25,gr5
   80| 00097C cmpld    7F28E840   1     CL8       cr6=gr8,gr29
   80| 000980 bc       4198FF80   1     BT        CL.52,cr6,0x8/llt,taken=80%(80,20)
    0|                              CL.180:
   74| 000984 bc       40850280   1     BF        CL.103,cr1,0x2/gt,taken=30%(30,70)
    0| 000988 mulld    7D0539D2   1     M         gr8=gr5,gr7
    0| 00098C add      7D4A5A14   1     A         gr10=gr10,gr11
    0| 000990 mulld    7CE4F1D2   1     M         gr7=gr4,gr30
    0| 000994 add      7D405214   1     A         gr10=gr0,gr10
   80| 000998 addi     38C6FFFF   1     AI        gr6=gr6,-1
    0| 00099C add      7D085214   1     A         gr8=gr8,gr10
   80| 0009A0 sradi    7CC61674   1     SRA8CA    gr6,ca=gr6,2
    0| 0009A4 add      7CE74214   1     A         gr7=gr7,gr8
   75| 0009A8 cmpd     7CA9F000   1     C8        cr1=gr9,gr30
    0| 0009AC rldicr   789E0FA4   1     SLL8      gr30=gr4,1
    0| 0009B0 rldicr   78BD1764   1     SLL8      gr29=gr5,2
    0| 0009B4 addi     3B63FFFF   1     AI        gr27=gr3,-1
   80| 0009B8 addze    7C660194   1     ADDE      gr3,ca=gr6,0,ca
    0| 0009BC add      7CC43A14   1     A         gr6=gr4,gr7
    0| 0009C0 add      7D042A14   1     A         gr8=gr4,gr5
   74| 0009C4 addi     38800000   1     LI        gr4=0
   75| 0009C8 crand    4C3D2A02   1     CR_N      cr0=cr[71],0x2/gt,0x2/gt,0x2/gt,cr0
   74| 0009CC std      F88102B8   1     ST8       #SPILL30(gr1,696)=gr4
    0| 0009D0 add      7D27F214   1     A         gr9=gr7,gr30
    0| 0009D4 rldicr   78AA0FA4   1     SLL8      gr10=gr5,1
    0| 0009D8 subf     7D65E850   1     S         gr11=gr29,gr5
    0| 0009DC bc       40810228   1     BF        CL.103,cr0,0x2/gt,taken=20%(20,80)
    0| 0009E0 sradi    7F640E74   1     SRA8CA    gr4,ca=gr27,1
   77| 0009E4 add      7F8CE214   1     A         gr28=gr12,gr28
    0| 0009E8 add      7D89EA14   1     A         gr12=gr9,gr29
    0| 0009EC add      7EC65214   1     A         gr22=gr6,gr10
    0| 0009F0 std      F98102C0   1     ST8       #SPILL31(gr1,704)=gr12
    0| 0009F4 std      FAC102C8   1     ST8       #SPILL32(gr1,712)=gr22
    0| 0009F8 add      7EA65A14   1     A         gr21=gr6,gr11
    0| 0009FC add      7F254A14   1     A         gr25=gr5,gr9
    0| 000A00 std      FAA102D0   1     ST8       #SPILL33(gr1,720)=gr21
    0| 000A04 addi     38A30001   1     AI        gr5=gr3,1
    0| 000A08 addze    7C640194   1     ADDE      gr3,ca=gr4,0,ca
    0| 000A0C std      F8A102D8   1     ST8       #SPILL34(gr1,728)=gr5
    0| 000A10 std      F86102E0   1     ST8       #SPILL35(gr1,736)=gr3
    0| 000A14 add      7F695A14   1     A         gr27=gr9,gr11
    0| 000A18 add      7F495214   1     A         gr26=gr9,gr10
    0| 000A1C add      7F074214   1     A         gr24=gr7,gr8
    0| 000A20 add      7EE6EA14   1     A         gr23=gr6,gr29
    0| 000A24 cmpdi    2C3F0000   1     C8        cr0=gr31,0
   74|                              CL.35:
   75| 000A28 addi     38600000   1     LI        gr3=0
    0| 000A2C bc       40810188   1     BF        CL.36,cr0,0x2/gt,taken=20%(20,80)
    0| 000A30 ld       E88102E0   1     L8        gr4=#SPILL35(gr1,736)
    0| 000A34 ld       EAC102C0   1     L8        gr22=#SPILL31(gr1,704)
    0| 000A38 ld       EAA102D0   1     L8        gr21=#SPILL33(gr1,720)
    0| 000A3C ld       EA8102C8   1     L8        gr20=#SPILL32(gr1,712)
    0| 000A40 or       7EF3BB78   1     LR        gr19=gr23
    0| 000A44 or       7F12C378   1     LR        gr18=gr24
    0| 000A48 or       7F71DB78   1     LR        gr17=gr27
    0| 000A4C or       7F30CB78   1     LR        gr16=gr25
    0| 000A50 or       7F4FD378   1     LR        gr15=gr26
    0| 000A54 addi     39C40001   1     AI        gr14=gr4,1
   75|                              CL.37:
   77| 000A58 or       7F84E378   1     LR        gr4=gr28
    0| 000A5C mtspr    7FE903A6   1     LCTR      ctr=gr31
    0| 000A60 or       7E459378   1     LR        gr5=gr18
    0| 000A64 or       7E068378   1     LR        gr6=gr16
    0| 000A68 or       7E87A378   1     LR        gr7=gr20
    0| 000A6C or       7DE87B78   1     LR        gr8=gr15
    0| 000A70 or       7EA9AB78   1     LR        gr9=gr21
    0| 000A74 or       7E2A8B78   1     LR        gr10=gr17
    0| 000A78 or       7E6B9B78   1     LR        gr11=gr19
    0| 000A7C or       7ECCB378   1     LR        gr12=gr22
   77| 000A80 lfdu     CC440008   1     LFDU      fp2,gr4=x1b(gr4,8)
    0| 000A84 bc       4240009C   1     BCF       ctr=CL.444,taken=0%(0,100)
    0| 000A88 ori      60210000   1     XNOP      
    0| 000A8C ori      60210000   1     XNOP      
   76|                              CL.39:
   77| 000A90 fcmpu    FC82E800   1     CFL       cr1=fp2,fp29
   77| 000A94 lfdu     CC440008   1     LFDU      fp2,gr4=x1b(gr4,8)
   77| 000A98 bc       40840064   1     BF        CL.77,cr1,0x20/flt,taken=50%(0,0)
   77| 000A9C lfd      C8250000   1     LFL       fp1=e(gr5,0)
   77| 000AA0 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AA4 stfd     D8250000   1     STFL      e(gr5,0)=fp1
   77| 000AA8 lfd      C8260000   1     LFL       fp1=e(gr6,0)
   77| 000AAC fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AB0 stfd     D8260000   1     STFL      e(gr6,0)=fp1
   77| 000AB4 lfd      C8270000   1     LFL       fp1=e(gr7,0)
   77| 000AB8 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000ABC stfd     D8270000   1     STFL      e(gr7,0)=fp1
   77| 000AC0 lfd      C8280000   1     LFL       fp1=e(gr8,0)
   77| 000AC4 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AC8 stfd     D8280000   1     STFL      e(gr8,0)=fp1
   77| 000ACC lfd      C8290000   1     LFL       fp1=e(gr9,0)
   77| 000AD0 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AD4 stfd     D8290000   1     STFL      e(gr9,0)=fp1
   77| 000AD8 lfd      C82A0000   1     LFL       fp1=e(gr10,0)
   77| 000ADC fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AE0 stfd     D82A0000   1     STFL      e(gr10,0)=fp1
   77| 000AE4 lfd      C82B0000   1     LFL       fp1=e(gr11,0)
   77| 000AE8 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AEC stfd     D82B0000   1     STFL      e(gr11,0)=fp1
   77| 000AF0 lfd      C82C0000   1     LFL       fp1=e(gr12,0)
   77| 000AF4 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000AF8 stfd     D82C0000   1     STFL      e(gr12,0)=fp1
   77|                              CL.77:
    0| 000AFC add      7CA02A14   1     A         gr5=gr0,gr5
    0| 000B00 add      7D605A14   1     A         gr11=gr0,gr11
    0| 000B04 add      7CE03A14   1     A         gr7=gr0,gr7
    0| 000B08 add      7D806214   1     A         gr12=gr0,gr12
    0| 000B0C add      7D405214   1     A         gr10=gr0,gr10
    0| 000B10 add      7CC03214   1     A         gr6=gr0,gr6
    0| 000B14 add      7D004214   1     A         gr8=gr0,gr8
    0| 000B18 add      7D204A14   1     A         gr9=gr0,gr9
    0| 000B1C bc       4200FF74   1     BCT       ctr=CL.39,taken=100%(100,0)
    0|                              CL.444:
   77| 000B20 fcmpu    FC82E800   1     CFL       cr1=fp2,fp29
   79| 000B24 addi     38630001   1     AI        gr3=gr3,1
    0| 000B28 add      7ED6F214   1     A         gr22=gr22,gr30
   79| 000B2C cmpld    7F237040   1     CL8       cr6=gr3,gr14
    0| 000B30 add      7EB5F214   1     A         gr21=gr21,gr30
    0| 000B34 add      7E94F214   1     A         gr20=gr20,gr30
   77| 000B38 bc       40840064   1     BF        CL.443,cr1,0x20/flt,taken=50%(0,0)
   77| 000B3C lfd      C8250000   1     LFL       fp1=e(gr5,0)
   77| 000B40 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B44 stfd     D8250000   1     STFL      e(gr5,0)=fp1
   77| 000B48 lfd      C8260000   1     LFL       fp1=e(gr6,0)
   77| 000B4C fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B50 stfd     D8260000   1     STFL      e(gr6,0)=fp1
   77| 000B54 lfd      C8270000   1     LFL       fp1=e(gr7,0)
   77| 000B58 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B5C stfd     D8270000   1     STFL      e(gr7,0)=fp1
   77| 000B60 lfd      C8280000   1     LFL       fp1=e(gr8,0)
   77| 000B64 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B68 stfd     D8280000   1     STFL      e(gr8,0)=fp1
   77| 000B6C lfd      C8290000   1     LFL       fp1=e(gr9,0)
   77| 000B70 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B74 stfd     D8290000   1     STFL      e(gr9,0)=fp1
   77| 000B78 lfd      C82A0000   1     LFL       fp1=e(gr10,0)
   77| 000B7C fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B80 stfd     D82A0000   1     STFL      e(gr10,0)=fp1
   77| 000B84 lfd      C82B0000   1     LFL       fp1=e(gr11,0)
   77| 000B88 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B8C stfd     D82B0000   1     STFL      e(gr11,0)=fp1
   77| 000B90 lfd      C82C0000   1     LFL       fp1=e(gr12,0)
   77| 000B94 fadd     FC20082A   1     AFL       fp1=fp0,fp1,fcr
   77| 000B98 stfd     D82C0000   1     STFL      e(gr12,0)=fp1
   77|                              CL.443:
    0| 000B9C add      7E73F214   1     A         gr19=gr19,gr30
    0| 000BA0 add      7E52F214   1     A         gr18=gr18,gr30
    0| 000BA4 add      7E31F214   1     A         gr17=gr17,gr30
    0| 000BA8 add      7E10F214   1     A         gr16=gr16,gr30
    0| 000BAC add      7DEFF214   1     A         gr15=gr15,gr30
   79| 000BB0 bc       4198FEA8   1     BT        CL.37,cr6,0x8/llt,taken=80%(80,20)
   79|                              CL.36:
   80| 000BB4 ld       E86102B8   1     L8        gr3=#SPILL30(gr1,696)
    0| 000BB8 ld       E88102C0   1     L8        gr4=#SPILL31(gr1,704)
   80| 000BBC ld       E8A102D8   1     L8        gr5=#SPILL34(gr1,728)
    0| 000BC0 ld       E8C102C8   1     L8        gr6=#SPILL32(gr1,712)
    0| 000BC4 ld       E8E102D0   1     L8        gr7=#SPILL33(gr1,720)
    0| 000BC8 add      7F7BEA14   1     A         gr27=gr27,gr29
   80| 000BCC addi     38630001   1     AI        gr3=gr3,1
    0| 000BD0 add      7C84EA14   1     A         gr4=gr4,gr29
   80| 000BD4 std      F86102B8   1     ST8       #SPILL30(gr1,696)=gr3
   80| 000BD8 cmpld    7CA32840   1     CL8       cr1=gr3,gr5
    0| 000BDC std      F88102C0   1     ST8       #SPILL31(gr1,704)=gr4
    0| 000BE0 add      7CC6EA14   1     A         gr6=gr6,gr29
    0| 000BE4 add      7CE7EA14   1     A         gr7=gr7,gr29
    0| 000BE8 std      F8C102C8   1     ST8       #SPILL32(gr1,712)=gr6
    0| 000BEC std      F8E102D0   1     ST8       #SPILL33(gr1,720)=gr7
    0| 000BF0 add      7F5AEA14   1     A         gr26=gr26,gr29
    0| 000BF4 add      7F39EA14   1     A         gr25=gr25,gr29
    0| 000BF8 add      7F18EA14   1     A         gr24=gr24,gr29
    0| 000BFC add      7EF7EA14   1     A         gr23=gr23,gr29
   80| 000C00 bc       4184FE28   1     BT        CL.35,cr1,0x8/llt,taken=80%(80,20)
   83|                              CL.103:
   83| 000C04 ld       E98103D0   1     L8        gr12=#stack(gr1,976)
   83| 000C08 lfd      CBE103B8   1     LFL       fp31=#stack(gr1,952)
   83| 000C0C lfd      CBC103B0   1     LFL       fp30=#stack(gr1,944)
   83| 000C10 lfd      CBA103A8   1     LFL       fp29=#stack(gr1,936)
   83| 000C14 lfd      CB8103A0   1     LFL       fp28=#stack(gr1,928)
   83| 000C18 addi     382103C0   1     AI        gr1=gr1,960
   83| 000C1C mtspr    7D8803A6   1     LLR       lr=gr12
   83| 000C20 ld       E9C1FF50   1     L8        gr14=#stack(gr1,-176)
   83| 000C24 ld       E9E1FF58   1     L8        gr15=#stack(gr1,-168)
   83| 000C28 ld       EA01FF60   1     L8        gr16=#stack(gr1,-160)
   83| 000C2C ld       EA21FF68   1     L8        gr17=#stack(gr1,-152)
   83| 000C30 ld       EA41FF70   1     L8        gr18=#stack(gr1,-144)
   83| 000C34 ld       EA61FF78   1     L8        gr19=#stack(gr1,-136)
   83| 000C38 ld       EA81FF80   1     L8        gr20=#stack(gr1,-128)
   83| 000C3C ld       EAA1FF88   1     L8        gr21=#stack(gr1,-120)
   83| 000C40 ld       EAC1FF90   1     L8        gr22=#stack(gr1,-112)
   83| 000C44 ld       EAE1FF98   1     L8        gr23=#stack(gr1,-104)
   83| 000C48 ld       EB01FFA0   1     L8        gr24=#stack(gr1,-96)
   83| 000C4C ld       EB21FFA8   1     L8        gr25=#stack(gr1,-88)
   83| 000C50 ld       EB41FFB0   1     L8        gr26=#stack(gr1,-80)
   83| 000C54 ld       EB61FFB8   1     L8        gr27=#stack(gr1,-72)
   83| 000C58 ld       EB81FFC0   1     L8        gr28=#stack(gr1,-64)
   83| 000C5C ld       EBA1FFC8   1     L8        gr29=#stack(gr1,-56)
   83| 000C60 ld       EBC1FFD0   1     L8        gr30=#stack(gr1,-48)
   83| 000C64 ld       EBE1FFD8   1     L8        gr31=#stack(gr1,-40)
   83| 000C68 bclr     4E800020   1     BA        lr
   79|                              CL.274:
    0| 000C6C mtspr    7CE903A6   1     LCTR      ctr=gr7
   79|                              CL.222:
   80| 000C70 addi     38630001   1     AI        gr3=gr3,1
   80| 000C74 cmpd     7C233800   1     C8        cr0=gr3,gr7
   80| 000C78 bc       4100FFF8   1     BCTT      ctr=CL.222,cr0,0x1/lt,taken=80%(80,20)
    0| 000C7C b        4BFFFBE4   1     B         CL.65,-1
   68|                              CL.264:
    0| 000C80 ld       E80101D8   1     L8        gr0=#SPILL2(gr1,472)
    0| 000C84 mtspr    7C0903A6   1     LCTR      ctr=gr0
   68|                              CL.253:
   69| 000C88 addi     38630001   1     AI        gr3=gr3,1
   69| 000C8C cmpd     7F230000   1     C8        cr6=gr3,gr0
   69| 000C90 bc       4118FFF8   1     BCTT      ctr=CL.253,cr6,0x1/lt,taken=80%(80,20)
    0| 000C94 b        4BFFF780   1     B         CL.89,-1
     |               Tag Table
     | 000C98        00000000 00012201 84120000 00000C98
     |               Instruction count          806
     |               Straight-line exec time    809
     |               Constant Area
     | 000000        7365646F 762E6639 30000000 7067656E 64307430 65626C61
     | 000018        73747262 6C617374 7365646F 762E6639 3049424D 2049424D
     | 000030        3E45798E E2308C3A 42480000 49424D20 4A511B0E C57E649A
     | 000048        426D1A94 A2000000 251F2DE4 18007A2F 00000000 40400000
     | 000060        41490FDB

 
 
>>>>> COMPILATION UNIT EPILOGUE SECTION <<<<<
 
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    sedov.f90                   07/08/15   15:48:48
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................      84
1501-510  Compilation successful for file sedov.f90.
1501-543  Object file created.
