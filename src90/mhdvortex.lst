IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- mhdvortex.f90 07/08/15 15:48:50
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** mhdvortex   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at mhdvortex.f90 <line 53> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at mhdvortex.f90 <line 54> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 55> was not SIMD vectorized because it contains memory references ((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns2.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] =  5.0000000000000000E+000 / (gamm1 *  3.7699112892150878E+001); with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 55> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns3.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] = - _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x2b%addr  + d-x2b%rvo))->x2b[].rns4.[$$CIV1 + 1ll]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 55> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns7.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 55> was not SIMD vectorized because it contains memory references ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns1.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] =  2.2104852592084123E-001; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 55> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns5.[$$CIV2 + 1ll][$$CIV1 + 1ll][$$CIV0 + 1ll] = _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV0 + 1ll]); with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at mhdvortex.f90 <line 57> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 57> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at mhdvortex.f90 <line 57> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at mhdvortex.f90 <line 57> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at mhdvortex.f90 <line 58> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 58> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at mhdvortex.f90 <line 58> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at mhdvortex.f90 <line 58> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at mhdvortex.f90 <line 61> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 61> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at mhdvortex.f90 <line 61> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at mhdvortex.f90 <line 61> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at mhdvortex.f90 <line 56> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 56> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at mhdvortex.f90 <line 56> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at mhdvortex.f90 <line 56> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
1586-536 (I) Loop (loop index 3) at mhdvortex.f90 <line 59> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at mhdvortex.f90 <line 59> was not SIMD vectorized because it contains operation in  6.2831854820251464E+000 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV0 + 1ll] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at mhdvortex.f90 <line 59> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 3) at mhdvortex.f90 <line 59> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 3) at mhdvortex.f90 <line 59> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
1586-534 (I) Loop (loop index 4) at mhdvortex.f90 <line 71> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at mhdvortex.f90 <line 72> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns8.[1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = - (( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x2b%addr  + d-x2b%rvo))->x2b[].rns4.[$$CIV4 + 1ll])); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns10.[1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns9.[1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns8.[2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = - (( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x2b%addr  + d-x2b%rvo))->x2b[].rns4.[$$CIV4 + 1ll])); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns9.[2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns8.[4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = - (( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x2b%addr  + d-x2b%rvo))->x2b[].rns4.[$$CIV4 + 1ll])); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns9.[4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns10.[4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns10.[2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns8.[3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = - (( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x2b%addr  + d-x2b%rvo))->x2b[].rns4.[$$CIV4 + 1ll])); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns10.[3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns9.[3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)][$$CIV4 + 1ll][$$CIV3 + 1ll] = ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]); with non-vectorizable strides.
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains operation in ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains operation in ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains operation in ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(4ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains operation in ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + ($$CIV6 * 4ll + (long long) kn % 4ll)) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
1586-534 (I) Loop (loop index 10) at mhdvortex.f90 <line 71> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 11) at mhdvortex.f90 <line 72> was not SIMD vectorized because the loop is not the innermost loop.
1586-540 (I) Loop (loop index 12) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b2%addr  + d-b2%rvo))->b2[].rns9.[$$CIV5 + 1ll][$$CIV4 + 1ll][$$CIV3 + 1ll] = ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b3%addr  + d-b3%rvo))->b3[].rns10.[$$CIV5 + 1ll][$$CIV4 + 1ll][$$CIV3 + 1ll] =  0.0000000000000000E+000; with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at mhdvortex.f90 <line 73> was not SIMD vectorized because it contains memory references ((double *)((char *)d-b1%addr  + d-b1%rvo))->b1[].rns8.[$$CIV5 + 1ll][$$CIV4 + 1ll][$$CIV3 + 1ll] = - (( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 6.2831854820251464E+000 * ((double *)((char *)d-x2b%addr  + d-x2b%rvo))->x2b[].rns4.[$$CIV4 + 1ll])); with non-vectorizable strides.
1586-536 (I) Loop (loop index 12) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*($$CIV5 + 1ll) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains operation in ( 1.0000000000000000E+000 / _sqrt@18( 1.2566370964050292E+001)) * _sin@7( 1.2566370964050292E+001 * ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 1ll]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains memory references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*($$CIV5 + 1ll) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at mhdvortex.f90 <line 75> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at mhdvortex.f90 <line 75> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b2%addr  + d-b2%rvo + (d-b2%bounds%mult[].off880)*($$CIV5 + 1ll) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 12) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*($$CIV5 + 1ll) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains memory references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*($$CIV5 + 1ll) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at mhdvortex.f90 <line 76> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at mhdvortex.f90 <line 76> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b3%addr  + d-b3%rvo + (d-b3%bounds%mult[].off984)*($$CIV5 + 1ll) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
1586-536 (I) Loop (loop index 12) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*($$CIV5 + 1ll) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable alignment.
1586-540 (I) Loop (loop index 12) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains memory references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*($$CIV5 + 1ll) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at mhdvortex.f90 <line 74> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at mhdvortex.f90 <line 74> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-b1%addr  + d-b1%rvo + (d-b1%bounds%mult[].off776)*($$CIV5 + 1ll) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"3">. Total number of the innermost loops SIMD vectorized <"0">.


    14|         SUBROUTINE mhdvortex ()
                  _alloca((8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0)))))
                  _alloca((8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0)))))
                  _alloca((8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0)))))
                  _alloca((8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0)))))
    40|           IF ((myid == 0)) THEN
    14|             |pgen%version = 129
                    |pgen%name_addr = "pgenfooImhdvortex.f90"
                    |pgen%name_len = 4
                    |pgen%num_of_items = 1
                    |pgen%nlitems%type.off32 = 4
                    |pgen%nlitems%kind.off40 = 
                    |pgen%nlitems%size.off48 = 
                    |pgen%nlitems%name_addr.off56 = "pgenfooImhdvortex.f90" + 4
                    |pgen%nlitems%name_len.off64 = 3
                    |pgen%nlitems%item_addr.off72 = loc(foo)
    41|             |pgen%name_flags = 0
                    #14 = _xlfBeginIO(1,2,#13,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#14))
    42|             |pgen%name_flags = 0
                    #16 = _xlfBeginIO(2,258,#15,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#16))
    43|           ENDIF
    53|           IF ((int(kn) > 0)) THEN
                    $$CIV2 = 0
       Id=1         DO $$CIV2 = $$CIV2, int(int(kn))-1
    54|               IF ((int(jn) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int(int(jn))-1
    55|                   IF ((int(in) > 0)) THEN
                            $$CIV0 = 0
       Id=3                 DO $$CIV0 = $$CIV0, int(int(in))-1
    56|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               2.2104852592084123E-001
    57|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               5.0000000000000000E+000 / (gamm1 *  &
                &               3.7699112892150878E+001)
    58|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = &
                &               -_sin( 6.2831854820251464E+000 * d-x2b%addr%x2b(&
                &               %VAL($$CIV1) + 1))
    59|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = &
                &               _sin( 6.2831854820251464E+000 * d-x1b%addr%x1b(&
                &               %VAL($$CIV0) + 1))
    61|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    62|                     ENDDO
                          ENDIF
    63|                 ENDDO
                      ENDIF
    64|             ENDDO
                  ENDIF
    71|           IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
                    $$CIV5 = 0
       Id=10        DO $$CIV5 = $$CIV5, MOD(int(kn), int(4))-1
    72|               IF ((int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=11            DO $$CIV4 = $$CIV4, int(int(jn))-1
    73|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=12                DO $$CIV3 = $$CIV3, int(int(in))-1
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = &
                &               -(( 1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               6.2831854820251464E+000 * d-x2b%addr%x2b(%VAL(&
                &               $$CIV4) + 1)))
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = (&
                &                1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               1.2566370964050292E+001 * d-x1b%addr%x1b(%VAL(&
                &               $$CIV3) + 1))
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) =  &
                &               0.0000000000000000E+000
    77|                     ENDDO
                          ENDIF
    78|                 ENDDO
                      ENDIF
    79|             ENDDO
                  ENDIF
    71|           IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))) THEN
                    $$CIV6 = int(0)
       Id=4         DO $$CIV6 = $$CIV6, int((((int(kn) - MOD(int(kn), 4)) - 1)&
                &        / 4 + 1))-1
    72|               IF ((int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int(int(jn))-1
    73|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(int(in))-1
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,1 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = -(( &
                &               1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               6.2831854820251464E+000 * d-x2b%addr%x2b(%VAL(&
                &               $$CIV4) + 1)))
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,1 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = ( 1.0000000000000000E+000 &
                &               / _sqrt( 1.2566370964050292E+001)) * _sin( &
                &               1.2566370964050292E+001 * d-x1b%addr%x1b(%VAL(&
                &               $$CIV3) + 1))
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,1 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,2 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = -(( &
                &               1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               6.2831854820251464E+000 * d-x2b%addr%x2b(%VAL(&
                &               $$CIV4) + 1)))
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,2 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = ( 1.0000000000000000E+000 &
                &               / _sqrt( 1.2566370964050292E+001)) * _sin( &
                &               1.2566370964050292E+001 * d-x1b%addr%x1b(%VAL(&
                &               $$CIV3) + 1))
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,2 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,3 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = -(( &
                &               1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               6.2831854820251464E+000 * d-x2b%addr%x2b(%VAL(&
                &               $$CIV4) + 1)))
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,3 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = ( 1.0000000000000000E+000 &
                &               / _sqrt( 1.2566370964050292E+001)) * _sin( &
                &               1.2566370964050292E+001 * d-x1b%addr%x1b(%VAL(&
                &               $$CIV3) + 1))
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,3 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,4 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = -(( &
                &               1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               6.2831854820251464E+000 * d-x2b%addr%x2b(%VAL(&
                &               $$CIV4) + 1)))
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,4 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = ( 1.0000000000000000E+000 &
                &               / _sqrt( 1.2566370964050292E+001)) * _sin( &
                &               1.2566370964050292E+001 * d-x1b%addr%x1b(%VAL(&
                &               $$CIV3) + 1))
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,4 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    77|                     ENDDO
                          ENDIF
    78|                 ENDDO
                      ENDIF
    79|             ENDDO
                  ENDIF
    82|           RETURN
                END SUBROUTINE mhdvortex


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            53             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            54             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            56                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            56                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            56                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            56                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-d%addr  + 
                                          d-d%rvo + (d-d%bounds%mult[].off48)*($$CIV2 + 1ll) + 
                                          (d-d%bounds%mult[].off72)*($$CIV1 + 1ll) + 
                                          (d-d%bounds%mult[].off96)*($$CIV0 + 1ll)).
         0            57                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            57                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            57                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            57                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-e%addr  + 
                                          d-e%rvo + (d-e%bounds%mult[].off152)*($$CIV2 + 1ll) + 
                                          (d-e%bounds%mult[].off176)*($$CIV1 + 1ll) + 
                                          (d-e%bounds%mult[].off200)*($$CIV0 + 1ll)).
         0            58                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            58                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*($$CIV2 + 1ll) + 
                                          (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            58                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            58                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*($$CIV2 + 
                                          1ll) + (d-v1%bounds%mult[].off488)*($$CIV1 + 1ll) + 
                                          (d-v1%bounds%mult[].off512)*($$CIV0 + 1ll)).
         0            59                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            59                  Loop was not SIMD vectorized because it contains 
                                          operation in  6.2831854820251464E+000 * ((double 
                                          *)((char *)d-x1b%addr  + 
                                          d-x1b%rvo))->x1b[].rns6.[$$CIV0 + 1ll] which is not  
                                          suitable for SIMD vectorization.
         0            59                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*($$CIV2 + 1ll) + 
                                          (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            59                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            59                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*($$CIV2 + 
                                          1ll) + (d-v2%bounds%mult[].off592)*($$CIV1 + 1ll) + 
                                          (d-v2%bounds%mult[].off616)*($$CIV0 + 1ll)).
         0            61                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll))  with 
                                          non-vectorizable alignment.
         0            61                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*($$CIV2 + 1ll) + 
                                          (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)) with  
                                          non-vectorizable strides.
         0            61                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            61                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*($$CIV2 + 
                                          1ll) + (d-v3%bounds%mult[].off696)*($$CIV1 + 1ll) + 
                                          (d-v3%bounds%mult[].off720)*($$CIV0 + 1ll)).
         0            71            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            72            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*($$CIV5 + 1ll) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*($$CIV5 + 1ll) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            74                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*($$CIV5 + 
                                          1ll) + (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*($$CIV5 + 1ll) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          operation in ( 1.0000000000000000E+000 / _sqrt@18( 
                                          1.2566370964050292E+001)) * _sin@7( 
                                          1.2566370964050292E+001 * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*($$CIV5 + 1ll) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            75                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*($$CIV5 + 
                                          1ll) + (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*($$CIV5 + 1ll) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*($$CIV5 + 1ll) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            76                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*($$CIV5 + 
                                          1ll) + (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
         0            71             4    Outer loop has been unrolled 4 time(s).
         0            71             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            72             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(1ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(1ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            74                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(1ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(1ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          operation in ( 1.0000000000000000E+000 / _sqrt@18( 
                                          1.2566370964050292E+001)) * _sin@7( 
                                          1.2566370964050292E+001 * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(1ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            75                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(1ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(1ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(1ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            76                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(1ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(2ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(2ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            74                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(2ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(2ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          operation in ( 1.0000000000000000E+000 / _sqrt@18( 
                                          1.2566370964050292E+001)) * _sin@7( 
                                          1.2566370964050292E+001 * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(2ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            75                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(2ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(2ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(2ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            76                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(2ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(3ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(3ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            74                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(3ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(3ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          operation in ( 1.0000000000000000E+000 / _sqrt@18( 
                                          1.2566370964050292E+001)) * _sin@7( 
                                          1.2566370964050292E+001 * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(3ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            75                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(3ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(3ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(3ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            76                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(3ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(4ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b1%addr  + d-b1%rvo + 
                                          (d-b1%bounds%mult[].off776)*(4ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            74                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            74                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b1%addr  
                                          + d-b1%rvo + (d-b1%bounds%mult[].off776)*(4ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b1%bounds%mult[].off800)*($$CIV4 + 1ll) + 
                                          (d-b1%bounds%mult[].off824)*($$CIV3 + 1ll)).
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(4ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          operation in ( 1.0000000000000000E+000 / _sqrt@18( 
                                          1.2566370964050292E+001)) * _sin@7( 
                                          1.2566370964050292E+001 * ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns6.[$$CIV3 + 
                                          1ll]) which is not  suitable for SIMD vectorization.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b2%addr  + d-b2%rvo + 
                                          (d-b2%bounds%mult[].off880)*(4ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            75                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            75                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b2%addr  
                                          + d-b2%rvo + (d-b2%bounds%mult[].off880)*(4ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b2%bounds%mult[].off904)*($$CIV4 + 1ll) + 
                                          (d-b2%bounds%mult[].off928)*($$CIV3 + 1ll)).
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(4ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll))  with 
                                          non-vectorizable alignment.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-b3%addr  + d-b3%rvo + 
                                          (d-b3%bounds%mult[].off984)*(4ll + ($$CIV6 * 4ll + 
                                          (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)) with  
                                          non-vectorizable strides.
         0            76                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            76                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-b3%addr  
                                          + d-b3%rvo + (d-b3%bounds%mult[].off984)*(4ll + 
                                          ($$CIV6 * 4ll + (long long) kn % 4ll)) + 
                                          (d-b3%bounds%mult[].off1008)*($$CIV4 + 1ll) + 
                                          (d-b3%bounds%mult[].off1032)*($$CIV3 + 1ll)).


    14|         SUBROUTINE mhdvortex ()
                  _alloca(8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0))))
                  _alloca(8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0))))
                  _alloca(8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0))))
                  _alloca(8 * (max(int(%VAL(kn)),0) * (max(int(%VAL(jn)),0) * &
                &   max(int(%VAL(in)),0))))
    40|           IF ((myid == 0)) THEN
    14|             |pgen%version = 129
                    |pgen%name_addr = "pgenfooImhdvortex.f90"
                    |pgen%name_len = 4
                    |pgen%num_of_items = 1
                    |pgen%nlitems%type.off32 = 4
                    |pgen%nlitems%kind.off40 = 
                    |pgen%nlitems%size.off48 = 
                    |pgen%nlitems%name_addr.off56 = "pgenfooImhdvortex.f90" + 4
                    |pgen%nlitems%name_len.off64 = 3
                    |pgen%nlitems%item_addr.off72 = loc(foo)
    41|             |pgen%name_flags = 0
                    #14 = _xlfBeginIO(1,2,#13,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#14))
    42|             |pgen%name_flags = 0
                    #16 = _xlfBeginIO(2,258,#15,32768,NULL,0,|pgen)
                    _xlfEndIO(%VAL(#16))
    43|           ENDIF
    53|           IF ((int(kn) > 0)) THEN
                    $$CIV2 = 0
       Id=1         DO $$CIV2 = $$CIV2, int(int(kn))-1
    54|               IF ((int(jn) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int(int(jn))-1
    55|                   IF ((int(in) > 0)) THEN
                            $$CIV0 = 0
    58|                     $$csx0 = -_sin( 6.2831854820251464E+000 * &
                &             d-x2b%addr%x2b(%VAL($$CIV1) + 1))
    55|Id=3                 DO $$CIV0 = $$CIV0, int(int(in))-1
    56|                       d-d%addr%d($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               2.2104852592084123E-001
    57|                       d-e%addr%e($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               5.0000000000000000E+000 / (gamm1 *  &
                &               3.7699112892150878E+001)
    58|                       d-v1%addr%v1($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = &
                &               $$csx0
    59|                       d-v2%addr%v2($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) = &
                &               _sin( 6.2831854820251464E+000 * d-x1b%addr%x1b(&
                &               %VAL($$CIV0) + 1))
    61|                       d-v3%addr%v3($$CIV0 + 1,$$CIV1 + 1,$$CIV2 + 1) =  &
                &               0.0000000000000000E+000
    62|                     ENDDO
                          ENDIF
    63|                 ENDDO
                      ENDIF
    64|             ENDDO
                  ENDIF
    71|           IF ((MOD(int(kn), 4) > 0  .AND.  int(kn) > 0)) THEN
                    $$CIV5 = 0
       Id=10        DO $$CIV5 = $$CIV5, MOD(int(kn), int(4))-1
    72|               IF ((int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=11            DO $$CIV4 = $$CIV4, int(int(jn))-1
    73|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
    74|                     $$csx1 = -(( 1.0000000000000000E+000 / _sqrt( &
                &             1.2566370964050292E+001)) * _sin( &
                &             6.2831854820251464E+000 * d-x2b%addr%x2b(%VAL(&
                &             $$CIV4) + 1)))
    73|Id=12                DO $$CIV3 = $$CIV3, int(int(in))-1
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = &
                &               $$csx1
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) = (&
                &                1.0000000000000000E+000 / _sqrt( &
                &               1.2566370964050292E+001)) * _sin( &
                &               1.2566370964050292E+001 * d-x1b%addr%x1b(%VAL(&
                &               $$CIV3) + 1))
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,$$CIV5 + 1) =  &
                &               0.0000000000000000E+000
    77|                     ENDDO
                          ENDIF
    78|                 ENDDO
                      ENDIF
    79|             ENDDO
                  ENDIF
    71|           IF ((int(kn) > 0  .AND.  int(kn) > MOD(int(kn), 4))) THEN
                    $$CIV6 = int(0)
       Id=4         DO $$CIV6 = $$CIV6, int((((int(kn) - MOD(int(kn), 4)) - 1)&
                &        / 4 + 1))-1
    72|               IF ((int(jn) > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int(int(jn))-1
    73|                   IF ((int(in) > 0)) THEN
                            $$CIV3 = 0
    74|                     $$csx3 =  1.0000000000000000E+000 / _sqrt( &
                &             1.2566370964050292E+001)
                            $$csx2 = -($$csx3 * _sin( 6.2831854820251464E+000 * &
                &             d-x2b%addr%x2b(%VAL($$CIV4) + 1)))
    73|Id=6                 DO $$CIV3 = $$CIV3, int(int(in))-1
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,1 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx2
    75|                       $$csx4 = $$csx3 * _sin( 1.2566370964050292E+001 * &
                &               d-x1b%addr%x1b(%VAL($$CIV3) + 1))
                              d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,1 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx4
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,1 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,2 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx2
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,2 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx4
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,2 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,3 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx2
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,3 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx4
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,3 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    74|                       d-b1%addr%b1($$CIV3 + 1,$$CIV4 + 1,4 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx2
    75|                       d-b2%addr%b2($$CIV3 + 1,$$CIV4 + 1,4 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) = $$csx4
    76|                       d-b3%addr%b3($$CIV3 + 1,$$CIV4 + 1,4 + ($$CIV6 * &
                &               4 + MOD(int(kn), 4))) =  0.0000000000000000E+000
    77|                     ENDDO
                          ENDIF
    78|                 ENDDO
                      ENDIF
    79|             ENDDO
                  ENDIF
    82|           RETURN
                END SUBROUTINE mhdvortex

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   suus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---s ssss ssss
 CCR's set/used:   sss- ssss
     | 000000                           PDEF     mhdvortex
   14|                                  PROC      
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 stfd     DB61FFD8   1     STFL      #stack(gr1,-40)=fp27
    0| 000014 stfd     DB41FFD0   1     STFL      #stack(gr1,-48)=fp26
    0| 000018 stfd     DB21FFC8   1     STFL      #stack(gr1,-56)=fp25
    0| 00001C stfd     DB01FFC0   1     STFL      #stack(gr1,-64)=fp24
    0| 000020 stfd     DAE1FFB8   1     STFL      #stack(gr1,-72)=fp23
    0| 000024 std      FBE1FFB0   1     ST8       #stack(gr1,-80)=gr31
    0| 000028 std      FBC1FFA8   1     ST8       #stack(gr1,-88)=gr30
    0| 00002C std      FBA1FFA0   1     ST8       #stack(gr1,-96)=gr29
    0| 000030 std      FB81FF98   1     ST8       #stack(gr1,-104)=gr28
    0| 000034 std      FB61FF90   1     ST8       #stack(gr1,-112)=gr27
    0| 000038 std      FB41FF88   1     ST8       #stack(gr1,-120)=gr26
    0| 00003C std      FB21FF80   1     ST8       #stack(gr1,-128)=gr25
    0| 000040 std      FB01FF78   1     ST8       #stack(gr1,-136)=gr24
    0| 000044 std      FAE1FF70   1     ST8       #stack(gr1,-144)=gr23
    0| 000048 std      FAC1FF68   1     ST8       #stack(gr1,-152)=gr22
    0| 00004C std      FAA1FF60   1     ST8       #stack(gr1,-160)=gr21
    0| 000050 std      FA81FF58   1     ST8       #stack(gr1,-168)=gr20
    0| 000054 std      FA61FF50   1     ST8       #stack(gr1,-176)=gr19
    0| 000058 std      FA41FF48   1     ST8       #stack(gr1,-184)=gr18
    0| 00005C std      FA21FF40   1     ST8       #stack(gr1,-192)=gr17
    0| 000060 std      FA01FF38   1     ST8       #stack(gr1,-200)=gr16
    0| 000064 std      F9E1FF30   1     ST8       #stack(gr1,-208)=gr15
    0| 000068 std      F9C1FF28   1     ST8       #stack(gr1,-216)=gr14
    0| 00006C mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 000070 mfcr     7D800026   1     LFCR      gr12=cr[24],2
    0| 000074 stw      91810008   1     ST4A      #stack(gr1,8)=gr12
    0| 000078 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 00007C stdu     F821FCA1   1     ST8U      gr1,#stack(gr1,-864)=gr1
    0| 000080 or       7C3F0B78   1     LR        gr31=gr1
    0| 000084 ld       E8A20000   1     L8        gr5=.&&N&&param(gr2,0)
   40| 000088 ld       E8820000   1     L8        gr4=.&&N&&mpipar(gr2,0)
   74| 00008C ld       EAE20000   1     L8        gr23=.&&N&field(gr2,0)
   59| 000090 ld       E8620000   1     L8        gr3=.&&N&grid(gr2,0)
    0| 000094 lwa      E8C5000A   1     L4A       gr6=<s11:d8:l4>(gr5,8)
   40| 000098 lwz      80840004   1     L4Z       gr4=<s154:d4:l4>(gr4,4)
    0| 00009C lwa      E8E50002   1     L4A       gr7=<s11:d0:l4>(gr5,0)
    0| 0000A0 lwa      E9050006   1     L4A       gr8=<s11:d4:l4>(gr5,4)
   74| 0000A4 ld       E93702D8   1     L8        gr9=<s57:d728:l8>(gr23,728)
   75| 0000A8 ld       E9570340   1     L8        gr10=<s57:d832:l8>(gr23,832)
    0| 0000AC std      F8DF0100   1     ST8       #SPILL0(gr31,256)=gr6
   76| 0000B0 sradi    7CC01674   1     SRA8CA    gr0,ca=gr6,2
    0| 0000B4 std      F8FF0108   1     ST8       #SPILL1(gr31,264)=gr7
    0| 0000B8 std      F91F0110   1     ST8       #SPILL2(gr31,272)=gr8
   74| 0000BC std      F93F0118   1     ST8       #SPILL3(gr31,280)=gr9
   75| 0000C0 std      F95F0120   1     ST8       #SPILL4(gr31,288)=gr10
   76| 0000C4 addze    7C000194   1     ADDE      gr0,ca=gr0,0,ca
   76| 0000C8 ld       E97703A8   1     L8        gr11=<s57:d936:l8>(gr23,936)
   59| 0000CC ld       EB6306C8   1     L8        gr27=<s131:d1736:l8>(gr3,1736)
   58| 0000D0 ld       EB430700   1     L8        gr26=<s131:d1792:l8>(gr3,1792)
   58| 0000D4 ld       EB230718   1     L8        gr25=<s131:d1816:l8>(gr3,1816)
   59| 0000D8 ld       EB0306E0   1     L8        gr24=<s131:d1760:l8>(gr3,1760)
   74| 0000DC ld       EAD702F0   1     L8        gr22=<s57:d752:l8>(gr23,752)
   76| 0000E0 std      F97F0128   1     ST8       #SPILL5(gr31,296)=gr11
   59| 0000E4 std      FB7F0138   1     ST8       #SPILL7(gr31,312)=gr27
   58| 0000E8 std      FB5F0140   1     ST8       #SPILL8(gr31,320)=gr26
   58| 0000EC std      FB3F0148   1     ST8       #SPILL9(gr31,328)=gr25
   75| 0000F0 ld       EAB70358   1     L8        gr21=<s57:d856:l8>(gr23,856)
   76| 0000F4 ld       EA9703C0   1     L8        gr20=<s57:d960:l8>(gr23,960)
   59| 0000F8 std      FB1F0150   1     ST8       #SPILL10(gr31,336)=gr24
   74| 0000FC ld       EA770308   1     L8        gr19=<s57:d776:l8>(gr23,776)
   74| 000100 ld       EA570320   1     L8        gr18=<s57:d800:l8>(gr23,800)
   74| 000104 std      FADF0158   1     ST8       #SPILL11(gr31,344)=gr22
   75| 000108 ld       EA370370   1     L8        gr17=<s57:d880:l8>(gr23,880)
   75| 00010C ld       EA170388   1     L8        gr16=<s57:d904:l8>(gr23,904)
   75| 000110 std      FABF0160   1     ST8       #SPILL12(gr31,352)=gr21
   76| 000114 ld       E9F703D8   1     L8        gr15=<s57:d984:l8>(gr23,984)
   76| 000118 ld       E9D703F0   1     L8        gr14=<s57:d1008:l8>(gr23,1008)
   76| 00011C std      FA9F0168   1     ST8       #SPILL13(gr31,360)=gr20
   74| 000120 std      FA7F0170   1     ST8       #SPILL14(gr31,368)=gr19
   71| 000124 rldicr   780C1764   1     SLL8      gr12=gr0,2
   74| 000128 std      FA5F0178   1     ST8       #SPILL15(gr31,376)=gr18
   40| 00012C cmpdi    2C240000   1     C8        cr0=gr4,0
   71| 000130 subf     7CAC3050   1     S         gr5=gr6,gr12
   71| 000134 std      F99F0130   1     ST8       #SPILL6(gr31,304)=gr12
   74| 000138 ld       EBD70338   1     L8        gr30=<s57:d824:l8>(gr23,824)
   75| 00013C std      FA3F0180   1     ST8       #SPILL16(gr31,384)=gr17
   75| 000140 std      FA1F0188   1     ST8       #SPILL17(gr31,392)=gr16
   75| 000144 ld       EBB703A0   1     L8        gr29=<s57:d928:l8>(gr23,928)
   76| 000148 std      F9FF0190   1     ST8       #SPILL18(gr31,400)=gr15
   76| 00014C std      F9DF0260   1     ST8       #SPILL44(gr31,608)=gr14
   76| 000150 ld       EB970408   1     L8        gr28=<s57:d1032:l8>(gr23,1032)
   71| 000154 std      F8BF0198   1     ST8       #SPILL19(gr31,408)=gr5
   40| 000158 bc       408200B8   1     BF        CL.1,cr0,0x4/eq,taken=60%(60,40)
    0| 00015C ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
    0| 000160 addi     38000081   1     LI        gr0=129
   41| 000164 ld       EB620000   1     L8        gr27=.$STATIC(gr2,0)
    0| 000168 stw      901F00A0   1     ST4Z      <a1:d160:l4>(gr31,160)=gr0
   41| 00016C addi     38800000   1     LI        gr4=0
   41| 000170 addi     38E00000   1     LI        gr7=0
    0| 000174 addi     38030010   1     AI        gr0=gr3,16
    0| 000178 addi     38630014   1     AI        gr3=gr3,20
    0| 00017C std      F81F00A8   1     ST8       <a1:d168:l8>(gr31,168)=gr0
    0| 000180 addi     38000004   1     LI        gr0=4
    0| 000184 std      F87F00D8   1     ST8       <a1:d216:l8>(gr31,216)=gr3
    0| 000188 std      F81F00B0   1     ST8       <a1:d176:l8>(gr31,176)=gr0
    0| 00018C std      F81F00C0   1     ST8       <a1:d192:l8>(gr31,192)=gr0
    0| 000190 addi     38000001   1     LI        gr0=1
    0| 000194 addi     38600003   1     LI        gr3=3
    0| 000198 std      F81F00B8   1     ST8       <a1:d184:l8>(gr31,184)=gr0
    0| 00019C addi     38000008   1     LI        gr0=8
   41| 0001A0 ori      609A8000   1     OIL       gr26=gr4,0x8000
    0| 0001A4 std      F81F00C8   1     ST8       <a1:d200:l8>(gr31,200)=gr0
    0| 0001A8 std      F81F00D0   1     ST8       <a1:d208:l8>(gr31,208)=gr0
    0| 0001AC addi     381F0080   1     AI        gr0=gr31,128
   41| 0001B0 stw      909F00A4   1     ST4Z      <a1:d164:l4>(gr31,164)=gr4
   41| 0001B4 or       7F65DB78   1     LR        gr5=gr27
   41| 0001B8 or       7F46D378   1     LR        gr6=gr26
    0| 0001BC std      F87F00E0   1     ST8       <a1:d224:l8>(gr31,224)=gr3
    0| 0001C0 std      F81F00E8   1     ST8       <a1:d232:l8>(gr31,232)=gr0
   41| 0001C4 addi     38600001   1     LI        gr3=1
   41| 0001C8 addi     38800002   1     LI        gr4=2
   41| 0001CC addi     39000000   1     LI        gr8=0
   41| 0001D0 addi     393F00A0   1     AI        gr9=gr31,160
   41| 0001D4 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#13",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|pgen,gr9,#def_xlfBeginIO11",_xlfBeginIO",fcr",#MX_TEMP1",gr1,gr31,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   41| 0001D8 ori      60000000   1
   41| 0001DC bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,gr31,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   41| 0001E0 ori      60000000   1
   42| 0001E4 addi     38BB0040   1     AI        gr5=gr27,64
   42| 0001E8 or       7F46D378   1     LR        gr6=gr26
   42| 0001EC addi     38600002   1     LI        gr3=2
   42| 0001F0 addi     38800102   1     LI        gr4=258
   42| 0001F4 addi     38E00000   1     LI        gr7=0
   42| 0001F8 addi     39000000   1     LI        gr8=0
   42| 0001FC addi     393F00A0   1     AI        gr9=gr31,160
   42| 000200 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#15",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|pgen,gr9,#use_xlfBeginIO21,_xlfBeginIO",fcr",#MX_TEMP1",gr1,gr31,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   42| 000204 ori      60000000   1
   42| 000208 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,gr31,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   42| 00020C ori      60000000   1
   43|                              CL.1:
   53| 000210 ld       E81F0100   1     L8        gr0=#SPILL0(gr31,256)
   53| 000214 cmpwi    2C000000   1     C4        cr0=gr0,0
   53| 000218 bc       40810300   1     BF        CL.27,cr0,0x2/gt,taken=40%(40,60)
   56| 00021C ld       EB770030   1     L8        gr27=<s57:d48:l8>(gr23,48)
   56| 000220 ld       EB570048   1     L8        gr26=<s57:d72:l8>(gr23,72)
   57| 000224 ld       E8C20000   1     L8        gr6=.&&N&&root(gr2,0)
   57| 000228 ld       EB370098   1     L8        gr25=<s57:d152:l8>(gr23,152)
   57| 00022C ld       E9570068   1     L8        gr10=<s57:d104:l8>(gr23,104)
   59| 000230 ld       E8B70208   1     L8        gr5=<s57:d520:l8>(gr23,520)
   56| 000234 std      FB7F01A0   1     ST8       #SPILL20(gr31,416)=gr27
   56| 000238 std      FB5F01A8   1     ST8       #SPILL21(gr31,424)=gr26
   59| 00023C ld       EB770238   1     L8        gr27=<s57:d568:l8>(gr23,568)
   61| 000240 ld       EB5702A0   1     L8        gr26=<s57:d672:l8>(gr23,672)
   57| 000244 std      FB3F01B0   1     ST8       #SPILL22(gr31,432)=gr25
   61| 000248 ld       E8770270   1     L8        gr3=<s57:d624:l8>(gr23,624)
   57| 00024C ld       E9370080   1     L8        gr9=<s57:d128:l8>(gr23,128)
   59| 000250 ld       E8170220   1     L8        gr0=<s57:d544:l8>(gr23,544)
    0| 000254 ld       EB3F0110   1     L8        gr25=#SPILL2(gr31,272)
   57| 000258 lfd      CBE600E8   1     LFL       fp31=<s168:d232:l8>(gr6,232)
   57| 00025C ld       E8D700B0   1     L8        gr6=<s57:d176:l8>(gr23,176)
   59| 000260 std      FB7F01C0   1     ST8       #SPILL24(gr31,448)=gr27
   61| 000264 std      FB5F01C8   1     ST8       #SPILL25(gr31,456)=gr26
   61| 000268 ld       E8970288   1     L8        gr4=<s57:d648:l8>(gr23,648)
   58| 00026C ld       EB1701D0   1     L8        gr24=<s57:d464:l8>(gr23,464)
   58| 000270 ld       E91701A0   1     L8        gr8=<s57:d416:l8>(gr23,416)
   57| 000274 std      F8DF01D8   1     ST8       #SPILL27(gr31,472)=gr6
   58| 000278 ld       E8F701B8   1     L8        gr7=<s57:d440:l8>(gr23,440)
    0| 00027C cmpwi    2C190000   1     C4        cr0=gr25,0
   58| 000280 ld       EB3701E8   1     L8        gr25=<s57:d488:l8>(gr23,488)
    0| 000284 add      7CC32214   1     A         gr6=gr3,gr4
    0| 000288 add      7CA50214   1     A         gr5=gr5,gr0
   59| 00028C ld       E8770250   1     L8        gr3=<s57:d592:l8>(gr23,592)
    0| 000290 add      7C095214   1     A         gr0=gr9,gr10
   61| 000294 ld       E95702B8   1     L8        gr10=<s57:d696:l8>(gr23,696)
   58| 000298 std      FB1F01B8   1     ST8       #SPILL23(gr31,440)=gr24
   58| 00029C std      FB3F01E0   1     ST8       #SPILL28(gr31,480)=gr25
   53| 0002A0 addi     3B000000   1     LI        gr24=0
   56| 0002A4 ld       E9970000   1     L8        gr12=<s57:d0:l8>(gr23,0)
   56| 0002A8 ld       E9770018   1     L8        gr11=<s57:d24:l8>(gr23,24)
   53| 0002AC std      FB1F01D0   1     ST8       #SPILL26(gr31,464)=gr24
   56| 0002B0 ld       EB770060   1     L8        gr27=<s57:d96:l8>(gr23,96)
   57| 0002B4 ld       EB5700C8   1     L8        gr26=<s57:d200:l8>(gr23,200)
   58| 0002B8 ld       EB370200   1     L8        gr25=<s57:d512:l8>(gr23,512)
   59| 0002BC std      F87F01E8   1     ST8       #SPILL29(gr31,488)=gr3
    0| 0002C0 add      7C874214   1     A         gr4=gr7,gr8
   59| 0002C4 ld       EB170268   1     L8        gr24=<s57:d616:l8>(gr23,616)
   61| 0002C8 std      F95F01F0   1     ST8       #SPILL30(gr31,496)=gr10
   61| 0002CC ld       EAF702D0   1     L8        gr23=<s57:d720:l8>(gr23,720)
    0| 0002D0 bc       40810248   1     BF        CL.27,cr0,0x2/gt,taken=20%(20,80)
    0| 0002D4 ld       E8FF01C8   1     L8        gr7=#SPILL25(gr31,456)
    0| 0002D8 ld       E91F01C0   1     L8        gr8=#SPILL24(gr31,448)
    0| 0002DC or       7C691B78   1     LR        gr9=gr3
    0| 0002E0 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
    0| 0002E4 add      7CC63A14   1     A         gr6=gr6,gr7
    0| 0002E8 add      7CA54214   1     A         gr5=gr5,gr8
    0| 0002EC ld       E8FF01B8   1     L8        gr7=#SPILL23(gr31,440)
    0| 0002F0 ld       E91F01B0   1     L8        gr8=#SPILL22(gr31,432)
    0| 0002F4 add      7CA54A14   1     A         gr5=gr5,gr9
    0| 0002F8 ld       E93F01E0   1     L8        gr9=#SPILL28(gr31,480)
    0| 0002FC std      F8BF0210   1     ST8       #SPILL34(gr31,528)=gr5
    0| 000300 add      7CC65214   1     A         gr6=gr6,gr10
    0| 000304 add      7C843A14   1     A         gr4=gr4,gr7
    0| 000308 std      F8DF0208   1     ST8       #SPILL33(gr31,520)=gr6
    0| 00030C add      7C004214   1     A         gr0=gr0,gr8
    0| 000310 ld       E8FF01A0   1     L8        gr7=#SPILL20(gr31,416)
    0| 000314 ld       E91F01A8   1     L8        gr8=#SPILL21(gr31,424)
    0| 000318 add      7C844A14   1     A         gr4=gr4,gr9
    0| 00031C ld       E93F01D8   1     L8        gr9=#SPILL27(gr31,472)
    0| 000320 std      F89F0218   1     ST8       #SPILL35(gr31,536)=gr4
    0| 000324 lfs      C3C30028   1     LFS       fp30=+CONSTANT_AREA(gr3,40)
    0| 000328 lfd      CBA30030   1     LFL       fp29=+CONSTANT_AREA(gr3,48)
    0| 00032C add      7D074214   1     A         gr8=gr7,gr8
    0| 000330 add      7CEB6214   1     A         gr7=gr11,gr12
   59| 000334 ld       E97F0138   1     L8        gr11=#SPILL7(gr31,312)
   59| 000338 ld       E99F0150   1     L8        gr12=#SPILL10(gr31,336)
    0| 00033C add      7CE74214   1     A         gr7=gr7,gr8
    0| 000340 ld       E91F0108   1     L8        gr8=#SPILL1(gr31,264)
    0| 000344 std      F8FF0228   1     ST8       #SPILL37(gr31,552)=gr7
    0| 000348 add      7C004A14   1     A         gr0=gr0,gr9
    0| 00034C lfs      C3830038   1     LFS       fp28=+CONSTANT_AREA(gr3,56)
    0| 000350 std      F81F0220   1     ST8       #SPILL36(gr31,544)=gr0
   59| 000354 add      7DCB6214   1     A         gr14=gr11,gr12
   58| 000358 ld       E97F0140   1     L8        gr11=#SPILL8(gr31,320)
   58| 00035C ld       E99F0148   1     L8        gr12=#SPILL9(gr31,328)
   59| 000360 std      F9DF01F8   1     ST8       #SPILL31(gr31,504)=gr14
    0| 000364 cmpdi    2E280000   1     C8        cr4=gr8,0
    0| 000368 lfd      CB630040   1     LFL       fp27=+CONSTANT_AREA(gr3,64)
    0| 00036C lfs      C3430048   1     LFS       fp26=+CONSTANT_AREA(gr3,72)
    0| 000370 lfs      C2E30068   1     LFS       fp23=+CONSTANT_AREA(gr3,104)
   58| 000374 add      7DEB6214   1     A         gr15=gr11,gr12
   58| 000378 std      F9FF0200   1     ST8       #SPILL32(gr31,512)=gr15
   53|                              CL.28:
   54| 00037C addi     38000000   1     LI        gr0=0
   54| 000380 std      F81F0230   1     ST8       #SPILL38(gr31,560)=gr0
    0| 000384 bc       4091012C   1     BF        CL.29,cr4,0x2/gt,taken=20%(20,80)
    0| 000388 fmul     FC5F0772   1     MFL       fp2=fp31,fp29,fcr
   58| 00038C ld       E87F0200   1     L8        gr3=#SPILL32(gr31,512)
    0| 000390 ld       E89F0228   1     L8        gr4=#SPILL37(gr31,552)
    0| 000394 ld       E8BF0220   1     L8        gr5=#SPILL36(gr31,544)
    0| 000398 ld       E8DF0218   1     L8        gr6=#SPILL35(gr31,536)
    0| 00039C ld       E9DF0210   1     L8        gr14=#SPILL34(gr31,528)
    0| 0003A0 qvfre    10001030   1     QVFRE     fp0=fp2
   58| 0003A4 std      F87F0238   1     ST8       #SPILL39(gr31,568)=gr3
    0| 0003A8 std      F89F0240   1     ST8       #SPILL40(gr31,576)=gr4
    0| 0003AC std      F8BF0248   1     ST8       #SPILL41(gr31,584)=gr5
    0| 0003B0 std      F8DF0250   1     ST8       #SPILL42(gr31,592)=gr6
    0| 0003B4 ld       EADF0208   1     L8        gr22=#SPILL33(gr31,520)
    0| 0003B8 fmsub    FC22B838   1     FMS       fp1=fp23,fp2,fp0,fcr
    0| 0003BC fnmsub   FC00007C   2     FNMS      fp0=fp0,fp0,fp1,fcr
    0| 0003C0 fmsub    FC22B838   2     FMS       fp1=fp23,fp2,fp0,fcr
    0| 0003C4 fnmsub   FC00007C   2     FNMS      fp0=fp0,fp0,fp1,fcr
    0| 0003C8 fmul     FC200732   2     MFL       fp1=fp0,fp28,fcr
    0| 0003CC fmsub    FC42E078   2     FMS       fp2=fp28,fp2,fp1,fcr
    0| 0003D0 fnmsub   FF2008BC   2     FNMS      fp25=fp1,fp0,fp2,fcr
    0| 0003D4 ori      60210000   2     XNOP      
    0| 0003D8 ori      60210000   1     XNOP      
    0| 0003DC ori      60210000   1     XNOP      
   54|                              CL.30:
   58| 0003E0 ld       E87F0238   1     L8        gr3=#SPILL39(gr31,568)
    0| 0003E4 ld       E89F0230   1     L8        gr4=#SPILL38(gr31,560)
   58| 0003E8 lfdu     CC030008   1     LFDU      fp0,gr3=x2b(gr3,8)
    0| 0003EC addi     38840001   1     AI        gr4=gr4,1
    0| 0003F0 std      F89F0230   1     ST8       #SPILL38(gr31,560)=gr4
   58| 0003F4 fmul     FC2007B2   1     MFL       fp1=fp0,fp30,fcr
   58| 0003F8 std      F87F0238   1     ST8       #SPILL39(gr31,568)=gr3
   58| 0003FC bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,gr31,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   58| 000400 ori      60000000   1
   61| 000404 or       7ED5B378   1     LR        gr21=gr22
   59| 000408 or       7DD47378   1     LR        gr20=gr14
   58| 00040C ld       EA7F0250   1     L8        gr19=#SPILL42(gr31,592)
   58| 000410 fneg     FF000850   1     COMPFL    fp24=fp1
   56| 000414 std      FBDF0258   1     ST8       #SPILL43(gr31,600)=gr30
   57| 000418 ld       EA5F0248   1     L8        gr18=#SPILL41(gr31,584)
   56| 00041C ld       EA3F0240   1     L8        gr17=#SPILL40(gr31,576)
   59| 000420 ld       EA1F01F8   1     L8        gr16=#SPILL31(gr31,504)
   56| 000424 addi     39E00001   1     LI        gr15=1
   56| 000428 ld       EBDF0108   1     L8        gr30=#SPILL1(gr31,264)
   55|                              CL.32:
   57| 00042C stfdux   7F32D5EE   1     STFDU     gr18,e(gr18,gr26,0)=fp25
   59| 000430 lfdu     CC100008   1     LFDU      fp0,gr16=x1b(gr16,8)
   56| 000434 stfdux   7F71DDEE   1     STFDU     gr17,d(gr17,gr27,0)=fp27
   58| 000438 stfdux   7F13CDEE   1     STFDU     gr19,v1(gr19,gr25,0)=fp24
   59| 00043C fmul     FC2007B2   1     MFL       fp1=fp0,fp30,fcr
   59| 000440 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,gr31,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   59| 000444 ori      60000000   1
   62| 000448 cmpld    7C2FF040   1     CL8       cr0=gr15,gr30
   61| 00044C stfdux   7F55BDEE   1     STFDU     gr21,v3(gr21,gr23,0)=fp26
   56| 000450 addi     39EF0001   1     AI        gr15=gr15,1
   59| 000454 stfdux   7C34C5EE   1     STFDU     gr20,v2(gr20,gr24,0)=fp1
   62| 000458 bc       4180FFD4   1     BT        CL.32,cr0,0x8/llt,taken=80%(80,20)
   63| 00045C ld       E81F0230   1     L8        gr0=#SPILL38(gr31,560)
   63| 000460 ld       E87F0110   1     L8        gr3=#SPILL2(gr31,272)
    0| 000464 ld       E89F01A8   1     L8        gr4=#SPILL21(gr31,424)
    0| 000468 ld       E8BF0240   1     L8        gr5=#SPILL40(gr31,576)
    0| 00046C ld       E8DF01D8   1     L8        gr6=#SPILL27(gr31,472)
    0| 000470 ld       E8FF0248   1     L8        gr7=#SPILL41(gr31,584)
    0| 000474 ld       E91F01E0   1     L8        gr8=#SPILL28(gr31,480)
    0| 000478 ld       E93F0250   1     L8        gr9=#SPILL42(gr31,592)
    0| 00047C ld       E95F01E8   1     L8        gr10=#SPILL29(gr31,488)
    0| 000480 ld       E97F01F0   1     L8        gr11=#SPILL30(gr31,496)
   63| 000484 cmpld    7C201840   1     CL8       cr0=gr0,gr3
    0| 000488 add      7CA42A14   1     A         gr5=gr4,gr5
    0| 00048C add      7CE63A14   1     A         gr7=gr6,gr7
    0| 000490 std      F8BF0240   1     ST8       #SPILL40(gr31,576)=gr5
    0| 000494 std      F8FF0248   1     ST8       #SPILL41(gr31,584)=gr7
    0| 000498 add      7D284A14   1     A         gr9=gr8,gr9
   62| 00049C ld       EBDF0258   1     L8        gr30=#SPILL43(gr31,600)
    0| 0004A0 std      F93F0250   1     ST8       #SPILL42(gr31,592)=gr9
    0| 0004A4 add      7DCA7214   1     A         gr14=gr10,gr14
    0| 0004A8 add      7ECBB214   1     A         gr22=gr11,gr22
   63| 0004AC bc       4180FF34   1     BT        CL.30,cr0,0x8/llt,taken=80%(80,20)
   63|                              CL.29:
   64| 0004B0 ld       E87F01D0   1     L8        gr3=#SPILL26(gr31,464)
    0| 0004B4 ld       E81F01C8   1     L8        gr0=#SPILL25(gr31,456)
    0| 0004B8 ld       E89F0208   1     L8        gr4=#SPILL33(gr31,520)
   64| 0004BC ld       E8BF0100   1     L8        gr5=#SPILL0(gr31,256)
    0| 0004C0 ld       E8DF01C0   1     L8        gr6=#SPILL24(gr31,448)
    0| 0004C4 ld       E8FF0210   1     L8        gr7=#SPILL34(gr31,528)
    0| 0004C8 ld       E91F01B8   1     L8        gr8=#SPILL23(gr31,440)
    0| 0004CC ld       E93F0218   1     L8        gr9=#SPILL35(gr31,536)
    0| 0004D0 ld       E95F01B0   1     L8        gr10=#SPILL22(gr31,432)
    0| 0004D4 ld       E97F0220   1     L8        gr11=#SPILL36(gr31,544)
    0| 0004D8 ld       E99F01A0   1     L8        gr12=#SPILL20(gr31,416)
    0| 0004DC ld       EADF0228   1     L8        gr22=#SPILL37(gr31,552)
   64| 0004E0 addi     38630001   1     AI        gr3=gr3,1
    0| 0004E4 add      7C802214   1     A         gr4=gr0,gr4
   64| 0004E8 std      F87F01D0   1     ST8       #SPILL26(gr31,464)=gr3
    0| 0004EC std      F89F0208   1     ST8       #SPILL33(gr31,520)=gr4
   64| 0004F0 cmpld    7C232840   1     CL8       cr0=gr3,gr5
    0| 0004F4 add      7CE63A14   1     A         gr7=gr6,gr7
    0| 0004F8 add      7D284A14   1     A         gr9=gr8,gr9
    0| 0004FC std      F8FF0210   1     ST8       #SPILL34(gr31,528)=gr7
    0| 000500 std      F93F0218   1     ST8       #SPILL35(gr31,536)=gr9
    0| 000504 add      7D6A5A14   1     A         gr11=gr10,gr11
    0| 000508 add      7ECCB214   1     A         gr22=gr12,gr22
    0| 00050C std      F97F0220   1     ST8       #SPILL36(gr31,544)=gr11
    0| 000510 std      FADF0228   1     ST8       #SPILL37(gr31,552)=gr22
   64| 000514 bc       4180FE68   1     BT        CL.28,cr0,0x8/llt,taken=80%(80,20)
   64|                              CL.27:
   71| 000518 ld       E81F0198   1     L8        gr0=#SPILL19(gr31,408)
   71| 00051C ld       E87F0100   1     L8        gr3=#SPILL0(gr31,256)
   71| 000520 cmpdi    2C200000   1     C8        cr0=gr0,0
   71| 000524 cmpwi    2E030000   1     C4        cr4=gr3,0
   71| 000528 crand    4C310A02   1     CR_N      cr0=cr[40],0x2/gt,0x2/gt,0x2/gt,cr0
   71| 00052C bc       4081017C   1     BF        CL.62,cr0,0x2/gt,taken=50%(0,0)
    0| 000530 ld       E89F0110   1     L8        gr4=#SPILL2(gr31,272)
   71| 000534 addi     3A000000   1     LI        gr16=0
    0| 000538 cmpwi    2C040000   1     C4        cr0=gr4,0
    0| 00053C bc       408105FC   1     BF        CL.176,cr0,0x2/gt,taken=40%(40,60)
    0| 000540 ld       E8DF0128   1     L8        gr6=#SPILL5(gr31,296)
    0| 000544 ld       E8FF0168   1     L8        gr7=#SPILL13(gr31,360)
    0| 000548 ld       E91F0120   1     L8        gr8=#SPILL4(gr31,288)
    0| 00054C ld       E93F0160   1     L8        gr9=#SPILL12(gr31,352)
    0| 000550 ld       E99F0260   1     L8        gr12=#SPILL44(gr31,608)
    0| 000554 ld       EB7F0188   1     L8        gr27=#SPILL17(gr31,392)
    0| 000558 ld       EA9F0190   1     L8        gr20=#SPILL18(gr31,400)
    0| 00055C ld       EA5F0180   1     L8        gr18=#SPILL16(gr31,384)
    0| 000560 add      7CA63A14   1     A         gr5=gr6,gr7
    0| 000564 add      7C884A14   1     A         gr4=gr8,gr9
    0| 000568 ld       E95F0118   1     L8        gr10=#SPILL3(gr31,280)
    0| 00056C ld       E97F0158   1     L8        gr11=#SPILL11(gr31,344)
    0| 000570 add      7CA56214   1     A         gr5=gr5,gr12
    0| 000574 add      7C84DA14   1     A         gr4=gr4,gr27
    0| 000578 ld       EB5F0178   1     L8        gr26=#SPILL15(gr31,376)
   74| 00057C ld       EAFF0140   1     L8        gr23=#SPILL8(gr31,320)
   74| 000580 ld       EADF0148   1     L8        gr22=#SPILL9(gr31,328)
    0| 000584 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
   75| 000588 ld       EB3F0138   1     L8        gr25=#SPILL7(gr31,312)
   75| 00058C ld       EB1F0150   1     L8        gr24=#SPILL10(gr31,336)
    0| 000590 add      7E65A214   1     A         gr19=gr5,gr20
    0| 000594 add      7DC49214   1     A         gr14=gr4,gr18
    0| 000598 std      FA7F01A8   1     ST8       #SPILL21(gr31,424)=gr19
    0| 00059C ld       E8BF0170   1     L8        gr5=#SPILL14(gr31,368)
    0| 0005A0 ld       E89F0108   1     L8        gr4=#SPILL1(gr31,264)
    0| 0005A4 add      7C0A5A14   1     A         gr0=gr10,gr11
   74| 0005A8 add      7EB6BA14   1     A         gr21=gr22,gr23
    0| 0005AC add      7C00D214   1     A         gr0=gr0,gr26
   74| 0005B0 std      FABF01A0   1     ST8       #SPILL20(gr31,416)=gr21
   75| 0005B4 add      7E38CA14   1     A         gr17=gr24,gr25
    0| 0005B8 add      7DE02A14   1     A         gr15=gr0,gr5
    0| 0005BC cmpdi    2D240000   1     C8        cr2=gr4,0
    0| 0005C0 lfs      C3E30028   1     LFS       fp31=+CONSTANT_AREA(gr3,40)
    0| 0005C4 lfs      C3C30048   1     LFS       fp30=+CONSTANT_AREA(gr3,72)
    0| 0005C8 lfd      CBA30050   1     LFL       fp29=+CONSTANT_AREA(gr3,80)
    0| 0005CC lfs      C3830058   1     LFS       fp28=+CONSTANT_AREA(gr3,88)
    0| 0005D0 lfd      CB630060   1     LFL       fp27=+CONSTANT_AREA(gr3,96)
   71|                              CL.57:
   72| 0005D4 addi     3B600000   1     LI        gr27=0
    0| 0005D8 bc       408900A0   1     BF        CL.61,cr2,0x2/gt,taken=50%(0,0)
   74| 0005DC ld       EB5F01A0   1     L8        gr26=#SPILL20(gr31,416)
    0| 0005E0 or       7DF97B78   1     LR        gr25=gr15
    0| 0005E4 or       7DD87378   1     LR        gr24=gr14
    0| 0005E8 ld       EAFF01A8   1     L8        gr23=#SPILL21(gr31,424)
   72|                              CL.58:
   74| 0005EC lfdu     CC1A0008   1     LFDU      fp0,gr26=x2b(gr26,8)
    0| 0005F0 addi     3B7B0001   1     AI        gr27=gr27,1
   73| 0005F4 addi     3AC00000   1     LI        gr22=0
   74| 0005F8 fmul     FC2007F2   1     MFL       fp1=fp0,fp31,fcr
   74| 0005FC bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,gr31,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   74| 000600 ori      60000000   1
   74| 000604 std      FB7F01B0   1     ST8       #SPILL22(gr31,432)=gr27
   76| 000608 or       7EF5BB78   1     LR        gr21=gr23
   75| 00060C or       7F14C378   1     LR        gr20=gr24
   74| 000610 or       7F33CB78   1     LR        gr19=gr25
   74| 000614 fmul     FF410772   1     MFL       fp26=fp1,fp29,fcr
   75| 000618 or       7E328B78   2     LR        gr18=gr17
   74| 00061C ld       EB7F0108   1     L8        gr27=#SPILL1(gr31,264)
    0| 000620 ori      60210000   1     XNOP      
   73|                              CL.59:
   74| 000624 stfdux   7F53F5EE   1     STFDU     gr19,b1(gr19,gr30,0)=fp26
   75| 000628 lfdu     CC120008   1     LFDU      fp0,gr18=x1b(gr18,8)
   74| 00062C addi     3AD60001   1     AI        gr22=gr22,1
   75| 000630 fmul     FC200732   1     MFL       fp1=fp0,fp28,fcr
   75| 000634 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,gr31,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   75| 000638 ori      60000000   1
   76| 00063C stfdux   7FD5E5EE   1     STFDU     gr21,b3(gr21,gr28,0)=fp30
   77| 000640 cmpld    7CB6D840   1     CL8       cr1=gr22,gr27
   75| 000644 fmul     FC0106F2   1     MFL       fp0=fp1,fp27,fcr
   75| 000648 stfdux   7C14EDEE   2     STFDU     gr20,b2(gr20,gr29,0)=fp0
   77| 00064C bc       4184FFD8   1     BT        CL.59,cr1,0x8/llt,taken=80%(80,20)
   77| 000650 ld       EB7F01B0   1     L8        gr27=#SPILL22(gr31,432)
   78| 000654 ld       E81F0110   1     L8        gr0=#SPILL2(gr31,272)
    0| 000658 ld       E87F0178   1     L8        gr3=#SPILL15(gr31,376)
    0| 00065C ld       E89F0188   1     L8        gr4=#SPILL17(gr31,392)
    0| 000660 ld       E8BF0260   1     L8        gr5=#SPILL44(gr31,608)
   78| 000664 cmpld    7CBB0040   1     CL8       cr1=gr27,gr0
    0| 000668 add      7F23CA14   1     A         gr25=gr3,gr25
    0| 00066C add      7F04C214   1     A         gr24=gr4,gr24
    0| 000670 add      7EE5BA14   1     A         gr23=gr5,gr23
   78| 000674 bc       4184FF78   1     BT        CL.58,cr1,0x8/llt,taken=80%(80,20)
   78|                              CL.61:
   79| 000678 ld       E87F0198   1     L8        gr3=#SPILL19(gr31,408)
    0| 00067C ld       E8BF0190   1     L8        gr5=#SPILL18(gr31,400)
    0| 000680 ld       E8DF01A8   1     L8        gr6=#SPILL21(gr31,424)
    0| 000684 ld       E81F0170   1     L8        gr0=#SPILL14(gr31,368)
    0| 000688 ld       E89F0180   1     L8        gr4=#SPILL16(gr31,384)
   79| 00068C addi     3A100001   1     AI        gr16=gr16,1
   79| 000690 cmpd     7CA38000   1     C8        cr1=gr3,gr16
    0| 000694 add      7CC53214   1     A         gr6=gr5,gr6
    0| 000698 add      7DE07A14   1     A         gr15=gr0,gr15
    0| 00069C std      F8DF01A8   1     ST8       #SPILL21(gr31,424)=gr6
    0| 0006A0 add      7DC47214   1     A         gr14=gr4,gr14
   79| 0006A4 bc       4185FF30   1     BT        CL.57,cr1,0x2/gt,taken=80%(80,20)
   79|                              CL.62:
   71| 0006A8 ld       E81F0100   1     L8        gr0=#SPILL0(gr31,256)
   71| 0006AC ld       E87F0198   1     L8        gr3=#SPILL19(gr31,408)
   71| 0006B0 cmpd     7CA01800   1     C8        cr1=gr0,gr3
   71| 0006B4 crand    4C312A02   1     CR_N      cr0=cr[41],0x2/gt,0x2/gt,0x2/gt,cr0
   71| 0006B8 bc       408103F8   1     BF        CL.76,cr0,0x2/gt,taken=30%(30,70)
   79| 0006BC ld       E91F0130   1     L8        gr8=#SPILL6(gr31,304)
    0| 0006C0 ld       E99F0190   1     L8        gr12=#SPILL18(gr31,400)
    0| 0006C4 ld       EB7F0198   1     L8        gr27=#SPILL19(gr31,408)
    0| 0006C8 ld       EB5F0180   1     L8        gr26=#SPILL16(gr31,384)
    0| 0006CC ld       EB3F0170   1     L8        gr25=#SPILL14(gr31,368)
    0| 0006D0 ld       E97F0110   1     L8        gr11=#SPILL2(gr31,272)
    0| 0006D4 ld       EB1F0128   1     L8        gr24=#SPILL5(gr31,296)
    0| 0006D8 ld       EAFF0168   1     L8        gr23=#SPILL13(gr31,360)
    0| 0006DC ld       EADF0120   1     L8        gr22=#SPILL4(gr31,288)
    0| 0006E0 ld       EABF0160   1     L8        gr21=#SPILL12(gr31,352)
    0| 0006E4 ld       EA9F0118   1     L8        gr20=#SPILL3(gr31,280)
    0| 0006E8 ld       EA7F0158   1     L8        gr19=#SPILL11(gr31,344)
   79| 0006EC addi     3808FFFF   1     AI        gr0=gr8,-1
    0| 0006F0 cmpwi    2C0B0000   1     C4        cr0=gr11,0
   79| 0006F4 sradi    7C031674   1     SRA8CA    gr3,ca=gr0,2
   71| 0006F8 addi     3A400000   1     LI        gr18=0
    0| 0006FC mulld    7CECD9D2   1     M         gr7=gr12,gr27
   71| 000700 std      FA5F0100   1     ST8       #SPILL0(gr31,256)=gr18
    0| 000704 mulld    7C9AD9D2   1     M         gr4=gr26,gr27
    0| 000708 mulld    7C19D9D2   1     M         gr0=gr25,gr27
    0| 00070C add      7CB7C214   1     A         gr5=gr23,gr24
    0| 000710 add      7D55B214   1     A         gr10=gr21,gr22
    0| 000714 add      7D33A214   1     A         gr9=gr19,gr20
   79| 000718 addze    7CC30194   1     ADDE      gr6,ca=gr3,0,ca
    0| 00071C bc       40810394   1     BF        CL.76,cr0,0x2/gt,taken=20%(20,80)
   75| 000720 ld       EA3F0138   1     L8        gr17=#SPILL7(gr31,312)
   75| 000724 ld       EA1F0150   1     L8        gr16=#SPILL10(gr31,336)
    0| 000728 ld       EB1F0260   1     L8        gr24=#SPILL44(gr31,608)
    0| 00072C ld       EADF0188   1     L8        gr22=#SPILL17(gr31,392)
    0| 000730 ld       EA9F0178   1     L8        gr20=#SPILL15(gr31,376)
    0| 000734 ld       EA7F0190   1     L8        gr19=#SPILL18(gr31,400)
   74| 000738 ld       E9DF0140   1     L8        gr14=#SPILL8(gr31,320)
   75| 00073C add      7DF08A14   1     A         gr15=gr16,gr17
   74| 000740 ld       EA3F0148   1     L8        gr17=#SPILL9(gr31,328)
   75| 000744 std      F9FF0130   1     ST8       #SPILL6(gr31,304)=gr15
    0| 000748 rldicr   799B1764   1     SLL8      gr27=gr12,2
    0| 00074C add      7D05C214   1     A         gr8=gr5,gr24
    0| 000750 std      FB7F0118   1     ST8       #SPILL3(gr31,280)=gr27
    0| 000754 rldicr   7B571764   1     SLL8      gr23=gr26,2
    0| 000758 add      7CAAB214   1     A         gr5=gr10,gr22
    0| 00075C std      FAFF0120   1     ST8       #SPILL4(gr31,288)=gr23
    0| 000760 rldicr   7B351764   1     SLL8      gr21=gr25,2
    0| 000764 add      7D89A214   1     A         gr12=gr9,gr20
    0| 000768 std      FABF0128   1     ST8       #SPILL5(gr31,296)=gr21
    0| 00076C add      7D074214   1     A         gr8=gr7,gr8
    0| 000770 rldicr   7A6A0FA4   1     SLL8      gr10=gr19,1
    0| 000774 subf     7D33D850   1     S         gr9=gr27,gr19
    0| 000778 rldicr   7B470FA4   1     SLL8      gr7=gr26,1
    0| 00077C add      7C842A14   1     A         gr4=gr4,gr5
    0| 000780 subf     7CBAB850   1     S         gr5=gr23,gr26
    0| 000784 rldicr   7B2B0FA4   1     SLL8      gr11=gr25,1
    0| 000788 add      7C006214   1     A         gr0=gr0,gr12
    0| 00078C subf     7D99A850   1     S         gr12=gr21,gr25
    0| 000790 ld       E8620000   1     L8        gr3=.+CONSTANT_AREA(gr2,0)
   74| 000794 add      7E0E8A14   1     A         gr16=gr14,gr17
    0| 000798 add      7E28DA14   1     A         gr17=gr8,gr27
   74| 00079C std      FA1F0138   1     ST8       #SPILL7(gr31,312)=gr16
    0| 0007A0 std      FA3F0140   1     ST8       #SPILL8(gr31,320)=gr17
    0| 0007A4 add      7E734214   1     A         gr19=gr19,gr8
    0| 0007A8 add      7DC85214   1     A         gr14=gr8,gr10
    0| 0007AC std      FA7F0148   1     ST8       #SPILL9(gr31,328)=gr19
    0| 0007B0 std      F9DF0150   1     ST8       #SPILL10(gr31,336)=gr14
    0| 0007B4 add      7D484A14   1     A         gr10=gr8,gr9
    0| 0007B8 add      7D043A14   1     A         gr8=gr4,gr7
    0| 0007BC std      F95F0158   1     ST8       #SPILL11(gr31,344)=gr10
    0| 0007C0 std      F91F0160   1     ST8       #SPILL12(gr31,352)=gr8
    0| 0007C4 add      7D24BA14   1     A         gr9=gr4,gr23
    0| 0007C8 add      7F5A2214   1     A         gr26=gr26,gr4
    0| 0007CC std      F93F0168   1     ST8       #SPILL13(gr31,360)=gr9
    0| 0007D0 std      FB5F0170   1     ST8       #SPILL14(gr31,368)=gr26
    0| 0007D4 add      7CE42A14   1     A         gr7=gr4,gr5
    0| 0007D8 add      7C805A14   1     A         gr4=gr0,gr11
    0| 0007DC std      F8FF0180   1     ST8       #SPILL16(gr31,384)=gr7
    0| 0007E0 std      F89F0190   1     ST8       #SPILL18(gr31,400)=gr4
    0| 0007E4 add      7CA06214   1     A         gr5=gr0,gr12
    0| 0007E8 add      7D60AA14   1     A         gr11=gr0,gr21
    0| 0007EC std      F8BF0198   1     ST8       #SPILL19(gr31,408)=gr5
    0| 0007F0 add      7F390214   1     A         gr25=gr25,gr0
    0| 0007F4 addi     38060001   1     AI        gr0=gr6,1
    0| 0007F8 std      F97F01A0   1     ST8       #SPILL20(gr31,416)=gr11
    0| 0007FC ld       E8DF0108   1     L8        gr6=#SPILL1(gr31,264)
    0| 000800 std      FB3F01A8   1     ST8       #SPILL21(gr31,424)=gr25
    0| 000804 std      F81F01B0   1     ST8       #SPILL22(gr31,432)=gr0
    0| 000808 lfs      C3E30028   1     LFS       fp31=+CONSTANT_AREA(gr3,40)
    0| 00080C lfs      C3C30048   1     LFS       fp30=+CONSTANT_AREA(gr3,72)
    0| 000810 lfd      CBA30050   1     LFL       fp29=+CONSTANT_AREA(gr3,80)
    0| 000814 cmpdi    2E260000   1     C8        cr4=gr6,0
    0| 000818 lfs      C3830058   1     LFS       fp28=+CONSTANT_AREA(gr3,88)
    0| 00081C lfd      CB630060   1     LFL       fp27=+CONSTANT_AREA(gr3,96)
   71|                              CL.34:
   72| 000820 addi     38000000   1     LI        gr0=0
   72| 000824 std      F81F01B8   1     ST8       #SPILL23(gr31,440)=gr0
    0| 000828 bc       409101D4   1     BF        CL.35,cr4,0x2/gt,taken=20%(20,80)
   74| 00082C ld       E87F0138   1     L8        gr3=#SPILL7(gr31,312)
    0| 000830 ld       E89F0198   1     L8        gr4=#SPILL19(gr31,408)
    0| 000834 ld       E8BF01A0   1     L8        gr5=#SPILL20(gr31,416)
    0| 000838 ld       E8DF01A8   1     L8        gr6=#SPILL21(gr31,424)
    0| 00083C ld       E8FF0190   1     L8        gr7=#SPILL18(gr31,400)
    0| 000840 ld       E91F0168   1     L8        gr8=#SPILL13(gr31,360)
   74| 000844 std      F87F01C0   1     ST8       #SPILL24(gr31,448)=gr3
    0| 000848 std      F89F01C8   1     ST8       #SPILL25(gr31,456)=gr4
    0| 00084C std      F8BF01D0   1     ST8       #SPILL26(gr31,464)=gr5
    0| 000850 std      F8DF01D8   1     ST8       #SPILL27(gr31,472)=gr6
    0| 000854 std      F8FF01E0   1     ST8       #SPILL28(gr31,480)=gr7
    0| 000858 std      F91F01E8   1     ST8       #SPILL29(gr31,488)=gr8
    0| 00085C ld       E93F0170   1     L8        gr9=#SPILL14(gr31,368)
    0| 000860 ld       E95F0180   1     L8        gr10=#SPILL16(gr31,384)
    0| 000864 ld       E97F0160   1     L8        gr11=#SPILL12(gr31,352)
    0| 000868 ld       E99F0140   1     L8        gr12=#SPILL8(gr31,320)
    0| 00086C ld       EB7F0148   1     L8        gr27=#SPILL9(gr31,328)
    0| 000870 ld       EB5F0158   1     L8        gr26=#SPILL11(gr31,344)
    0| 000874 std      F93F01F0   1     ST8       #SPILL30(gr31,496)=gr9
    0| 000878 std      F95F01F8   1     ST8       #SPILL31(gr31,504)=gr10
    0| 00087C std      F97F0200   1     ST8       #SPILL32(gr31,512)=gr11
    0| 000880 std      F99F0208   1     ST8       #SPILL33(gr31,520)=gr12
    0| 000884 std      FB7F0210   1     ST8       #SPILL34(gr31,528)=gr27
    0| 000888 std      FB5F0218   1     ST8       #SPILL35(gr31,536)=gr26
    0| 00088C ld       EB3F0150   1     L8        gr25=#SPILL10(gr31,336)
    0| 000890 std      FB3F0220   1     ST8       #SPILL36(gr31,544)=gr25
   72|                              CL.36:
   74| 000894 ld       E87F01C0   1     L8        gr3=#SPILL24(gr31,448)
    0| 000898 ld       E89F01B8   1     L8        gr4=#SPILL23(gr31,440)
   74| 00089C lfdu     CC030008   1     LFDU      fp0,gr3=x2b(gr3,8)
    0| 0008A0 addi     38840001   1     AI        gr4=gr4,1
    0| 0008A4 std      F89F01B8   1     ST8       #SPILL23(gr31,440)=gr4
   74| 0008A8 fmul     FC2007F2   1     MFL       fp1=fp0,fp31,fcr
   74| 0008AC std      F87F01C0   1     ST8       #SPILL24(gr31,448)=gr3
   74| 0008B0 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,gr31,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   74| 0008B4 ori      60000000   1
   76| 0008B8 ld       EB7F0208   1     L8        gr27=#SPILL33(gr31,520)
   76| 0008BC ld       EB5F0210   1     L8        gr26=#SPILL34(gr31,528)
   76| 0008C0 ld       EB3F0220   1     L8        gr25=#SPILL36(gr31,544)
   74| 0008C4 fmul     FF410772   1     MFL       fp26=fp1,fp29,fcr
   76| 0008C8 ld       EB1F0218   1     L8        gr24=#SPILL35(gr31,536)
   75| 0008CC ld       EAFF01E8   1     L8        gr23=#SPILL29(gr31,488)
   75| 0008D0 ld       EADF01F0   1     L8        gr22=#SPILL30(gr31,496)
   75| 0008D4 ld       EABF0200   1     L8        gr21=#SPILL32(gr31,512)
   75| 0008D8 ld       EA9F01F8   1     L8        gr20=#SPILL31(gr31,504)
   74| 0008DC ld       EA7F01D0   1     L8        gr19=#SPILL26(gr31,464)
   74| 0008E0 ld       EA5F01D8   1     L8        gr18=#SPILL27(gr31,472)
   74| 0008E4 ld       EA3F01E0   1     L8        gr17=#SPILL28(gr31,480)
   74| 0008E8 ld       EA1F01C8   1     L8        gr16=#SPILL25(gr31,456)
   75| 0008EC ld       E9FF0130   1     L8        gr15=#SPILL6(gr31,304)
   74| 0008F0 addi     39C00001   1     LI        gr14=1
    0| 0008F4 ori      60210000   1     XNOP      
    0| 0008F8 ori      60210000   1     XNOP      
   73|                              CL.38:
   75| 0008FC lfdu     CC0F0008   1     LFDU      fp0,gr15=x1b(gr15,8)
   74| 000900 stfdux   7F52F5EE   1     STFDU     gr18,b1(gr18,gr30,0)=fp26
   75| 000904 fmul     FC200732   1     MFL       fp1=fp0,fp28,fcr
   75| 000908 bl       48000001   1     CALLN     fp1=__xl_sin,1,fp1,__xl_sin,#MX_TEMP1",__xl_sin",gr1,gr31,cr[01567]",gr0",gr3"-gr12",fp0",fp2"-fp13",mq",lr",xer",fsr",ca",ctr"
   75| 00090C ori      60000000   1
   76| 000910 stfdux   7FDAE5EE   1     STFDU     gr26,b3(gr26,gr28,0)=fp30
   74| 000914 stfdux   7F51F5EE   1     STFDU     gr17,b1(gr17,gr30,0)=fp26
   76| 000918 stfdux   7FD9E5EE   1     STFDU     gr25,b3(gr25,gr28,0)=fp30
   74| 00091C stfdux   7F50F5EE   1     STFDU     gr16,b1(gr16,gr30,0)=fp26
   75| 000920 fmul     FC0106F2   1     MFL       fp0=fp1,fp27,fcr
   76| 000924 stfdux   7FD8E5EE   1     STFDU     gr24,b3(gr24,gr28,0)=fp30
   74| 000928 stfdux   7F53F5EE   1     STFDU     gr19,b1(gr19,gr30,0)=fp26
   77| 00092C ld       E81F0108   1     L8        gr0=#SPILL1(gr31,264)
   76| 000930 stfdux   7FDBE5EE   1     STFDU     gr27,b3(gr27,gr28,0)=fp30
   75| 000934 stfdux   7C16EDEE   1     STFDU     gr22,b2(gr22,gr29,0)=fp0
   75| 000938 stfdux   7C15EDEE   1     STFDU     gr21,b2(gr21,gr29,0)=fp0
   77| 00093C cmpld    7C2E0040   1     CL8       cr0=gr14,gr0
   75| 000940 stfdux   7C14EDEE   1     STFDU     gr20,b2(gr20,gr29,0)=fp0
   74| 000944 addi     39CE0001   1     AI        gr14=gr14,1
   75| 000948 stfdux   7C17EDEE   1     STFDU     gr23,b2(gr23,gr29,0)=fp0
   77| 00094C bc       4180FFB0   1     BT        CL.38,cr0,0x8/llt,taken=80%(80,20)
   78| 000950 ld       E81F01B8   1     L8        gr0=#SPILL23(gr31,440)
   78| 000954 ld       E87F0110   1     L8        gr3=#SPILL2(gr31,272)
    0| 000958 ld       E89F0178   1     L8        gr4=#SPILL15(gr31,376)
    0| 00095C ld       E8BF01C8   1     L8        gr5=#SPILL25(gr31,456)
    0| 000960 ld       E8DF01D0   1     L8        gr6=#SPILL26(gr31,464)
    0| 000964 ld       E8FF01D8   1     L8        gr7=#SPILL27(gr31,472)
    0| 000968 ld       E91F01E0   1     L8        gr8=#SPILL28(gr31,480)
    0| 00096C ld       E93F0188   1     L8        gr9=#SPILL17(gr31,392)
    0| 000970 ld       E95F01E8   1     L8        gr10=#SPILL29(gr31,488)
    0| 000974 ld       E97F01F0   1     L8        gr11=#SPILL30(gr31,496)
    0| 000978 ld       E99F01F8   1     L8        gr12=#SPILL31(gr31,504)
    0| 00097C ld       EB7F0200   1     L8        gr27=#SPILL32(gr31,512)
    0| 000980 ld       EB5F0260   1     L8        gr26=#SPILL44(gr31,608)
    0| 000984 ld       EB3F0208   1     L8        gr25=#SPILL33(gr31,520)
    0| 000988 ld       EB1F0210   1     L8        gr24=#SPILL34(gr31,528)
    0| 00098C ld       EAFF0218   1     L8        gr23=#SPILL35(gr31,536)
    0| 000990 ld       EADF0220   1     L8        gr22=#SPILL36(gr31,544)
   78| 000994 cmpld    7C201840   1     CL8       cr0=gr0,gr3
    0| 000998 add      7CA42A14   1     A         gr5=gr4,gr5
    0| 00099C add      7CC43214   1     A         gr6=gr4,gr6
    0| 0009A0 std      F8BF01C8   1     ST8       #SPILL25(gr31,456)=gr5
    0| 0009A4 std      F8DF01D0   1     ST8       #SPILL26(gr31,464)=gr6
    0| 0009A8 add      7CE43A14   1     A         gr7=gr4,gr7
    0| 0009AC add      7D044214   1     A         gr8=gr4,gr8
    0| 0009B0 std      F8FF01D8   1     ST8       #SPILL27(gr31,472)=gr7
    0| 0009B4 std      F91F01E0   1     ST8       #SPILL28(gr31,480)=gr8
    0| 0009B8 add      7D495214   1     A         gr10=gr9,gr10
    0| 0009BC add      7D695A14   1     A         gr11=gr9,gr11
    0| 0009C0 std      F95F01E8   1     ST8       #SPILL29(gr31,488)=gr10
    0| 0009C4 std      F97F01F0   1     ST8       #SPILL30(gr31,496)=gr11
    0| 0009C8 add      7D896214   1     A         gr12=gr9,gr12
    0| 0009CC add      7F69DA14   1     A         gr27=gr9,gr27
    0| 0009D0 std      F99F01F8   1     ST8       #SPILL31(gr31,504)=gr12
    0| 0009D4 std      FB7F0200   1     ST8       #SPILL32(gr31,512)=gr27
    0| 0009D8 add      7F39D214   1     A         gr25=gr25,gr26
    0| 0009DC add      7F18D214   1     A         gr24=gr24,gr26
    0| 0009E0 std      FB3F0208   1     ST8       #SPILL33(gr31,520)=gr25
    0| 0009E4 std      FB1F0210   1     ST8       #SPILL34(gr31,528)=gr24
    0| 0009E8 add      7EF7D214   1     A         gr23=gr23,gr26
    0| 0009EC add      7ED6D214   1     A         gr22=gr22,gr26
    0| 0009F0 std      FAFF0218   1     ST8       #SPILL35(gr31,536)=gr23
    0| 0009F4 std      FADF0220   1     ST8       #SPILL36(gr31,544)=gr22
   78| 0009F8 bc       4180FE9C   1     BT        CL.36,cr0,0x8/llt,taken=80%(80,20)
   78|                              CL.35:
   79| 0009FC ld       E87F0100   1     L8        gr3=#SPILL0(gr31,256)
    0| 000A00 ld       E81F0118   1     L8        gr0=#SPILL3(gr31,280)
    0| 000A04 ld       E89F0140   1     L8        gr4=#SPILL8(gr31,320)
   79| 000A08 ld       E8BF01B0   1     L8        gr5=#SPILL22(gr31,432)
    0| 000A0C ld       E8DF0148   1     L8        gr6=#SPILL9(gr31,328)
    0| 000A10 ld       E8FF0150   1     L8        gr7=#SPILL10(gr31,336)
    0| 000A14 ld       E91F0158   1     L8        gr8=#SPILL11(gr31,344)
    0| 000A18 ld       E93F0120   1     L8        gr9=#SPILL4(gr31,288)
    0| 000A1C ld       E95F0160   1     L8        gr10=#SPILL12(gr31,352)
    0| 000A20 ld       E97F0168   1     L8        gr11=#SPILL13(gr31,360)
    0| 000A24 ld       E99F0170   1     L8        gr12=#SPILL14(gr31,368)
    0| 000A28 ld       EB7F0180   1     L8        gr27=#SPILL16(gr31,384)
    0| 000A2C ld       EB5F0128   1     L8        gr26=#SPILL5(gr31,296)
    0| 000A30 ld       EB3F0190   1     L8        gr25=#SPILL18(gr31,400)
    0| 000A34 ld       EB1F0198   1     L8        gr24=#SPILL19(gr31,408)
    0| 000A38 ld       EAFF01A0   1     L8        gr23=#SPILL20(gr31,416)
    0| 000A3C ld       EADF01A8   1     L8        gr22=#SPILL21(gr31,424)
   79| 000A40 addi     38630001   1     AI        gr3=gr3,1
    0| 000A44 add      7C802214   1     A         gr4=gr0,gr4
   79| 000A48 std      F87F0100   1     ST8       #SPILL0(gr31,256)=gr3
    0| 000A4C std      F89F0140   1     ST8       #SPILL8(gr31,320)=gr4
   79| 000A50 cmpld    7C232840   1     CL8       cr0=gr3,gr5
    0| 000A54 add      7CC03214   1     A         gr6=gr0,gr6
    0| 000A58 add      7CE03A14   1     A         gr7=gr0,gr7
    0| 000A5C std      F8DF0148   1     ST8       #SPILL9(gr31,328)=gr6
    0| 000A60 std      F8FF0150   1     ST8       #SPILL10(gr31,336)=gr7
    0| 000A64 add      7D004214   1     A         gr8=gr0,gr8
    0| 000A68 add      7D495214   1     A         gr10=gr9,gr10
    0| 000A6C std      F91F0158   1     ST8       #SPILL11(gr31,344)=gr8
    0| 000A70 std      F95F0160   1     ST8       #SPILL12(gr31,352)=gr10
    0| 000A74 add      7D695A14   1     A         gr11=gr9,gr11
    0| 000A78 add      7D896214   1     A         gr12=gr9,gr12
    0| 000A7C std      F97F0168   1     ST8       #SPILL13(gr31,360)=gr11
    0| 000A80 std      F99F0170   1     ST8       #SPILL14(gr31,368)=gr12
    0| 000A84 add      7F69DA14   1     A         gr27=gr9,gr27
    0| 000A88 add      7F39D214   1     A         gr25=gr25,gr26
    0| 000A8C std      FB7F0180   1     ST8       #SPILL16(gr31,384)=gr27
    0| 000A90 std      FB3F0190   1     ST8       #SPILL18(gr31,400)=gr25
    0| 000A94 add      7F18D214   1     A         gr24=gr24,gr26
    0| 000A98 add      7EF7D214   1     A         gr23=gr23,gr26
    0| 000A9C std      FB1F0198   1     ST8       #SPILL19(gr31,408)=gr24
    0| 000AA0 std      FAFF01A0   1     ST8       #SPILL20(gr31,416)=gr23
    0| 000AA4 add      7ED6D214   1     A         gr22=gr22,gr26
    0| 000AA8 std      FADF01A8   1     ST8       #SPILL21(gr31,424)=gr22
   79| 000AAC bc       4180FD74   1     BT        CL.34,cr0,0x8/llt,taken=80%(80,20)
   82|                              CL.76:
   82| 000AB0 ld       E8210000   1     L8        gr1=#stack(gr1,0),gr31"
   82| 000AB4 ld       E8010010   1     L8        gr0=#stack(gr1,16)
   82| 000AB8 lwa      E981000A   1     L4A       gr12=#stack(gr1,8)
   82| 000ABC ld       E9C1FF28   1     L8        gr14=#stack(gr1,-216)
   82| 000AC0 ld       E9E1FF30   1     L8        gr15=#stack(gr1,-208)
   82| 000AC4 ld       EA01FF38   1     L8        gr16=#stack(gr1,-200)
   82| 000AC8 ld       EA21FF40   1     L8        gr17=#stack(gr1,-192)
   82| 000ACC mtspr    7C0803A6   1     LLR       lr=gr0
   82| 000AD0 ld       EA41FF48   1     L8        gr18=#stack(gr1,-184)
   82| 000AD4 ld       EA61FF50   1     L8        gr19=#stack(gr1,-176)
   82| 000AD8 ld       EA81FF58   1     L8        gr20=#stack(gr1,-168)
   82| 000ADC ld       EAA1FF60   1     L8        gr21=#stack(gr1,-160)
   82| 000AE0 ld       EAC1FF68   1     L8        gr22=#stack(gr1,-152)
   82| 000AE4 ld       EAE1FF70   1     L8        gr23=#stack(gr1,-144)
   82| 000AE8 ld       EB01FF78   1     L8        gr24=#stack(gr1,-136)
   82| 000AEC ld       EB21FF80   1     L8        gr25=#stack(gr1,-128)
   82| 000AF0 ld       EB41FF88   1     L8        gr26=#stack(gr1,-120)
   82| 000AF4 ld       EB61FF90   1     L8        gr27=#stack(gr1,-112)
   82| 000AF8 ld       EB81FF98   1     L8        gr28=#stack(gr1,-104)
   82| 000AFC ld       EBA1FFA0   1     L8        gr29=#stack(gr1,-96)
   82| 000B00 ld       EBC1FFA8   1     L8        gr30=#stack(gr1,-88)
   82| 000B04 ld       EBE1FFB0   1     L8        gr31=#stack(gr1,-80)
   82| 000B08 mtcrf    7D820120   1     MTCRF     cr2=gr12
   82| 000B0C mtcrf    7D808120   1     MTCRF     cr4=gr12
   82| 000B10 lfd      CBE1FFF8   1     LFL       fp31=#stack(gr1,-8)
   82| 000B14 lfd      CBC1FFF0   1     LFL       fp30=#stack(gr1,-16)
   82| 000B18 lfd      CBA1FFE8   1     LFL       fp29=#stack(gr1,-24)
   82| 000B1C lfd      CB81FFE0   1     LFL       fp28=#stack(gr1,-32)
   82| 000B20 lfd      CB61FFD8   1     LFL       fp27=#stack(gr1,-40)
   82| 000B24 lfd      CB41FFD0   1     LFL       fp26=#stack(gr1,-48)
   82| 000B28 lfd      CB21FFC8   1     LFL       fp25=#stack(gr1,-56)
   82| 000B2C lfd      CB01FFC0   1     LFL       fp24=#stack(gr1,-64)
   82| 000B30 lfd      CAE1FFB8   1     LFL       fp23=#stack(gr1,-72)
   82| 000B34 bclr     4E800020   1     BA        lr
   78|                              CL.176:
    0| 000B38 mtspr    7C0903A6   1     LCTR      ctr=gr0
   78|                              CL.145:
   79| 000B3C addi     3A100001   1     AI        gr16=gr16,1
   79| 000B40 cmpd     7CB00000   1     C8        cr1=gr16,gr0
   79| 000B44 bc       4104FFF8   1     BCTT      ctr=CL.145,cr1,0x1/lt,taken=80%(80,20)
    0| 000B48 b        4BFFFB60   1     B         CL.62,-1
     |               Tag Table
     | 000B4C        00000000 00012223 89120000 00000B4C 1F
     |               Instruction count          723
     |               Straight-line exec time    732
     |               Constant Area
     | 000000        6D686476 6F727465 782E6639 30000000 7067656E 666F6F49
     | 000018        6D686476 6F727465 782E6639 3049424D 40C90FDB 49424D20
     | 000030        4042D97C 88000000 40A00000 49424D20 3FCC4B51 6ED45AD0
     | 000048        00000000 49424D20 BFD20DD7 4C0BB604 41490FDB 49424D20
     | 000060        3FD20DD7 4C0BB604 3F800000

 
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    mhdvortex.f90               07/08/15   15:48:50
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................      82
1501-510  Compilation successful for file mhdvortex.f90.
1501-543  Object file created.
