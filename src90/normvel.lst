IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- normvel.f90 07/08/15 15:48:57
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
** normvel   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at normvel.f90 <line 17> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at normvel.f90 <line 19> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 3) at normvel.f90 <line 21> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 3) at normvel.f90 <line 21> was not SIMD vectorized because it contains memory references norm = ((norm + (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) * (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) + (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) * (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) + (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) * (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at normvel.f90 <line 23> was not SIMD vectorized because it contains memory references norm = ((norm + (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) * (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) + (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) * (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) + (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) * (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at normvel.f90 <line 23> was not SIMD vectorized because it contains operation in ((norm + (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) * (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) + (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) * (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) + (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) * (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) ks)][(long long) js + $$CIV1][(long long) is + $$CIV0]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at normvel.f90 <line 23> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*((long long) ks + $$CIV2) + (d-v1%bounds%mult[].off488)*((long long) js + $$CIV1) + (d-v1%bounds%mult[].off512)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at normvel.f90 <line 23> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at normvel.f90 <line 23> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 4) at normvel.f90 <line 31> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at normvel.f90 <line 31> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[1ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[1ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[2ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[2ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[3ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[3ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[1ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[2ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)).
1586-536 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(3ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[3ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(3ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 6) at normvel.f90 <line 31> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(3ll + (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + d-v1%bounds%lbound[].off448)) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)).
1586-534 (I) Loop (loop index 7) at normvel.f90 <line 32> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 8) at normvel.f90 <line 32> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[1ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] = ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[1ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[3ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] = ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[3ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] = ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[2ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] = ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[2ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[1ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)).
1586-536 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(3ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[3ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(3ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(3ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)).
1586-536 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)).
1586-536 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[2ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 9) at normvel.f90 <line 32> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) + d-v2%bounds%lbound[].off552)) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)).
1586-534 (I) Loop (loop index 10) at normvel.f90 <line 33> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 11) at normvel.f90 <line 33> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[3ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] = ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[3ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[2ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] = ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[2ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] = ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] = ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[3ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[2ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656)) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)).
1586-536 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 12) at normvel.f90 <line 33> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) + d-v3%bounds%lbound[].off656) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)).
1586-534 (I) Loop (loop index 13) at normvel.f90 <line 36> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 14) at normvel.f90 <line 38> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 15) at normvel.f90 <line 40> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 15) at normvel.f90 <line 40> was not SIMD vectorized because it contains memory references av = av + (((((((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) *  5.0000000000000000E-001; with non-vectorizable strides.
1586-540 (I) Loop (loop index 15) at normvel.f90 <line 40> was not SIMD vectorized because it contains memory references $$SCREP0 = $$SCREP0 + (((((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) * (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) * (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC])) + (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) * (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC])) *  2.5000000000000000E-001; with non-vectorizable strides.
1586-536 (I) Loop (loop index 15) at normvel.f90 <line 42> was not SIMD vectorized because it contains memory references av = av + (((((((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) *  5.0000000000000000E-001; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 15) at normvel.f90 <line 42> was not SIMD vectorized because it contains operation in av + (((((((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC]) + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) *  5.0000000000000000E-001 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 15) at normvel.f90 <line 42> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*((long long) ks + $$CIVE) + (d-v1%bounds%mult[].off488)*((long long) js + $$CIVD) + (d-v1%bounds%mult[].off512)*((long long) is + $$CIVC)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 15) at normvel.f90 <line 42> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 15) at normvel.f90 <line 42> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 15) at normvel.f90 <line 45> was not SIMD vectorized because it contains memory references $$SCREP0 = $$SCREP0 + (((((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) * (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) * (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC])) + (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) * (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC])) *  2.5000000000000000E-001; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 15) at normvel.f90 <line 45> was not SIMD vectorized because it contains operation in $$SCREP0 + (((((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) * (((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) + (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC]) * (((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + ($$CIVD + (long long) js)][(long long) is + $$CIVC])) + (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC]) * (((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long long) js + $$CIVD][(long long) is + $$CIVC] + ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) ks)][(long long) js + $$CIVD][(long long) is + $$CIVC])) *  2.5000000000000000E-001 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 15) at normvel.f90 <line 45> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*((long long) ks + $$CIVE) + (d-v1%bounds%mult[].off488)*((long long) js + $$CIVD) + (d-v1%bounds%mult[].off512)*((long long) is + $$CIVC)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 15) at normvel.f90 <line 45> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 15) at normvel.f90 <line 45> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 19) at normvel.f90 <line 33> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 20) at normvel.f90 <line 33> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[d-v3%bounds%lbound[].off656 + $$CIVB][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] = ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[d-v3%bounds%lbound[].off656 + $$CIVB][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off656 + $$CIVB) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v3%addr  + d-v3%rvo))->v3[].rns2.[d-v3%bounds%lbound[].off656 + $$CIVB][d-v3%bounds%lbound[].off680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because it contains memory references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off656 + $$CIVB) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 21) at normvel.f90 <line 33> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v3%addr  + d-v3%rvo + (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off656 + $$CIVB) + (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off680 + $$CIVA) + (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off704 + $$CIV9)).
1586-534 (I) Loop (loop index 25) at normvel.f90 <line 32> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 26) at normvel.f90 <line 32> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[d-v2%bounds%lbound[].off552 + $$CIV8][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] = ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[d-v2%bounds%lbound[].off552 + $$CIV8][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off552 + $$CIV8) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v2%addr  + d-v2%rvo))->v2[].rns3.[d-v2%bounds%lbound[].off552 + $$CIV8][d-v2%bounds%lbound[].off576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because it contains memory references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off552 + $$CIV8) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 27) at normvel.f90 <line 32> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v2%addr  + d-v2%rvo + (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off552 + $$CIV8) + (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off576 + $$CIV7) + (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off600 + $$CIV6)).
1586-534 (I) Loop (loop index 31) at normvel.f90 <line 31> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 32) at normvel.f90 <line 31> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[d-v1%bounds%lbound[].off448 + $$CIV5][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[d-v1%bounds%lbound[].off448 + $$CIV5][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)); with non-vectorizable strides.
1586-536 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(d-v1%bounds%lbound[].off448 + $$CIV5) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable alignment.
1586-537 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[d-v1%bounds%lbound[].off448 + $$CIV5][d-v1%bounds%lbound[].off472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / _sqrt@18(((norm *  2.5000000000000000E-001) / (double) ((float) ((double) ((float) ijkn) EXPI  3))) / (.rms->rms * .rms->rms)) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(d-v1%bounds%lbound[].off448 + $$CIV5) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)) with non-vectorizable strides.
1586-554 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-556 (I) Loop (loop index 33) at normvel.f90 <line 31> was not SIMD vectorized because it contains non-stride-one store references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*(d-v1%bounds%lbound[].off448 + $$CIV5) + (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off472 + $$CIV4) + (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off496 + $$CIV3)).
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"8">. Total number of the innermost loops SIMD vectorized <"0">.


     1|         SUBROUTINE normvel (rms)
    16|           norm =  0.0000000000000000E+000
    17|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIV2 = 0
       Id=1         DO $$CIV2 = $$CIV2, int((1 + (int(ke) - int(ks))))-1
    19|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int((1 + (int(je) - int(js))))-1
    21|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV0 = 0
                            $$PRC2 = d-v1%addr%v1(int(is),$$CIV1 + int(js),&
                &             $$CIV2 + int(ks))
       Id=3                 DO $$CIV0 = $$CIV0, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC3 = d-v1%addr%v1(1 + (int(is) + $$CIV0),&
                &               $$CIV1 + int(js),$$CIV2 + int(ks))
    23|                       norm = ((norm + ($$PRC2 + $$PRC3) * ($$PRC2 + &
                &               $$PRC3)) + (d-v2%addr%v2(int(is) + $$CIV0,int(js) &
                &               + $$CIV1,int(ks) + $$CIV2) + d-v2%addr%v2(int(is) &
                &               + $$CIV0,1 + ($$CIV1 + int(js)),int(ks) + $$CIV2))&
                &                * (d-v2%addr%v2(int(is) + $$CIV0,int(js) + &
                &               $$CIV1,int(ks) + $$CIV2) + d-v2%addr%v2(int(is) + &
                &               $$CIV0,1 + ($$CIV1 + int(js)),int(ks) + $$CIV2))) &
                &               + (d-v3%addr%v3(int(is) + $$CIV0,int(js) + $$CIV1,&
                &               int(ks) + $$CIV2) + d-v3%addr%v3(int(is) + $$CIV0,&
                &               int(js) + $$CIV1,1 + ($$CIV2 + int(ks)))) * (&
                &               d-v3%addr%v3(int(is) + $$CIV0,int(js) + $$CIV1,&
                &               int(ks) + $$CIV2) + d-v3%addr%v3(int(is) + $$CIV0,&
                &               int(js) + $$CIV1,1 + ($$CIV2 + int(ks))))
                              $$PRC2 = $$PRC3
    27|                     ENDDO
                          ENDIF
    28|                 ENDDO
                      ENDIF
    29|             ENDDO
                  ENDIF
    30|           norm.rnn13 = _sqrt(((%VAL(norm) *  2.5000000000000000E-001) / &
                &   real(real((real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
    31|           IF ((MOD(d-v1%bounds%extent[].off456, 4) > 0  .AND.  &
                &   d-v1%bounds%extent[].off456 > 0)) THEN
                    $$CIV5 = 0
       Id=31        DO $$CIV5 = $$CIV5, MOD(d-v1%bounds%extent[].off456, int(&
                &       4))-1
                      IF ((d-v1%bounds%extent[].off480 > 0)) THEN
                        $$CIV4 = 0
       Id=32            DO $$CIV4 = $$CIV4, int(d-v1%bounds%extent[].off480)&
                &           -1
                          IF ((d-v1%bounds%extent[].off504 > 0)) THEN
                            $$CIV3 = 0
       Id=33                DO $$CIV3 = $$CIV3, int(&
                &               d-v1%bounds%extent[].off504)-1
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,&
                &               d-v1%bounds%lbound[].off448 + $$CIV5) = &
                &               d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,&
                &               d-v1%bounds%lbound[].off448 + $$CIV5) / _sqrt(((&
                &               %VAL(norm) *  2.5000000000000000E-001) / real(&
                &               real((real(real(%VAL(ijkn))) ** 3)))) / (rms * &
                &               rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-v1%bounds%extent[].off456 > 0  .AND.  &
                &   d-v1%bounds%extent[].off456 > MOD(d-v1%bounds%extent[].off456,&
                &    4))) THEN
                    $$CIVF = int(0)
       Id=4         DO $$CIVF = $$CIVF, int((((d-v1%bounds%extent[].off456 - &
                &       MOD(d-v1%bounds%extent[].off456, 4)) - 1) / 4 + 1))-1
                      IF ((d-v1%bounds%extent[].off480 > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int(d-v1%bounds%extent[].off480)&
                &           -1
                          IF ((d-v1%bounds%extent[].off504 > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(&
                &               d-v1%bounds%extent[].off504)-1
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448) / _sqrt(((%VAL(norm) &
                &               *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) / _sqrt(((%VAL(norm)&
                &                *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) / _sqrt(((%VAL(norm)&
                &                *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) / _sqrt(((%VAL(norm)&
                &                *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    32|           IF ((MOD(d-v2%bounds%extent[].off560, 4) > 0  .AND.  &
                &   d-v2%bounds%extent[].off560 > 0)) THEN
                    $$CIV8 = 0
       Id=25        DO $$CIV8 = $$CIV8, MOD(d-v2%bounds%extent[].off560, int(&
                &       4))-1
                      IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                        $$CIV7 = 0
       Id=26            DO $$CIV7 = $$CIV7, int(d-v2%bounds%extent[].off584)&
                &           -1
                          IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                            $$CIV6 = 0
       Id=27                DO $$CIV6 = $$CIV6, int(&
                &               d-v2%bounds%extent[].off608)-1
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,&
                &               d-v2%bounds%lbound[].off552 + $$CIV8) = &
                &               d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,&
                &               d-v2%bounds%lbound[].off552 + $$CIV8) / _sqrt(((&
                &               %VAL(norm) *  2.5000000000000000E-001) / real(&
                &               real((real(real(%VAL(ijkn))) ** 3)))) / (rms * &
                &               rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-v2%bounds%extent[].off560 > 0  .AND.  &
                &   d-v2%bounds%extent[].off560 > MOD(d-v2%bounds%extent[].off560,&
                &    4))) THEN
                    $$CIV10 = int(0)
       Id=7         DO $$CIV10 = $$CIV10, int((((d-v2%bounds%extent[].off560 &
                &       - MOD(d-v2%bounds%extent[].off560, 4)) - 1) / 4 + 1))-1
                      IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                        $$CIV7 = 0
       Id=8             DO $$CIV7 = $$CIV7, int(d-v2%bounds%extent[].off584)&
                &           -1
                          IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                            $$CIV6 = 0
       Id=9                 DO $$CIV6 = $$CIV6, int(&
                &               d-v2%bounds%extent[].off608)-1
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-v2%bounds%extent[].off560, 4)) + &
                &               d-v2%bounds%lbound[].off552) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-v2%bounds%extent[].off560, 4)) + &
                &               d-v2%bounds%lbound[].off552) / _sqrt(((%VAL(norm) &
                &               *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    33|           IF ((MOD(d-v3%bounds%extent[].off664, 4) > 0  .AND.  &
                &   d-v3%bounds%extent[].off664 > 0)) THEN
                    $$CIVB = 0
       Id=19        DO $$CIVB = $$CIVB, MOD(d-v3%bounds%extent[].off664, int(&
                &       4))-1
                      IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                        $$CIVA = 0
       Id=20            DO $$CIVA = $$CIVA, int(d-v3%bounds%extent[].off688)&
                &           -1
                          IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                            $$CIV9 = 0
       Id=21                DO $$CIV9 = $$CIV9, int(&
                &               d-v3%bounds%extent[].off712)-1
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,&
                &               d-v3%bounds%lbound[].off656 + $$CIVB) = &
                &               d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,&
                &               d-v3%bounds%lbound[].off656 + $$CIVB) / _sqrt(((&
                &               %VAL(norm) *  2.5000000000000000E-001) / real(&
                &               real((real(real(%VAL(ijkn))) ** 3)))) / (rms * &
                &               rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-v3%bounds%extent[].off664 > 0  .AND.  &
                &   d-v3%bounds%extent[].off664 > MOD(d-v3%bounds%extent[].off664,&
                &    4))) THEN
                    $$CIV11 = int(0)
       Id=10        DO $$CIV11 = $$CIV11, int((((d-v3%bounds%extent[].off664 &
                &       - MOD(d-v3%bounds%extent[].off664, 4)) - 1) / 4 + 1))-1
                      IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                        $$CIVA = 0
       Id=11            DO $$CIVA = $$CIVA, int(d-v3%bounds%extent[].off688)&
                &           -1
                          IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                            $$CIV9 = 0
       Id=12                DO $$CIV9 = $$CIV9, int(&
                &               d-v3%bounds%extent[].off712)-1
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,($$CIV11 * 4 &
                &               + MOD(d-v3%bounds%extent[].off664, 4)) + &
                &               d-v3%bounds%lbound[].off656) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,($$CIV11 * 4 &
                &               + MOD(d-v3%bounds%extent[].off664, 4)) + &
                &               d-v3%bounds%lbound[].off656) / _sqrt(((%VAL(norm) &
                &               *  2.5000000000000000E-001) / real(real((real(&
                &               real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) / _sqrt(((%VAL(&
                &               norm) *  2.5000000000000000E-001) / real(real((&
                &               real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    34|           av =  0.0000000000000000E+000
    35|           rms =  0.0000000000000000E+000
    36|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIVE = 0
       Id=13        DO $$CIVE = $$CIVE, int((1 + (int(ke) - int(ks))))-1
    38|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVD = 0
       Id=14            DO $$CIVD = $$CIVD, int((1 + (int(je) - int(js))))-1
    40|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIVC = 0
                            $$SCREP0 = rms
                            $$PRC0 = d-v1%addr%v1(int(is),$$CIVD + int(js),&
                &             $$CIVE + int(ks))
       Id=15                DO $$CIVC = $$CIVC, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC1 = d-v1%addr%v1(1 + (int(is) + $$CIVC),&
                &               $$CIVD + int(js),$$CIVE + int(ks))
    42|                       av = av + ((((($$PRC0 + $$PRC1) + d-v2%addr%v2(&
                &               int(is) + $$CIVC,int(js) + $$CIVD,int(ks) + &
                &               $$CIVE)) + d-v2%addr%v2(int(is) + $$CIVC,1 + (&
                &               $$CIVD + int(js)),int(ks) + $$CIVE)) + &
                &               d-v3%addr%v3(int(is) + $$CIVC,int(js) + $$CIVD,&
                &               int(ks) + $$CIVE)) + d-v3%addr%v3(int(is) + &
                &               $$CIVC,int(js) + $$CIVD,1 + ($$CIVE + int(ks)))) &
                &               *  5.0000000000000000E-001
    45|                       $$SCREP0 = $$SCREP0 + ((($$PRC0 + $$PRC1) * (&
                &               $$PRC0 + $$PRC1) + (d-v2%addr%v2(int(is) + $$CIVC,&
                &               int(js) + $$CIVD,int(ks) + $$CIVE) + d-v2%addr%v2(&
                &               int(is) + $$CIVC,1 + ($$CIVD + int(js)),int(ks) + &
                &               $$CIVE)) * (d-v2%addr%v2(int(is) + $$CIVC,int(js) &
                &               + $$CIVD,int(ks) + $$CIVE) + d-v2%addr%v2(int(is) &
                &               + $$CIVC,1 + ($$CIVD + int(js)),int(ks) + $$CIVE))&
                &               ) + (d-v3%addr%v3(int(is) + $$CIVC,int(js) + &
                &               $$CIVD,int(ks) + $$CIVE) + d-v3%addr%v3(int(is) + &
                &               $$CIVC,int(js) + $$CIVD,1 + ($$CIVE + int(ks)))) &
                &               * (d-v3%addr%v3(int(is) + $$CIVC,int(js) + $$CIVD,&
                &               int(ks) + $$CIVE) + d-v3%addr%v3(int(is) + $$CIVC,&
                &               int(js) + $$CIVD,1 + ($$CIVE + int(ks))))) *  &
                &               2.5000000000000000E-001
                              $$PRC0 = $$PRC1
    48|                     ENDDO
                            rms = $$SCREP0
                          ENDIF
    49|                 ENDDO
                      ENDIF
    50|             ENDDO
                  ENDIF
    52|           rms = _sqrt(rms / real(real((real(real(%VAL(ijkn))) ** 3))))
    53|           IF ((myid_w == 0)) THEN
    54|             #2 = _xlfBeginIO(6,257,#1,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#2),"NORMVEL  :",10,1)
                    _xlfEndIO(%VAL(#2))
    55|             #4 = _xlfBeginIO(6,257,#3,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#4),"NORMVEL  : Norm     = ",22,1)&
                &     
                    #5 = norm.rnn13
                    CALL _xlfWriteLDReal(%VAL(#4),#5,8,8)
                    _xlfEndIO(%VAL(#4))
    56|             #7 = _xlfBeginIO(6,257,#6,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#7),"NORMVEL  : V_av     = ",22,1)&
                &     
                    #8 = av / real(( 3.00000000E+00 * real((real(real(ijkn)) ** &
                &     3))))
                    CALL _xlfWriteLDReal(%VAL(#7),#8,8,8)
                    _xlfEndIO(%VAL(#7))
    57|             #10 = _xlfBeginIO(6,257,#9,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#10),"NORMVEL  : V_rms    = ",22,&
                &     1)
                    CALL _xlfWriteLDReal(%VAL(#10),rms,8,8)
                    _xlfEndIO(%VAL(#10))
    58|             #12 = _xlfBeginIO(6,257,#11,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#12),"NORMVEL  :",10,1)
                    _xlfEndIO(%VAL(#12))
    59|           ENDIF
    61|           RETURN
                END SUBROUTINE normvel


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0            17             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            19             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            21             3    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            23                  Loop was not SIMD vectorized because it contains 
                                          memory references norm = ((norm + (((double *)((char 
                                          *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks 
                                          + $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) 
                                          * (((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) 
                                          + (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) 
                                          * (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) 
                                          + (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) * (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]);  with non-vectorizable alignment.
         0            23                  Loop was not SIMD vectorized because it contains 
                                          operation in ((norm + (((double *)((char *)d-v1%addr  
                                          + d-v1%rvo))->v1[].rns4.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)]) 
                                          * (((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][1ll + ($$CIV0 + (long long) is)])) 
                                          + (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0]) 
                                          * (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIV2][1ll + 
                                          ($$CIV1 + (long long) js)][(long long) is + $$CIV0])) 
                                          + (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) * (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIV2 + (long long) 
                                          ks)][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) which is not  suitable for SIMD 
                                          vectorization.
         0            23                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*((long long) ks + $$CIV2) 
                                          + (d-v1%bounds%mult[].off488)*((long long) js + 
                                          $$CIV1) + (d-v1%bounds%mult[].off512)*((long long) is 
                                          + $$CIV0)) with  non-vectorizable strides.
         0            23                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0            23                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            31            31    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            31            32    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            31            33    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(d-v1%bounds%lbound[].off4
                                          48 + $$CIV5) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3))  with non-vectorizable alignment.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[d-v1%bounds%lbound[].off448 + 
                                          $$CIV5][d-v1%bounds%lbound[].off472 + 
                                          $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(d-v1%bounds%lbound[].off4
                                          48 + $$CIV5) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)) with  non-vectorizable strides.
         0            31                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(d-v1%bounds%lbound[].off4
                                          48 + $$CIV5) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)).
         0            31             4    Outer loop has been unrolled 4 time(s).
         0            31             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            31             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            31             6    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3))  with non-vectorizable alignment.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448][d-v1%bounds%lbound[].off4
                                          72 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)) with  non-vectorizable strides.
         0            31                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*(($$CIVF * 
                                          4ll + d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)).
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(1ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3))  with non-vectorizable alignment.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[1ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off
                                          472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(1ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)) with  non-vectorizable strides.
         0            31                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*(1ll + 
                                          (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)).
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(2ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3))  with non-vectorizable alignment.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[2ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off
                                          472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(2ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)) with  non-vectorizable strides.
         0            31                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*(2ll + 
                                          (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)).
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(3ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3))  with non-vectorizable alignment.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[3ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)][d-v1%bounds%lbound[].off
                                          472 + $$CIV4][d-v1%bounds%lbound[].off496 + $$CIV3] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*(3ll + (($$CIVF * 4ll + 
                                          d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)) with  non-vectorizable strides.
         0            31                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            31                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v1%addr  
                                          + d-v1%rvo + (d-v1%bounds%mult[].off464)*(3ll + 
                                          (($$CIVF * 4ll + d-v1%bounds%extent[].off456 % 4ll) + 
                                          d-v1%bounds%lbound[].off448)) + 
                                          (d-v1%bounds%mult[].off488)*(d-v1%bounds%lbound[].off4
                                          72 + $$CIV4) + 
                                          (d-v1%bounds%mult[].off512)*(d-v1%bounds%lbound[].off4
                                          96 + $$CIV3)).
         0            32            25    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            32            26    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            32            27    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off5
                                          52 + $$CIV8) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6))  with non-vectorizable alignment.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[d-v2%bounds%lbound[].off552 + 
                                          $$CIV8][d-v2%bounds%lbound[].off576 + 
                                          $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off5
                                          52 + $$CIV8) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)) with  non-vectorizable strides.
         0            32                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(d-v2%bounds%lbound[].off5
                                          52 + $$CIV8) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)).
         0            32             7    Outer loop has been unrolled 4 time(s).
         0            32             7    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            32             8    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            32             9    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6))  with non-vectorizable alignment.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552][d-v2%bounds%lbound[].off5
                                          76 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)) with  non-vectorizable strides.
         0            32                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(($$CIV10 * 
                                          4ll + d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)).
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(1ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6))  with non-vectorizable alignment.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[1ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off
                                          576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(1ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)) with  non-vectorizable strides.
         0            32                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(1ll + 
                                          (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) 
                                          + d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)).
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(2ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6))  with non-vectorizable alignment.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[2ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off
                                          576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(2ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)) with  non-vectorizable strides.
         0            32                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(2ll + 
                                          (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) 
                                          + d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)).
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(3ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6))  with non-vectorizable alignment.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[3ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)][d-v2%bounds%lbound[].off
                                          576 + $$CIV7][d-v2%bounds%lbound[].off600 + $$CIV6] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v2%addr  + d-v2%rvo + 
                                          (d-v2%bounds%mult[].off568)*(3ll + (($$CIV10 * 4ll + 
                                          d-v2%bounds%extent[].off560 % 4ll) + 
                                          d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)) with  non-vectorizable strides.
         0            32                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            32                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v2%addr  
                                          + d-v2%rvo + (d-v2%bounds%mult[].off568)*(3ll + 
                                          (($$CIV10 * 4ll + d-v2%bounds%extent[].off560 % 4ll) 
                                          + d-v2%bounds%lbound[].off552)) + 
                                          (d-v2%bounds%mult[].off592)*(d-v2%bounds%lbound[].off5
                                          76 + $$CIV7) + 
                                          (d-v2%bounds%mult[].off616)*(d-v2%bounds%lbound[].off6
                                          00 + $$CIV6)).
         0            33            19    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            33            20    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            33            21    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off6
                                          56 + $$CIVB) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9))  with non-vectorizable alignment.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[d-v3%bounds%lbound[].off656 + 
                                          $$CIVB][d-v3%bounds%lbound[].off680 + 
                                          $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off6
                                          56 + $$CIVB) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)) with  non-vectorizable strides.
         0            33                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(d-v3%bounds%lbound[].off6
                                          56 + $$CIVB) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)).
         0            33            10    Outer loop has been unrolled 4 time(s).
         0            33            10    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            33            11    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            33            12    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9))  with non-vectorizable alignment.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656][d-v3%bounds%lbound[].off6
                                          80 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)) with  non-vectorizable strides.
         0            33                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(($$CIV11 * 
                                          4ll + d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)).
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(1ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9))  with non-vectorizable alignment.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off
                                          680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(1ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)) with  non-vectorizable strides.
         0            33                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(1ll + 
                                          (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) 
                                          + d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)).
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(2ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9))  with non-vectorizable alignment.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[2ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off
                                          680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(2ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)) with  non-vectorizable strides.
         0            33                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(2ll + 
                                          (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) 
                                          + d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)).
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(3ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9))  with non-vectorizable alignment.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[3ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)][d-v3%bounds%lbound[].off
                                          680 + $$CIVA][d-v3%bounds%lbound[].off704 + $$CIV9] / 
                                          _sqrt@18(((norm *  2.5000000000000000E-001) / 
                                          (double) ((float) ((double) ((float) ijkn) EXPI  3))) 
                                          / (.rms->rms * .rms->rms)) which is not  suitable for 
                                          SIMD vectorization.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v3%addr  + d-v3%rvo + 
                                          (d-v3%bounds%mult[].off672)*(3ll + (($$CIV11 * 4ll + 
                                          d-v3%bounds%extent[].off664 % 4ll) + 
                                          d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)) with  non-vectorizable strides.
         0            33                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            33                  Loop was not SIMD vectorized because it contains 
                                          non-stride-one store  references ((char *)d-v3%addr  
                                          + d-v3%rvo + (d-v3%bounds%mult[].off672)*(3ll + 
                                          (($$CIV11 * 4ll + d-v3%bounds%extent[].off664 % 4ll) 
                                          + d-v3%bounds%lbound[].off656)) + 
                                          (d-v3%bounds%mult[].off696)*(d-v3%bounds%lbound[].off6
                                          80 + $$CIVA) + 
                                          (d-v3%bounds%mult[].off720)*(d-v3%bounds%lbound[].off7
                                          04 + $$CIV9)).
         0            36            13    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            38            14    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0            40            15    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0            42                  Loop was not SIMD vectorized because it contains 
                                          memory references av = av + (((((((double *)((char 
                                          *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks 
                                          + $$CIVE][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC] + ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          + ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) *  5.0000000000000000E-001;  with 
                                          non-vectorizable alignment.
         0            42                  Loop was not SIMD vectorized because it contains 
                                          operation in av + (((((((double *)((char *)d-v1%addr  
                                          + d-v1%rvo))->v1[].rns4.[(long long) ks + 
                                          $$CIVE][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC] + ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          + ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC]) + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) *  5.0000000000000000E-001 which is not  
                                          suitable for SIMD vectorization.
         0            42                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*((long long) ks + $$CIVE) 
                                          + (d-v1%bounds%mult[].off488)*((long long) js + 
                                          $$CIVD) + (d-v1%bounds%mult[].off512)*((long long) is 
                                          + $$CIVC)) with  non-vectorizable strides.
         0            42                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0            42                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0            45                  Loop was not SIMD vectorized because it contains 
                                          memory references $$SCREP0 = $$SCREP0 + (((((double 
                                          *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long 
                                          long) ks + $$CIVE][(long long) js + $$CIVD][(long 
                                          long) is + $$CIVC] + ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          * (((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          * (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC])) 
                                          + (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) * (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC])) *  2.5000000000000000E-001;  with 
                                          non-vectorizable alignment.
         0            45                  Loop was not SIMD vectorized because it contains 
                                          operation in $$SCREP0 + (((((double *)((char 
                                          *)d-v1%addr  + d-v1%rvo))->v1[].rns4.[(long long) ks 
                                          + $$CIVE][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC] + ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          * (((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns4.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][1ll + ($$CIVC + (long long) is)]) 
                                          + (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC]) 
                                          * (((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v2%addr  + 
                                          d-v2%rvo))->v2[].rns3.[(long long) ks + $$CIVE][1ll + 
                                          ($$CIVD + (long long) js)][(long long) is + $$CIVC])) 
                                          + (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC]) * (((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[(long long) ks + $$CIVE][(long 
                                          long) js + $$CIVD][(long long) is + $$CIVC] + 
                                          ((double *)((char *)d-v3%addr  + 
                                          d-v3%rvo))->v3[].rns2.[1ll + ($$CIVE + (long long) 
                                          ks)][(long long) js + $$CIVD][(long long) is + 
                                          $$CIVC])) *  2.5000000000000000E-001 which is not  
                                          suitable for SIMD vectorization.
         0            45                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*((long long) ks + $$CIVE) 
                                          + (d-v1%bounds%mult[].off488)*((long long) js + 
                                          $$CIVD) + (d-v1%bounds%mult[].off512)*((long long) is 
                                          + $$CIVC)) with  non-vectorizable strides.
         0            45                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0            45                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.


     1|         SUBROUTINE normvel (rms)
    16|           norm =  0.0000000000000000E+000
    17|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIV2 = 0
       Id=1         DO $$CIV2 = $$CIV2, int((1 + (int(ke) - int(ks))))-1
    19|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIV1 = 0
       Id=2             DO $$CIV1 = $$CIV1, int((1 + (int(je) - int(js))))-1
    21|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIV0 = 0
                            $$PRC2 = d-v1%addr%v1(int(is),$$CIV1 + int(js),&
                &             $$CIV2 + int(ks))
                            $$ICM0 = $$CIV1 + int(js)
                            $$ICM1 = $$CIV2 + int(ks)
    23|                     $$ICM2 = 1 + ($$CIV2 + int(ks))
                            $$ICM3 = 1 + ($$CIV1 + int(js))
    21|Id=3                 DO $$CIV0 = $$CIV0, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC3 = d-v1%addr%v1(1 + (int(is) + $$CIV0),&
                &               $$ICM0,$$ICM1)
    23|                       norm = ((norm + ($$PRC2 + $$PRC3) * ($$PRC2 + &
                &               $$PRC3)) + (d-v2%addr%v2(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-v2%addr%v2(int(is) + $$CIV0,$$ICM3,&
                &               $$ICM1)) * (d-v2%addr%v2(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-v2%addr%v2(int(is) + $$CIV0,$$ICM3,&
                &               $$ICM1))) + (d-v3%addr%v3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-v3%addr%v3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM2)) * (d-v3%addr%v3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM1) + d-v3%addr%v3(int(is) + $$CIV0,$$ICM0,&
                &               $$ICM2))
                              $$PRC2 = $$PRC3
    27|                     ENDDO
                          ENDIF
    28|                 ENDDO
                      ENDIF
    29|             ENDDO
                  ENDIF
    30|           $$csx0 = _sqrt(((%VAL(norm) *  2.5000000000000000E-001) / &
                &   real(real((real(real(%VAL(ijkn))) ** 3)))) / (rms * rms))
                  norm.rnn13 = $$csx0
    31|           IF ((MOD(d-v1%bounds%extent[].off456, 4) > 0  .AND.  &
                &   d-v1%bounds%extent[].off456 > 0)) THEN
                    $$CIV5 = 0
       Id=31        DO $$CIV5 = $$CIV5, MOD(d-v1%bounds%extent[].off456, int(&
                &       4))-1
                      IF ((d-v1%bounds%extent[].off480 > 0)) THEN
                        $$CIV4 = 0
       Id=32            DO $$CIV4 = $$CIV4, int(d-v1%bounds%extent[].off480)&
                &           -1
                          IF ((d-v1%bounds%extent[].off504 > 0)) THEN
                            $$CIV3 = 0
       Id=33                DO $$CIV3 = $$CIV3, int(&
                &               d-v1%bounds%extent[].off504)-1
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,&
                &               d-v1%bounds%lbound[].off448 + $$CIV5) = &
                &               d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,&
                &               d-v1%bounds%lbound[].off448 + $$CIV5) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-v1%bounds%extent[].off456 > 0  .AND.  &
                &   d-v1%bounds%extent[].off456 > MOD(d-v1%bounds%extent[].off456,&
                &    4))) THEN
                    $$CIVF = int(0)
       Id=4         DO $$CIVF = $$CIVF, int((((d-v1%bounds%extent[].off456 - &
                &       MOD(d-v1%bounds%extent[].off456, 4)) - 1) / 4 + 1))-1
                      IF ((d-v1%bounds%extent[].off480 > 0)) THEN
                        $$CIV4 = 0
       Id=5             DO $$CIV4 = $$CIV4, int(d-v1%bounds%extent[].off480)&
                &           -1
                          IF ((d-v1%bounds%extent[].off504 > 0)) THEN
                            $$CIV3 = 0
       Id=6                 DO $$CIV3 = $$CIV3, int(&
                &               d-v1%bounds%extent[].off504)-1
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,($$CIVF * 4 &
                &               + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448) / $$csx0
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,1 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) / $$csx0
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,2 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) / $$csx0
                              d-v1%addr%v1(d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) = d-v1%addr%v1(&
                &               d-v1%bounds%lbound[].off496 + $$CIV3,&
                &               d-v1%bounds%lbound[].off472 + $$CIV4,3 + (($$CIVF &
                &               * 4 + MOD(d-v1%bounds%extent[].off456, 4)) + &
                &               d-v1%bounds%lbound[].off448)) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    32|           IF ((MOD(d-v2%bounds%extent[].off560, 4) > 0  .AND.  &
                &   d-v2%bounds%extent[].off560 > 0)) THEN
                    $$CIV8 = 0
       Id=25        DO $$CIV8 = $$CIV8, MOD(d-v2%bounds%extent[].off560, int(&
                &       4))-1
                      IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                        $$CIV7 = 0
       Id=26            DO $$CIV7 = $$CIV7, int(d-v2%bounds%extent[].off584)&
                &           -1
                          IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                            $$CIV6 = 0
       Id=27                DO $$CIV6 = $$CIV6, int(&
                &               d-v2%bounds%extent[].off608)-1
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,&
                &               d-v2%bounds%lbound[].off552 + $$CIV8) = &
                &               d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,&
                &               d-v2%bounds%lbound[].off552 + $$CIV8) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-v2%bounds%extent[].off560 > 0  .AND.  &
                &   d-v2%bounds%extent[].off560 > MOD(d-v2%bounds%extent[].off560,&
                &    4))) THEN
                    $$CIV10 = int(0)
       Id=7         DO $$CIV10 = $$CIV10, int((((d-v2%bounds%extent[].off560 &
                &       - MOD(d-v2%bounds%extent[].off560, 4)) - 1) / 4 + 1))-1
                      IF ((d-v2%bounds%extent[].off584 > 0)) THEN
                        $$CIV7 = 0
       Id=8             DO $$CIV7 = $$CIV7, int(d-v2%bounds%extent[].off584)&
                &           -1
                          IF ((d-v2%bounds%extent[].off608 > 0)) THEN
                            $$CIV6 = 0
       Id=9                 DO $$CIV6 = $$CIV6, int(&
                &               d-v2%bounds%extent[].off608)-1
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-v2%bounds%extent[].off560, 4)) + &
                &               d-v2%bounds%lbound[].off552) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,($$CIV10 * 4 &
                &               + MOD(d-v2%bounds%extent[].off560, 4)) + &
                &               d-v2%bounds%lbound[].off552) / $$csx0
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,1 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) / $$csx0
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,2 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) / $$csx0
                              d-v2%addr%v2(d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) = d-v2%addr%v2(&
                &               d-v2%bounds%lbound[].off600 + $$CIV6,&
                &               d-v2%bounds%lbound[].off576 + $$CIV7,3 + ((&
                &               $$CIV10 * 4 + MOD(d-v2%bounds%extent[].off560, 4))&
                &                + d-v2%bounds%lbound[].off552)) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    33|           IF ((MOD(d-v3%bounds%extent[].off664, 4) > 0  .AND.  &
                &   d-v3%bounds%extent[].off664 > 0)) THEN
                    $$CIVB = 0
       Id=19        DO $$CIVB = $$CIVB, MOD(d-v3%bounds%extent[].off664, int(&
                &       4))-1
                      IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                        $$CIVA = 0
       Id=20            DO $$CIVA = $$CIVA, int(d-v3%bounds%extent[].off688)&
                &           -1
                          IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                            $$CIV9 = 0
       Id=21                DO $$CIV9 = $$CIV9, int(&
                &               d-v3%bounds%extent[].off712)-1
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,&
                &               d-v3%bounds%lbound[].off656 + $$CIVB) = &
                &               d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,&
                &               d-v3%bounds%lbound[].off656 + $$CIVB) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
                  IF ((d-v3%bounds%extent[].off664 > 0  .AND.  &
                &   d-v3%bounds%extent[].off664 > MOD(d-v3%bounds%extent[].off664,&
                &    4))) THEN
                    $$CIV11 = int(0)
       Id=10        DO $$CIV11 = $$CIV11, int((((d-v3%bounds%extent[].off664 &
                &       - MOD(d-v3%bounds%extent[].off664, 4)) - 1) / 4 + 1))-1
                      IF ((d-v3%bounds%extent[].off688 > 0)) THEN
                        $$CIVA = 0
       Id=11            DO $$CIVA = $$CIVA, int(d-v3%bounds%extent[].off688)&
                &           -1
                          IF ((d-v3%bounds%extent[].off712 > 0)) THEN
                            $$CIV9 = 0
       Id=12                DO $$CIV9 = $$CIV9, int(&
                &               d-v3%bounds%extent[].off712)-1
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,($$CIV11 * 4 &
                &               + MOD(d-v3%bounds%extent[].off664, 4)) + &
                &               d-v3%bounds%lbound[].off656) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,($$CIV11 * 4 &
                &               + MOD(d-v3%bounds%extent[].off664, 4)) + &
                &               d-v3%bounds%lbound[].off656) / $$csx0
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,1 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) / $$csx0
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,2 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) / $$csx0
                              d-v3%addr%v3(d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) = d-v3%addr%v3(&
                &               d-v3%bounds%lbound[].off704 + $$CIV9,&
                &               d-v3%bounds%lbound[].off680 + $$CIVA,3 + ((&
                &               $$CIV11 * 4 + MOD(d-v3%bounds%extent[].off664, 4))&
                &                + d-v3%bounds%lbound[].off656)) / $$csx0
                            ENDDO
                          ENDIF
                        ENDDO
                      ENDIF
                    ENDDO
                  ENDIF
    34|           av =  0.0000000000000000E+000
    35|           rms =  0.0000000000000000E+000
    36|           IF ((1 + (int(ke) - int(ks)) > 0)) THEN
                    $$CIVE = 0
       Id=13        DO $$CIVE = $$CIVE, int((1 + (int(ke) - int(ks))))-1
    38|               IF ((1 + (int(je) - int(js)) > 0)) THEN
                        $$CIVD = 0
       Id=14            DO $$CIVD = $$CIVD, int((1 + (int(je) - int(js))))-1
    40|                   IF ((1 + (int(ie) - int(is)) > 0)) THEN
                            $$CIVC = 0
                            $$SCREP0 = rms
                            $$PRC0 = d-v1%addr%v1(int(is),$$CIVD + int(js),&
                &             $$CIVE + int(ks))
                            $$ICM4 = $$CIVD + int(js)
                            $$ICM5 = $$CIVE + int(ks)
    42|                     $$ICM6 = 1 + ($$CIVE + int(ks))
                            $$ICM7 = 1 + ($$CIVD + int(js))
    40|Id=15                DO $$CIVC = $$CIVC, int((1 + (int(ie) - int(is))))&
                &               -1
                              $$PRC1 = d-v1%addr%v1(1 + (int(is) + $$CIVC),&
                &               $$ICM4,$$ICM5)
    42|                       av = av + ((((($$PRC0 + $$PRC1) + d-v2%addr%v2(&
                &               int(is) + $$CIVC,$$ICM4,$$ICM5)) + d-v2%addr%v2(&
                &               int(is) + $$CIVC,$$ICM7,$$ICM5)) + d-v3%addr%v3(&
                &               int(is) + $$CIVC,$$ICM4,$$ICM5)) + d-v3%addr%v3(&
                &               int(is) + $$CIVC,$$ICM4,$$ICM6)) *  &
                &               5.0000000000000000E-001
    45|                       $$SCREP0 = $$SCREP0 + ((($$PRC0 + $$PRC1) * (&
                &               $$PRC0 + $$PRC1) + (d-v2%addr%v2(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-v2%addr%v2(int(is) + $$CIVC,&
                &               $$ICM7,$$ICM5)) * (d-v2%addr%v2(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-v2%addr%v2(int(is) + $$CIVC,&
                &               $$ICM7,$$ICM5))) + (d-v3%addr%v3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-v3%addr%v3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM6)) * (d-v3%addr%v3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM5) + d-v3%addr%v3(int(is) + $$CIVC,&
                &               $$ICM4,$$ICM6))) *  2.5000000000000000E-001
                              $$PRC0 = $$PRC1
    48|                     ENDDO
                            rms = $$SCREP0
                          ENDIF
    49|                 ENDDO
                      ENDIF
    50|             ENDDO
                  ENDIF
    52|           rms = _sqrt(rms / real(real((real(real(%VAL(ijkn))) ** 3))))
    53|           IF ((myid_w == 0)) THEN
    54|             #2 = _xlfBeginIO(6,257,#1,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#2),"NORMVEL  :",10,1)
                    _xlfEndIO(%VAL(#2))
    55|             #4 = _xlfBeginIO(6,257,#3,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#4),"NORMVEL  : Norm     = ",22,1)&
                &     
                    #5 = norm.rnn13
                    CALL _xlfWriteLDReal(%VAL(#4),#5,8,8)
                    _xlfEndIO(%VAL(#4))
    56|             #7 = _xlfBeginIO(6,257,#6,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#7),"NORMVEL  : V_av     = ",22,1)&
                &     
                    #8 = av / real(( 3.00000000E+00 * real((real(real(ijkn)) ** &
                &     3))))
                    CALL _xlfWriteLDReal(%VAL(#7),#8,8,8)
                    _xlfEndIO(%VAL(#7))
    57|             #10 = _xlfBeginIO(6,257,#9,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#10),"NORMVEL  : V_rms    = ",22,&
                &     1)
                    CALL _xlfWriteLDReal(%VAL(#10),rms,8,8)
                    _xlfEndIO(%VAL(#10))
    58|             #12 = _xlfBeginIO(6,257,#11,32768,NULL,0,NULL)
                    CALL _xlfWriteLDChar(%VAL(#12),"NORMVEL  :",10,1)
                    _xlfEndIO(%VAL(#12))
    59|           ENDIF
    61|           RETURN
                END SUBROUTINE normvel

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ss--  ---- ---- -sss ssss
 CCR's set/used:   sss- ssss
     | 000000                           PDEF     normvel
    1|                                  PROC      .rms,gr3
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 stfd     DB61FFD8   1     STFL      #stack(gr1,-40)=fp27
    0| 000014 stfd     DB41FFD0   1     STFL      #stack(gr1,-48)=fp26
    0| 000018 stfd     DB21FFC8   1     STFL      #stack(gr1,-56)=fp25
    0| 00001C std      FBE1FFC0   1     ST8       #stack(gr1,-64)=gr31
    0| 000020 std      FBC1FFB8   1     ST8       #stack(gr1,-72)=gr30
    0| 000024 std      FBA1FFB0   1     ST8       #stack(gr1,-80)=gr29
    0| 000028 std      FB81FFA8   1     ST8       #stack(gr1,-88)=gr28
    0| 00002C std      FB61FFA0   1     ST8       #stack(gr1,-96)=gr27
    0| 000030 std      FB41FF98   1     ST8       #stack(gr1,-104)=gr26
    0| 000034 std      FB21FF90   1     ST8       #stack(gr1,-112)=gr25
    0| 000038 std      FB01FF88   1     ST8       #stack(gr1,-120)=gr24
    0| 00003C std      FAE1FF80   1     ST8       #stack(gr1,-128)=gr23
    0| 000040 std      FAC1FF78   1     ST8       #stack(gr1,-136)=gr22
    0| 000044 std      FAA1FF70   1     ST8       #stack(gr1,-144)=gr21
    0| 000048 std      FA81FF68   1     ST8       #stack(gr1,-152)=gr20
    0| 00004C std      FA61FF60   1     ST8       #stack(gr1,-160)=gr19
    0| 000050 std      FA41FF58   1     ST8       #stack(gr1,-168)=gr18
    0| 000054 std      FA21FF50   1     ST8       #stack(gr1,-176)=gr17
    0| 000058 std      FA01FF48   1     ST8       #stack(gr1,-184)=gr16
    0| 00005C std      F9E1FF40   1     ST8       #stack(gr1,-192)=gr15
    0| 000060 std      F9C1FF38   1     ST8       #stack(gr1,-200)=gr14
    0| 000064 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 000068 mfcr     7D800026   1     LFCR      gr12=cr[24],2
    0| 00006C stw      91810008   1     ST4A      #stack(gr1,8)=gr12
    0| 000070 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000074 stdu     F821FD41   1     ST8U      gr1,#stack(gr1,-704)=gr1
    0| 000078 std      F86100A8   1     ST8       #SPILL0(gr1,168)=gr3
   21| 00007C ld       E8E20000   1     L8        gr7=.&&N&field(gr2,0)
   17| 000080 ld       E9220000   1     L8        gr9=.&&N&&grid(gr2,0)
   30| 000084 ld       E9020000   1     L8        gr8=.&&N&&param(gr2,0)
   16| 000088 ld       EBA20000   1     L8        gr29=.+CONSTANT_AREA(gr2,0)
   33| 00008C ld       EB870298   1     L8        gr28=<s17:d664:l8>(gr7,664)
   32| 000090 ld       EB670230   1     L8        gr27=<s17:d560:l8>(gr7,560)
   31| 000094 ld       EB4701C8   1     L8        gr26=<s17:d456:l8>(gr7,456)
   17| 000098 lwa      EB290012   1     L4A       gr25=<s46:d16:l4>(gr9,16)
   17| 00009C lwa      E8A90016   1     L4A       gr5=<s46:d20:l4>(gr9,20)
   19| 0000A0 lwa      EA89000E   1     L4A       gr20=<s46:d12:l4>(gr9,12)
   33| 0000A4 std      FB8100B0   1     ST8       #SPILL1(gr1,176)=gr28
   32| 0000A8 std      FB6100B8   1     ST8       #SPILL2(gr1,184)=gr27
   31| 0000AC std      FB4100C0   1     ST8       #SPILL3(gr1,192)=gr26
   33| 0000B0 sradi    7F801674   1     SRA8CA    gr0,ca=gr28,2
   17| 0000B4 std      FB2100C8   1     ST8       #SPILL4(gr1,200)=gr25
   21| 0000B8 ld       EA6701B8   1     L8        gr19=<s17:d440:l8>(gr7,440)
   23| 0000BC ld       E9C70208   1     L8        gr14=<s17:d520:l8>(gr7,520)
   19| 0000C0 std      FA8100E0   1     ST8       #SPILL7(gr1,224)=gr20
   33| 0000C4 addze    7FE00194   1     ADDE      gr31,ca=gr0,0,ca
   33| 0000C8 sradi    7F641674   1     SRA8CA    gr4,ca=gr27,2
   17| 0000CC subf     7D592850   1     S         gr10=gr5,gr25
   21| 0000D0 std      FA6100E8   1     ST8       #SPILL8(gr1,232)=gr19
   23| 0000D4 ld       EAE702A0   1     L8        gr23=<s17:d672:l8>(gr7,672)
   23| 0000D8 std      F9C10110   1     ST8       #SPILL13(gr1,272)=gr14
   33| 0000DC addze    7D840194   1     ADDE      gr12,ca=gr4,0,ca
   32| 0000E0 sradi    7F461674   1     SRA8CA    gr6,ca=gr26,2
   33| 0000E4 rldicr   7BF21764   1     SLL8      gr18=gr31,2
   32| 0000E8 addze    7D660194   1     ADDE      gr11,ca=gr6,0,ca
   33| 0000EC std      FA4100F0   1     ST8       #SPILL9(gr1,240)=gr18
   23| 0000F0 std      FAE10148   1     ST8       #SPILL20(gr1,328)=gr23
   31| 0000F4 rldicr   79701764   1     SLL8      gr16=gr11,2
   23| 0000F8 ld       E9670288   1     L8        gr11=<s17:d648:l8>(gr7,648)
   31| 0000FC std      FA010100   1     ST8       #SPILL11(gr1,256)=gr16
   17| 000100 addic.   35EA0001   1     AI_R      gr15,cr0=gr10,1,ca"
   30| 000104 lwa      E948000E   1     L4A       gr10=<s57:d12:l4>(gr8,12)
   17| 000108 std      F9E10108   1     ST8       #SPILL12(gr1,264)=gr15
   21| 00010C lwa      EB090002   1     L4A       gr24=<s46:d0:l4>(gr9,0)
   23| 000110 std      F9610130   1     ST8       #SPILL17(gr1,304)=gr11
   33| 000114 ld       E96100B0   1     L8        gr11=#SPILL1(gr1,176)
   19| 000118 lwa      E809000A   1     L4A       gr0=<s46:d8:l4>(gr9,8)
   30| 00011C std      F9410138   1     ST8       #SPILL18(gr1,312)=gr10
   21| 000120 ld       EAA701A0   1     L8        gr21=<s17:d416:l8>(gr7,416)
   32| 000124 rldicr   79911764   1     SLL8      gr17=gr12,2
   23| 000128 ld       EBE70270   1     L8        gr31=<s17:d624:l8>(gr7,624)
   21| 00012C std      FB0100D0   1     ST8       #SPILL5(gr1,208)=gr24
   33| 000130 subf     7D525850   1     S         gr10=gr11,gr18
   32| 000134 ld       E96100B8   1     L8        gr11=#SPILL2(gr1,184)
   21| 000138 std      FAA100D8   1     ST8       #SPILL6(gr1,216)=gr21
   33| 00013C std      F9410198   1     ST8       #SPILL30(gr1,408)=gr10
   21| 000140 lwa      E9890006   1     L4A       gr12=<s46:d4:l4>(gr9,4)
   23| 000144 ld       E9270220   1     L8        gr9=<s17:d544:l8>(gr7,544)
   23| 000148 std      FBE10118   1     ST8       #SPILL14(gr1,280)=gr31
   32| 00014C subf     7E515850   1     S         gr18=gr11,gr17
   31| 000150 ld       E96100C0   1     L8        gr11=#SPILL3(gr1,192)
   32| 000154 std      FA4101A0   1     ST8       #SPILL31(gr1,416)=gr18
   16| 000158 lfs      C3FD000C   1     LFS       fp31=+CONSTANT_AREA(gr29,12)
   21| 00015C std      F9810120   1     ST8       #SPILL15(gr1,288)=gr12
   21| 000160 ld       E8670200   1     L8        gr3=<s17:d512:l8>(gr7,512)
   21| 000164 ld       EBC701D0   1     L8        gr30=<s17:d464:l8>(gr7,464)
   23| 000168 std      F9210128   1     ST8       #SPILL16(gr1,296)=gr9
   21| 00016C ld       E88701E8   1     L8        gr4=<s17:d488:l8>(gr7,488)
   23| 000170 ld       E8A70268   1     L8        gr5=<s17:d616:l8>(gr7,616)
   16| 000174 fmr      FC40F890   1     LRFL      fp2=fp31
   31| 000178 ld       EBA701F8   1     L8        gr29=<s17:d504:l8>(gr7,504)
   31| 00017C ld       EB8701E0   1     L8        gr28=<s17:d480:l8>(gr7,480)
   32| 000180 std      FA2100F8   1     ST8       #SPILL10(gr1,248)=gr17
   23| 000184 ld       E8C702D0   1     L8        gr6=<s17:d720:l8>(gr7,720)
   23| 000188 ld       E9070238   1     L8        gr8=<s17:d568:l8>(gr7,568)
   23| 00018C ld       EB670250   1     L8        gr27=<s17:d592:l8>(gr7,592)
   32| 000190 ld       EB470260   1     L8        gr26=<s17:d608:l8>(gr7,608)
   32| 000194 ld       EB270248   1     L8        gr25=<s17:d584:l8>(gr7,584)
   23| 000198 ld       EB0702B8   1     L8        gr24=<s17:d696:l8>(gr7,696)
   33| 00019C ld       EAE702C8   1     L8        gr23=<s17:d712:l8>(gr7,712)
   23| 0001A0 std      F9010140   1     ST8       #SPILL19(gr1,320)=gr8
   33| 0001A4 ld       EAC702B0   1     L8        gr22=<s17:d688:l8>(gr7,688)
   31| 0001A8 ld       EAA701F0   1     L8        gr21=<s17:d496:l8>(gr7,496)
   31| 0001AC ld       EA8701D8   1     L8        gr20=<s17:d472:l8>(gr7,472)
   31| 0001B0 ld       EA6701C0   1     L8        gr19=<s17:d448:l8>(gr7,448)
   32| 0001B4 ld       E9E70258   1     L8        gr15=<s17:d600:l8>(gr7,600)
   32| 0001B8 ld       E9C70240   1     L8        gr14=<s17:d576:l8>(gr7,576)
   32| 0001BC ld       EBE70228   1     L8        gr31=<s17:d552:l8>(gr7,552)
   31| 0001C0 std      FAA10150   1     ST8       #SPILL21(gr1,336)=gr21
   31| 0001C4 std      FA810158   1     ST8       #SPILL22(gr1,344)=gr20
   31| 0001C8 std      FA610160   1     ST8       #SPILL23(gr1,352)=gr19
   32| 0001CC std      F9E10168   1     ST8       #SPILL24(gr1,360)=gr15
   32| 0001D0 std      F9C10170   1     ST8       #SPILL25(gr1,368)=gr14
   32| 0001D4 std      FBE10178   1     ST8       #SPILL26(gr1,376)=gr31
   33| 0001D8 ld       E98702C0   1     L8        gr12=<s17:d704:l8>(gr7,704)
   33| 0001DC ld       E92702A8   1     L8        gr9=<s17:d680:l8>(gr7,680)
   33| 0001E0 ld       E8E70290   1     L8        gr7=<s17:d656:l8>(gr7,656)
   31| 0001E4 subf     7E305850   1     S         gr17=gr11,gr16
   17| 0001E8 mcrf     4C800000   1     LRCR      cr1=cr0
   31| 0001EC std      FA2101A8   1     ST8       #SPILL32(gr1,424)=gr17
   33| 0001F0 std      F9810180   1     ST8       #SPILL27(gr1,384)=gr12
   33| 0001F4 std      F9210188   1     ST8       #SPILL28(gr1,392)=gr9
   33| 0001F8 std      F8E10190   1     ST8       #SPILL29(gr1,400)=gr7
   17| 0001FC bc       40810258   1     BF        CL.62,cr0,0x2/gt,taken=50%(0,0)
   19| 000200 ld       E90100E0   1     L8        gr8=#SPILL7(gr1,224)
   17| 000204 addi     3AA00000   1     LI        gr21=0
   19| 000208 subf     7CE04050   1     S         gr7=gr8,gr0
   17| 00020C std      FAA101B0   1     ST8       #SPILL33(gr1,432)=gr21
   19| 000210 addic.   36A70001   1     AI_R      gr21,cr0=gr7,1,ca"
    0| 000214 bc       40810240   1     BF        CL.62,cr0,0x2/gt,taken=20%(20,80)
    0| 000218 ld       E96100C8   1     L8        gr11=#SPILL4(gr1,200)
    0| 00021C ld       E8E100D0   1     L8        gr7=#SPILL5(gr1,208)
    0| 000220 mulld    7D2021D2   1     M         gr9=gr0,gr4
    0| 000224 mulld    7D4BF1D2   1     M         gr10=gr11,gr30
    0| 000228 ld       EA6100D8   1     L8        gr19=#SPILL6(gr1,216)
    0| 00022C ld       EA4100E8   1     L8        gr18=#SPILL8(gr1,232)
    0| 000230 add      7D895214   1     A         gr12=gr9,gr10
    0| 000234 mulld    7D0339D2   1     M         gr8=gr3,gr7
    0| 000238 add      7D329A14   1     A         gr9=gr18,gr19
   23| 00023C ld       EA4100D0   1     L8        gr18=#SPILL5(gr1,208)
    0| 000240 add      7E684A14   1     A         gr19=gr8,gr9
   23| 000244 ld       E9010110   1     L8        gr8=#SPILL13(gr1,272)
    0| 000248 ld       EA810120   1     L8        gr20=#SPILL15(gr1,288)
    0| 00024C subfic   20E70001   1     SFI       gr7=1,gr7,ca"
   23| 000250 mulld    7D2591D2   1     M         gr9=gr5,gr18
   23| 000254 mulld    7D6691D2   1     M         gr11=gr6,gr18
   23| 000258 ld       EA410118   1     L8        gr18=#SPILL14(gr1,280)
   23| 00025C subf     7D454050   1     S         gr10=gr8,gr5
    0| 000260 add      7CE7A214   1     A         gr7=gr7,gr20
    0| 000264 rldicl   78F4F842   1     SRL8      gr20=gr7,1
   23| 000268 subf     7D069050   1     S         gr8=gr18,gr6
   23| 00026C ld       EA410128   1     L8        gr18=#SPILL16(gr1,296)
    0| 000270 cmpdi    2F340000   1     C8        cr6=gr20,0
   23| 000274 add      7D4A9214   1     A         gr10=gr10,gr18
   23| 000278 ld       EA410130   1     L8        gr18=#SPILL17(gr1,304)
   23| 00027C add      7D295214   1     A         gr9=gr9,gr10
    0| 000280 ld       E94100D0   1     L8        gr10=#SPILL5(gr1,208)
   23| 000284 std      F92101B8   1     ST8       #SPILL34(gr1,440)=gr9
   23| 000288 add      7D089214   1     A         gr8=gr8,gr18
   23| 00028C add      7E485A14   1     A         gr18=gr8,gr11
    0| 000290 ld       E9610120   1     L8        gr11=#SPILL15(gr1,288)
    0| 000294 subf     7D049850   1     S         gr8=gr19,gr4
    0| 000298 subf     7D2A5850   1     S         gr9=gr11,gr10
    0| 00029C addic.   35290001   1     AI_R      gr9,cr0=gr9,1,ca"
    0| 0002A0 add      7D286214   1     A         gr9=gr8,gr12
    0| 0002A4 mcrf     4F800000   1     LRCR      cr7=cr0
    0| 0002A8 std      F92101C0   1     ST8       #SPILL35(gr1,448)=gr9
    0| 0002AC andi.    70E70001   1     RN4_R     gr7,cr0=gr7,0,0x1
   17|                              CL.63:
    0| 0002B0 ld       E90100C8   1     L8        gr8=#SPILL4(gr1,200)
    0| 0002B4 ld       E94101B0   1     L8        gr10=#SPILL33(gr1,432)
   19| 0002B8 addi     38E00000   1     LI        gr7=0
    0| 0002BC add      7D285214   1     A         gr9=gr8,gr10
    0| 0002C0 bc       409D0170   1     BF        CL.64,cr7,0x2/gt,taken=20%(20,80)
    0| 0002C4 ld       E9810140   1     L8        gr12=#SPILL19(gr1,320)
    0| 0002C8 ld       EBE10148   1     L8        gr31=#SPILL20(gr1,328)
    0| 0002CC addi     39690001   1     AI        gr11=gr9,1
    0| 0002D0 mulld    7E29F1D2   1     M         gr17=gr9,gr30
    0| 0002D4 mulld    7D4961D2   1     M         gr10=gr9,gr12
    0| 0002D8 mulld    7DE9F9D2   1     M         gr15=gr9,gr31
    0| 0002DC ld       E92101B8   1     L8        gr9=#SPILL34(gr1,440)
   21| 0002E0 ld       E90101C0   1     L8        gr8=#SPILL35(gr1,448)
    0| 0002E4 mulld    7E0BF9D2   1     M         gr16=gr11,gr31
    0| 0002E8 add      7DC95214   1     A         gr14=gr9,gr10
    0| 0002EC ori      60210000   1     XNOP      
    0| 0002F0 ori      60210000   1     XNOP      
    0| 0002F4 ori      60210000   1     XNOP      
   19|                              CL.65:
   21| 0002F8 add      7D203A14   1     A         gr9=gr0,gr7
   21| 0002FC lfdux    7C0824EE   1     LFDU      fp0,gr8=v1(gr8,gr4,0)
   23| 000300 addi     39490001   1     AI        gr10=gr9,1
   21| 000304 mulld    7D8449D2   1     M         gr12=gr4,gr9
   23| 000308 mulld    7FE9C1D2   1     M         gr31=gr9,gr24
   23| 00030C mulld    7D4AD9D2   1     M         gr10=gr10,gr27
   23| 000310 mulld    7D69D9D2   1     M         gr11=gr9,gr27
   21| 000314 add      7D2C9A14   1     A         gr9=gr12,gr19
   23| 000318 add      7FFF9214   1     A         gr31=gr31,gr18
   21| 00031C add      7D298A14   1     A         gr9=gr9,gr17
   23| 000320 add      7D4A7214   1     A         gr10=gr10,gr14
   23| 000324 add      7D6B7214   1     A         gr11=gr11,gr14
   23| 000328 add      7D90FA14   1     A         gr12=gr16,gr31
   23| 00032C add      7FFF7A14   1     A         gr31=gr31,gr15
    0| 000330 mtspr    7E8903A6   1     LCTR      ctr=gr20
    0| 000334 bc       41820038   1     BT        CL.623,cr0,0x4/eq,taken=50%(0,0)
   21| 000338 lfdux    7C291CEE   1     LFDU      fp1,gr9=v1(gr9,gr3,0)
   23| 00033C lfdux    7C6B2CEE   1     LFDU      fp3,gr11=v2(gr11,gr5,0)
   23| 000340 lfdux    7C8A2CEE   1     LFDU      fp4,gr10=v2(gr10,gr5,0)
   23| 000344 lfdux    7CBF34EE   1     LFDU      fp5,gr31=v3(gr31,gr6,0)
   23| 000348 lfdux    7CCC34EE   1     LFDU      fp6,gr12=v3(gr12,gr6,0)
   23| 00034C fadd     FCE0082A   1     AFL       fp7=fp0,fp1,fcr
   23| 000350 fmr      FC000890   2     LRFL      fp0=fp1
   23| 000354 fadd     FC23202A   2     AFL       fp1=fp3,fp4,fcr
   23| 000358 fadd     FC65302A   2     AFL       fp3=fp5,fp6,fcr
   23| 00035C fmadd    FC4711FA   2     FMA       fp2=fp2,fp7,fp7,fcr
   23| 000360 fmadd    FC21107A   2     FMA       fp1=fp2,fp1,fp1,fcr
   23| 000364 fmadd    FC4308FA   2     FMA       fp2=fp1,fp3,fp3,fcr
    0| 000368 bc       419A00BC   1     BT        CL.524,cr6,0x4/eq,taken=20%(20,80)
    0|                              CL.623:
   21| 00036C lfdux    7D091CEE   1     LFDU      fp8,gr9=v1(gr9,gr3,0)
   23| 000370 lfdux    7C6B2CEE   1     LFDU      fp3,gr11=v2(gr11,gr5,0)
   23| 000374 lfdux    7C8A2CEE   1     LFDU      fp4,gr10=v2(gr10,gr5,0)
   23| 000378 lfdux    7C3F34EE   1     LFDU      fp1,gr31=v3(gr31,gr6,0)
   23| 00037C lfdux    7CAC34EE   1     LFDU      fp5,gr12=v3(gr12,gr6,0)
   23| 000380 fadd     FCC0402A   1     AFL       fp6=fp0,fp8,fcr
   21| 000384 lfdux    7C091CEE   1     LFDU      fp0,gr9=v1(gr9,gr3,0)
   23| 000388 fadd     FC63202A   1     AFL       fp3=fp3,fp4,fcr
   23| 00038C lfdux    7C8B2CEE   1     LFDU      fp4,gr11=v2(gr11,gr5,0)
   23| 000390 fadd     FC21282A   1     AFL       fp1=fp1,fp5,fcr
   23| 000394 lfdux    7CAA2CEE   1     LFDU      fp5,gr10=v2(gr10,gr5,0)
   23| 000398 fmadd    FCE611BA   1     FMA       fp7=fp2,fp6,fp6,fcr
   23| 00039C lfdux    7CDF34EE   1     LFDU      fp6,gr31=v3(gr31,gr6,0)
   23| 0003A0 fadd     FC48002A   1     AFL       fp2=fp8,fp0,fcr
   23| 0003A4 lfdux    7D0C34EE   1     LFDU      fp8,gr12=v3(gr12,gr6,0)
   23| 0003A8 fmadd    FC6338FA   1     FMA       fp3=fp7,fp3,fp3,fcr
   23| 0003AC fadd     FCE4282A   2     AFL       fp7=fp4,fp5,fcr
   23| 0003B0 fadd     FCC6402A   2     AFL       fp6=fp6,fp8,fcr
   23| 0003B4 fmadd    FC21187A   2     FMA       fp1=fp3,fp1,fp1,fcr
   23| 0003B8 fmadd    FC4208BA   2     FMA       fp2=fp1,fp2,fp2,fcr
    0| 0003BC bc       42400060   1     BCF       ctr=CL.655,taken=0%(0,100)
    0|                              CL.656:
   21| 0003C0 lfdux    7C291CEE   1     LFDU      fp1,gr9=v1(gr9,gr3,0)
   23| 0003C4 fmadd    FCE711FA   1     FMA       fp7=fp2,fp7,fp7,fcr
   23| 0003C8 lfdux    7C4B2CEE   1     LFDU      fp2,gr11=v2(gr11,gr5,0)
   23| 0003CC lfdux    7C6A2CEE   1     LFDU      fp3,gr10=v2(gr10,gr5,0)
   23| 0003D0 lfdux    7C9F34EE   1     LFDU      fp4,gr31=v3(gr31,gr6,0)
   23| 0003D4 fadd     FCA0082A   1     AFL       fp5=fp0,fp1,fcr
   23| 0003D8 fmadd    FCC639BA   2     FMA       fp6=fp7,fp6,fp6,fcr
   23| 0003DC lfdux    7CEC34EE   1     LFDU      fp7,gr12=v3(gr12,gr6,0)
   23| 0003E0 lfdux    7D0B2CEE   1     LFDU      fp8,gr11=v2(gr11,gr5,0)
   23| 0003E4 lfdux    7D2A2CEE   1     LFDU      fp9,gr10=v2(gr10,gr5,0)
   23| 0003E8 fadd     FC42182A   1     AFL       fp2=fp2,fp3,fcr
   21| 0003EC lfdux    7C091CEE   1     LFDU      fp0,gr9=v1(gr9,gr3,0)
   23| 0003F0 fmadd    FC65317A   1     FMA       fp3=fp6,fp5,fp5,fcr
   23| 0003F4 lfdux    7CBF34EE   1     LFDU      fp5,gr31=v3(gr31,gr6,0)
   23| 0003F8 lfdux    7CCC34EE   1     LFDU      fp6,gr12=v3(gr12,gr6,0)
   23| 0003FC fadd     FC84382A   1     AFL       fp4=fp4,fp7,fcr
   23| 000400 fadd     FCE8482A   2     AFL       fp7=fp8,fp9,fcr
   23| 000404 fmadd    FC4218BA   2     FMA       fp2=fp3,fp2,fp2,fcr
   23| 000408 fadd     FC21002A   2     AFL       fp1=fp1,fp0,fcr
   23| 00040C fmadd    FC44113A   2     FMA       fp2=fp2,fp4,fp4,fcr
   23| 000410 fadd     FCC5302A   2     AFL       fp6=fp5,fp6,fcr
   23| 000414 fmadd    FC41107A   2     FMA       fp2=fp2,fp1,fp1,fcr
    0| 000418 bc       4200FFA8   1     BCT       ctr=CL.656,taken=100%(100,0)
    0|                              CL.655:
   23| 00041C fmadd    FC0711FA   1     FMA       fp0=fp2,fp7,fp7,fcr
   23| 000420 fmadd    FC4601BA   2     FMA       fp2=fp0,fp6,fp6,fcr
    0|                              CL.524:
   28| 000424 addi     38E70001   1     AI        gr7=gr7,1
   28| 000428 cmpld    7EA7A840   1     CL8       cr5=gr7,gr21
   28| 00042C bc       4194FECC   1     BT        CL.65,cr5,0x8/llt,taken=80%(80,20)
   28|                              CL.64:
   29| 000430 ld       E8E101B0   1     L8        gr7=#SPILL33(gr1,432)
    0| 000434 ld       E90101C0   1     L8        gr8=#SPILL35(gr1,448)
   29| 000438 ld       E9210108   1     L8        gr9=#SPILL12(gr1,264)
   29| 00043C addi     38E70001   1     AI        gr7=gr7,1
    0| 000440 add      7D08F214   1     A         gr8=gr8,gr30
   29| 000444 std      F8E101B0   1     ST8       #SPILL33(gr1,432)=gr7
   29| 000448 cmpld    7EA74840   1     CL8       cr5=gr7,gr9
    0| 00044C std      F90101C0   1     ST8       #SPILL35(gr1,448)=gr8
   29| 000450 bc       4194FE60   1     BT        CL.63,cr5,0x8/llt,taken=80%(80,20)
   29|                              CL.62:
   30| 000454 ld       E8E10138   1     L8        gr7=#SPILL18(gr1,312)
   30| 000458 ld       E92100A8   1     L8        gr9=#SPILL0(gr1,168)
   30| 00045C ld       E9020000   1     L8        gr8=.+CONSTANT_AREA(gr2,0)
   31| 000460 ld       E94101A8   1     L8        gr10=#SPILL32(gr1,424)
   31| 000464 ld       E96100C0   1     L8        gr11=#SPILL3(gr1,192)
   30| 000468 std      F8E10090   1     ST8       #MX_CONVF1_0(gr1,144)=gr7
   30| 00046C lfd      C9090000   1     LFL       fp8=rms(gr9,0)
   30| 000470 lfs      C0080010   1     LFS       fp0=+CONSTANT_AREA(gr8,16)
   30| 000474 lfs      C0680014   1     LFS       fp3=+CONSTANT_AREA(gr8,20)
   30| 000478 lfs      C0880018   1     LFS       fp4=+CONSTANT_AREA(gr8,24)
   30| 00047C lfs      C0A8001C   1     LFS       fp5=+CONSTANT_AREA(gr8,28)
   30| 000480 lfs      C0C80020   1     LFS       fp6=+CONSTANT_AREA(gr8,32)
   30| 000484 lfs      C0E80024   1     LFS       fp7=+CONSTANT_AREA(gr8,36)
   30| 000488 fmul     FD080232   1     MFL       fp8=fp8,fp8,fcr
   30| 00048C lfd      CBC10090   2     LFL       fp30=#MX_CONVF1_0(gr1,144)
   30| 000490 fmul     FD220032   1     MFL       fp9=fp2,fp0,fcr
   30| 000494 lfs      C0280028   1     LFS       fp1=+CONSTANT_AREA(gr8,40)
   31| 000498 cmpdi    2C2A0000   1     C8        cr0=gr10,0
   30| 00049C qvfre    10404030   1     QVFRE     fp2=fp8
   31| 0004A0 cror     4E210B82   1     CR_O      cr4=cr[00],0x2/gt,0x2/gt,0x2/gt,cr4
   30| 0004A4 fcfid    FD60F69C   1     FCFID     fp11=fp30,fcr
   31| 0004A8 cmpdi    2C2B0000   1     C8        cr0=gr11,0
   31| 0004AC cror     4D210B82   1     CR_O      cr2=cr[00],0x2/gt,0x2/gt,0x2/gt,cr2
   30| 0004B0 fmsub    FD4818B8   1     FMS       fp10=fp3,fp8,fp2,fcr
   31| 0004B4 crand    4E298A02   1     CR_N      cr4=cr[24],0x2/gt,0x2/gt,0x2/gt,cr4
   30| 0004B8 frsp     FD805818   1     CVLS      fp12=fp11,fcr
   30| 0004BC fnmsub   FD4212BC   2     FNMS      fp10=fp2,fp2,fp10,fcr
   30| 0004C0 fmul     FC4C0332   2     MFL       fp2=fp12,fp12,fcr
   30| 0004C4 fmsub    FD681AB8   2     FMS       fp11=fp3,fp8,fp10,fcr
   30| 0004C8 fmul     FC4C00B2   2     MFL       fp2=fp12,fp2,fcr
   30| 0004CC fnmsub   FD8A52FC   2     FNMS      fp12=fp10,fp10,fp11,fcr
   30| 0004D0 frsp     FC401018   2     CVLS      fp2=fp2,fcr
   30| 0004D4 qvfre    11401030   1     QVFRE     fp10=fp2
   30| 0004D8 fmsub    FD621AB8   1     FMS       fp11=fp3,fp2,fp10,fcr
   30| 0004DC fnmsub   FD4A52FC   2     FNMS      fp10=fp10,fp10,fp11,fcr
   30| 0004E0 fmsub    FD621AB8   2     FMS       fp11=fp3,fp2,fp10,fcr
   30| 0004E4 fnmsub   FD4A52FC   2     FNMS      fp10=fp10,fp10,fp11,fcr
   30| 0004E8 fmul     FD6902B2   2     MFL       fp11=fp9,fp10,fcr
   30| 0004EC fmsub    FD224AF8   2     FMS       fp9=fp9,fp2,fp11,fcr
   30| 0004F0 fnmsub   FD2A5A7C   2     FNMS      fp9=fp11,fp10,fp9,fcr
   30| 0004F4 fmul     FD490332   2     MFL       fp10=fp9,fp12,fcr
   30| 0004F8 fmsub    FD084AB8   2     FMS       fp8=fp9,fp8,fp10,fcr
   30| 0004FC fnmsub   FD0C523C   2     FNMS      fp8=fp10,fp12,fp8,fcr
   30| 000500 frsqrte  FD204034   2     FRSQRE    fp9=fp8
   30| 000504 fnabs    FD404110   2     NABSFL    fp10=fp8
   30| 000508 fmul     FD680272   2     MFL       fp11=fp8,fp9,fcr
   30| 00050C fmadd    FC8922FA   2     FMA       fp4=fp4,fp9,fp11,fcr
   30| 000510 fmadd    FCA4317A   2     FMA       fp5=fp6,fp4,fp5,fcr
   30| 000514 fmadd    FCA4397A   2     FMA       fp5=fp7,fp4,fp5,fcr
   30| 000518 fmul     FC840172   2     MFL       fp4=fp4,fp5,fcr
   30| 00051C fmul     FCAB0132   2     MFL       fp5=fp11,fp4,fcr
   30| 000520 fmadd    FC89493A   2     FMA       fp4=fp9,fp9,fp4,fcr
   30| 000524 fmadd    FCA82A7A   2     FMA       fp5=fp5,fp8,fp9,fcr
   30| 000528 fmul     FC840072   2     MFL       fp4=fp4,fp1,fcr
   30| 00052C fmsub    FCC54178   2     FMS       fp6=fp8,fp5,fp5,fcr
   30| 000530 fnmsub   FC86293C   2     FNMS      fp4=fp5,fp6,fp4,fcr
   30| 000534 fsel     FFAA222E   2     FSEL      fp29=fp10,fp4,fp8
   31| 000538 bc       4091009C   1     BF        CL.129,cr4,0x2/gt,taken=50%(0,0)
    0| 00053C cmpdi    2C3C0000   1     C8        cr0=gr28,0
   31| 000540 addi     38E00000   1     LI        gr7=0
    0| 000544 bc       40810E40   1     BF        CL.380,cr0,0x2/gt,taken=40%(40,60)
    0| 000548 ld       E9810158   1     L8        gr12=#SPILL22(gr1,344)
    0| 00054C ld       EBE10160   1     L8        gr31=#SPILL23(gr1,352)
    0| 000550 ld       EAA10150   1     L8        gr21=#SPILL21(gr1,336)
    0| 000554 ld       EA8100D8   1     L8        gr20=#SPILL6(gr1,216)
    0| 000558 ld       EA6100E8   1     L8        gr19=#SPILL8(gr1,232)
    0| 00055C cmpdi    2C3D0000   1     C8        cr0=gr29,0
    0| 000560 mulld    7D0461D2   1     M         gr8=gr4,gr12
    0| 000564 mulld    7D3EF9D2   1     M         gr9=gr30,gr31
    0| 000568 mulld    7D43A9D2   1     M         gr10=gr3,gr21
    0| 00056C add      7D73A214   1     A         gr11=gr19,gr20
    0| 000570 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000574 subf     7D235850   1     S         gr9=gr11,gr3
    0| 000578 add      7D295214   1     A         gr9=gr9,gr10
    0| 00057C add      7D484A14   1     A         gr10=gr8,gr9
   31|                              CL.124:
   31| 000580 addi     39000000   1     LI        gr8=0
    0| 000584 bc       4081003C   1     BF        CL.128,cr0,0x2/gt,taken=20%(20,80)
    0| 000588 or       7D4B5378   1     LR        gr11=gr10
   31|                              CL.125:
   31| 00058C or       7D695B78   1     LR        gr9=gr11
    0| 000590 mtspr    7FA903A6   1     LCTR      ctr=gr29
    0| 000594 ori      60210000   1     XNOP      
    0| 000598 ori      60210000   1     XNOP      
    0| 00059C ori      60210000   1     XNOP      
    0|                              CL.657:
   31| 0005A0 lfdux    7C891CEE   1     LFDU      fp4,gr9=v1(gr9,gr3,0)
   31| 0005A4 fdiv     FC84E824   1     DFL       fp4=fp4,fp29,fcr
   31| 0005A8 stfd     D8890000   1     STFL      v1(gr9,0)=fp4
    0| 0005AC bc       4200FFF4   1     BCT       ctr=CL.657,taken=100%(100,0)
   31| 0005B0 addi     39080001   1     AI        gr8=gr8,1
    0| 0005B4 add      7D645A14   1     A         gr11=gr4,gr11
   31| 0005B8 cmpld    7F28E040   1     CL8       cr6=gr8,gr28
   31| 0005BC bc       4198FFD0   1     BT        CL.125,cr6,0x8/llt,taken=80%(80,20)
   31|                              CL.128:
   31| 0005C0 ld       E90101A8   1     L8        gr8=#SPILL32(gr1,424)
   31| 0005C4 addi     38E70001   1     AI        gr7=gr7,1
    0| 0005C8 add      7D4AF214   1     A         gr10=gr10,gr30
   31| 0005CC cmpd     7F283800   1     C8        cr6=gr8,gr7
   31| 0005D0 bc       4199FFB0   1     BT        CL.124,cr6,0x2/gt,taken=80%(80,20)
   31|                              CL.129:
   31| 0005D4 ld       E8E100C0   1     L8        gr7=#SPILL3(gr1,192)
   31| 0005D8 ld       E90101A8   1     L8        gr8=#SPILL32(gr1,424)
   31| 0005DC cmpd     7C274000   1     C8        cr0=gr7,gr8
   31| 0005E0 crand    4D214A02   1     CR_N      cr2=cr[02],0x2/gt,0x2/gt,0x2/gt,cr2
   31| 0005E4 bc       4089020C   1     BF        CL.68,cr2,0x2/gt,taken=50%(0,0)
    0| 0005E8 ld       EA810158   1     L8        gr20=#SPILL22(gr1,344)
    0| 0005EC ld       EA610160   1     L8        gr19=#SPILL23(gr1,352)
    0| 0005F0 ld       EA410150   1     L8        gr18=#SPILL21(gr1,336)
    0| 0005F4 ld       EA2100D8   1     L8        gr17=#SPILL6(gr1,216)
    0| 0005F8 ld       EA0100E8   1     L8        gr16=#SPILL8(gr1,232)
    0| 0005FC ld       E9E101A8   1     L8        gr15=#SPILL32(gr1,424)
   31| 000600 ld       E9C10100   1     L8        gr14=#SPILL11(gr1,256)
    0| 000604 mulld    7CE4A1D2   1     M         gr7=gr4,gr20
    0| 000608 mulld    7D13F1D2   1     M         gr8=gr19,gr30
    0| 00060C mulld    7D6391D2   1     M         gr11=gr3,gr18
    0| 000610 add      7D508A14   1     A         gr10=gr16,gr17
   31| 000614 addi     398EFFFF   1     AI        gr12=gr14,-1
    0| 000618 subf     7D435050   1     S         gr10=gr10,gr3
    0| 00061C mulld    7D2FF1D2   1     M         gr9=gr15,gr30
    0| 000620 add      7CE74214   1     A         gr7=gr7,gr8
    0| 000624 add      7D4A5A14   1     A         gr10=gr10,gr11
   31| 000628 sradi    7D8B1674   1     SRA8CA    gr11,ca=gr12,2
    0| 00062C rldicr   7BC81764   1     SLL8      gr8=gr30,2
    0| 000630 add      7CE75214   1     A         gr7=gr7,gr10
   31| 000634 addze    7EAB0194   1     ADDE      gr21,ca=gr11,0,ca
    0| 000638 cmpdi    2C3C0000   1     C8        cr0=gr28,0
    0| 00063C add      7D293A14   1     A         gr9=gr9,gr7
    0| 000640 subf     7D9E4050   1     S         gr12=gr8,gr30
    0| 000644 rldicr   7BDF0FA4   1     SLL8      gr31=gr30,1
    0| 000648 rldicl   7BAAF842   1     SRL8      gr10=gr29,1
   31| 00064C addi     38E00000   1     LI        gr7=0
    0| 000650 bc       408101A0   1     BF        CL.68,cr0,0x2/gt,taken=20%(20,80)
    0| 000654 qvfre    1080E830   1     QVFRE     fp4=fp29
    0| 000658 addi     3A950001   1     AI        gr20=gr21,1
    0| 00065C add      7D69F214   1     A         gr11=gr9,gr30
    0| 000660 std      FA8101C8   1     ST8       #SPILL36(gr1,456)=gr20
    0| 000664 add      7D896214   1     A         gr12=gr9,gr12
    0| 000668 add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 00066C cmpdi    2F3D0000   1     C8        cr6=gr29,0
    0| 000670 andi.    73BD0001   1     RN4_R     gr29,cr0=gr29,0,0x1
    0| 000674 cmpdi    2EAA0000   1     C8        cr5=gr10,0
   31|                              CL.69:
   31| 000678 addi     3BA00000   1     LI        gr29=0
    0| 00067C bc       40990154   1     BF        CL.70,cr6,0x2/gt,taken=20%(20,80)
    0| 000680 fmsub    FCBD1938   1     FMS       fp5=fp3,fp29,fp4,fcr
    0| 000684 or       7D314B78   2     LR        gr17=gr9
    0| 000688 or       7D906378   1     LR        gr16=gr12
    0| 00068C or       7FEFFB78   1     LR        gr15=gr31
    0| 000690 or       7D6E5B78   1     LR        gr14=gr11
    0| 000694 fnmsub   FCA4217C   1     FNMS      fp5=fp4,fp4,fp5,fcr
    0| 000698 fmsub    FCDD1978   2     FMS       fp6=fp3,fp29,fp5,fcr
    0| 00069C fnmsub   FCA529BC   2     FNMS      fp5=fp5,fp5,fp6,fcr
    0| 0006A0 ori      60210000   2     XNOP      
   31|                              CL.71:
   31| 0006A4 or       7E358B78   1     LR        gr21=gr17
   31| 0006A8 or       7DD47378   1     LR        gr20=gr14
   31| 0006AC or       7DF37B78   1     LR        gr19=gr15
   31| 0006B0 or       7E128378   1     LR        gr18=gr16
    0| 0006B4 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 0006B8 bc       41820058   1     BT        CL.631,cr0,0x4/eq,taken=50%(0,0)
   31| 0006BC lfdux    7CD51CEE   1     LFDU      fp6,gr21=v1(gr21,gr3,0)
   31| 0006C0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 0006C4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 0006C8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 0006CC stfd     D8D50000   1     STFL      v1(gr21,0)=fp6
   31| 0006D0 lfdux    7CD41CEE   1     LFDU      fp6,gr20=v1(gr20,gr3,0)
   31| 0006D4 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 0006D8 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 0006DC fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 0006E0 stfd     D8D40000   1     STFL      v1(gr20,0)=fp6
   31| 0006E4 lfdux    7CD31CEE   1     LFDU      fp6,gr19=v1(gr19,gr3,0)
   31| 0006E8 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 0006EC fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 0006F0 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 0006F4 stfd     D8D30000   1     STFL      v1(gr19,0)=fp6
   31| 0006F8 lfdux    7CD21CEE   1     LFDU      fp6,gr18=v1(gr18,gr3,0)
   31| 0006FC fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 000700 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 000704 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000708 stfd     D8D20000   1     STFL      v1(gr18,0)=fp6
    0| 00070C bc       419600A8   1     BT        CL.530,cr5,0x4/eq,taken=20%(20,80)
    0|                              CL.631:
   31| 000710 lfdux    7CD51CEE   1     LFDU      fp6,gr21=v1(gr21,gr3,0)
   31| 000714 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 000718 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 00071C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000720 stfd     D8D50000   1     STFL      v1(gr21,0)=fp6
   31| 000724 lfdux    7CD41CEE   1     LFDU      fp6,gr20=v1(gr20,gr3,0)
   31| 000728 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 00072C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 000730 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000734 stfd     D8D40000   1     STFL      v1(gr20,0)=fp6
   31| 000738 lfdux    7CD31CEE   1     LFDU      fp6,gr19=v1(gr19,gr3,0)
   31| 00073C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 000740 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 000744 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000748 stfd     D8D30000   1     STFL      v1(gr19,0)=fp6
   31| 00074C lfdux    7CD21CEE   1     LFDU      fp6,gr18=v1(gr18,gr3,0)
   31| 000750 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 000754 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 000758 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 00075C stfd     D8D20000   1     STFL      v1(gr18,0)=fp6
   31| 000760 lfdux    7CD51CEE   1     LFDU      fp6,gr21=v1(gr21,gr3,0)
   31| 000764 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 000768 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 00076C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000770 stfd     D8D50000   1     STFL      v1(gr21,0)=fp6
   31| 000774 lfdux    7CD41CEE   1     LFDU      fp6,gr20=v1(gr20,gr3,0)
   31| 000778 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 00077C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 000780 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000784 stfd     D8D40000   1     STFL      v1(gr20,0)=fp6
   31| 000788 lfdux    7CD31CEE   1     LFDU      fp6,gr19=v1(gr19,gr3,0)
   31| 00078C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 000790 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 000794 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 000798 stfd     D8D30000   1     STFL      v1(gr19,0)=fp6
   31| 00079C lfdux    7CD21CEE   1     LFDU      fp6,gr18=v1(gr18,gr3,0)
   31| 0007A0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   31| 0007A4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   31| 0007A8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   31| 0007AC stfd     D8D20000   1     STFL      v1(gr18,0)=fp6
    0| 0007B0 bc       4200FF60   1     BCT       ctr=CL.631,taken=100%(100,0)
    0|                              CL.530:
   31| 0007B4 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 0007B8 add      7E248A14   1     A         gr17=gr4,gr17
   31| 0007BC cmpld    7FBDE040   1     CL8       cr7=gr29,gr28
    0| 0007C0 add      7E048214   1     A         gr16=gr4,gr16
    0| 0007C4 add      7DE47A14   1     A         gr15=gr4,gr15
    0| 0007C8 add      7DC47214   1     A         gr14=gr4,gr14
   31| 0007CC bc       419CFED8   1     BT        CL.71,cr7,0x8/llt,taken=80%(80,20)
   31|                              CL.70:
   31| 0007D0 ld       EBA101C8   1     L8        gr29=#SPILL36(gr1,456)
   31| 0007D4 addi     38E70001   1     AI        gr7=gr7,1
    0| 0007D8 add      7D685A14   1     A         gr11=gr8,gr11
    0| 0007DC add      7D886214   1     A         gr12=gr8,gr12
    0| 0007E0 add      7FE8FA14   1     A         gr31=gr8,gr31
    0| 0007E4 add      7D284A14   1     A         gr9=gr8,gr9
   31| 0007E8 cmpld    7FA7E840   1     CL8       cr7=gr7,gr29
   31| 0007EC bc       419CFE8C   1     BT        CL.69,cr7,0x8/llt,taken=80%(80,20)
   31|                              CL.68:
   32| 0007F0 ld       E8E101A0   1     L8        gr7=#SPILL31(gr1,416)
   32| 0007F4 ld       E90100B8   1     L8        gr8=#SPILL2(gr1,184)
   32| 0007F8 cmpdi    2C270000   1     C8        cr0=gr7,0
   32| 0007FC cror     4D210B82   1     CR_O      cr2=cr[00],0x2/gt,0x2/gt,0x2/gt,cr2
   32| 000800 cmpdi    2C280000   1     C8        cr0=gr8,0
   32| 000804 cror     4E210B82   1     CR_O      cr4=cr[00],0x2/gt,0x2/gt,0x2/gt,cr4
   32| 000808 crand    4D314A02   1     CR_N      cr2=cr[42],0x2/gt,0x2/gt,0x2/gt,cr2
   32| 00080C bc       4089009C   1     BF        CL.117,cr2,0x2/gt,taken=50%(0,0)
    0| 000810 cmpdi    2C390000   1     C8        cr0=gr25,0
   32| 000814 addi     38E00000   1     LI        gr7=0
    0| 000818 bc       40810B50   1     BF        CL.390,cr0,0x2/gt,taken=40%(40,60)
    0| 00081C ld       E9810170   1     L8        gr12=#SPILL25(gr1,368)
    0| 000820 ld       EBA10168   1     L8        gr29=#SPILL24(gr1,360)
    0| 000824 ld       EBE10110   1     L8        gr31=#SPILL13(gr1,272)
    0| 000828 ld       EAA10140   1     L8        gr21=#SPILL19(gr1,320)
    0| 00082C ld       EA810178   1     L8        gr20=#SPILL26(gr1,376)
    0| 000830 ld       EB810128   1     L8        gr28=#SPILL16(gr1,296)
    0| 000834 mulld    7D0CD9D2   1     M         gr8=gr12,gr27
    0| 000838 subf     7D45F850   1     S         gr10=gr31,gr5
    0| 00083C mulld    7D25E9D2   1     M         gr9=gr5,gr29
    0| 000840 add      7D4AE214   1     A         gr10=gr10,gr28
    0| 000844 mulld    7D74A9D2   1     M         gr11=gr20,gr21
    0| 000848 add      7D085214   1     A         gr8=gr8,gr10
    0| 00084C cmpdi    2C3A0000   1     C8        cr0=gr26,0
    0| 000850 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000854 add      7D485A14   1     A         gr10=gr8,gr11
   32|                              CL.112:
   32| 000858 addi     39000000   1     LI        gr8=0
    0| 00085C bc       40810034   1     BF        CL.116,cr0,0x2/gt,taken=20%(20,80)
    0| 000860 or       7D4B5378   1     LR        gr11=gr10
   32|                              CL.113:
   32| 000864 or       7D695B78   1     LR        gr9=gr11
    0| 000868 mtspr    7F4903A6   1     LCTR      ctr=gr26
    0| 00086C ori      60210000   1     XNOP      
    0|                              CL.659:
   32| 000870 lfdux    7C892CEE   1     LFDU      fp4,gr9=v2(gr9,gr5,0)
   32| 000874 fdiv     FC84E824   1     DFL       fp4=fp4,fp29,fcr
   32| 000878 stfd     D8890000   1     STFL      v2(gr9,0)=fp4
    0| 00087C bc       4200FFF4   1     BCT       ctr=CL.659,taken=100%(100,0)
   32| 000880 addi     39080001   1     AI        gr8=gr8,1
    0| 000884 add      7D6BDA14   1     A         gr11=gr11,gr27
   32| 000888 cmpld    7F28C840   1     CL8       cr6=gr8,gr25
   32| 00088C bc       4198FFD8   1     BT        CL.113,cr6,0x8/llt,taken=80%(80,20)
   32|                              CL.116:
   32| 000890 ld       E92101A0   1     L8        gr9=#SPILL31(gr1,416)
    0| 000894 ld       E9010140   1     L8        gr8=#SPILL19(gr1,320)
   32| 000898 addi     38E70001   1     AI        gr7=gr7,1
   32| 00089C cmpd     7F293800   1     C8        cr6=gr9,gr7
    0| 0008A0 add      7D485214   1     A         gr10=gr8,gr10
   32| 0008A4 bc       4199FFB4   1     BT        CL.112,cr6,0x2/gt,taken=80%(80,20)
   32|                              CL.117:
   32| 0008A8 ld       E8E100B8   1     L8        gr7=#SPILL2(gr1,184)
   32| 0008AC ld       E90101A0   1     L8        gr8=#SPILL31(gr1,416)
   32| 0008B0 cmpd     7D274000   1     C8        cr2=gr7,gr8
   32| 0008B4 crand    4D314A02   1     CR_N      cr2=cr[42],0x2/gt,0x2/gt,0x2/gt,cr2
   32| 0008B8 bc       40890210   1     BF        CL.74,cr2,0x2/gt,taken=50%(0,0)
    0| 0008BC ld       EB810170   1     L8        gr28=#SPILL25(gr1,368)
    0| 0008C0 ld       EA810168   1     L8        gr20=#SPILL24(gr1,360)
    0| 0008C4 ld       EAA10110   1     L8        gr21=#SPILL13(gr1,272)
    0| 0008C8 ld       EA410140   1     L8        gr18=#SPILL19(gr1,320)
    0| 0008CC ld       EA210178   1     L8        gr17=#SPILL26(gr1,376)
    0| 0008D0 ld       EA610128   1     L8        gr19=#SPILL16(gr1,296)
    0| 0008D4 or       7D0B4378   1     LR        gr11=gr8
    0| 0008D8 mulld    7D1BE1D2   1     M         gr8=gr27,gr28
    0| 0008DC subf     7CE5A850   1     S         gr7=gr21,gr5
    0| 0008E0 mulld    7D45A1D2   1     M         gr10=gr5,gr20
    0| 0008E4 add      7D279A14   1     A         gr9=gr7,gr19
   32| 0008E8 ld       EA0100F8   1     L8        gr16=#SPILL10(gr1,248)
    0| 0008EC mulld    7CF191D2   1     M         gr7=gr17,gr18
    0| 0008F0 add      7D084A14   1     A         gr8=gr8,gr9
    0| 0008F4 mulld    7D2B91D2   1     M         gr9=gr11,gr18
   32| 0008F8 addi     3970FFFF   1     AI        gr11=gr16,-1
    0| 0008FC add      7D4A4214   1     A         gr10=gr10,gr8
   32| 000900 sradi    7D6B1674   1     SRA8CA    gr11,ca=gr11,2
    0| 000904 rldicr   7A481764   1     SLL8      gr8=gr18,2
    0| 000908 add      7CE75214   1     A         gr7=gr7,gr10
   32| 00090C addze    7FAB0194   1     ADDE      gr29,ca=gr11,0,ca
    0| 000910 cmpdi    2C390000   1     C8        cr0=gr25,0
    0| 000914 add      7D293A14   1     A         gr9=gr9,gr7
    0| 000918 subf     7D924050   1     S         gr12=gr8,gr18
    0| 00091C rldicr   7A5F0FA4   1     SLL8      gr31=gr18,1
    0| 000920 rldicl   7B4AF842   1     SRL8      gr10=gr26,1
   32| 000924 addi     38E00000   1     LI        gr7=0
    0| 000928 bc       408101A0   1     BF        CL.74,cr0,0x2/gt,taken=20%(20,80)
    0| 00092C qvfre    1080E830   1     QVFRE     fp4=fp29
    0| 000930 add      7D699214   1     A         gr11=gr9,gr18
    0| 000934 add      7D896214   1     A         gr12=gr9,gr12
    0| 000938 add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 00093C addi     39FD0001   1     AI        gr15=gr29,1
    0| 000940 cmpdi    2F3A0000   1     C8        cr6=gr26,0
    0| 000944 andi.    735D0001   1     RN4_R     gr29,cr0=gr26,0,0x1
    0| 000948 cmpdi    2EAA0000   1     C8        cr5=gr10,0
   32|                              CL.75:
   32| 00094C addi     3BA00000   1     LI        gr29=0
    0| 000950 bc       4099015C   1     BF        CL.76,cr6,0x2/gt,taken=20%(20,80)
    0| 000954 fmsub    FCBD1938   1     FMS       fp5=fp3,fp29,fp4,fcr
    0| 000958 or       7D334B78   2     LR        gr19=gr9
    0| 00095C or       7D926378   1     LR        gr18=gr12
    0| 000960 or       7FF1FB78   1     LR        gr17=gr31
    0| 000964 or       7D705B78   1     LR        gr16=gr11
    0| 000968 fnmsub   FCA4217C   1     FNMS      fp5=fp4,fp4,fp5,fcr
    0| 00096C fmsub    FCDD1978   2     FMS       fp6=fp3,fp29,fp5,fcr
    0| 000970 fnmsub   FCA529BC   2     FNMS      fp5=fp5,fp5,fp6,fcr
   32|                              CL.77:
   32| 000974 or       7E7C9B78   1     LR        gr28=gr19
   32| 000978 or       7E1A8378   1     LR        gr26=gr16
   32| 00097C or       7E358B78   1     LR        gr21=gr17
   32| 000980 or       7E549378   1     LR        gr20=gr18
    0| 000984 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 000988 bc       41820064   1     BT        CL.639,cr0,0x4/eq,taken=50%(0,0)
   32| 00098C lfdux    7CDC2CEE   1     LFDU      fp6,gr28=v2(gr28,gr5,0)
   32| 000990 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000994 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000998 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 00099C stfd     D8DC0000   1     STFL      v2(gr28,0)=fp6
   32| 0009A0 lfdux    7CDA2CEE   1     LFDU      fp6,gr26=v2(gr26,gr5,0)
   32| 0009A4 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 0009A8 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 0009AC fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 0009B0 stfd     D8DA0000   1     STFL      v2(gr26,0)=fp6
   32| 0009B4 lfdux    7CD52CEE   1     LFDU      fp6,gr21=v2(gr21,gr5,0)
   32| 0009B8 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 0009BC fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 0009C0 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 0009C4 stfd     D8D50000   1     STFL      v2(gr21,0)=fp6
   32| 0009C8 lfdux    7CD42CEE   1     LFDU      fp6,gr20=v2(gr20,gr5,0)
   32| 0009CC fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 0009D0 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 0009D4 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 0009D8 stfd     D8D40000   1     STFL      v2(gr20,0)=fp6
    0| 0009DC bc       419600B4   1     BT        CL.536,cr5,0x4/eq,taken=20%(20,80)
    0| 0009E0 ori      60210000   1     XNOP      
    0| 0009E4 ori      60210000   1     XNOP      
    0| 0009E8 ori      60210000   1     XNOP      
    0|                              CL.639:
   32| 0009EC lfdux    7CDC2CEE   1     LFDU      fp6,gr28=v2(gr28,gr5,0)
   32| 0009F0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 0009F4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 0009F8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 0009FC stfd     D8DC0000   1     STFL      v2(gr28,0)=fp6
   32| 000A00 lfdux    7CDA2CEE   1     LFDU      fp6,gr26=v2(gr26,gr5,0)
   32| 000A04 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A08 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A0C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A10 stfd     D8DA0000   1     STFL      v2(gr26,0)=fp6
   32| 000A14 lfdux    7CD52CEE   1     LFDU      fp6,gr21=v2(gr21,gr5,0)
   32| 000A18 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A1C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A20 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A24 stfd     D8D50000   1     STFL      v2(gr21,0)=fp6
   32| 000A28 lfdux    7CD42CEE   1     LFDU      fp6,gr20=v2(gr20,gr5,0)
   32| 000A2C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A30 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A34 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A38 stfd     D8D40000   1     STFL      v2(gr20,0)=fp6
   32| 000A3C lfdux    7CDC2CEE   1     LFDU      fp6,gr28=v2(gr28,gr5,0)
   32| 000A40 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A44 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A48 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A4C stfd     D8DC0000   1     STFL      v2(gr28,0)=fp6
   32| 000A50 lfdux    7CDA2CEE   1     LFDU      fp6,gr26=v2(gr26,gr5,0)
   32| 000A54 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A58 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A5C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A60 stfd     D8DA0000   1     STFL      v2(gr26,0)=fp6
   32| 000A64 lfdux    7CD52CEE   1     LFDU      fp6,gr21=v2(gr21,gr5,0)
   32| 000A68 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A6C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A70 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A74 stfd     D8D50000   1     STFL      v2(gr21,0)=fp6
   32| 000A78 lfdux    7CD42CEE   1     LFDU      fp6,gr20=v2(gr20,gr5,0)
   32| 000A7C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   32| 000A80 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   32| 000A84 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   32| 000A88 stfd     D8D40000   1     STFL      v2(gr20,0)=fp6
    0| 000A8C bc       4200FF60   1     BCT       ctr=CL.639,taken=100%(100,0)
    0|                              CL.536:
   32| 000A90 addi     3BBD0001   1     AI        gr29=gr29,1
    0| 000A94 add      7E73DA14   1     A         gr19=gr19,gr27
   32| 000A98 cmpld    7FBDC840   1     CL8       cr7=gr29,gr25
    0| 000A9C add      7E52DA14   1     A         gr18=gr18,gr27
    0| 000AA0 add      7E31DA14   1     A         gr17=gr17,gr27
    0| 000AA4 add      7E10DA14   1     A         gr16=gr16,gr27
   32| 000AA8 bc       419CFECC   1     BT        CL.77,cr7,0x8/llt,taken=80%(80,20)
   32|                              CL.76:
   32| 000AAC addi     38E70001   1     AI        gr7=gr7,1
    0| 000AB0 add      7D685A14   1     A         gr11=gr8,gr11
   32| 000AB4 cmpld    7FA77840   1     CL8       cr7=gr7,gr15
    0| 000AB8 add      7D886214   1     A         gr12=gr8,gr12
    0| 000ABC add      7FE8FA14   1     A         gr31=gr8,gr31
    0| 000AC0 add      7D284A14   1     A         gr9=gr8,gr9
   32| 000AC4 bc       419CFE88   1     BT        CL.75,cr7,0x8/llt,taken=80%(80,20)
   32|                              CL.74:
   33| 000AC8 ld       E8E10198   1     L8        gr7=#SPILL30(gr1,408)
   33| 000ACC ld       E90100B0   1     L8        gr8=#SPILL1(gr1,176)
   33| 000AD0 cmpdi    2C270000   1     C8        cr0=gr7,0
   33| 000AD4 cror     4E210B82   1     CR_O      cr4=cr[00],0x2/gt,0x2/gt,0x2/gt,cr4
   33| 000AD8 cmpdi    2C280000   1     C8        cr0=gr8,0
   33| 000ADC cror     4D210B82   1     CR_O      cr2=cr[00],0x2/gt,0x2/gt,0x2/gt,cr2
   33| 000AE0 crand    4E298A02   1     CR_N      cr4=cr[24],0x2/gt,0x2/gt,0x2/gt,cr4
   33| 000AE4 bc       409100A4   1     BF        CL.105,cr4,0x2/gt,taken=50%(0,0)
    0| 000AE8 cmpdi    2C360000   1     C8        cr0=gr22,0
   33| 000AEC addi     38E00000   1     LI        gr7=0
    0| 000AF0 bc       4081085C   1     BF        CL.400,cr0,0x2/gt,taken=40%(40,60)
    0| 000AF4 ld       E9810188   1     L8        gr12=#SPILL28(gr1,392)
    0| 000AF8 ld       EBA10180   1     L8        gr29=#SPILL27(gr1,384)
    0| 000AFC ld       EBE10118   1     L8        gr31=#SPILL14(gr1,280)
    0| 000B00 ld       EB410148   1     L8        gr26=#SPILL20(gr1,328)
    0| 000B04 ld       EB210190   1     L8        gr25=#SPILL29(gr1,400)
    0| 000B08 ld       EB810130   1     L8        gr28=#SPILL17(gr1,304)
    0| 000B0C mulld    7D0CC1D2   1     M         gr8=gr12,gr24
    0| 000B10 subf     7D46F850   1     S         gr10=gr31,gr6
    0| 000B14 mulld    7D26E9D2   1     M         gr9=gr6,gr29
    0| 000B18 add      7D4AE214   1     A         gr10=gr10,gr28
    0| 000B1C mulld    7D79D1D2   1     M         gr11=gr25,gr26
    0| 000B20 add      7D085214   1     A         gr8=gr8,gr10
    0| 000B24 cmpdi    2C370000   1     C8        cr0=gr23,0
    0| 000B28 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000B2C add      7D485A14   1     A         gr10=gr8,gr11
   33|                              CL.100:
   33| 000B30 addi     39000000   1     LI        gr8=0
    0| 000B34 bc       4081003C   1     BF        CL.104,cr0,0x2/gt,taken=20%(20,80)
    0| 000B38 or       7D4B5378   1     LR        gr11=gr10
   33|                              CL.101:
   33| 000B3C or       7D695B78   1     LR        gr9=gr11
    0| 000B40 mtspr    7EE903A6   1     LCTR      ctr=gr23
    0| 000B44 ori      60210000   1     XNOP      
    0| 000B48 ori      60210000   1     XNOP      
    0| 000B4C ori      60210000   1     XNOP      
    0|                              CL.661:
   33| 000B50 lfdux    7C8934EE   1     LFDU      fp4,gr9=v3(gr9,gr6,0)
   33| 000B54 fdiv     FC84E824   1     DFL       fp4=fp4,fp29,fcr
   33| 000B58 stfd     D8890000   1     STFL      v3(gr9,0)=fp4
    0| 000B5C bc       4200FFF4   1     BCT       ctr=CL.661,taken=100%(100,0)
   33| 000B60 addi     39080001   1     AI        gr8=gr8,1
    0| 000B64 add      7D6BC214   1     A         gr11=gr11,gr24
   33| 000B68 cmpld    7F28B040   1     CL8       cr6=gr8,gr22
   33| 000B6C bc       4198FFD0   1     BT        CL.101,cr6,0x8/llt,taken=80%(80,20)
   33|                              CL.104:
   33| 000B70 ld       E9210198   1     L8        gr9=#SPILL30(gr1,408)
    0| 000B74 ld       E9010148   1     L8        gr8=#SPILL20(gr1,328)
   33| 000B78 addi     38E70001   1     AI        gr7=gr7,1
   33| 000B7C cmpd     7F293800   1     C8        cr6=gr9,gr7
    0| 000B80 add      7D485214   1     A         gr10=gr8,gr10
   33| 000B84 bc       4199FFAC   1     BT        CL.100,cr6,0x2/gt,taken=80%(80,20)
   33|                              CL.105:
   33| 000B88 ld       E8E100B0   1     L8        gr7=#SPILL1(gr1,176)
   33| 000B8C ld       E9010198   1     L8        gr8=#SPILL30(gr1,408)
   33| 000B90 cmpd     7E274000   1     C8        cr4=gr7,gr8
   33| 000B94 crand    4D314A02   1     CR_N      cr2=cr[42],0x2/gt,0x2/gt,0x2/gt,cr2
   33| 000B98 bc       40890210   1     BF        CL.80,cr2,0x2/gt,taken=50%(0,0)
    0| 000B9C ld       EB810188   1     L8        gr28=#SPILL28(gr1,392)
    0| 000BA0 ld       EB210180   1     L8        gr25=#SPILL27(gr1,384)
    0| 000BA4 ld       EB410118   1     L8        gr26=#SPILL14(gr1,280)
    0| 000BA8 ld       EA810148   1     L8        gr20=#SPILL20(gr1,328)
    0| 000BAC ld       EA610190   1     L8        gr19=#SPILL29(gr1,400)
    0| 000BB0 ld       EAA10130   1     L8        gr21=#SPILL17(gr1,304)
    0| 000BB4 ld       EA410198   1     L8        gr18=#SPILL30(gr1,408)
    0| 000BB8 mulld    7CF8E1D2   1     M         gr7=gr24,gr28
    0| 000BBC subf     7D06D050   1     S         gr8=gr26,gr6
    0| 000BC0 mulld    7D26C9D2   1     M         gr9=gr6,gr25
   33| 000BC4 ld       EA2100F0   1     L8        gr17=#SPILL9(gr1,240)
    0| 000BC8 add      7D08AA14   1     A         gr8=gr8,gr21
    0| 000BCC mulld    7D53A1D2   1     M         gr10=gr19,gr20
    0| 000BD0 add      7D074214   1     A         gr8=gr7,gr8
    0| 000BD4 mulld    7CF2A1D2   1     M         gr7=gr18,gr20
   33| 000BD8 addi     3971FFFF   1     AI        gr11=gr17,-1
    0| 000BDC add      7D294214   1     A         gr9=gr9,gr8
   33| 000BE0 sradi    7D6B1674   1     SRA8CA    gr11,ca=gr11,2
    0| 000BE4 rldicr   7A881764   1     SLL8      gr8=gr20,2
    0| 000BE8 add      7D295214   1     A         gr9=gr9,gr10
   33| 000BEC addze    7FAB0194   1     ADDE      gr29,ca=gr11,0,ca
    0| 000BF0 cmpdi    2C360000   1     C8        cr0=gr22,0
    0| 000BF4 add      7D274A14   1     A         gr9=gr7,gr9
    0| 000BF8 subf     7D944050   1     S         gr12=gr8,gr20
    0| 000BFC rldicr   7A9F0FA4   1     SLL8      gr31=gr20,1
    0| 000C00 rldicl   7AEAF842   1     SRL8      gr10=gr23,1
   33| 000C04 addi     38E00000   1     LI        gr7=0
    0| 000C08 bc       408101A0   1     BF        CL.80,cr0,0x2/gt,taken=20%(20,80)
    0| 000C0C qvfre    1080E830   1     QVFRE     fp4=fp29
    0| 000C10 add      7D69A214   1     A         gr11=gr9,gr20
    0| 000C14 add      7D896214   1     A         gr12=gr9,gr12
    0| 000C18 add      7FE9FA14   1     A         gr31=gr9,gr31
    0| 000C1C addi     3BBD0001   1     AI        gr29=gr29,1
    0| 000C20 cmpdi    2F370000   1     C8        cr6=gr23,0
    0| 000C24 andi.    72FC0001   1     RN4_R     gr28,cr0=gr23,0,0x1
    0| 000C28 cmpdi    2EAA0000   1     C8        cr5=gr10,0
   33|                              CL.81:
   33| 000C2C addi     3B800000   1     LI        gr28=0
    0| 000C30 bc       4099015C   1     BF        CL.82,cr6,0x2/gt,taken=20%(20,80)
    0| 000C34 fmsub    FCBD1938   1     FMS       fp5=fp3,fp29,fp4,fcr
    0| 000C38 or       7D344B78   1     LR        gr20=gr9
    0| 000C3C or       7D936378   1     LR        gr19=gr12
    0| 000C40 or       7FF2FB78   1     LR        gr18=gr31
    0| 000C44 or       7D715B78   1     LR        gr17=gr11
    0| 000C48 fnmsub   FCA4217C   1     FNMS      fp5=fp4,fp4,fp5,fcr
    0| 000C4C fmsub    FCDD1978   2     FMS       fp6=fp3,fp29,fp5,fcr
    0| 000C50 fnmsub   FCA529BC   2     FNMS      fp5=fp5,fp5,fp6,fcr
   33|                              CL.83:
   33| 000C54 or       7E9AA378   1     LR        gr26=gr20
   33| 000C58 or       7E398B78   1     LR        gr25=gr17
   33| 000C5C or       7E579378   1     LR        gr23=gr18
   33| 000C60 or       7E759B78   1     LR        gr21=gr19
    0| 000C64 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 000C68 bc       41820064   1     BT        CL.647,cr0,0x4/eq,taken=50%(0,0)
   33| 000C6C lfdux    7CDA34EE   1     LFDU      fp6,gr26=v3(gr26,gr6,0)
   33| 000C70 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000C74 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000C78 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000C7C stfd     D8DA0000   1     STFL      v3(gr26,0)=fp6
   33| 000C80 lfdux    7CD934EE   1     LFDU      fp6,gr25=v3(gr25,gr6,0)
   33| 000C84 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000C88 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000C8C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000C90 stfd     D8D90000   1     STFL      v3(gr25,0)=fp6
   33| 000C94 lfdux    7CD734EE   1     LFDU      fp6,gr23=v3(gr23,gr6,0)
   33| 000C98 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000C9C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000CA0 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000CA4 stfd     D8D70000   1     STFL      v3(gr23,0)=fp6
   33| 000CA8 lfdux    7CD534EE   1     LFDU      fp6,gr21=v3(gr21,gr6,0)
   33| 000CAC fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000CB0 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000CB4 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000CB8 stfd     D8D50000   1     STFL      v3(gr21,0)=fp6
    0| 000CBC bc       419600B4   1     BT        CL.542,cr5,0x4/eq,taken=20%(20,80)
    0| 000CC0 ori      60210000   1     XNOP      
    0| 000CC4 ori      60210000   1     XNOP      
    0| 000CC8 ori      60210000   1     XNOP      
    0|                              CL.647:
   33| 000CCC lfdux    7CDA34EE   1     LFDU      fp6,gr26=v3(gr26,gr6,0)
   33| 000CD0 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000CD4 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000CD8 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000CDC stfd     D8DA0000   1     STFL      v3(gr26,0)=fp6
   33| 000CE0 lfdux    7CD934EE   1     LFDU      fp6,gr25=v3(gr25,gr6,0)
   33| 000CE4 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000CE8 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000CEC fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000CF0 stfd     D8D90000   1     STFL      v3(gr25,0)=fp6
   33| 000CF4 lfdux    7CD734EE   1     LFDU      fp6,gr23=v3(gr23,gr6,0)
   33| 000CF8 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000CFC fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000D00 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000D04 stfd     D8D70000   1     STFL      v3(gr23,0)=fp6
   33| 000D08 lfdux    7CD534EE   1     LFDU      fp6,gr21=v3(gr21,gr6,0)
   33| 000D0C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000D10 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000D14 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000D18 stfd     D8D50000   1     STFL      v3(gr21,0)=fp6
   33| 000D1C lfdux    7CDA34EE   1     LFDU      fp6,gr26=v3(gr26,gr6,0)
   33| 000D20 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000D24 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000D28 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000D2C stfd     D8DA0000   1     STFL      v3(gr26,0)=fp6
   33| 000D30 lfdux    7CD934EE   1     LFDU      fp6,gr25=v3(gr25,gr6,0)
   33| 000D34 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000D38 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000D3C fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000D40 stfd     D8D90000   1     STFL      v3(gr25,0)=fp6
   33| 000D44 lfdux    7CD734EE   1     LFDU      fp6,gr23=v3(gr23,gr6,0)
   33| 000D48 fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000D4C fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000D50 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000D54 stfd     D8D70000   1     STFL      v3(gr23,0)=fp6
   33| 000D58 lfdux    7CD534EE   1     LFDU      fp6,gr21=v3(gr21,gr6,0)
   33| 000D5C fmul     FCE60172   1     MFL       fp7=fp6,fp5,fcr
   33| 000D60 fmsub    FCDD31F8   2     FMS       fp6=fp6,fp29,fp7,fcr
   33| 000D64 fnmsub   FCC539BC   2     FNMS      fp6=fp7,fp5,fp6,fcr
   33| 000D68 stfd     D8D50000   1     STFL      v3(gr21,0)=fp6
    0| 000D6C bc       4200FF60   1     BCT       ctr=CL.647,taken=100%(100,0)
    0|                              CL.542:
   33| 000D70 addi     3B9C0001   1     AI        gr28=gr28,1
    0| 000D74 add      7E94C214   1     A         gr20=gr20,gr24
   33| 000D78 cmpld    7FBCB040   1     CL8       cr7=gr28,gr22
    0| 000D7C add      7E73C214   1     A         gr19=gr19,gr24
    0| 000D80 add      7E52C214   1     A         gr18=gr18,gr24
    0| 000D84 add      7E31C214   1     A         gr17=gr17,gr24
   33| 000D88 bc       419CFECC   1     BT        CL.83,cr7,0x8/llt,taken=80%(80,20)
   33|                              CL.82:
   33| 000D8C addi     38E70001   1     AI        gr7=gr7,1
    0| 000D90 add      7D685A14   1     A         gr11=gr8,gr11
   33| 000D94 cmpld    7FA7E840   1     CL8       cr7=gr7,gr29
    0| 000D98 add      7D886214   1     A         gr12=gr8,gr12
    0| 000D9C add      7FE8FA14   1     A         gr31=gr8,gr31
    0| 000DA0 add      7D284A14   1     A         gr9=gr8,gr9
   33| 000DA4 bc       419CFE88   1     BT        CL.81,cr7,0x8/llt,taken=80%(80,20)
   33|                              CL.80:
   35| 000DA8 fmr      FC60F890   1     LRFL      fp3=fp31
   36| 000DAC bc       408502A0   1     BF        CL.86,cr1,0x2/gt,taken=50%(0,0)
    0| 000DB0 ld       EB2100D0   1     L8        gr25=#SPILL5(gr1,208)
   38| 000DB4 ld       EAC100E0   1     L8        gr22=#SPILL7(gr1,224)
    0| 000DB8 ld       EAE100C8   1     L8        gr23=#SPILL4(gr1,200)
    0| 000DBC ld       EAA100D8   1     L8        gr21=#SPILL6(gr1,216)
    0| 000DC0 ld       EA8100E8   1     L8        gr20=#SPILL8(gr1,232)
   42| 000DC4 ld       EA610110   1     L8        gr19=#SPILL13(gr1,272)
    0| 000DC8 mulld    7CE3C9D2   1     M         gr7=gr3,gr25
   42| 000DCC ld       EA410118   1     L8        gr18=#SPILL14(gr1,280)
    0| 000DD0 ld       EA210120   1     L8        gr17=#SPILL15(gr1,288)
   38| 000DD4 subf     7D80B050   1     S         gr12=gr22,gr0
    0| 000DD8 add      7FF4AA14   1     A         gr31=gr20,gr21
   38| 000DDC addic.   378C0001   1     AI_R      gr28,cr0=gr12,1,ca"
    0| 000DE0 mulld    7D0021D2   1     M         gr8=gr0,gr4
    0| 000DE4 mulld    7D37F1D2   1     M         gr9=gr23,gr30
   42| 000DE8 mulld    7D65C9D2   1     M         gr11=gr5,gr25
   42| 000DEC mulld    7D46C9D2   1     M         gr10=gr6,gr25
   42| 000DF0 subf     7D859850   1     S         gr12=gr19,gr5
   42| 000DF4 subf     7FA69050   1     S         gr29=gr18,gr6
    0| 000DF8 add      7F47FA14   1     A         gr26=gr7,gr31
    0| 000DFC subf     7FF98850   1     S         gr31=gr17,gr25
   36| 000E00 addi     38E00000   1     LI        gr7=0
    0| 000E04 bc       40810248   1     BF        CL.86,cr0,0x2/gt,taken=20%(20,80)
   42| 000E08 ld       EA810128   1     L8        gr20=#SPILL16(gr1,296)
   42| 000E0C ld       EA610130   1     L8        gr19=#SPILL17(gr1,304)
    0| 000E10 subfic   23390001   1     SFI       gr25=1,gr25,ca"
    0| 000E14 add      7D084A14   1     A         gr8=gr8,gr9
    0| 000E18 subf     7D24D050   1     S         gr9=gr26,gr4
    0| 000E1C addic.   37FF0001   1     AI_R      gr31,cr0=gr31,1,ca"
   42| 000E20 add      7EECA214   1     A         gr23=gr12,gr20
    0| 000E24 add      7D91CA14   1     A         gr12=gr17,gr25
   42| 000E28 add      7FB3EA14   1     A         gr29=gr19,gr29
    0| 000E2C rldicl   7999F842   1     SRL8      gr25=gr12,1
   42| 000E30 add      7EEBBA14   1     A         gr23=gr11,gr23
   42| 000E34 add      7ECAEA14   1     A         gr22=gr10,gr29
    0| 000E38 add      7EA84A14   1     A         gr21=gr8,gr9
    0| 000E3C mcrf     4C800000   1     LRCR      cr1=cr0
    0| 000E40 andi.    71880001   1     RN4_R     gr8,cr0=gr12,0,0x1
    0| 000E44 cmpdi    2FB90000   1     C8        cr7=gr25,0
   36|                              CL.87:
    0| 000E48 ld       E92100C8   1     L8        gr9=#SPILL4(gr1,200)
   38| 000E4C addi     39000000   1     LI        gr8=0
    0| 000E50 add      7D474A14   1     A         gr10=gr7,gr9
    0| 000E54 bc       408501E4   1     BF        CL.88,cr1,0x2/gt,taken=20%(20,80)
    0| 000E58 ld       EBE10140   1     L8        gr31=#SPILL19(gr1,320)
    0| 000E5C ld       EBA10148   1     L8        gr29=#SPILL20(gr1,328)
    0| 000E60 addi     398A0001   1     AI        gr12=gr10,1
   40| 000E64 or       7EA9AB78   1     LR        gr9=gr21
    0| 000E68 mulld    7E8AF1D2   1     M         gr20=gr10,gr30
    0| 000E6C mulld    7D6AF9D2   1     M         gr11=gr10,gr31
    0| 000E70 mulld    7E6CE9D2   1     M         gr19=gr12,gr29
    0| 000E74 mulld    7E4AE9D2   1     M         gr18=gr10,gr29
    0| 000E78 add      7E2BBA14   1     A         gr17=gr11,gr23
   38|                              CL.89:
   40| 000E7C add      7D404214   1     A         gr10=gr0,gr8
   40| 000E80 lfdux    7C8924EE   1     LFDU      fp4,gr9=v1(gr9,gr4,0)
   42| 000E84 addi     396A0001   1     AI        gr11=gr10,1
   40| 000E88 mulld    7FE451D2   1     M         gr31=gr4,gr10
   42| 000E8C mulld    7FAAC1D2   1     M         gr29=gr10,gr24
   42| 000E90 mulld    7D6BD9D2   1     M         gr11=gr11,gr27
   42| 000E94 mulld    7D8AD9D2   1     M         gr12=gr10,gr27
   40| 000E98 add      7D5AFA14   1     A         gr10=gr26,gr31
   42| 000E9C add      7FBDB214   1     A         gr29=gr29,gr22
   40| 000EA0 add      7D4AA214   1     A         gr10=gr10,gr20
   42| 000EA4 add      7D6B8A14   1     A         gr11=gr11,gr17
   42| 000EA8 add      7D8C8A14   1     A         gr12=gr12,gr17
    0| 000EAC mtspr    7F2903A6   1     LCTR      ctr=gr25
   42| 000EB0 add      7FF3EA14   1     A         gr31=gr19,gr29
   42| 000EB4 add      7FBD9214   1     A         gr29=gr29,gr18
    0| 000EB8 bc       41820050   1     BT        CL.651,cr0,0x4/eq,taken=50%(0,0)
   40| 000EBC lfdux    7D2A1CEE   1     LFDU      fp9,gr10=v1(gr10,gr3,0)
   42| 000EC0 lfdux    7D6C2CEE   1     LFDU      fp11,gr12=v2(gr12,gr5,0)
   42| 000EC4 lfdux    7CAB2CEE   1     LFDU      fp5,gr11=v2(gr11,gr5,0)
   42| 000EC8 lfdux    7CDD34EE   1     LFDU      fp6,gr29=v3(gr29,gr6,0)
   42| 000ECC lfdux    7CFF34EE   1     LFDU      fp7,gr31=v3(gr31,gr6,0)
   42| 000ED0 fadd     FD04482A   1     AFL       fp8=fp4,fp9,fcr
   45| 000ED4 fmr      FC804890   2     LRFL      fp4=fp9
   45| 000ED8 fadd     FD4B282A   2     AFL       fp10=fp11,fp5,fcr
   45| 000EDC fadd     FD26382A   2     AFL       fp9=fp6,fp7,fcr
   42| 000EE0 fadd     FD68582A   2     AFL       fp11=fp8,fp11,fcr
   45| 000EE4 fmul     FD4A02B2   2     MFL       fp10=fp10,fp10,fcr
   42| 000EE8 fadd     FCAB282A   2     AFL       fp5=fp11,fp5,fcr
   45| 000EEC fmadd    FD08523A   2     FMA       fp8=fp10,fp8,fp8,fcr
   42| 000EF0 fadd     FCA5302A   2     AFL       fp5=fp5,fp6,fcr
   45| 000EF4 fmadd    FCC9427A   2     FMA       fp6=fp8,fp9,fp9,fcr
   42| 000EF8 fadd     FCA5382A   2     AFL       fp5=fp5,fp7,fcr
   45| 000EFC fmadd    FC66183A   2     FMA       fp3=fp3,fp6,fp0,fcr
   42| 000F00 fmadd    FFE5F87A   2     FMA       fp31=fp31,fp5,fp1,fcr
    0| 000F04 bc       419E0128   1     BT        CL.545,cr7,0x4/eq,taken=20%(20,80)
    0|                              CL.651:
   40| 000F08 lfdux    7CCA1CEE   1     LFDU      fp6,gr10=v1(gr10,gr3,0)
   42| 000F0C lfdux    7CEC2CEE   1     LFDU      fp7,gr12=v2(gr12,gr5,0)
   42| 000F10 lfdux    7F6B2CEE   1     LFDU      fp27,gr11=v2(gr11,gr5,0)
   42| 000F14 lfdux    7D7D34EE   1     LFDU      fp11,gr29=v3(gr29,gr6,0)
   42| 000F18 lfdux    7D3F34EE   1     LFDU      fp9,gr31=v3(gr31,gr6,0)
   42| 000F1C fadd     FD44302A   1     AFL       fp10=fp4,fp6,fcr
   40| 000F20 lfdux    7C8A1CEE   1     LFDU      fp4,gr10=v1(gr10,gr3,0)
   45| 000F24 fadd     FF87D82A   1     AFL       fp28=fp7,fp27,fcr
   42| 000F28 lfdux    7D0C2CEE   1     LFDU      fp8,gr12=v2(gr12,gr5,0)
   45| 000F2C fadd     FCAB482A   1     AFL       fp5=fp11,fp9,fcr
   42| 000F30 lfdux    7D8B2CEE   1     LFDU      fp12,gr11=v2(gr11,gr5,0)
   42| 000F34 fadd     FF4A382A   1     AFL       fp26=fp10,fp7,fcr
   42| 000F38 lfdux    7CFD34EE   1     LFDU      fp7,gr29=v3(gr29,gr6,0)
   42| 000F3C fadd     FDA6202A   1     AFL       fp13=fp6,fp4,fcr
   42| 000F40 lfdux    7CDF34EE   1     LFDU      fp6,gr31=v3(gr31,gr6,0)
   45| 000F44 fmul     FF3C0732   1     MFL       fp25=fp28,fp28,fcr
   45| 000F48 fadd     FF88602A   2     AFL       fp28=fp8,fp12,fcr
   42| 000F4C fadd     FF7AD82A   2     AFL       fp27=fp26,fp27,fcr
   42| 000F50 fadd     FF4D402A   2     AFL       fp26=fp13,fp8,fcr
   45| 000F54 fadd     FD07302A   2     AFL       fp8=fp7,fp6,fcr
   45| 000F58 fmadd    FD4ACABA   2     FMA       fp10=fp25,fp10,fp10,fcr
   45| 000F5C fmul     FF9C0732   2     MFL       fp28=fp28,fp28,fcr
   42| 000F60 fadd     FD7B582A   2     AFL       fp11=fp27,fp11,fcr
   42| 000F64 fadd     FD9A602A   2     AFL       fp12=fp26,fp12,fcr
   45| 000F68 fmadd    FDADE37A   2     FMA       fp13=fp28,fp13,fp13,fcr
   42| 000F6C fadd     FD2B482A   2     AFL       fp9=fp11,fp9,fcr
   42| 000F70 fadd     FD8C382A   2     AFL       fp12=fp12,fp7,fcr
    0| 000F74 bc       4240009C   1     BCF       ctr=CL.663,taken=0%(0,100)
    0| 000F78 ori      60210000   1     XNOP      
    0| 000F7C ori      60210000   1     XNOP      
    0| 000F80 ori      60210000   1     XNOP      
    0|                              CL.664:
   40| 000F84 lfdux    7D6A1CEE   1     LFDU      fp11,gr10=v1(gr10,gr3,0)
   45| 000F88 fmadd    FCA5517A   1     FMA       fp5=fp10,fp5,fp5,fcr
   42| 000F8C lfdux    7CFD34EE   1     LFDU      fp7,gr29=v3(gr29,gr6,0)
   42| 000F90 fadd     FF8C302A   1     AFL       fp28=fp12,fp6,fcr
   42| 000F94 fmadd    FFE9F87A   2     FMA       fp31=fp31,fp9,fp1,fcr
   45| 000F98 fmadd    FD086A3A   2     FMA       fp8=fp13,fp8,fp8,fcr
   42| 000F9C lfdux    7D3F34EE   1     LFDU      fp9,gr31=v3(gr31,gr6,0)
   42| 000FA0 fadd     FD44582A   1     AFL       fp10=fp4,fp11,fcr
   40| 000FA4 lfdux    7C8A1CEE   1     LFDU      fp4,gr10=v1(gr10,gr3,0)
   42| 000FA8 lfdux    7CCC2CEE   1     LFDU      fp6,gr12=v2(gr12,gr5,0)
   42| 000FAC lfdux    7F6B2CEE   1     LFDU      fp27,gr11=v2(gr11,gr5,0)
   45| 000FB0 fmadd    FC65183A   1     FMA       fp3=fp3,fp5,fp0,fcr
   45| 000FB4 fadd     FCA7482A   2     AFL       fp5=fp7,fp9,fcr
   42| 000FB8 fadd     FD6B202A   2     AFL       fp11=fp11,fp4,fcr
   42| 000FBC lfdux    7F4C2CEE   1     LFDU      fp26,gr12=v2(gr12,gr5,0)
   42| 000FC0 lfdux    7D8B2CEE   1     LFDU      fp12,gr11=v2(gr11,gr5,0)
   42| 000FC4 fadd     FF2A302A   1     AFL       fp25=fp10,fp6,fcr
   45| 000FC8 fadd     FCC6D82A   2     AFL       fp6=fp6,fp27,fcr
   42| 000FCC lfdux    7DBD34EE   1     LFDU      fp13,gr29=v3(gr29,gr6,0)
   42| 000FD0 fmadd    FFFCF87A   1     FMA       fp31=fp31,fp28,fp1,fcr
   42| 000FD4 fadd     FF8BD02A   2     AFL       fp28=fp11,fp26,fcr
   42| 000FD8 fadd     FF79D82A   2     AFL       fp27=fp25,fp27,fcr
   45| 000FDC fadd     FF5A602A   2     AFL       fp26=fp26,fp12,fcr
   45| 000FE0 fmul     FF2601B2   2     MFL       fp25=fp6,fp6,fcr
   42| 000FE4 lfdux    7CDF34EE   1     LFDU      fp6,gr31=v3(gr31,gr6,0)
   45| 000FE8 fmadd    FC68183A   1     FMA       fp3=fp3,fp8,fp0,fcr
   42| 000FEC fadd     FD9C602A   2     AFL       fp12=fp28,fp12,fcr
   42| 000FF0 fadd     FCFB382A   2     AFL       fp7=fp27,fp7,fcr
   45| 000FF4 fmul     FF9A06B2   2     MFL       fp28=fp26,fp26,fcr
   45| 000FF8 fmadd    FD4ACABA   2     FMA       fp10=fp25,fp10,fp10,fcr
   45| 000FFC fadd     FD0D302A   2     AFL       fp8=fp13,fp6,fcr
   42| 001000 fadd     FD8C682A   2     AFL       fp12=fp12,fp13,fcr
   42| 001004 fadd     FD27482A   2     AFL       fp9=fp7,fp9,fcr
   45| 001008 fmadd    FDABE2FA   2     FMA       fp13=fp28,fp11,fp11,fcr
    0| 00100C bc       4200FF78   1     BCT       ctr=CL.664,taken=100%(100,0)
    0|                              CL.663:
   45| 001010 fmadd    FC85517A   1     FMA       fp4=fp10,fp5,fp5,fcr
   45| 001014 fmadd    FCA86A3A   2     FMA       fp5=fp13,fp8,fp8,fcr
   42| 001018 fmadd    FCE9F87A   2     FMA       fp7=fp31,fp9,fp1,fcr
   42| 00101C fadd     FCCC302A   2     AFL       fp6=fp12,fp6,fcr
   45| 001020 fmadd    FC64183A   2     FMA       fp3=fp3,fp4,fp0,fcr
   42| 001024 fmadd    FFE6387A   2     FMA       fp31=fp7,fp6,fp1,fcr
   45| 001028 fmadd    FC65183A   2     FMA       fp3=fp3,fp5,fp0,fcr
    0|                              CL.545:
   49| 00102C addi     39080001   1     AI        gr8=gr8,1
   49| 001030 cmpld    7F28E040   1     CL8       cr6=gr8,gr28
   49| 001034 bc       4198FE48   1     BT        CL.89,cr6,0x8/llt,taken=80%(80,20)
   49|                              CL.88:
   50| 001038 ld       E9010108   1     L8        gr8=#SPILL12(gr1,264)
   50| 00103C addi     38E70001   1     AI        gr7=gr7,1
    0| 001040 add      7EB5F214   1     A         gr21=gr21,gr30
   50| 001044 cmpld    7F274040   1     CL8       cr6=gr7,gr8
   50| 001048 bc       4198FE00   1     BT        CL.87,cr6,0x8/llt,taken=80%(80,20)
   50|                              CL.86:
   52| 00104C fdiv     FC031024   1     DFL       fp0=fp3,fp2,fcr
   53| 001050 ld       E8620000   1     L8        gr3=.&&N&&mpipar(gr2,0)
   52| 001054 ld       E88100A8   1     L8        gr4=#SPILL0(gr1,168)
   53| 001058 lwz      80030008   1     L4Z       gr0=<s61:d8:l4>(gr3,8)
   53| 00105C cmpdi    2C200000   1     C8        cr0=gr0,0
   52| 001060 fsqrt    FC00002C   1     SQRT      fp0=fp0,fcr
   52| 001064 stfd     D8040000   1     STFL      rms(gr4,0)=fp0
   53| 001068 bc       4182007C   1     BT        CL.896,cr0,0x4/eq,taken=40%(40,60)
   61| 00106C lwa      E98102CA   1     L4A       gr12=#stack(gr1,712)
   61| 001070 ld       E9C101F8   1     L8        gr14=#stack(gr1,504)
   61| 001074 ld       E9E10200   1     L8        gr15=#stack(gr1,512)
   61| 001078 ld       EA010208   1     L8        gr16=#stack(gr1,520)
   61| 00107C ld       EA210210   1     L8        gr17=#stack(gr1,528)
   61| 001080 ld       EA410218   1     L8        gr18=#stack(gr1,536)
   61| 001084 ld       EA610220   1     L8        gr19=#stack(gr1,544)
   61| 001088 ld       EA810228   1     L8        gr20=#stack(gr1,552)
   61| 00108C ld       EAA10230   1     L8        gr21=#stack(gr1,560)
   61| 001090 ld       EAC10238   1     L8        gr22=#stack(gr1,568)
   61| 001094 ld       EAE10240   1     L8        gr23=#stack(gr1,576)
   61| 001098 ld       EB010248   1     L8        gr24=#stack(gr1,584)
   61| 00109C ld       EB210250   1     L8        gr25=#stack(gr1,592)
   61| 0010A0 ld       EB410258   1     L8        gr26=#stack(gr1,600)
   61| 0010A4 ld       EB610260   1     L8        gr27=#stack(gr1,608)
   61| 0010A8 ld       EB810268   1     L8        gr28=#stack(gr1,616)
   61| 0010AC ld       EBA10270   1     L8        gr29=#stack(gr1,624)
   61| 0010B0 ld       EBC10278   1     L8        gr30=#stack(gr1,632)
   61| 0010B4 ld       EBE10280   1     L8        gr31=#stack(gr1,640)
   61| 0010B8 mtcrf    7D820120   1     MTCRF     cr2=gr12
   61| 0010BC mtcrf    7D808120   1     MTCRF     cr4=gr12
   61| 0010C0 lfd      CBE102B8   1     LFL       fp31=#stack(gr1,696)
   61| 0010C4 lfd      CBC102B0   1     LFL       fp30=#stack(gr1,688)
   61| 0010C8 lfd      CBA102A8   1     LFL       fp29=#stack(gr1,680)
   61| 0010CC lfd      CB8102A0   1     LFL       fp28=#stack(gr1,672)
   61| 0010D0 lfd      CB610298   1     LFL       fp27=#stack(gr1,664)
   61| 0010D4 lfd      CB410290   1     LFL       fp26=#stack(gr1,656)
   61| 0010D8 lfd      CB210288   1     LFL       fp25=#stack(gr1,648)
   61| 0010DC addi     382102C0   1     AI        gr1=gr1,704
   61| 0010E0 bclr     4E800020   1     BA        lr
    0|                              CL.896:
   54| 0010E4 ld       EBE20000   1     L8        gr31=.$STATIC(gr2,0)
   54| 0010E8 addi     38000000   1     LI        gr0=0
   54| 0010EC addi     38600006   1     LI        gr3=6
   54| 0010F0 ori      601D8000   1     OIL       gr29=gr0,0x8000
   54| 0010F4 addi     38800101   1     LI        gr4=257
   54| 0010F8 or       7FA6EB78   1     LR        gr6=gr29
   54| 0010FC or       7FE5FB78   1     LR        gr5=gr31
   54| 001100 addi     38E00000   1     LI        gr7=0
   54| 001104 addi     39000000   1     LI        gr8=0
   54| 001108 addi     39200000   1     LI        gr9=0
   54| 00110C bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#1",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   54| 001110 ori      60000000   1
   54| 001114 ld       EB820000   1     L8        gr28=.+CONSTANT_AREA(gr2,0)
   54| 001118 or       7C7E1B78   1     LR        gr30=gr3
   54| 00111C addi     38A0000A   1     LI        gr5=10
   54| 001120 addi     38C00001   1     LI        gr6=1
   54| 001124 addi     389C002C   1     AI        gr4=gr28,44
   54| 001128 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   54| 00112C ori      60000000   1
   54| 001130 or       7FC3F378   1     LR        gr3=gr30
   54| 001134 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   54| 001138 ori      60000000   1
   55| 00113C addi     38BF0040   1     AI        gr5=gr31,64
   55| 001140 addi     38600006   1     LI        gr3=6
   55| 001144 addi     38800101   1     LI        gr4=257
   55| 001148 or       7FA6EB78   1     LR        gr6=gr29
   55| 00114C addi     38E00000   1     LI        gr7=0
   55| 001150 addi     39000000   1     LI        gr8=0
   55| 001154 addi     39200000   1     LI        gr9=0
   55| 001158 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#3",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   55| 00115C ori      60000000   1
   55| 001160 or       7C7E1B78   1     LR        gr30=gr3
   55| 001164 addi     389C0038   1     AI        gr4=gr28,56
   55| 001168 addi     38A00016   1     LI        gr5=22
   55| 00116C addi     38C00001   1     LI        gr6=1
   55| 001170 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   55| 001174 ori      60000000   1
   55| 001178 stfd     DBA10080   1     STFL      #5(gr1,128)=fp29
   55| 00117C or       7FC3F378   1     LR        gr3=gr30
   55| 001180 addi     38810080   1     AI        gr4=gr1,128
   55| 001184 addi     38A00008   1     LI        gr5=8
   55| 001188 addi     38C00008   1     LI        gr6=8
   55| 00118C bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,#5,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   55| 001190 ori      60000000   1
   55| 001194 or       7FC3F378   1     LR        gr3=gr30
   55| 001198 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   55| 00119C ori      60000000   1
   56| 0011A0 addi     38BF0080   1     AI        gr5=gr31,128
   56| 0011A4 or       7FA6EB78   1     LR        gr6=gr29
   56| 0011A8 addi     38600006   1     LI        gr3=6
   56| 0011AC addi     38800101   1     LI        gr4=257
   56| 0011B0 addi     38E00000   1     LI        gr7=0
   56| 0011B4 addi     39000000   1     LI        gr8=0
   56| 0011B8 addi     39200000   1     LI        gr9=0
   56| 0011BC bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#6",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   56| 0011C0 ori      60000000   1
   56| 0011C4 or       7C7E1B78   1     LR        gr30=gr3
   56| 0011C8 addi     389C0050   1     AI        gr4=gr28,80
   56| 0011CC addi     38A00016   1     LI        gr5=22
   56| 0011D0 addi     38C00001   1     LI        gr6=1
   56| 0011D4 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   56| 0011D8 ori      60000000   1
   56| 0011DC fcfid    FC00F69C   1     FCFID     fp0=fp30,fcr
   56| 0011E0 lfs      C05C0068   1     LFS       fp2=+CONSTANT_AREA(gr28,104)
   56| 0011E4 or       7FC3F378   1     LR        gr3=gr30
   56| 0011E8 addi     38810088   1     AI        gr4=gr1,136
   56| 0011EC addi     38A00008   1     LI        gr5=8
   56| 0011F0 addi     38C00008   1     LI        gr6=8
   56| 0011F4 frsp     FC000018   1     CVLS      fp0=fp0,fcr
   56| 0011F8 fmul     FC200032   2     MFL       fp1=fp0,fp0,fcr
   56| 0011FC fmul     FC000072   2     MFL       fp0=fp0,fp1,fcr
   56| 001200 frsp     FC000018   2     CVLS      fp0=fp0,fcr
   56| 001204 fmuls    EC0000B2   1     MFS       fp0=fp0,fp2,fcr
   56| 001208 fdiv     FC1F0024   1     DFL       fp0=fp31,fp0,fcr
   56| 00120C stfd     D8010088   1     STFL      #8(gr1,136)=fp0
   56| 001210 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,#8,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   56| 001214 ori      60000000   1
   56| 001218 or       7FC3F378   1     LR        gr3=gr30
   56| 00121C bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   56| 001220 ori      60000000   1
   57| 001224 addi     38BF00C0   1     AI        gr5=gr31,192
   57| 001228 or       7FA6EB78   1     LR        gr6=gr29
   57| 00122C addi     38600006   1     LI        gr3=6
   57| 001230 addi     38800101   1     LI        gr4=257
   57| 001234 addi     38E00000   1     LI        gr7=0
   57| 001238 addi     39000000   1     LI        gr8=0
   57| 00123C addi     39200000   1     LI        gr9=0
   57| 001240 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#9",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   57| 001244 ori      60000000   1
   57| 001248 or       7C7E1B78   1     LR        gr30=gr3
   57| 00124C addi     389C006C   1     AI        gr4=gr28,108
   57| 001250 addi     38A00016   1     LI        gr5=22
   57| 001254 addi     38C00001   1     LI        gr6=1
   57| 001258 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   57| 00125C ori      60000000   1
   57| 001260 or       7FC3F378   1     LR        gr3=gr30
   57| 001264 ld       E88100A8   1     L8        gr4=#SPILL0(gr1,168)
   57| 001268 addi     38A00008   1     LI        gr5=8
   57| 00126C addi     38C00008   1     LI        gr6=8
   57| 001270 bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,rms,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   57| 001274 ori      60000000   1
   57| 001278 or       7FC3F378   1     LR        gr3=gr30
   57| 00127C bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   57| 001280 ori      60000000   1
   58| 001284 addi     38600006   1     LI        gr3=6
   58| 001288 addi     38BF0100   1     AI        gr5=gr31,256
   58| 00128C or       7FA6EB78   1     LR        gr6=gr29
   58| 001290 addi     38800101   1     LI        gr4=257
   58| 001294 addi     38E00000   1     LI        gr7=0
   58| 001298 addi     39000000   1     LI        gr8=0
   58| 00129C addi     39200000   1     LI        gr9=0
   58| 0012A0 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#11",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   58| 0012A4 ori      60000000   1
   58| 0012A8 addi     389C002C   1     AI        gr4=gr28,44
   58| 0012AC or       7C7F1B78   1     LR        gr31=gr3
   58| 0012B0 addi     38A0000A   1     LI        gr5=10
   58| 0012B4 addi     38C00001   1     LI        gr6=1
   58| 0012B8 bl       48000001   1     CALL      _xlfWriteLDChar,4,gr3-gr6,_xlfWriteLDChar",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   58| 0012BC ori      60000000   1
   58| 0012C0 or       7FE3FB78   1     LR        gr3=gr31
   58| 0012C4 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   58| 0012C8 ori      60000000   1
   61| 0012CC ld       E80102D0   1     L8        gr0=#stack(gr1,720)
   61| 0012D0 lwa      E98102CA   1     L4A       gr12=#stack(gr1,712)
   61| 0012D4 lfd      CBE102B8   1     LFL       fp31=#stack(gr1,696)
   61| 0012D8 lfd      CBC102B0   1     LFL       fp30=#stack(gr1,688)
   61| 0012DC lfd      CBA102A8   1     LFL       fp29=#stack(gr1,680)
   61| 0012E0 lfd      CB8102A0   1     LFL       fp28=#stack(gr1,672)
   61| 0012E4 lfd      CB610298   1     LFL       fp27=#stack(gr1,664)
   61| 0012E8 lfd      CB410290   1     LFL       fp26=#stack(gr1,656)
   61| 0012EC lfd      CB210288   1     LFL       fp25=#stack(gr1,648)
   61| 0012F0 addi     382102C0   1     AI        gr1=gr1,704
   61| 0012F4 mtspr    7C0803A6   1     LLR       lr=gr0
   61| 0012F8 mtcrf    7D820120   1     MTCRF     cr2=gr12
   61| 0012FC mtcrf    7D808120   1     MTCRF     cr4=gr12
   61| 001300 ld       E9C1FF38   1     L8        gr14=#stack(gr1,-200)
   61| 001304 ld       E9E1FF40   1     L8        gr15=#stack(gr1,-192)
   61| 001308 ld       EA01FF48   1     L8        gr16=#stack(gr1,-184)
   61| 00130C ld       EA21FF50   1     L8        gr17=#stack(gr1,-176)
   61| 001310 ld       EA41FF58   1     L8        gr18=#stack(gr1,-168)
   61| 001314 ld       EA61FF60   1     L8        gr19=#stack(gr1,-160)
   61| 001318 ld       EA81FF68   1     L8        gr20=#stack(gr1,-152)
   61| 00131C ld       EAA1FF70   1     L8        gr21=#stack(gr1,-144)
   61| 001320 ld       EAC1FF78   1     L8        gr22=#stack(gr1,-136)
   61| 001324 ld       EAE1FF80   1     L8        gr23=#stack(gr1,-128)
   61| 001328 ld       EB01FF88   1     L8        gr24=#stack(gr1,-120)
   61| 00132C ld       EB21FF90   1     L8        gr25=#stack(gr1,-112)
   61| 001330 ld       EB41FF98   1     L8        gr26=#stack(gr1,-104)
   61| 001334 ld       EB61FFA0   1     L8        gr27=#stack(gr1,-96)
   61| 001338 ld       EB81FFA8   1     L8        gr28=#stack(gr1,-88)
   61| 00133C ld       EBA1FFB0   1     L8        gr29=#stack(gr1,-80)
   61| 001340 ld       EBC1FFB8   1     L8        gr30=#stack(gr1,-72)
   61| 001344 ld       EBE1FFC0   1     L8        gr31=#stack(gr1,-64)
   61| 001348 bclr     4E800020   1     BA        lr
   33|                              CL.400:
    0| 00134C ld       E9210198   1     L8        gr9=#SPILL30(gr1,408)
    0| 001350 mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 001354 or       7D284B78   1     LR        gr8=gr9
   33|                              CL.289:
   33| 001358 addi     38E70001   1     AI        gr7=gr7,1
   33| 00135C cmpd     7C274000   1     C8        cr0=gr7,gr8
   33| 001360 bc       4100FFF8   1     BCTT      ctr=CL.289,cr0,0x1/lt,taken=80%(80,20)
    0| 001364 b        4BFFF824   1     B         CL.105,-1
   32|                              CL.390:
    0| 001368 ld       E92101A0   1     L8        gr9=#SPILL31(gr1,416)
    0| 00136C mtspr    7D2903A6   1     LCTR      ctr=gr9
    0| 001370 or       7D284B78   1     LR        gr8=gr9
   32|                              CL.319:
   32| 001374 addi     38E70001   1     AI        gr7=gr7,1
   32| 001378 cmpd     7C274000   1     C8        cr0=gr7,gr8
   32| 00137C bc       4100FFF8   1     BCTT      ctr=CL.319,cr0,0x1/lt,taken=80%(80,20)
    0| 001380 b        4BFFF528   1     B         CL.117,-1
   31|                              CL.380:
    0| 001384 mtspr    7D4903A6   1     LCTR      ctr=gr10
    0| 001388 or       7D485378   1     LR        gr8=gr10
   31|                              CL.349:
   31| 00138C addi     38E70001   1     AI        gr7=gr7,1
   31| 001390 cmpd     7C274000   1     C8        cr0=gr7,gr8
   31| 001394 bc       4100FFF8   1     BCTT      ctr=CL.349,cr0,0x1/lt,taken=80%(80,20)
    0| 001398 b        4BFFF23C   1     B         CL.129,-1
     |               Tag Table
     | 00139C        00000000 00012203 87120000 0000139C
     |               Instruction count         1255
     |               Straight-line exec time   1433
     |               Constant Area
     | 000000        6E6F726D 76656C2E 66393049 00000000 3E800000 3F800000
     | 000018        BF800000 BEA00000 3EC00000 BF000000 3F000000 4E4F524D
     | 000030        56454C20 203A4942 4E4F524D 56454C20 203A204E 6F726D20
     | 000048        20202020 3D204942 4E4F524D 56454C20 203A2056 5F617620
     | 000060        20202020 3D204942 40400000 4E4F524D 56454C20 203A2056
     | 000078        5F726D73 20202020 3D20

 
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    normvel.f90                 07/08/15   15:48:57
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    0           0         0           0          0            0
 
 
    Source records read.......................................      61
1501-510  Compilation successful for file normvel.f90.
1501-543  Object file created.
