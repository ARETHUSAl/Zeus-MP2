IBM XL Fortran for Blue Gene, V14.1 (5799-AH1) Version 14.01.0000.0012 --- textdmp2.f90 07/08/15 15:48:46
 
>>>>> OPTIONS SECTION <<<<<
***   Options In Effect   ***
  
         ==  On / Off Options  ==
         CR              NODIRECTSTORAG  ESCAPE          I4
         INLGLUE         NOLIBESSL       NOLIBPOSIX      OBJECT
         SWAPOMP         THREADED        UNWIND          ZEROSIZE
  
         ==  Options Of Integer Type ==
         ALIAS_SIZE(65536)     MAXMEM(-1)            OPTIMIZE(3)
         SPILLSIZE(512)        STACKTEMP(0)
  
         ==  Options of Integer and Character Type ==
         CACHE(LEVEL(1),TYPE(I),ASSOC(4),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(I),ASSOC(16),COST(80),LINE(128))
         CACHE(LEVEL(1),TYPE(D),ASSOC(8),COST(6),LINE(64),SIZE(16))
         CACHE(LEVEL(2),TYPE(D),ASSOC(16),COST(80),LINE(128))
         INLINE(NOAUTO,LEVEL(5))
         HOT(FASTMATH,LEVEL(0))
         SIMD(AUTO)
  
         ==  Options Of Character Type  ==
         64()                  ALIAS(STD,NOINTPTR)   ALIGN(BINDC(LINUXPPC),STRUCT(NATURAL))
         ARCH(QP)              AUTODBL(NONE)         DESCRIPTOR(V1)
         DIRECTIVE(IBM*,IBMT)  ENUM()                FLAG(I,I)
         FLOAT(MAF,FOLD,RSQRT,FLTINT)
         FREE(F90)             GNU_VERSION(DOT_TRIPLE)
         HALT(S)               IEEE(NEAR)            INTSIZE(4)
         LIST()                LANGLVL(EXTENDED)     REALSIZE(4)
         REPORT(HOTLIST)       STRICT(NONE,NOPRECISION,NOEXCEPTIONS,NOIEEEFP,NONANS,NOINFINITIES,NOSUBNORMALS,NOZEROSIGNS,NOOPERATIONPRECISION,ORDER,NOLIBRARY,NOCONSTRUCTCOPY,NOVECTORPRECISION)
         TUNE(QP)              UNROLL(AUTO)          XFLAG()
         XLF2003(NOPOLYMORPHIC,NOBOZLITARGS,NOSIGNDZEROINTR,NOSTOPEXCEPT,NOVOLATILE,NOAUTOREALLOC,OLDNANINF)
         XLF2008(NOCHECKPRESENCE)
         XLF77(LEADZERO,BLANKPAD)
         XLF90(SIGNEDZERO,AUTODEALLOC)
  
>>>>> SOURCE SECTION <<<<<
 "textdmp2.f90", line 105.1: 1515-021 (E) Syntax error: Token " & " is expected.
** textdmp2   === End of Compilation 1 ===
 
>>>>> LOOP TRANSFORMATION SECTION <<<<<

1586-534 (I) Loop (loop index 1) at textdmp2.f90 <line 107> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 2) at textdmp2.f90 <line 107> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_6 = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] * ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_8 = ( 7.6000000000000000E-001 * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_10 = (fh * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_5 = ((gamm1 * abs(((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) *  1.2027073445076573E-008) / ((((((((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[3ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) * ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_7 = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  1.0000000000000000E-005; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_4 = ((((((((((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[7ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[8ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[9ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) *  6.0221739328721606E+023; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_9 = (( 2.3999999999999999E-001 * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable strides.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_11 = ((( 1.0000000000000000E+000 - fh) * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable strides.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_6 = ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] * ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] * ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-d%addr  + d-d%rvo + (d-d%bounds%mult[].off48)*((long long) ks + $$CIV2) + (d-d%bounds%mult[].off72)*((long long) js + $$CIV1) + (d-d%bounds%mult[].off96)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_3 = ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns1.[(long long) is + $$CIV0] *  3.2425420493143010E-019; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ((double *)((char *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns1.[(long long) is + $$CIV0] *  3.2425420493143010E-019 which is not suitable for SIMD vectorization.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_8 = ( 7.6000000000000000E-001 * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ( 7.6000000000000000E-001 * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-abun%addr  + d-abun%rvo + (d-abun%bounds%mult[].off2128)*(10ll) + (d-abun%bounds%mult[].off2152)*((long long) ks + $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_10 = (fh * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in (fh * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-abun%addr  + d-abun%rvo + (d-abun%bounds%mult[].off2128)*(10ll) + (d-abun%bounds%mult[].off2152)*((long long) ks + $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_5 = ((gamm1 * abs(((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) *  1.2027073445076573E-008) / ((((((((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[3ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) * ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]); with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ((gamm1 * abs(((double *)((char *)d-e%addr  + d-e%rvo))->e[].rns4.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0])) *  1.2027073445076573E-008) / ((((((((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[3ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) * ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-e%addr  + d-e%rvo + (d-e%bounds%mult[].off152)*((long long) ks + $$CIV2) + (d-e%bounds%mult[].off176)*((long long) js + $$CIV1) + (d-e%bounds%mult[].off200)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_7 = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  1.0000000000000000E-005; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  1.0000000000000000E-005 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*((long long) ks + $$CIV2) + (d-v1%bounds%mult[].off488)*((long long) js + $$CIV1) + (d-v1%bounds%mult[].off512)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_4 = ((((((((((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[7ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[8ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[9ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) *  6.0221739328721606E+023; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ((((((((((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  2.5000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[7ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[8ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) + ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[9ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] *  5.0000000000000000E-001) * ((double *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) *  6.0221739328721606E+023 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-abun%addr  + d-abun%rvo + (d-abun%bounds%mult[].off2128)*(1ll) + (d-abun%bounds%mult[].off2152)*((long long) ks + $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_9 = (( 2.3999999999999999E-001 * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in (( 2.3999999999999999E-001 * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-abun%addr  + d-abun%rvo + (d-abun%bounds%mult[].off2128)*(10ll) + (d-abun%bounds%mult[].off2152)*((long long) ks + $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references T_11 = ((( 1.0000000000000000E+000 - fh) * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains operation in ((( 1.0000000000000000E+000 - fh) * ( 1.0000000000000000E+000 - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0]) - ((double *)((char *)d-abun%addr  + d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + $$CIV2][(long long) js + $$CIV1][(long long) is + $$CIV0] which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because it contains memory references ((char *)d-abun%addr  + d-abun%rvo + (d-abun%bounds%mult[].off2128)*(10ll) + (d-abun%bounds%mult[].off2152)*((long long) ks + $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long long) is + $$CIV0)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 3) at textdmp2.f90 <line 107> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-534 (I) Loop (loop index 4) at textdmp2.f90 <line 437> was not SIMD vectorized because the loop is not the innermost loop.
1586-534 (I) Loop (loop index 5) at textdmp2.f90 <line 437> was not SIMD vectorized because the loop is not the innermost loop.
1586-535 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because the aliasing-induced dependence prevents SIMD vectorization.
1586-540 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because it contains memory references T_13 = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV5][(long long) js + $$CIV4][(long long) is + $$CIV3] *  1.0000000000000000E-005; with non-vectorizable strides.
1586-536 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because it contains memory references T_12 = ((double *)((char *)d-x1a%addr  + d-x1a%rvo))->x1a[].rns7.[(long long) is + $$CIV3] *  3.2425420493143010E-019; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because it contains operation in ((double *)((char *)d-x1a%addr  + d-x1a%rvo))->x1a[].rns7.[(long long) is + $$CIV3] *  3.2425420493143010E-019 which is not suitable for SIMD vectorization.
1586-549 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-536 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because it contains memory references T_13 = ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV5][(long long) js + $$CIV4][(long long) is + $$CIV3] *  1.0000000000000000E-005; with non-vectorizable alignment.
1586-537 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because it contains operation in ((double *)((char *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV5][(long long) js + $$CIV4][(long long) is + $$CIV3] *  1.0000000000000000E-005 which is not suitable for SIMD vectorization.
1586-540 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because it contains memory references ((char *)d-v1%addr  + d-v1%rvo + (d-v1%bounds%mult[].off464)*((long long) ks + $$CIV5) + (d-v1%bounds%mult[].off488)*((long long) js + $$CIV4) + (d-v1%bounds%mult[].off512)*((long long) is + $$CIV3)) with non-vectorizable strides.
1586-549 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because a data dependence  prevents SIMD vectorization.
1586-554 (I) Loop (loop index 6) at textdmp2.f90 <line 437> was not SIMD vectorized because the floating point operation is not vectorizable under -qstrict.
1586-543 (I) <SIMD info> Total number of the innermost loops considered <"2">. Total number of the innermost loops SIMD vectorized <"0">.


     9|         SUBROUTINE textdmp2 ()
    43|           |speclist%version = 129
                  |speclist%spec_list%spec_key.off8 = 16
                  |speclist%spec_list%spec_value_addr.off16 = loc(usrfile)
                  |speclist%spec_list%spec_value_val.off24 = 16
                  |speclist%spec_list%spec_key.off32 = 118
                  |speclist%spec_list%spec_val_i4.off40 = 43
                  |speclist%spec_list%spec_key.off56 = 120
                  |speclist%spec_list%spec_value_addr.off64 = "textdmp2.f90"
                  |speclist%spec_list%spec_value_val.off72 = 12
                  |speclist%spec_count = 3
                  _xlfIOCmd(12,0,|speclist,268435456,NULL)
   102|           #2 = _xlfBeginIO(12,259,#1,32772,NULL,0,|1)
                  T_2 = time *  3.1709791983764585E-008
                  CALL _xlfWriteFmt(%VAL(#2),T_2,8,8,4)
                  _xlfEndIO(%VAL(#2))
   103|           #4 = _xlfBeginIO(12,257,#3,32768,NULL,0,NULL)
                  CALL _xlfWriteLDInt(%VAL(#4),nhy,4,4)
                  CALL _xlfWriteLDReal(%VAL(#4),dt,8,8)
                  _xlfEndIO(%VAL(#4))
   104|           |fmt_args%version = 129
                  |fmt_args%fmt_string_addr = NULL
                  |fmt_args%fmt_string_len = int(("textdmp2.f90 + 420))
                  |fmt_args%stack_frame = NULL
                  |fmt_args%call_back = NULL
                  #6 = _xlfBeginIO(12,259,#5,32772,NULL,0,|fmt_args)
                  _xlfEndIO(%VAL(#6))
   106|           |fmt_args%version = 129
                  |fmt_args%fmt_string_addr = NULL
                  |fmt_args%fmt_string_len = int(("textdmp2.f90 + 564))
                  |fmt_args%stack_frame = NULL
                  |fmt_args%call_back = NULL
                  #8 = _xlfBeginIO(12,259,#7,32772,NULL,0,|fmt_args)
                  _xlfEndIO(%VAL(#8))
   107|           #10 = _xlfBeginIO(12,259,#9,32772,NULL,0,|3)
                  IF (.FALSE.) GOTO lab_25
                  $$CIV2 = 0
       Id=1       DO $$CIV2 = $$CIV2, 0
                    IF (.FALSE.) GOTO lab_27
                    $$CIV1 = 0
       Id=2         DO $$CIV1 = $$CIV1, 0
                      IF ((1 + (int((ie + 2)) - int(is)) > 0)) THEN
                        $$CIV0 = 0
       Id=3             DO $$CIV0 = $$CIV0, int((1 + (int((ie + 2)) - int(is))&
                &           ))-1
                          T_3 = d-x1b%addr%x1b(int(is) + $$CIV0) *  &
                &           3.2425420493143010E-019
                          CALL _xlfWriteFmt(%VAL(#10),T_3,8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-d%addr + (%VAL(d-d%rvo)&
                &           ) + (%VAL(d-d%bounds%mult[].off48))*(int(%VAL(ks)) + &
                &           %VAL($$CIV2)) + (%VAL(d-d%bounds%mult[].off72))*(int(&
                &           %VAL(js)) + %VAL($$CIV1)) + (%VAL(&
                &           d-d%bounds%mult[].off96))*(int(%VAL(is)) + %VAL(&
                &           $$CIV0))),8,8,4)
                          T_4 = ((((((((d-abun%addr%abun(int(is) + $$CIV0,int(&
                &           js) + $$CIV1,int(ks) + $$CIV2,1) + d-abun%addr%abun(&
                &           int(is) + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,2))&
                &            + d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,5) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,7)) + d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,8) *  &
                &           5.0000000000000000E-001) + d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,9) *  &
                &           5.0000000000000000E-001) * d-d%addr%d(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2)) *  &
                &           6.0221739328721606E+023
                          CALL _xlfWriteFmt(%VAL(#10),T_4,8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-e%addr + (%VAL(d-e%rvo)&
                &           ) + (%VAL(d-e%bounds%mult[].off152))*(int(%VAL(ks)) + &
                &           %VAL($$CIV2)) + (%VAL(d-e%bounds%mult[].off176))*(int(&
                &           %VAL(js)) + %VAL($$CIV1)) + (%VAL(&
                &           d-e%bounds%mult[].off200))*(int(%VAL(is)) + %VAL(&
                &           $$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-tgas%addr + (%VAL(&
                &           d-tgas%rvo)) + (%VAL(d-tgas%bounds%mult[].off9856))*(&
                &           int(%VAL(ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-tgas%bounds%mult[].off9880))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-tgas%bounds%mult[].off9904))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          T_5 = ((gamm1 * abs(d-e%addr%e(int(is) + $$CIV0,int(&
                &           js) + $$CIV1,int(ks) + $$CIV2))) *  &
                &           1.2027073445076573E-008) / ((((((d-abun%addr%abun(int(&
                &           is) + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,1) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,2)) + d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,3)) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,5) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6) *  2.5000000000000000E-001) * &
                &           d-d%addr%d(int(is) + $$CIV0,int(js) + $$CIV1,int(ks) &
                &           + $$CIV2))
                          CALL _xlfWriteFmt(%VAL(#10),T_5,8,8,4)
                          T_6 = d-d%addr%d(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2) * d-abun%addr%abun(int(is) + $$CIV0,&
                &           int(js) + $$CIV1,int(ks) + $$CIV2,10)
                          CALL _xlfWriteFmt(%VAL(#10),T_6,8,8,4)
                          T_7 = d-v1%addr%v1(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2) *  1.0000000000000000E-005
                          CALL _xlfWriteFmt(%VAL(#10),T_7,8,8,4)
                          T_8 = ( 7.6000000000000000E-001 * ( &
                &           1.0000000000000000E+000 - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,10)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,1)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,2)
                          CALL _xlfWriteFmt(%VAL(#10),T_8,8,8,4)
                          T_9 = (( 2.3999999999999999E-001 * ( &
                &           1.0000000000000000E+000 - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,10)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,5)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6)
                          CALL _xlfWriteFmt(%VAL(#10),T_9,8,8,4)
                          T_10 = (fh * ( 1.0000000000000000E+000 - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,10)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,1)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,2)
                          CALL _xlfWriteFmt(%VAL(#10),T_10,8,8,4)
                          T_11 = ((( 1.0000000000000000E+000 - fh) * ( &
                &           1.0000000000000000E+000 - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,10)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,5)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6)
                          CALL _xlfWriteFmt(%VAL(#10),T_11,8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           1) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           2) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           3) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           4) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           5) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           6) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           10) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                        ENDDO
                      ENDIF
                    ENDDO
                    lab_27
                  ENDDO
                  lab_25
                  _xlfEndIO(%VAL(#10))
   434|           |fmt_args%version = 129
                  |fmt_args%fmt_string_addr = NULL
                  |fmt_args%fmt_string_len = int(("textdmp2.f90 + 676))
                  |fmt_args%stack_frame = NULL
                  |fmt_args%call_back = NULL
                  #12 = _xlfBeginIO(12,259,#11,32772,NULL,0,|fmt_args)
                  _xlfEndIO(%VAL(#12))
   437|           #14 = _xlfBeginIO(12,259,#13,32772,NULL,0,|2)
                  IF (.FALSE.) GOTO lab_31
                  $$CIV5 = 0
       Id=4       DO $$CIV5 = $$CIV5, 0
                    IF (.FALSE.) GOTO lab_33
                    $$CIV4 = 0
       Id=5         DO $$CIV4 = $$CIV4, 0
                      IF ((1 + (int(ie) - int(is)) > 0)) THEN
                        $$CIV3 = 0
       Id=6             DO $$CIV3 = $$CIV3, int((1 + (int(ie) - int(is))))-1
                          T_12 = d-x1a%addr%x1a(int(is) + $$CIV3) *  &
                &           3.2425420493143010E-019
                          CALL _xlfWriteFmt(%VAL(#14),T_12,8,8,4)
                          T_13 = d-v1%addr%v1(int(is) + $$CIV3,int(js) + $$CIV4,&
                &           int(ks) + $$CIV5) *  1.0000000000000000E-005
                          CALL _xlfWriteFmt(%VAL(#14),T_13,8,8,4)
                        ENDDO
                      ENDIF
                    ENDDO
                    lab_33
                  ENDDO
                  lab_31
                  _xlfEndIO(%VAL(#14))
   453|           _xlfIOCmd(12,1,#15,0,NULL)
   456|           RETURN
                END SUBROUTINE textdmp2


Source        Source        Loop Id       Action / Information                                      
File          Line                                                                                  
----------    ----------    ----------    ----------------------------------------------------------
         0           107             1    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           107             2    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           107             3    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_3 = ((double *)((char 
                                          *)d-x1b%addr  + d-x1b%rvo))->x1b[].rns1.[(long long) 
                                          is + $$CIV0] *  3.2425420493143010E-019;  with 
                                          non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-x1b%addr  + 
                                          d-x1b%rvo))->x1b[].rns1.[(long long) is + $$CIV0] *  
                                          3.2425420493143010E-019 which is not  suitable for 
                                          SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_4 = ((((((((((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[7ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[8ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  5.0000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[9ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  5.0000000000000000E-001) * ((double 
                                          *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long 
                                          long) ks + $$CIV2][(long long) js + $$CIV1][(long 
                                          long) is + $$CIV0]) *  6.0221739328721606E+023;  with 
                                          non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ((((((((((double *)((char *)d-abun%addr  
                                          + d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[7ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[8ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  5.0000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[9ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  5.0000000000000000E-001) * ((double 
                                          *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long 
                                          long) ks + $$CIV2][(long long) js + $$CIV1][(long 
                                          long) is + $$CIV0]) *  6.0221739328721606E+023 which 
                                          is not  suitable for SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-abun%addr  + d-abun%rvo 
                                          + (d-abun%bounds%mult[].off2128)*(1ll) + 
                                          (d-abun%bounds%mult[].off2152)*((long long) ks + 
                                          $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) 
                                          js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_5 = ((gamm1 * abs(((double 
                                          *)((char *)d-e%addr  + d-e%rvo))->e[].rns4.[(long 
                                          long) ks + $$CIV2][(long long) js + $$CIV1][(long 
                                          long) is + $$CIV0])) *  1.2027073445076573E-008) / 
                                          ((((((((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[3ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) * ((double 
                                          *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long 
                                          long) ks + $$CIV2][(long long) js + $$CIV1][(long 
                                          long) is + $$CIV0]);  with non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ((gamm1 * abs(((double *)((char 
                                          *)d-e%addr  + d-e%rvo))->e[].rns4.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0])) *  1.2027073445076573E-008) / 
                                          ((((((((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[3ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) + ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) + ((double 
                                          *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  2.5000000000000000E-001) * ((double 
                                          *)((char *)d-d%addr  + d-d%rvo))->d[].rns2.[(long 
                                          long) ks + $$CIV2][(long long) js + $$CIV1][(long 
                                          long) is + $$CIV0]) which is not  suitable for SIMD 
                                          vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-e%addr  + d-e%rvo + 
                                          (d-e%bounds%mult[].off152)*((long long) ks + $$CIV2) 
                                          + (d-e%bounds%mult[].off176)*((long long) js + 
                                          $$CIV1) + (d-e%bounds%mult[].off200)*((long long) is 
                                          + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_6 = ((double *)((char *)d-d%addr  
                                          + d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] * 
                                          ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0];  with non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-d%addr  + 
                                          d-d%rvo))->d[].rns2.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] * 
                                          ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] which is not  suitable for SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-d%addr  + d-d%rvo + 
                                          (d-d%bounds%mult[].off48)*((long long) ks + $$CIV2) + 
                                          (d-d%bounds%mult[].off72)*((long long) js + $$CIV1) + 
                                          (d-d%bounds%mult[].off96)*((long long) is + $$CIV0)) 
                                          with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_7 = ((double *)((char *)d-v1%addr 
                                          + d-v1%rvo))->v1[].rns6.[(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] *  1.0000000000000000E-005;  with 
                                          non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV2][(long 
                                          long) js + $$CIV1][(long long) is + $$CIV0] *  
                                          1.0000000000000000E-005 which is not  suitable for 
                                          SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*((long long) ks + $$CIV2) 
                                          + (d-v1%bounds%mult[].off488)*((long long) js + 
                                          $$CIV1) + (d-v1%bounds%mult[].off512)*((long long) is 
                                          + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_8 = ( 7.6000000000000000E-001 * ( 
                                          1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0];  with non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ( 7.6000000000000000E-001 * ( 
                                          1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] which is not  suitable for SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-abun%addr  + d-abun%rvo 
                                          + (d-abun%bounds%mult[].off2128)*(10ll) + 
                                          (d-abun%bounds%mult[].off2152)*((long long) ks + 
                                          $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) 
                                          js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_9 = (( 2.3999999999999999E-001 * 
                                          ( 1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0];  with non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in (( 2.3999999999999999E-001 * ( 
                                          1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] which is not  suitable for SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-abun%addr  + d-abun%rvo 
                                          + (d-abun%bounds%mult[].off2128)*(10ll) + 
                                          (d-abun%bounds%mult[].off2152)*((long long) ks + 
                                          $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) 
                                          js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_10 = (fh * ( 
                                          1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0];  with non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in (fh * ( 1.0000000000000000E+000 - 
                                          ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[1ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[2ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] which is not  suitable for SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-abun%addr  + d-abun%rvo 
                                          + (d-abun%bounds%mult[].off2128)*(10ll) + 
                                          (d-abun%bounds%mult[].off2152)*((long long) ks + 
                                          $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) 
                                          js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references T_11 = ((( 1.0000000000000000E+000 
                                          - fh) * ( 1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0];  with non-vectorizable alignment.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          operation in ((( 1.0000000000000000E+000 - fh) * ( 
                                          1.0000000000000000E+000 - ((double *)((char 
                                          *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[10ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[4ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[5ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0]) - ((double *)((char *)d-abun%addr  + 
                                          d-abun%rvo))->abun[].rns3.[6ll][(long long) ks + 
                                          $$CIV2][(long long) js + $$CIV1][(long long) is + 
                                          $$CIV0] which is not  suitable for SIMD vectorization.
         0           107                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-abun%addr  + d-abun%rvo 
                                          + (d-abun%bounds%mult[].off2128)*(10ll) + 
                                          (d-abun%bounds%mult[].off2152)*((long long) ks + 
                                          $$CIV2) + (d-abun%bounds%mult[].off2176)*((long long) 
                                          js + $$CIV1) + (d-abun%bounds%mult[].off2200)*((long 
                                          long) is + $$CIV0)) with  non-vectorizable strides.
         0           107                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           107                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           437             4    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           437             5    Loop was not SIMD vectorized because the loop is not 
                                          the innermost loop.
         0           437             6    Loop was not SIMD vectorized because the 
                                          aliasing-induced dependence  prevents SIMD 
                                          vectorization.
         0           437                  Loop was not SIMD vectorized because it contains 
                                          memory references T_12 = ((double *)((char 
                                          *)d-x1a%addr  + d-x1a%rvo))->x1a[].rns7.[(long long) 
                                          is + $$CIV3] *  3.2425420493143010E-019;  with 
                                          non-vectorizable alignment.
         0           437                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-x1a%addr  + 
                                          d-x1a%rvo))->x1a[].rns7.[(long long) is + $$CIV3] *  
                                          3.2425420493143010E-019 which is not  suitable for 
                                          SIMD vectorization.
         0           437                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           437                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.
         0           437                  Loop was not SIMD vectorized because it contains 
                                          memory references T_13 = ((double *)((char 
                                          *)d-v1%addr  + d-v1%rvo))->v1[].rns6.[(long long) ks 
                                          + $$CIV5][(long long) js + $$CIV4][(long long) is + 
                                          $$CIV3] *  1.0000000000000000E-005;  with 
                                          non-vectorizable alignment.
         0           437                  Loop was not SIMD vectorized because it contains 
                                          operation in ((double *)((char *)d-v1%addr  + 
                                          d-v1%rvo))->v1[].rns6.[(long long) ks + $$CIV5][(long 
                                          long) js + $$CIV4][(long long) is + $$CIV3] *  
                                          1.0000000000000000E-005 which is not  suitable for 
                                          SIMD vectorization.
         0           437                  Loop was not SIMD vectorized because it contains 
                                          memory references ((char *)d-v1%addr  + d-v1%rvo + 
                                          (d-v1%bounds%mult[].off464)*((long long) ks + $$CIV5) 
                                          + (d-v1%bounds%mult[].off488)*((long long) js + 
                                          $$CIV4) + (d-v1%bounds%mult[].off512)*((long long) is 
                                          + $$CIV3)) with  non-vectorizable strides.
         0           437                  Loop was not SIMD vectorized because a data 
                                          dependence  prevents SIMD  vectorization.
         0           437                  Loop was not SIMD vectorized because the floating 
                                          point operation is not  vectorizable under -qstrict.


     9|         SUBROUTINE textdmp2 ()
    43|           |speclist%version = 129
                  |speclist%spec_list%spec_key.off8 = 16
                  |speclist%spec_list%spec_value_addr.off16 = loc(usrfile)
                  |speclist%spec_list%spec_value_val.off24 = 16
                  |speclist%spec_list%spec_key.off32 = 118
                  |speclist%spec_list%spec_val_i4.off40 = 43
                  |speclist%spec_list%spec_key.off56 = 120
                  |speclist%spec_list%spec_value_addr.off64 = "textdmp2.f90"
                  |speclist%spec_list%spec_value_val.off72 = 12
                  |speclist%spec_count = 3
                  _xlfIOCmd(12,0,|speclist,268435456,NULL)
   102|           #2 = _xlfBeginIO(12,259,#1,32772,NULL,0,|1)
                  T_2 = time *  3.1709791983764585E-008
                  CALL _xlfWriteFmt(%VAL(#2),T_2,8,8,4)
                  _xlfEndIO(%VAL(#2))
   103|           #4 = _xlfBeginIO(12,257,#3,32768,NULL,0,NULL)
                  CALL _xlfWriteLDInt(%VAL(#4),nhy,4,4)
                  CALL _xlfWriteLDReal(%VAL(#4),dt,8,8)
                  _xlfEndIO(%VAL(#4))
   104|           |fmt_args%version = 129
                  |fmt_args%fmt_string_addr = NULL
                  |fmt_args%fmt_string_len = int(("textdmp2.f90 + 420))
                  |fmt_args%stack_frame = NULL
                  |fmt_args%call_back = NULL
                  #6 = _xlfBeginIO(12,259,#5,32772,NULL,0,|fmt_args)
                  _xlfEndIO(%VAL(#6))
   106|           |fmt_args%version = 129
                  |fmt_args%fmt_string_addr = NULL
                  |fmt_args%fmt_string_len = int(("textdmp2.f90 + 564))
                  |fmt_args%stack_frame = NULL
                  |fmt_args%call_back = NULL
                  #8 = _xlfBeginIO(12,259,#7,32772,NULL,0,|fmt_args)
                  _xlfEndIO(%VAL(#8))
   107|           #10 = _xlfBeginIO(12,259,#9,32772,NULL,0,|3)
                  IF (.FALSE.) GOTO lab_25
                  $$CIV2 = 0
       Id=1       DO $$CIV2 = $$CIV2, 0
                    IF (.FALSE.) GOTO lab_27
                    $$CIV1 = 0
       Id=2         DO $$CIV1 = $$CIV1, 0
                      IF ((1 + (int((ie + 2)) - int(is)) > 0)) THEN
                        $$CIV0 = 0
       Id=3             DO $$CIV0 = $$CIV0, int((1 + (int((ie + 2)) - int(is))&
                &           ))-1
                          T_3 = d-x1b%addr%x1b(int(is) + $$CIV0) *  &
                &           3.2425420493143010E-019
                          CALL _xlfWriteFmt(%VAL(#10),T_3,8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-d%addr + (%VAL(d-d%rvo)&
                &           ) + (%VAL(d-d%bounds%mult[].off48))*(int(%VAL(ks)) + &
                &           %VAL($$CIV2)) + (%VAL(d-d%bounds%mult[].off72))*(int(&
                &           %VAL(js)) + %VAL($$CIV1)) + (%VAL(&
                &           d-d%bounds%mult[].off96))*(int(%VAL(is)) + %VAL(&
                &           $$CIV0))),8,8,4)
                          T_4 = ((((((((d-abun%addr%abun(int(is) + $$CIV0,int(&
                &           js) + $$CIV1,int(ks) + $$CIV2,1) + d-abun%addr%abun(&
                &           int(is) + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,2))&
                &            + d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,5) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,7)) + d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,8) *  &
                &           5.0000000000000000E-001) + d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,9) *  &
                &           5.0000000000000000E-001) * d-d%addr%d(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2)) *  &
                &           6.0221739328721606E+023
                          CALL _xlfWriteFmt(%VAL(#10),T_4,8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-e%addr + (%VAL(d-e%rvo)&
                &           ) + (%VAL(d-e%bounds%mult[].off152))*(int(%VAL(ks)) + &
                &           %VAL($$CIV2)) + (%VAL(d-e%bounds%mult[].off176))*(int(&
                &           %VAL(js)) + %VAL($$CIV1)) + (%VAL(&
                &           d-e%bounds%mult[].off200))*(int(%VAL(is)) + %VAL(&
                &           $$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-tgas%addr + (%VAL(&
                &           d-tgas%rvo)) + (%VAL(d-tgas%bounds%mult[].off9856))*(&
                &           int(%VAL(ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-tgas%bounds%mult[].off9880))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-tgas%bounds%mult[].off9904))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          T_5 = ((gamm1 * abs(d-e%addr%e(int(is) + $$CIV0,int(&
                &           js) + $$CIV1,int(ks) + $$CIV2))) *  &
                &           1.2027073445076573E-008) / ((((((d-abun%addr%abun(int(&
                &           is) + $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,1) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,2)) + d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,3)) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,5) *  2.5000000000000000E-001) + &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6) *  2.5000000000000000E-001) * &
                &           d-d%addr%d(int(is) + $$CIV0,int(js) + $$CIV1,int(ks) &
                &           + $$CIV2))
                          CALL _xlfWriteFmt(%VAL(#10),T_5,8,8,4)
                          T_6 = d-d%addr%d(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2) * d-abun%addr%abun(int(is) + $$CIV0,&
                &           int(js) + $$CIV1,int(ks) + $$CIV2,10)
                          CALL _xlfWriteFmt(%VAL(#10),T_6,8,8,4)
                          T_7 = d-v1%addr%v1(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2) *  1.0000000000000000E-005
                          CALL _xlfWriteFmt(%VAL(#10),T_7,8,8,4)
                          T_8 = ( 7.6000000000000000E-001 * ( &
                &           1.0000000000000000E+000 - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,10)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,1)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,2)
                          CALL _xlfWriteFmt(%VAL(#10),T_8,8,8,4)
                          T_9 = (( 2.3999999999999999E-001 * ( &
                &           1.0000000000000000E+000 - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,10)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,5)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6)
                          CALL _xlfWriteFmt(%VAL(#10),T_9,8,8,4)
                          T_10 = (fh * ( 1.0000000000000000E+000 - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,10)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,1)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,2)
                          CALL _xlfWriteFmt(%VAL(#10),T_10,8,8,4)
                          T_11 = ((( 1.0000000000000000E+000 - fh) * ( &
                &           1.0000000000000000E+000 - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,10)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,4)) - d-abun%addr%abun(int(is) + &
                &           $$CIV0,int(js) + $$CIV1,int(ks) + $$CIV2,5)) - &
                &           d-abun%addr%abun(int(is) + $$CIV0,int(js) + $$CIV1,&
                &           int(ks) + $$CIV2,6)
                          CALL _xlfWriteFmt(%VAL(#10),T_11,8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           1) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           2) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           3) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           4) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           5) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           6) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                          CALL _xlfWriteFmt(%VAL(#10),(d-abun%addr + (%VAL(&
                &           d-abun%rvo)) + (%VAL(d-abun%bounds%mult[].off2128))*(&
                &           10) + (%VAL(d-abun%bounds%mult[].off2152))*(int(%VAL(&
                &           ks)) + %VAL($$CIV2)) + (%VAL(&
                &           d-abun%bounds%mult[].off2176))*(int(%VAL(js)) + %VAL(&
                &           $$CIV1)) + (%VAL(d-abun%bounds%mult[].off2200))*(int(&
                &           %VAL(is)) + %VAL($$CIV0))),8,8,4)
                        ENDDO
                      ENDIF
                    ENDDO
                    lab_27
                  ENDDO
                  lab_25
                  _xlfEndIO(%VAL(#10))
   434|           |fmt_args%version = 129
                  |fmt_args%fmt_string_addr = NULL
                  |fmt_args%fmt_string_len = int(("textdmp2.f90 + 676))
                  |fmt_args%stack_frame = NULL
                  |fmt_args%call_back = NULL
                  #12 = _xlfBeginIO(12,259,#11,32772,NULL,0,|fmt_args)
                  _xlfEndIO(%VAL(#12))
   437|           #14 = _xlfBeginIO(12,259,#13,32772,NULL,0,|2)
                  IF (.FALSE.) GOTO lab_31
                  $$CIV5 = 0
       Id=4       DO $$CIV5 = $$CIV5, 0
                    IF (.FALSE.) GOTO lab_33
                    $$CIV4 = 0
       Id=5         DO $$CIV4 = $$CIV4, 0
                      IF ((1 + (int(ie) - int(is)) > 0)) THEN
                        $$CIV3 = 0
       Id=6             DO $$CIV3 = $$CIV3, int((1 + (int(ie) - int(is))))-1
                          T_12 = d-x1a%addr%x1a(int(is) + $$CIV3) *  &
                &           3.2425420493143010E-019
                          CALL _xlfWriteFmt(%VAL(#14),T_12,8,8,4)
                          T_13 = d-v1%addr%v1(int(is) + $$CIV3,int(js) + $$CIV4,&
                &           int(ks) + $$CIV5) *  1.0000000000000000E-005
                          CALL _xlfWriteFmt(%VAL(#14),T_13,8,8,4)
                        ENDDO
                      ENDIF
                    ENDDO
                    lab_33
                  ENDDO
                  lab_31
                  _xlfEndIO(%VAL(#14))
   453|           _xlfIOCmd(12,1,#15,0,NULL)
   456|           RETURN
                END SUBROUTINE textdmp2

 
 
>>>>> OBJECT SECTION <<<<<
 GPR's set/used:   ssus ssss ssss s-ss  ssss ssss ssss ssss
 FPR's set/used:   ssss ssss ssss ssss  ssss ssss ssss ssss
 CCR's set/used:   ss-- -sss
     | 000000                           PDEF     textdmp2
    9|                                  PROC      
    0| 000000 stfd     DBE1FFF8   1     STFL      #stack(gr1,-8)=fp31
    0| 000004 stfd     DBC1FFF0   1     STFL      #stack(gr1,-16)=fp30
    0| 000008 stfd     DBA1FFE8   1     STFL      #stack(gr1,-24)=fp29
    0| 00000C stfd     DB81FFE0   1     STFL      #stack(gr1,-32)=fp28
    0| 000010 stfd     DB61FFD8   1     STFL      #stack(gr1,-40)=fp27
    0| 000014 stfd     DB41FFD0   1     STFL      #stack(gr1,-48)=fp26
    0| 000018 stfd     DB21FFC8   1     STFL      #stack(gr1,-56)=fp25
    0| 00001C stfd     DB01FFC0   1     STFL      #stack(gr1,-64)=fp24
    0| 000020 stfd     DAE1FFB8   1     STFL      #stack(gr1,-72)=fp23
    0| 000024 stfd     DAC1FFB0   1     STFL      #stack(gr1,-80)=fp22
    0| 000028 stfd     DAA1FFA8   1     STFL      #stack(gr1,-88)=fp21
    0| 00002C stfd     DA81FFA0   1     STFL      #stack(gr1,-96)=fp20
    0| 000030 stfd     DA61FF98   1     STFL      #stack(gr1,-104)=fp19
    0| 000034 stfd     DA41FF90   1     STFL      #stack(gr1,-112)=fp18
    0| 000038 stfd     DA21FF88   1     STFL      #stack(gr1,-120)=fp17
    0| 00003C stfd     DA01FF80   1     STFL      #stack(gr1,-128)=fp16
    0| 000040 stfd     D9E1FF78   1     STFL      #stack(gr1,-136)=fp15
    0| 000044 stfd     D9C1FF70   1     STFL      #stack(gr1,-144)=fp14
    0| 000048 std      FBE1FF68   1     ST8       #stack(gr1,-152)=gr31
    0| 00004C std      FBC1FF60   1     ST8       #stack(gr1,-160)=gr30
    0| 000050 std      FBA1FF58   1     ST8       #stack(gr1,-168)=gr29
    0| 000054 std      FB81FF50   1     ST8       #stack(gr1,-176)=gr28
    0| 000058 std      FB61FF48   1     ST8       #stack(gr1,-184)=gr27
    0| 00005C std      FB41FF40   1     ST8       #stack(gr1,-192)=gr26
    0| 000060 std      FB21FF38   1     ST8       #stack(gr1,-200)=gr25
    0| 000064 std      FB01FF30   1     ST8       #stack(gr1,-208)=gr24
    0| 000068 std      FAE1FF28   1     ST8       #stack(gr1,-216)=gr23
    0| 00006C std      FAC1FF20   1     ST8       #stack(gr1,-224)=gr22
    0| 000070 std      FAA1FF18   1     ST8       #stack(gr1,-232)=gr21
    0| 000074 std      FA81FF10   1     ST8       #stack(gr1,-240)=gr20
    0| 000078 std      FA61FF08   1     ST8       #stack(gr1,-248)=gr19
    0| 00007C std      FA41FF00   1     ST8       #stack(gr1,-256)=gr18
    0| 000080 std      FA21FEF8   1     ST8       #stack(gr1,-264)=gr17
    0| 000084 std      FA01FEF0   1     ST8       #stack(gr1,-272)=gr16
    0| 000088 std      F9E1FEE8   1     ST8       #stack(gr1,-280)=gr15
    0| 00008C std      F9C1FEE0   1     ST8       #stack(gr1,-288)=gr14
    0| 000090 mfspr    7C0802A6   1     LFLR      gr0=lr
    0| 000094 std      F8010010   1     ST8       #stack(gr1,16)=gr0
    0| 000098 stdu     F821F861   1     ST8U      gr1,#stack(gr1,-1952)=gr1
  107| 00009C ld       E9020000   1     L8        gr8=.&&N&&grid(gr2,0)
   43| 0000A0 ld       EBC20000   1     L8        gr30=.&&N&&root(gr2,0)
   43| 0000A4 ld       EB820000   1     L8        gr28=.+CONSTANT_AREA(gr2,0)
   43| 0000A8 addi     39200010   1     LI        gr9=16
   43| 0000AC addi     3BE00081   1     LI        gr31=129
   43| 0000B0 stw      91210128   1     ST4Z      <a1:d296:l4>(gr1,296)=gr9
  107| 0000B4 lwz      83A80004   1     L4Z       gr29=<s131:d4:l4>(gr8,4)
  107| 0000B8 lwa      EB480012   1     L4A       gr26=<s131:d16:l4>(gr8,16)
   43| 0000BC std      F9210138   1     ST8       <a1:d312:l8>(gr1,312)=gr9
  107| 0000C0 lwa      EB28000A   1     L4A       gr25=<s131:d8:l4>(gr8,8)
  107| 0000C4 lwa      EB080002   1     L4A       gr24=<s131:d0:l4>(gr8,0)
   43| 0000C8 addi     38000076   1     LI        gr0=118
   43| 0000CC stw      93E10120   1     ST4Z      <a1:d288:l4>(gr1,288)=gr31
   43| 0000D0 stw      90010140   1     ST4Z      <a1:d320:l4>(gr1,320)=gr0
   43| 0000D4 addi     3860002B   1     LI        gr3=43
   43| 0000D8 addi     38800078   1     LI        gr4=120
   43| 0000DC stw      90610148   1     ST4Z      <a1:d328:l4>(gr1,328)=gr3
   43| 0000E0 addi     38BC02E4   1     AI        gr5=gr28,740
   43| 0000E4 addi     38C0000C   1     LI        gr6=12
   43| 0000E8 addi     38E00003   1     LI        gr7=3
   43| 0000EC stw      90810158   1     ST4Z      <a1:d344:l4>(gr1,344)=gr4
   43| 0000F0 addi     393E025C   1     AI        gr9=gr30,604
  107| 0000F4 extsw    7FB707B4   1     EXTS4     gr23=gr29
  107| 0000F8 std      FB4104F0   1     ST8       #SPILL0(gr1,1264)=gr26
   43| 0000FC std      F9210130   1     ST8       <a1:d304:l8>(gr1,304)=gr9
  107| 000100 std      FB2104F8   1     ST8       #SPILL1(gr1,1272)=gr25
  107| 000104 std      FB010500   1     ST8       #SPILL2(gr1,1280)=gr24
  107| 000108 std      FAE10508   1     ST8       #SPILL3(gr1,1288)=gr23
   43| 00010C std      F8A10160   1     ST8       <a1:d352:l8>(gr1,352)=gr5
   43| 000110 std      F8C10168   1     ST8       <a1:d360:l8>(gr1,360)=gr6
   43| 000114 stw      90E10124   1     ST4Z      <a1:d292:l4>(gr1,292)=gr7
   43| 000118 addi     38A10120   1     AI        gr5=gr1,288
   43| 00011C addi     3860000C   1     LI        gr3=12
   43| 000120 addi     38800000   1     LI        gr4=0
   43| 000124 addis    3CC01000   1     LIU       gr6=4096
   43| 000128 addi     38E00000   1     LI        gr7=0
   43| 00012C bl       48000001   1     CALL      gr3=_xlfIOCmd,5,gr3,gr4,|speclist",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,#use_xlfIOCmd11,_xlfIOCmd",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
   43| 000130 ori      60000000   1
  102| 000134 ld       EAC20000   1     L8        gr22=.$STATIC(gr2,0)
  102| 000138 addi     3B600000   1     LI        gr27=0
  102| 00013C addi     3860000C   1     LI        gr3=12
  102| 000140 ori      63758004   1     OIL       gr21=gr27,0x8004
  102| 000144 std      FAA10510   1     ST8       #SPILL4(gr1,1296)=gr21
  102| 000148 addi     38800103   1     LI        gr4=259
  102| 00014C or       7EA6AB78   1     LR        gr6=gr21
  102| 000150 addi     38B600C0   1     AI        gr5=gr22,192
  102| 000154 addi     38E00000   1     LI        gr7=0
  102| 000158 addi     39000000   1     LI        gr8=0
  102| 00015C or       7EC9B378   1     LR        gr9=gr22
  102| 000160 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#1",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|1,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  102| 000164 ori      60000000   1
  102| 000168 lfd      C81E0118   1     LFL       fp0=<s82:d280:l8>(gr30,280)
  102| 00016C lfd      C83C02F0   1     LFL       fp1=+CONSTANT_AREA(gr28,752)
  102| 000170 or       7C7C1B78   1     LR        gr28=gr3
  102| 000174 addi     38810080   1     AI        gr4=gr1,128
  102| 000178 addi     38A00008   1     LI        gr5=8
  102| 00017C addi     38C00008   1     LI        gr6=8
  102| 000180 addi     38E00004   1     LI        gr7=4
  102| 000184 fmul     FC000072   1     MFL       fp0=fp0,fp1,fcr
  102| 000188 stfd     D8010080   1     STFL      T_2(gr1,128)=fp0
  102| 00018C bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_2,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  102| 000190 ori      60000000   1
  102| 000194 or       7F83E378   1     LR        gr3=gr28
  102| 000198 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  102| 00019C ori      60000000   1
  103| 0001A0 ori      63668000   1     OIL       gr6=gr27,0x8000
  103| 0001A4 addi     38B60100   1     AI        gr5=gr22,256
  103| 0001A8 addi     3860000C   1     LI        gr3=12
  103| 0001AC addi     38800101   1     LI        gr4=257
  103| 0001B0 addi     38E00000   1     LI        gr7=0
  103| 0001B4 addi     39000000   1     LI        gr8=0
  103| 0001B8 addi     39200000   1     LI        gr9=0
  103| 0001BC bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#3",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,@PALI_SHADOW_CONST.rns0.,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  103| 0001C0 ori      60000000   1
  103| 0001C4 or       7C7C1B78   1     LR        gr28=gr3
  103| 0001C8 addi     389E01E8   1     AI        gr4=gr30,488
  103| 0001CC addi     38A00004   1     LI        gr5=4
  103| 0001D0 addi     38C00004   1     LI        gr6=4
  103| 0001D4 bl       48000001   1     CALL      _xlfWriteLDInt,4,gr3,nhy,gr4-gr6,_xlfWriteLDInt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  103| 0001D8 ori      60000000   1
  103| 0001DC addi     389E0078   1     AI        gr4=gr30,120
  103| 0001E0 or       7F83E378   1     LR        gr3=gr28
  103| 0001E4 addi     38A00008   1     LI        gr5=8
  103| 0001E8 addi     38C00008   1     LI        gr6=8
  103| 0001EC bl       48000001   1     CALL      _xlfWriteLDReal,4,gr3,dt,gr4-gr6,_xlfWriteLDReal",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  103| 0001F0 ori      60000000   1
  103| 0001F4 or       7F83E378   1     LR        gr3=gr28
  103| 0001F8 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  103| 0001FC ori      60000000   1
  104| 000200 stw      93E100E0   1     ST4Z      <a1:d224:l4>(gr1,224)=gr31
  104| 000204 std      FB6100E8   1     ST8       <a1:d232:l8>(gr1,232)=gr27
  104| 000208 ld       EA820000   1     L8        gr20=.+CONSTANT_AREA(gr2,0)
  104| 00020C std      FB6100F8   1     ST8       <a1:d248:l8>(gr1,248)=gr27
  104| 000210 std      FB610100   1     ST8       <a1:d256:l8>(gr1,256)=gr27
  104| 000214 addi     38B60140   1     AI        gr5=gr22,320
  104| 000218 addi     3860000C   1     LI        gr3=12
  104| 00021C addi     38800103   1     LI        gr4=259
  104| 000220 addi     381401A4   1     AI        gr0=gr20,420
  104| 000224 or       7EA6AB78   1     LR        gr6=gr21
  104| 000228 std      F80100F0   1     ST8       <a1:d240:l8>(gr1,240)=gr0
  104| 00022C addi     38E00000   1     LI        gr7=0
  104| 000230 addi     39000000   1     LI        gr8=0
  104| 000234 addi     392100E0   1     AI        gr9=gr1,224
  104| 000238 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#5",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|fmt_args,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  104| 00023C ori      60000000   1
  104| 000240 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  104| 000244 ori      60000000   1
  106| 000248 addi     38140234   1     AI        gr0=gr20,564
  106| 00024C addi     38B60180   1     AI        gr5=gr22,384
  106| 000250 std      F80100F0   1     ST8       <a1:d240:l8>(gr1,240)=gr0
  106| 000254 addi     3860000C   1     LI        gr3=12
  106| 000258 addi     38800103   1     LI        gr4=259
  106| 00025C or       7EA6AB78   1     LR        gr6=gr21
  106| 000260 addi     38E00000   1     LI        gr7=0
  106| 000264 addi     39000000   1     LI        gr8=0
  106| 000268 addi     392100E0   1     AI        gr9=gr1,224
  106| 00026C bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#7",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|fmt_args,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  106| 000270 ori      60000000   1
  106| 000274 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  106| 000278 ori      60000000   1
  107| 00027C addi     39360040   1     AI        gr9=gr22,64
  107| 000280 addi     38B601C0   1     AI        gr5=gr22,448
  107| 000284 addi     3860000C   1     LI        gr3=12
  107| 000288 addi     38800103   1     LI        gr4=259
  107| 00028C or       7EA6AB78   1     LR        gr6=gr21
  107| 000290 addi     38E00000   1     LI        gr7=0
  107| 000294 addi     39000000   1     LI        gr8=0
  107| 000298 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#9",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|3,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 00029C ori      60000000   1
    0| 0002A0 addi     381D0002   1     AI        gr0=gr29,2
  107| 0002A4 or       7C7F1B78   1     LR        gr31=gr3
    0| 0002A8 extsw    7C0007B4   1     EXTS4     gr0=gr0
  107| 0002AC ld       E8620000   1     L8        gr3=.&&N&&chem(gr2,0)
    0| 0002B0 subf     7C980050   1     S         gr4=gr0,gr24
  107| 0002B4 addi     3A600000   1     LI        gr19=0
    0| 0002B8 addic.   36440001   1     AI_R      gr18,cr0=gr4,1,ca"
  107| 0002BC std      FA610518   1     ST8       #SPILL5(gr1,1304)=gr19
    0| 0002C0 std      FA410520   1     ST8       #SPILL6(gr1,1312)=gr18
  107| 0002C4 lfd      CBE30070   1     LFL       fp31=<s151:d112:l8>(gr3,112)
    0| 0002C8 bc       40810680   1     BF        CL.90,cr0,0x2/gt,taken=50%(0,0)
  107| 0002CC ld       E9220000   1     L8        gr9=.&&N&chem(gr2,0)
  107| 0002D0 ld       E8620000   1     L8        gr3=.&&N&field(gr2,0)
  107| 0002D4 ld       E8820000   1     L8        gr4=.&&N&grid(gr2,0)
  107| 0002D8 rldicr   7B051F24   1     SLL8      gr5=gr24,3
    0| 0002DC addi     38D8FFFF   1     AI        gr6=gr24,-1
  107| 0002E0 addi     3945FFF8   1     AI        gr10=gr5,-8
    0| 0002E4 ld       E9092698   1     L8        gr8=<s70:d9880:l8>(gr9,9880)
    0| 0002E8 ld       E8E300C8   1     L8        gr7=<s12:d200:l8>(gr3,200)
    0| 0002EC ld       E96406C8   1     L8        gr11=<s50:d1736:l8>(gr4,1736)
    0| 0002F0 ld       E80300B0   1     L8        gr0=<s12:d176:l8>(gr3,176)
    0| 0002F4 ld       E8A30000   1     L8        gr5=<s12:d0:l8>(gr3,0)
    0| 0002F8 ld       E9892680   1     L8        gr12=<s70:d9856:l8>(gr9,9856)
    0| 0002FC std      F9010570   1     ST8       #SPILL16(gr1,1392)=gr8
    0| 000300 std      F8E10568   1     ST8       #SPILL15(gr1,1384)=gr7
    0| 000304 ld       E90406E0   1     L8        gr8=<s50:d1760:l8>(gr4,1760)
    0| 000308 std      F8010560   1     ST8       #SPILL14(gr1,1376)=gr0
    0| 00030C ld       E8E30018   1     L8        gr7=<s12:d24:l8>(gr3,24)
    0| 000310 std      F9810578   1     ST8       #SPILL17(gr1,1400)=gr12
    0| 000314 add      7D4A5A14   1     A         gr10=gr10,gr11
    0| 000318 ld       EBA30060   1     L8        gr29=<s12:d96:l8>(gr3,96)
    0| 00031C add      7D685214   1     A         gr11=gr8,gr10
    0| 000320 ld       EA0926B0   1     L8        gr16=<s70:d9904:l8>(gr9,9904)
    0| 000324 std      F9610580   1     ST8       #SPILL18(gr1,1408)=gr11
    0| 000328 ld       E8092650   1     L8        gr0=<s70:d9808:l8>(gr9,9808)
    0| 00032C ld       E8892668   1     L8        gr4=<s70:d9832:l8>(gr9,9832)
    0| 000330 std      FBA10528   1     ST8       #SPILL7(gr1,1320)=gr29
    0| 000334 ld       E9230068   1     L8        gr9=<s12:d104:l8>(gr3,104)
    0| 000338 std      FA010548   1     ST8       #SPILL11(gr1,1352)=gr16
    0| 00033C add      7CE53A14   1     A         gr7=gr5,gr7
    0| 000340 ld       E8A301A0   1     L8        gr5=<s12:d416:l8>(gr3,416)
    0| 000344 ld       E9630820   1     L8        gr11=<s12:d2080:l8>(gr3,2080)
    0| 000348 ld       E9830838   1     L8        gr12=<s12:d2104:l8>(gr3,2104)
    0| 00034C ld       E9430080   1     L8        gr10=<s12:d128:l8>(gr3,128)
    0| 000350 ld       E90301B8   1     L8        gr8=<s12:d440:l8>(gr3,440)
    0| 000354 lfd      CBDE00E8   1     LFL       fp30=<s82:d232:l8>(gr30,232)
    0| 000358 ld       EB630200   1     L8        gr27=<s12:d512:l8>(gr3,512)
    0| 00035C ld       EBC30898   1     L8        gr30=<s12:d2200:l8>(gr3,2200)
    0| 000360 add      7D6B6214   1     A         gr11=gr11,gr12
    0| 000364 add      7D895214   1     A         gr12=gr9,gr10
    0| 000368 std      F9610588   1     ST8       #SPILL19(gr1,1416)=gr11
    0| 00036C std      F9810590   1     ST8       #SPILL20(gr1,1424)=gr12
    0| 000370 std      FB610538   1     ST8       #SPILL9(gr1,1336)=gr27
    0| 000374 add      7D254214   1     A         gr9=gr5,gr8
    0| 000378 mulld    7D06E9D2   1     M         gr8=gr6,gr29
    0| 00037C std      F9210598   1     ST8       #SPILL21(gr1,1432)=gr9
    0| 000380 add      7D474214   1     A         gr10=gr7,gr8
    0| 000384 ld       E90300C8   1     L8        gr8=<s12:d200:l8>(gr3,200)
    0| 000388 std      F94105A0   1     ST8       #SPILL22(gr1,1440)=gr10
    0| 00038C mulld    7CE681D2   1     M         gr7=gr6,gr16
    0| 000390 add      7C002214   1     A         gr0=gr0,gr4
    0| 000394 ld       EB8301E8   1     L8        gr28=<s12:d488:l8>(gr3,488)
    0| 000398 ld       EA230880   1     L8        gr17=<s12:d2176:l8>(gr3,2176)
    0| 00039C ld       E9E30850   1     L8        gr15=<s12:d2128:l8>(gr3,2128)
    0| 0003A0 ld       E9C30048   1     L8        gr14=<s12:d72:l8>(gr3,72)
    0| 0003A4 mulld    7C86F1D2   1     M         gr4=gr6,gr30
    0| 0003A8 std      FB810530   1     ST8       #SPILL8(gr1,1328)=gr28
    0| 0003AC std      FA210540   1     ST8       #SPILL10(gr1,1344)=gr17
    0| 0003B0 std      F9E10550   1     ST8       #SPILL12(gr1,1360)=gr15
    0| 0003B4 mulld    7CA6D9D2   1     M         gr5=gr6,gr27
    0| 0003B8 mulld    7CC641D2   1     M         gr6=gr6,gr8
    0| 0003BC std      F9C10558   1     ST8       #SPILL13(gr1,1368)=gr14
    0| 0003C0 add      7C003A14   1     A         gr0=gr0,gr7
    0| 0003C4 ld       E8E30030   1     L8        gr7=<s12:d48:l8>(gr3,48)
    0| 0003C8 std      F8C105A8   1     ST8       #SPILL23(gr1,1448)=gr6
    0| 0003CC lfd      CBB402F8   1     LFL       fp29=+CONSTANT_AREA(gr20,760)
    0| 0003D0 std      F80105B0   1     ST8       #SPILL24(gr1,1456)=gr0
    0| 0003D4 lfs      C3940300   1     LFS       fp28=+CONSTANT_AREA(gr20,768)
    0| 0003D8 lfs      C3740304   1     LFS       fp27=+CONSTANT_AREA(gr20,772)
    0| 0003DC lfd      CB540308   1     LFL       fp26=+CONSTANT_AREA(gr20,776)
    0| 0003E0 lfd      CB340310   1     LFL       fp25=+CONSTANT_AREA(gr20,784)
    0| 0003E4 std      F8E105B8   1     ST8       #SPILL25(gr1,1464)=gr7
    0| 0003E8 lfd      CB140318   1     LFL       fp24=+CONSTANT_AREA(gr20,792)
    0| 0003EC lfs      C2F40320   1     LFS       fp23=+CONSTANT_AREA(gr20,800)
    0| 0003F0 lfd      CAD40328   1     LFL       fp22=+CONSTANT_AREA(gr20,808)
    0| 0003F4 lfd      CAB40330   1     LFL       fp21=+CONSTANT_AREA(gr20,816)
    0| 0003F8 ld       E8030098   1     L8        gr0=<s12:d152:l8>(gr3,152)
    0| 0003FC ld       EAC10560   1     L8        gr22=#SPILL14(gr1,1376)
    0| 000400 ld       EBA10570   1     L8        gr29=#SPILL16(gr1,1392)
    0| 000404 rldicr   79F30FA4   1     SLL8      gr19=gr15,1
    0| 000408 std      FA610608   1     ST8       #SPILL35(gr1,1544)=gr19
    0| 00040C std      F80105C0   1     ST8       #SPILL26(gr1,1472)=gr0
    0| 000410 ld       E80301D0   1     L8        gr0=<s12:d464:l8>(gr3,464)
    0| 000414 ld       E8630868   1     L8        gr3=<s12:d2152:l8>(gr3,2152)
    0| 000418 mulld    7EF6C9D2   1     M         gr23=gr22,gr25
    0| 00041C std      F80105C8   1     ST8       #SPILL27(gr1,1480)=gr0
    0| 000420 std      F86105D0   1     ST8       #SPILL28(gr1,1488)=gr3
    0| 000424 std      FAE10620   1     ST8       #SPILL38(gr1,1568)=gr23
    0| 000428 mulld    7C11C9D2   1     M         gr0=gr17,gr25
    0| 00042C mulld    7C79E1D2   1     M         gr3=gr25,gr28
    0| 000430 add      7C002214   1     A         gr0=gr0,gr4
    0| 000434 add      7C832A14   1     A         gr4=gr3,gr5
    0| 000438 std      F80105D8   1     ST8       #SPILL29(gr1,1496)=gr0
    0| 00043C std      F88105E0   1     ST8       #SPILL30(gr1,1504)=gr4
    0| 000440 rldicr   79E51764   1     SLL8      gr5=gr15,2
    0| 000444 mulld    7ECEC9D2   1     M         gr22=gr14,gr25
    0| 000448 std      F8A105E8   1     ST8       #SPILL31(gr1,1512)=gr5
    0| 00044C std      FAC10628   1     ST8       #SPILL39(gr1,1576)=gr22
    0| 000450 mulld    7F99E9D2   1     M         gr28=gr25,gr29
    0| 000454 rldicr   79E31F24   1     SLL8      gr3=gr15,3
    0| 000458 std      FB810640   1     ST8       #SPILL42(gr1,1600)=gr28
    0| 00045C std      F86105F0   1     ST8       #SPILL32(gr1,1520)=gr3
    0| 000460 subf     7D0F2850   1     S         gr8=gr5,gr15
    0| 000464 add      7E457A14   1     A         gr18=gr5,gr15
    0| 000468 std      F90105F8   1     ST8       #SPILL33(gr1,1528)=gr8
    0| 00046C std      FA410600   1     ST8       #SPILL34(gr1,1536)=gr18
    0| 000470 rldicr   7A540FA4   1     SLL8      gr20=gr18,1
    0| 000474 rldicr   79150FA4   1     SLL8      gr21=gr8,1
    0| 000478 std      FA810610   1     ST8       #SPILL36(gr1,1552)=gr20
    0| 00047C std      FAA10618   1     ST8       #SPILL37(gr1,1560)=gr21
    0| 000480 add      7F037A14   1     A         gr24=gr3,gr15
    0| 000484 subf     7F4F1850   1     S         gr26=gr3,gr15
    0| 000488 std      FB010630   1     ST8       #SPILL40(gr1,1584)=gr24
    0| 00048C std      FB410638   1     ST8       #SPILL41(gr1,1592)=gr26
  107|                              CL.26:
    0| 000490 ld       E8A104F0   1     L8        gr5=#SPILL0(gr1,1264)
    0| 000494 ld       E8C10518   1     L8        gr6=#SPILL5(gr1,1304)
    0| 000498 ld       E90105D0   1     L8        gr8=#SPILL28(gr1,1488)
    0| 00049C ld       E92105C0   1     L8        gr9=#SPILL26(gr1,1472)
    0| 0004A0 ld       EB8105B8   1     L8        gr28=#SPILL25(gr1,1464)
    0| 0004A4 ld       EB4105C8   1     L8        gr26=#SPILL27(gr1,1480)
    0| 0004A8 ld       EB010578   1     L8        gr24=#SPILL17(gr1,1400)
    0| 0004AC add      7C053214   1     A         gr0=gr5,gr6
    0| 0004B0 ld       E9410588   1     L8        gr10=#SPILL19(gr1,1416)
    0| 0004B4 mulld    7C6041D2   1     M         gr3=gr0,gr8
    0| 0004B8 mulld    7C8049D2   1     M         gr4=gr0,gr9
    0| 0004BC ld       E9810620   1     L8        gr12=#SPILL38(gr1,1568)
    0| 0004C0 mulld    7F60E1D2   1     M         gr27=gr0,gr28
    0| 0004C4 mulld    7F20D1D2   1     M         gr25=gr0,gr26
    0| 0004C8 std      FB610660   1     ST8       #SPILL46(gr1,1632)=gr27
    0| 0004CC std      FB210668   1     ST8       #SPILL47(gr1,1640)=gr25
    0| 0004D0 mulld    7EE0C1D2   1     M         gr23=gr0,gr24
  107| 0004D4 addi     38E00000   1     LI        gr7=0
    0| 0004D8 std      FAE10670   1     ST8       #SPILL48(gr1,1648)=gr23
  107| 0004DC std      F8E10648   1     ST8       #SPILL43(gr1,1608)=gr7
    0| 0004E0 add      7D635214   1     A         gr11=gr3,gr10
    0| 0004E4 add      7FA46214   1     A         gr29=gr4,gr12
    0| 0004E8 std      F9610650   1     ST8       #SPILL44(gr1,1616)=gr11
    0| 0004EC std      FBA10658   1     ST8       #SPILL45(gr1,1624)=gr29
  107|                              CL.28:
  107| 0004F0 ld       E8C10648   1     L8        gr6=#SPILL43(gr1,1608)
  107| 0004F4 ld       E8E10560   1     L8        gr7=#SPILL14(gr1,1376)
  107| 0004F8 ld       E8A10540   1     L8        gr5=#SPILL10(gr1,1344)
  107| 0004FC ld       E9210590   1     L8        gr9=#SPILL20(gr1,1424)
  107| 000500 ld       E90105D8   1     L8        gr8=#SPILL29(gr1,1496)
  107| 000504 ld       E96105A8   1     L8        gr11=#SPILL23(gr1,1448)
  107| 000508 ld       E9410650   1     L8        gr10=#SPILL44(gr1,1616)
  107| 00050C mulld    7C6639D2   1     M         gr3=gr6,gr7
  107| 000510 mulld    7C0531D2   1     M         gr0=gr5,gr6
  107| 000514 ld       EA210558   1     L8        gr17=#SPILL13(gr1,1368)
  107| 000518 add      7C634A14   1     A         gr3=gr3,gr9
  107| 00051C ld       EA010530   1     L8        gr16=#SPILL8(gr1,1328)
  107| 000520 add      7C004214   1     A         gr0=gr0,gr8
  107| 000524 ld       E8810658   1     L8        gr4=#SPILL45(gr1,1624)
  107| 000528 add      7C635A14   1     A         gr3=gr3,gr11
  107| 00052C ld       E9810608   1     L8        gr12=#SPILL35(gr1,1544)
  107| 000530 ld       EB4105E8   1     L8        gr26=#SPILL31(gr1,1512)
  107| 000534 ld       EB2105F8   1     L8        gr25=#SPILL33(gr1,1528)
  107| 000538 ld       EB010550   1     L8        gr24=#SPILL12(gr1,1360)
  107| 00053C ld       EAE10610   1     L8        gr23=#SPILL36(gr1,1552)
  107| 000540 ld       EAC10600   1     L8        gr22=#SPILL34(gr1,1536)
  107| 000544 ld       EAA10618   1     L8        gr21=#SPILL37(gr1,1560)
  107| 000548 ld       EA810630   1     L8        gr20=#SPILL40(gr1,1584)
  107| 00054C ld       EA610638   1     L8        gr19=#SPILL41(gr1,1592)
  107| 000550 ld       EA4105F0   1     L8        gr18=#SPILL32(gr1,1520)
  107| 000554 add      7C005214   1     A         gr0=gr0,gr10
  107| 000558 add      7F832214   1     A         gr28=gr3,gr4
  107| 00055C mulld    7C6689D2   1     M         gr3=gr6,gr17
  107| 000560 ld       E9E10660   1     L8        gr15=#SPILL46(gr1,1632)
  107| 000564 add      7FA06214   1     A         gr29=gr0,gr12
  107| 000568 add      7F60D214   1     A         gr27=gr0,gr26
  107| 00056C add      7F40CA14   1     A         gr26=gr0,gr25
  107| 000570 add      7F20C214   1     A         gr25=gr0,gr24
  107| 000574 add      7F00BA14   1     A         gr24=gr0,gr23
  107| 000578 add      7EE0B214   1     A         gr23=gr0,gr22
  107| 00057C add      7EC0AA14   1     A         gr22=gr0,gr21
  107| 000580 add      7EA0A214   1     A         gr21=gr0,gr20
  107| 000584 add      7E809A14   1     A         gr20=gr0,gr19
  107| 000588 add      7E609214   1     A         gr19=gr0,gr18
  107| 00058C mulld    7C0681D2   1     M         gr0=gr6,gr16
  107| 000590 ld       E9C10598   1     L8        gr14=#SPILL21(gr1,1432)
  107| 000594 ld       EA410628   1     L8        gr18=#SPILL39(gr1,1576)
  107| 000598 ld       EA010570   1     L8        gr16=#SPILL16(gr1,1392)
  107| 00059C add      7C637A14   1     A         gr3=gr3,gr15
  107| 0005A0 ld       E9E105A0   1     L8        gr15=#SPILL22(gr1,1440)
  107| 0005A4 ld       EA210668   1     L8        gr17=#SPILL47(gr1,1640)
  107| 0005A8 add      7C007214   1     A         gr0=gr0,gr14
  107| 0005AC add      7C839214   1     A         gr4=gr3,gr18
  107| 0005B0 ld       E9C10640   1     L8        gr14=#SPILL42(gr1,1600)
  107| 0005B4 add      7E447A14   1     A         gr18=gr4,gr15
  107| 0005B8 ld       E9E105E0   1     L8        gr15=#SPILL30(gr1,1504)
  107| 0005BC add      7C608A14   1     A         gr3=gr0,gr17
  107| 0005C0 mulld    7C0681D2   1     M         gr0=gr6,gr16
  107| 0005C4 ld       E8810670   1     L8        gr4=#SPILL48(gr1,1648)
  107| 0005C8 add      7E037A14   1     A         gr16=gr3,gr15
  107| 0005CC ld       E86105B0   1     L8        gr3=#SPILL24(gr1,1456)
  107| 0005D0 addi     3A200000   1     LI        gr17=0
  107| 0005D4 add      7C002214   1     A         gr0=gr0,gr4
  107| 0005D8 add      7C007214   1     A         gr0=gr0,gr14
  107| 0005DC ld       E9C10580   1     L8        gr14=#SPILL18(gr1,1408)
  107| 0005E0 add      7DE01A14   1     A         gr15=gr0,gr3
    0| 0005E4 ori      60210000   1     XNOP      
    0| 0005E8 ori      60210000   1     XNOP      
    0| 0005EC ori      60210000   1     XNOP      
  107|                              CL.30:
  107| 0005F0 lfdu     CC0E0008   1     LFDU      fp0,gr14=x1b(gr14,8)
  107| 0005F4 or       7FE3FB78   1     LR        gr3=gr31
  107| 0005F8 addi     38810088   1     AI        gr4=gr1,136
  107| 0005FC addi     38A00008   1     LI        gr5=8
  107| 000600 addi     38C00008   1     LI        gr6=8
  107| 000604 addi     38E00004   1     LI        gr7=4
  107| 000608 fmul     FC000772   1     MFL       fp0=fp0,fp29,fcr
  107| 00060C stfd     D8010088   1     STFL      T_3(gr1,136)=fp0
  107| 000610 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_3,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000614 ori      60000000   1
  107| 000618 ld       E8010528   1     L8        gr0=#SPILL7(gr1,1320)
  107| 00061C or       7FE3FB78   1     LR        gr3=gr31
  107| 000620 addi     38A00008   1     LI        gr5=8
  107| 000624 addi     38C00008   1     LI        gr6=8
  107| 000628 addi     38E00004   1     LI        gr7=4
  107| 00062C add      7E409214   1     A         gr18=gr0,gr18
  107| 000630 or       7E449378   1     LR        gr4=gr18
  107| 000634 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,d,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000638 ori      60000000   1
  107| 00063C lfdux    7E79F4EE   1     LFDU      fp19,gr25=abun(gr25,gr30,0)
  107| 000640 lfdux    7E5DF4EE   1     LFDU      fp18,gr29=abun(gr29,gr30,0)
  107| 000644 lfdux    7E3BF4EE   1     LFDU      fp17,gr27=abun(gr27,gr30,0)
  107| 000648 lfdux    7E17F4EE   1     LFDU      fp16,gr23=abun(gr23,gr30,0)
  107| 00064C lfdux    7DD6F4EE   1     LFDU      fp14,gr22=abun(gr22,gr30,0)
  107| 000650 lfdux    7C14F4EE   1     LFDU      fp0,gr20=abun(gr20,gr30,0)
  107| 000654 lfdux    7C33F4EE   1     LFDU      fp1,gr19=abun(gr19,gr30,0)
  107| 000658 lfdux    7C75F4EE   1     LFDU      fp3,gr21=abun(gr21,gr30,0)
  107| 00065C fadd     FC53902A   1     AFL       fp2=fp19,fp18,fcr
  107| 000660 lfd      CA920000   2     LFL       fp20=d(gr18,0)
  107| 000664 or       7FE3FB78   1     LR        gr3=gr31
  107| 000668 addi     38810090   1     AI        gr4=gr1,144
  107| 00066C addi     38A00008   1     LI        gr5=8
  107| 000670 addi     38C00008   1     LI        gr6=8
  107| 000674 fmadd    FC51173A   1     FMA       fp2=fp2,fp17,fp28,fcr
  107| 000678 addi     38E00004   1     LI        gr7=4
  107| 00067C fmadd    FC50173A   1     FMA       fp2=fp2,fp16,fp28,fcr
  107| 000680 fmadd    FC4E173A   2     FMA       fp2=fp2,fp14,fp28,fcr
  107| 000684 fadd     FC02002A   2     AFL       fp0=fp2,fp0,fcr
  107| 000688 fmadd    FC0106FA   2     FMA       fp0=fp0,fp1,fp27,fcr
  107| 00068C fmadd    FC0306FA   2     FMA       fp0=fp0,fp3,fp27,fcr
  107| 000690 fmul     FC000532   2     MFL       fp0=fp0,fp20,fcr
  107| 000694 fmul     FC0006B2   2     MFL       fp0=fp0,fp26,fcr
  107| 000698 stfd     D8010090   1     STFL      T_4(gr1,144)=fp0
  107| 00069C bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_4,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0006A0 ori      60000000   1
  107| 0006A4 ld       E8010568   1     L8        gr0=#SPILL15(gr1,1384)
  107| 0006A8 or       7FE3FB78   1     LR        gr3=gr31
  107| 0006AC addi     38A00008   1     LI        gr5=8
  107| 0006B0 addi     38C00008   1     LI        gr6=8
  107| 0006B4 addi     38E00004   1     LI        gr7=4
  107| 0006B8 add      7F80E214   1     A         gr28=gr0,gr28
  107| 0006BC or       7F84E378   1     LR        gr4=gr28
  107| 0006C0 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,e,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0006C4 ori      60000000   1
  107| 0006C8 ld       E8010548   1     L8        gr0=#SPILL11(gr1,1352)
  107| 0006CC or       7FE3FB78   1     LR        gr3=gr31
  107| 0006D0 addi     38A00008   1     LI        gr5=8
  107| 0006D4 addi     38C00008   1     LI        gr6=8
  107| 0006D8 addi     38E00004   1     LI        gr7=4
  107| 0006DC add      7DE07A14   1     A         gr15=gr0,gr15
  107| 0006E0 or       7DE47B78   1     LR        gr4=gr15
  107| 0006E4 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,tgas,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0006E8 ori      60000000   1
  107| 0006EC fadd     FC13902A   1     AFL       fp0=fp19,fp18,fcr
  107| 0006F0 lfdux    7C5AF4EE   1     LFDU      fp2,gr26=abun(gr26,gr30,0)
  107| 0006F4 lfd      C83C0000   1     LFL       fp1=e(gr28,0)
  107| 0006F8 or       7FE3FB78   1     LR        gr3=gr31
  107| 0006FC addi     38810098   1     AI        gr4=gr1,152
  107| 000700 addi     38A00008   1     LI        gr5=8
  107| 000704 addi     38C00008   1     LI        gr6=8
  107| 000708 addi     38E00004   1     LI        gr7=4
  107| 00070C fadd     FC00102A   1     AFL       fp0=fp0,fp2,fcr
  107| 000710 fabs     FC200A10   2     ABSFL     fp1=fp1
  107| 000714 fmul     FC3E0072   2     MFL       fp1=fp30,fp1,fcr
  107| 000718 fmadd    FC11073A   2     FMA       fp0=fp0,fp17,fp28,fcr
  107| 00071C fmul     FC210672   2     MFL       fp1=fp1,fp25,fcr
  107| 000720 fmadd    FC10073A   2     FMA       fp0=fp0,fp16,fp28,fcr
  107| 000724 fmadd    FC0E073A   2     FMA       fp0=fp0,fp14,fp28,fcr
  107| 000728 fmul     FC140032   2     MFL       fp0=fp20,fp0,fcr
  107| 00072C fdiv     FC010024   2     DFL       fp0=fp1,fp0,fcr
  107| 000730 stfd     D8010098   1     STFL      T_5(gr1,152)=fp0
  107| 000734 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_5,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000738 ori      60000000   1
  107| 00073C lfdux    7DF8F4EE   1     LFDU      fp15,gr24=abun(gr24,gr30,0)
  107| 000740 or       7FE3FB78   1     LR        gr3=gr31
  107| 000744 addi     388100A0   1     AI        gr4=gr1,160
  107| 000748 addi     38A00008   1     LI        gr5=8
  107| 00074C addi     38C00008   1     LI        gr6=8
  107| 000750 addi     38E00004   1     LI        gr7=4
  107| 000754 fmul     FC1403F2   1     MFL       fp0=fp20,fp15,fcr
  107| 000758 stfd     D80100A0   1     STFL      T_6(gr1,160)=fp0
  107| 00075C bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_6,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000760 ori      60000000   1
  107| 000764 ld       E8610538   1     L8        gr3=#SPILL9(gr1,1336)
  107| 000768 addi     388100A8   1     AI        gr4=gr1,168
  107| 00076C addi     38A00008   1     LI        gr5=8
  107| 000770 addi     38C00008   1     LI        gr6=8
  107| 000774 addi     38E00004   1     LI        gr7=4
  107| 000778 lfdux    7C101CEE   1     LFDU      fp0,gr16=v1(gr16,gr3,0)
  107| 00077C or       7FE3FB78   1     LR        gr3=gr31
  107| 000780 fmul     FC000632   1     MFL       fp0=fp0,fp24,fcr
  107| 000784 stfd     D80100A8   1     STFL      T_7(gr1,168)=fp0
  107| 000788 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_7,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 00078C ori      60000000   1
  107| 000790 fsub     FC177828   1     SFL       fp0=fp23,fp15,fcr
  107| 000794 or       7FE3FB78   2     LR        gr3=gr31
  107| 000798 addi     388100B0   1     AI        gr4=gr1,176
  107| 00079C addi     38A00008   1     LI        gr5=8
  107| 0007A0 addi     38C00008   1     LI        gr6=8
  107| 0007A4 fmsub    FC009DB8   1     FMS       fp0=fp19,fp0,fp22,fcr
  107| 0007A8 addi     38E00004   1     LI        gr7=4
  107| 0007AC fsub     FC009028   1     SFL       fp0=fp0,fp18,fcr
  107| 0007B0 stfd     D80100B0   1     STFL      T_8(gr1,176)=fp0
  107| 0007B4 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_8,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0007B8 ori      60000000   1
  107| 0007BC fsub     FC177828   1     SFL       fp0=fp23,fp15,fcr
  107| 0007C0 or       7FE3FB78   2     LR        gr3=gr31
  107| 0007C4 addi     388100B8   1     AI        gr4=gr1,184
  107| 0007C8 addi     38A00008   1     LI        gr5=8
  107| 0007CC addi     38C00008   1     LI        gr6=8
  107| 0007D0 addi     38E00004   1     LI        gr7=4
  107| 0007D4 fmsub    FC008D78   1     FMS       fp0=fp17,fp0,fp21,fcr
  107| 0007D8 fsub     FC008028   2     SFL       fp0=fp0,fp16,fcr
  107| 0007DC fsub     FC007028   2     SFL       fp0=fp0,fp14,fcr
  107| 0007E0 stfd     D80100B8   1     STFL      T_9(gr1,184)=fp0
  107| 0007E4 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_9,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0007E8 ori      60000000   1
  107| 0007EC fsub     FC177828   1     SFL       fp0=fp23,fp15,fcr
  107| 0007F0 or       7FE3FB78   2     LR        gr3=gr31
  107| 0007F4 addi     388100C0   1     AI        gr4=gr1,192
  107| 0007F8 addi     38A00008   1     LI        gr5=8
  107| 0007FC addi     38C00008   1     LI        gr6=8
  107| 000800 addi     38E00004   1     LI        gr7=4
  107| 000804 fmsub    FC1F9838   1     FMS       fp0=fp19,fp31,fp0,fcr
  107| 000808 fsub     FC009028   2     SFL       fp0=fp0,fp18,fcr
  107| 00080C stfd     D80100C0   1     STFL      T_10(gr1,192)=fp0
  107| 000810 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_10,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000814 ori      60000000   1
  107| 000818 fsub     FC17F828   1     SFL       fp0=fp23,fp31,fcr
  107| 00081C or       7FE3FB78   2     LR        gr3=gr31
  107| 000820 fsub     FC377828   1     SFL       fp1=fp23,fp15,fcr
  107| 000824 addi     388100C8   1     AI        gr4=gr1,200
  107| 000828 addi     38A00008   1     LI        gr5=8
  107| 00082C addi     38C00008   1     LI        gr6=8
  107| 000830 addi     38E00004   1     LI        gr7=4
  107| 000834 fmsub    FC008878   1     FMS       fp0=fp17,fp0,fp1,fcr
  107| 000838 fsub     FC008028   2     SFL       fp0=fp0,fp16,fcr
  107| 00083C fsub     FC007028   2     SFL       fp0=fp0,fp14,fcr
  107| 000840 stfd     D80100C8   1     STFL      T_11(gr1,200)=fp0
  107| 000844 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_11,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000848 ori      60000000   1
  107| 00084C or       7F24CB78   1     LR        gr4=gr25
  107| 000850 or       7FE3FB78   1     LR        gr3=gr31
  107| 000854 addi     38A00008   1     LI        gr5=8
  107| 000858 addi     38C00008   1     LI        gr6=8
  107| 00085C addi     38E00004   1     LI        gr7=4
  107| 000860 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000864 ori      60000000   1
  107| 000868 or       7FA4EB78   1     LR        gr4=gr29
  107| 00086C or       7FE3FB78   1     LR        gr3=gr31
  107| 000870 addi     38A00008   1     LI        gr5=8
  107| 000874 addi     38C00008   1     LI        gr6=8
  107| 000878 addi     38E00004   1     LI        gr7=4
  107| 00087C bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000880 ori      60000000   1
  107| 000884 or       7F44D378   1     LR        gr4=gr26
  107| 000888 or       7FE3FB78   1     LR        gr3=gr31
  107| 00088C addi     38A00008   1     LI        gr5=8
  107| 000890 addi     38C00008   1     LI        gr6=8
  107| 000894 addi     38E00004   1     LI        gr7=4
  107| 000898 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 00089C ori      60000000   1
  107| 0008A0 or       7F64DB78   1     LR        gr4=gr27
  107| 0008A4 or       7FE3FB78   1     LR        gr3=gr31
  107| 0008A8 addi     38A00008   1     LI        gr5=8
  107| 0008AC addi     38C00008   1     LI        gr6=8
  107| 0008B0 addi     38E00004   1     LI        gr7=4
  107| 0008B4 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0008B8 ori      60000000   1
  107| 0008BC or       7EE4BB78   1     LR        gr4=gr23
  107| 0008C0 or       7FE3FB78   1     LR        gr3=gr31
  107| 0008C4 addi     38A00008   1     LI        gr5=8
  107| 0008C8 addi     38C00008   1     LI        gr6=8
  107| 0008CC addi     38E00004   1     LI        gr7=4
  107| 0008D0 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0008D4 ori      60000000   1
  107| 0008D8 or       7EC4B378   1     LR        gr4=gr22
  107| 0008DC or       7FE3FB78   1     LR        gr3=gr31
  107| 0008E0 addi     38A00008   1     LI        gr5=8
  107| 0008E4 addi     38C00008   1     LI        gr6=8
  107| 0008E8 addi     38E00004   1     LI        gr7=4
  107| 0008EC bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 0008F0 ori      60000000   1
  107| 0008F4 or       7F04C378   1     LR        gr4=gr24
  107| 0008F8 or       7FE3FB78   1     LR        gr3=gr31
  107| 0008FC addi     38A00008   1     LI        gr5=8
  107| 000900 addi     38C00008   1     LI        gr6=8
  107| 000904 addi     38E00004   1     LI        gr7=4
  107| 000908 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,abun,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 00090C ori      60000000   1
  107| 000910 ld       E8010520   1     L8        gr0=#SPILL6(gr1,1312)
  107| 000914 addi     3A310001   1     AI        gr17=gr17,1
  107| 000918 cmpld    7C310040   1     CL8       cr0=gr17,gr0
  107| 00091C bc       4180FCD4   1     BT        CL.30,cr0,0x8/llt,taken=80%(80,20)
  107| 000920 ld       E8610648   1     L8        gr3=#SPILL43(gr1,1608)
  107| 000924 addi     38630001   1     AI        gr3=gr3,1
  107| 000928 cmpldi   28230001   1     CL8       cr0=gr3,1
  107| 00092C std      F8610648   1     ST8       #SPILL43(gr1,1608)=gr3
  107| 000930 bc       4180FBC0   1     BT        CL.28,cr0,0x8/llt,taken=80%(80,20)
  107| 000934 ld       E8610518   1     L8        gr3=#SPILL5(gr1,1304)
  107| 000938 addi     38630001   1     AI        gr3=gr3,1
  107| 00093C cmpldi   28230001   1     CL8       cr0=gr3,1
  107| 000940 std      F8610518   1     ST8       #SPILL5(gr1,1304)=gr3
  107| 000944 bc       4180FB4C   1     BT        CL.26,cr0,0x8/llt,taken=80%(80,20)
    0|                              CL.90:
  107| 000948 or       7FE3FB78   1     LR        gr3=gr31
  107| 00094C bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  107| 000950 ori      60000000   1
  434| 000954 ld       EB820000   1     L8        gr28=.+CONSTANT_AREA(gr2,0)
  434| 000958 ld       EB620000   1     L8        gr27=.$STATIC(gr2,0)
  434| 00095C addi     3860000C   1     LI        gr3=12
  434| 000960 addi     38800103   1     LI        gr4=259
  434| 000964 ld       E8C10510   1     L8        gr6=#SPILL4(gr1,1296)
  434| 000968 addi     38E00000   1     LI        gr7=0
  434| 00096C addi     381C02A4   1     AI        gr0=gr28,676
  434| 000970 addi     38BB0200   1     AI        gr5=gr27,512
  434| 000974 std      F80100F0   1     ST8       <a1:d240:l8>(gr1,240)=gr0
  434| 000978 addi     39000000   1     LI        gr8=0
  434| 00097C addi     392100E0   1     AI        gr9=gr1,224
  434| 000980 bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#11",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|fmt_args,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  434| 000984 ori      60000000   1
  434| 000988 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  434| 00098C ori      60000000   1
  437| 000990 addi     393B0080   1     AI        gr9=gr27,128
  437| 000994 addi     38BB0240   1     AI        gr5=gr27,576
  437| 000998 addi     3860000C   1     LI        gr3=12
  437| 00099C addi     38800103   1     LI        gr4=259
  437| 0009A0 ld       E8C10510   1     L8        gr6=#SPILL4(gr1,1296)
  437| 0009A4 addi     38E00000   1     LI        gr7=0
  437| 0009A8 addi     39000000   1     LI        gr8=0
  437| 0009AC bl       48000001   1     CALL      gr3=_xlfBeginIO,7,gr3,gr4,#13",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,gr8,|2,gr9,_xlfBeginIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  437| 0009B0 ori      60000000   1
    0| 0009B4 ld       EB410508   1     L8        gr26=#SPILL3(gr1,1288)
    0| 0009B8 ld       EB210500   1     L8        gr25=#SPILL2(gr1,1280)
  437| 0009BC or       7C7F1B78   1     LR        gr31=gr3
  437| 0009C0 addi     3BA00000   1     LI        gr29=0
    0| 0009C4 subf     7C99D050   1     S         gr4=gr26,gr25
    0| 0009C8 addic.   37C40001   1     AI_R      gr30,cr0=gr4,1,ca"
    0| 0009CC bc       40810100   1     BF        CL.98,cr0,0x2/gt,taken=50%(0,0)
  437| 0009D0 ld       E8620000   1     L8        gr3=.&&N&field(gr2,0)
    0| 0009D4 lfd      CBFC02F8   1     LFL       fp31=+CONSTANT_AREA(gr28,760)
    0| 0009D8 ld       E94104F8   1     L8        gr10=#SPILL1(gr1,1272)
  437| 0009DC ld       E8820000   1     L8        gr4=.&&N&grid(gr2,0)
  437| 0009E0 rldicr   7B261F24   1     SLL8      gr6=gr25,3
    0| 0009E4 addi     38B9FFFF   1     AI        gr5=gr25,-1
    0| 0009E8 ld       EB8301E8   1     L8        gr28=<s12:d488:l8>(gr3,488)
    0| 0009EC ld       EB630200   1     L8        gr27=<s12:d512:l8>(gr3,512)
    0| 0009F0 ld       E8E301A0   1     L8        gr7=<s12:d416:l8>(gr3,416)
    0| 0009F4 ld       E90301B8   1     L8        gr8=<s12:d440:l8>(gr3,440)
    0| 0009F8 ld       E9240000   1     L8        gr9=<s50:d0:l8>(gr4,0)
    0| 0009FC ld       E8040018   1     L8        gr0=<s50:d24:l8>(gr4,24)
    0| 000A00 mulld    7C8AE1D2   1     M         gr4=gr10,gr28
    0| 000A04 ld       E9620000   1     L8        gr11=.+CONSTANT_AREA(gr2,0)
  437| 000A08 addi     38C6FFF8   1     AI        gr6=gr6,-8
    0| 000A0C add      7CE74214   1     A         gr7=gr7,gr8
    0| 000A10 add      7CC64A14   1     A         gr6=gr6,gr9
    0| 000A14 ld       EB4301D0   1     L8        gr26=<s12:d464:l8>(gr3,464)
    0| 000A18 mulld    7F25D9D2   1     M         gr25=gr5,gr27
    0| 000A1C lfd      CBCB0318   1     LFL       fp30=+CONSTANT_AREA(gr11,792)
    0| 000A20 add      7E443A14   1     A         gr18=gr4,gr7
    0| 000A24 add      7F003214   1     A         gr24=gr0,gr6
  437|                              CL.32:
    0| 000A28 ld       E86104F0   1     L8        gr3=#SPILL0(gr1,1264)
  437| 000A2C addi     3AE00000   1     LI        gr23=0
    0| 000A30 add      7C03EA14   1     A         gr0=gr3,gr29
    0| 000A34 mulld    7EC0D1D2   1     M         gr22=gr0,gr26
    0| 000A38 ori      60210000   1     XNOP      
  437|                              CL.34:
  437| 000A3C mulld    7C17E1D2   1     M         gr0=gr23,gr28
  437| 000A40 addi     3AA00000   1     LI        gr21=0
  437| 000A44 add      7C00B214   1     A         gr0=gr0,gr22
  437| 000A48 or       7F14C378   1     LR        gr20=gr24
  437| 000A4C add      7C00CA14   1     A         gr0=gr0,gr25
  437| 000A50 or       7FE3FB78   1     LR        gr3=gr31
  437| 000A54 add      7E609214   1     A         gr19=gr0,gr18
  437|                              CL.36:
  437| 000A58 lfdu     CC140008   1     LFDU      fp0,gr20=x1a(gr20,8)
  437| 000A5C addi     388100D0   1     AI        gr4=gr1,208
  437| 000A60 addi     38A00008   1     LI        gr5=8
  437| 000A64 addi     38C00008   1     LI        gr6=8
  437| 000A68 addi     38E00004   1     LI        gr7=4
  437| 000A6C fmul     FC0007F2   1     MFL       fp0=fp0,fp31,fcr
  437| 000A70 stfd     D80100D0   1     STFL      T_12(gr1,208)=fp0
  437| 000A74 bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_12,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  437| 000A78 ori      60000000   1
  437| 000A7C lfdux    7C13DCEE   1     LFDU      fp0,gr19=v1(gr19,gr27,0)
  437| 000A80 or       7FE3FB78   1     LR        gr3=gr31
  437| 000A84 addi     388100D8   1     AI        gr4=gr1,216
  437| 000A88 addi     38A00008   1     LI        gr5=8
  437| 000A8C addi     38C00008   1     LI        gr6=8
  437| 000A90 addi     38E00004   1     LI        gr7=4
  437| 000A94 fmul     FC0007B2   1     MFL       fp0=fp0,fp30,fcr
  437| 000A98 stfd     D80100D8   1     STFL      T_13(gr1,216)=fp0
  437| 000A9C bl       48000001   1     CALL      _xlfWriteFmt,5,gr3,T_13,gr4-gr7,_xlfWriteFmt",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr3"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  437| 000AA0 ori      60000000   1
  437| 000AA4 addi     3AB50001   1     AI        gr21=gr21,1
  437| 000AA8 or       7FE3FB78   1     LR        gr3=gr31
  437| 000AAC cmpld    7C35F040   1     CL8       cr0=gr21,gr30
  437| 000AB0 bc       4180FFA8   1     BT        CL.36,cr0,0x8/llt,taken=80%(80,20)
  437| 000AB4 addi     3AF70001   1     AI        gr23=gr23,1
  437| 000AB8 cmpldi   28370001   1     CL8       cr0=gr23,1
  437| 000ABC bc       4180FF80   1     BT        CL.34,cr0,0x8/llt,taken=80%(80,20)
  437| 000AC0 addi     3BBD0001   1     AI        gr29=gr29,1
  437| 000AC4 cmpldi   283D0001   1     CL8       cr0=gr29,1
  437| 000AC8 bc       4180FF60   1     BT        CL.32,cr0,0x8/llt,taken=80%(80,20)
    0|                              CL.98:
  437| 000ACC or       7FE3FB78   1     LR        gr3=gr31
  437| 000AD0 bl       48000001   1     CALL      gr3=_xlfEndIO,1,gr3,_xlfEndIO",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  437| 000AD4 ori      60000000   1
  453| 000AD8 ld       EBE20000   1     L8        gr31=.$STATIC(gr2,0)
  453| 000ADC addi     3860000C   1     LI        gr3=12
  453| 000AE0 addi     38800001   1     LI        gr4=1
  453| 000AE4 addi     38C00000   1     LI        gr6=0
  453| 000AE8 addi     38E00000   1     LI        gr7=0
  453| 000AEC addi     38BF0280   1     AI        gr5=gr31,640
  453| 000AF0 bl       48000001   1     CALL      gr3=_xlfIOCmd,5,gr3,gr4,#15",gr5,gr6,@PALI_SHADOW_CONST.rns0.,gr7,_xlfIOCmd",fcr",#MX_TEMP1",gr1,cr[01567]",gr0",gr4"-gr12",fp0"-fp13",mq",lr",xer",fsr",ca",ctr"
  453| 000AF4 ori      60000000   1
  456| 000AF8 ld       E98107B0   1     L8        gr12=#stack(gr1,1968)
  456| 000AFC lfd      CBE10798   1     LFL       fp31=#stack(gr1,1944)
  456| 000B00 lfd      CBC10790   1     LFL       fp30=#stack(gr1,1936)
  456| 000B04 lfd      CBA10788   1     LFL       fp29=#stack(gr1,1928)
  456| 000B08 lfd      CB810780   1     LFL       fp28=#stack(gr1,1920)
  456| 000B0C lfd      CB610778   1     LFL       fp27=#stack(gr1,1912)
  456| 000B10 lfd      CB410770   1     LFL       fp26=#stack(gr1,1904)
  456| 000B14 lfd      CB210768   1     LFL       fp25=#stack(gr1,1896)
  456| 000B18 lfd      CB010760   1     LFL       fp24=#stack(gr1,1888)
  456| 000B1C lfd      CAE10758   1     LFL       fp23=#stack(gr1,1880)
  456| 000B20 lfd      CAC10750   1     LFL       fp22=#stack(gr1,1872)
  456| 000B24 lfd      CAA10748   1     LFL       fp21=#stack(gr1,1864)
  456| 000B28 lfd      CA810740   1     LFL       fp20=#stack(gr1,1856)
  456| 000B2C lfd      CA610738   1     LFL       fp19=#stack(gr1,1848)
  456| 000B30 lfd      CA410730   1     LFL       fp18=#stack(gr1,1840)
  456| 000B34 lfd      CA210728   1     LFL       fp17=#stack(gr1,1832)
  456| 000B38 lfd      CA010720   1     LFL       fp16=#stack(gr1,1824)
  456| 000B3C lfd      C9E10718   1     LFL       fp15=#stack(gr1,1816)
  456| 000B40 lfd      C9C10710   1     LFL       fp14=#stack(gr1,1808)
  456| 000B44 addi     382107A0   1     AI        gr1=gr1,1952
  456| 000B48 mtspr    7D8803A6   1     LLR       lr=gr12
  456| 000B4C ld       E9C1FEE0   1     L8        gr14=#stack(gr1,-288)
  456| 000B50 ld       E9E1FEE8   1     L8        gr15=#stack(gr1,-280)
  456| 000B54 ld       EA01FEF0   1     L8        gr16=#stack(gr1,-272)
  456| 000B58 ld       EA21FEF8   1     L8        gr17=#stack(gr1,-264)
  456| 000B5C ld       EA41FF00   1     L8        gr18=#stack(gr1,-256)
  456| 000B60 ld       EA61FF08   1     L8        gr19=#stack(gr1,-248)
  456| 000B64 ld       EA81FF10   1     L8        gr20=#stack(gr1,-240)
  456| 000B68 ld       EAA1FF18   1     L8        gr21=#stack(gr1,-232)
  456| 000B6C ld       EAC1FF20   1     L8        gr22=#stack(gr1,-224)
  456| 000B70 ld       EAE1FF28   1     L8        gr23=#stack(gr1,-216)
  456| 000B74 ld       EB01FF30   1     L8        gr24=#stack(gr1,-208)
  456| 000B78 ld       EB21FF38   1     L8        gr25=#stack(gr1,-200)
  456| 000B7C ld       EB41FF40   1     L8        gr26=#stack(gr1,-192)
  456| 000B80 ld       EB61FF48   1     L8        gr27=#stack(gr1,-184)
  456| 000B84 ld       EB81FF50   1     L8        gr28=#stack(gr1,-176)
  456| 000B88 ld       EBA1FF58   1     L8        gr29=#stack(gr1,-168)
  456| 000B8C ld       EBC1FF60   1     L8        gr30=#stack(gr1,-160)
  456| 000B90 ld       EBE1FF68   1     L8        gr31=#stack(gr1,-152)
  456| 000B94 bclr     4E800020   1     BA        lr
     |               Tag Table
     | 000B98        00000000 00012201 92120000 00000B98
     |               Instruction count          742
     |               Straight-line exec time    766
     |               Constant Area
     | 000000        74657874 646D7032 2E663930 00000000 80010000 00000000
     | 000018        0000000D 00000000 00000001 00000000 00000006 00000000
     | 000030        00000000 00000000 80020000 00000000 800B0001 00000000
     | 000048        00000001 00000000 00200111 00000000 0000000B 00000000
     | 000060        00000003 00000000 00000003 00000000 80050000 00000000
     | 000078        80010000 00000000 00000013 00000000 00000002 00000000
     | 000090        00000006 00000000 00000000 00000000 80020000 00000000
     | 0000A8        80040001 00000000 00000002 00000000 80020000 00000000
     | 0000C0        800B0001 00000000 00000001 00000000 00200111 00000000
     | 0000D8        0000000B 00000000 00000003 00000000 00000003 00000000
     | 0000F0        80120001 00000000 00000001 00000000 80030000 00000000
     | 000108        80050000 00000000 80010000 00000000 00000013 00000000
     | 000120        00000002 00000000 00000006 00000000 00000000 00000000
     | 000138        80020000 00000000 80040001 00000000 00000003 00000000
     | 000150        80020000 00000000 800B0001 00000000 00000001 00000000
     | 000168        00200111 00000000 0000000B 00000000 00000003 00000000
     | 000180        00000003 00000000 80120001 00000000 00000001 00000000
     | 000198        80030000 00000000 80050000 00000000 80010000 00000000
     | 0001B0        00000012 00000000 00000001 00000000 00000006 00000000
     | 0001C8        00000000 00000000 80020000 00000000 802F0011 00000000
     | 0001E0        0000003D 00000000 0000000B 20202020 20783162 20202020
     | 0001F8        20202020 64656E73 20202020 20202065 6E657267 20202020
     | 000210        2020206D 6574616C 20202020 20202020 76312020 20202020
     | 000228        20000001 00000000 80050000 00000000 80010000 00000000
     | 000240        0000000E 00000000 00000001 00000000 00000006 00000000
     | 000258        00000000 00000000 80020000 00000000 802F0011 00000000
     | 000270        00000020 00000000 00000007 20202020 20783162 20202020
     | 000288        20202020 64656E73 20202020 20202065 6E657267 00000000
     | 0002A0        80050000 00000000 80010000 00000000 00000008 00000000
     | 0002B8        00000001 00000000 00000006 00000000 00000000 00000000
     | 0002D0        80020000 00000000 002C0000 00000000 80050000 74657874
     | 0002E8        646D7032 2E663930 3E610629 1EB9624E 3C17ECFD 1F30F6B3
     | 000300        3E800000 3F000000 44DFE191 4CAF8E84 3E49D3F4 1BE28495
     | 000318        3EE4F8B5 88E368F1 3F800000 49424D20 3FE851EB 851EB852
     | 000330        3FCEB851 EB851EB8

 
 
>>>>> FILE TABLE SECTION <<<<<
 
 
                                       FILE CREATION        FROM
FILE NO   FILENAME                    DATE       TIME       FILE    LINE
     0    textdmp2.f90                07/08/15   15:48:46
 
 
>>>>> COMPILATION EPILOGUE SECTION <<<<<
 
 
FORTRAN Summary of Diagnosed Conditions
 
TOTAL   UNRECOVERABLE  SEVERE       ERROR     WARNING    INFORMATIONAL
               (U)       (S)         (E)        (W)          (I)
    1           0         0           1          0            0
 
 
    Source records read.......................................     456
1501-510  Compilation successful for file textdmp2.f90.
1501-543  Object file created.
